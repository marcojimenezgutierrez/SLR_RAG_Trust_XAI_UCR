
@inproceedings{lo_ai_2025,
	title = {{AI} {Hiring} with {LLMs}: {A} {Context}-{Aware} and {Explainable} {Multi}-{Agent} {Framework} for {Resume} {Screening}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017862444&doi=10.1109%2FCVPRW67362.2025.00402&partnerID=40&md5=cad5486d717cde909d02ee1702adb951},
	doi = {10.1109/CVPRW67362.2025.00402},
	booktitle = {{IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Lo, Frank Po Wen and Qiu, Jianing and Wang, Zeyu and Yu, Haibao and Chen, Yeming and Zhang, Gao and Lo, Benny Ping Lai},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {agentic system, ai hiring, bias migration, Computational modeling, Computer vision, Conferences, Context modeling, Data mining, explainable ai, Knowledge based systems, Large language models, llm, rag, Recruitment, Resumes, Retrieval augmented generation},
	pages = {4184 -- 4193},
	annote = {Cited by: 2},
}

@article{ogdu_adaptive_2025,
	title = {An {Adaptive} {Multi}-{Agent} {LLM}-{Based} {Clinical} {Decision} {Support} {System} {Integrating} {Biomedical} {RAG} and {Web} {Intelligence}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017394937&doi=10.1109%2FACCESS.2025.3613340&partnerID=40&md5=0233d2e832316669a43b057afe0f98c2},
	doi = {10.1109/ACCESS.2025.3613340},
	journal = {IEEE Access},
	author = {Ogdu, Çağatay Umut and Arslanoǧlu, Kübra and Karakose, Mehmet},
	year = {2025},
	note = {Type: Article},
	keywords = {Accuracy, Clinical decision support system, Data integration, Databases, Decision support systems, Heuristic algorithms, large language models, Large language models, Medical diagnostic imaging, multi-agent system, Protocols, Real-time systems, Reliability},
	pages = {167390 -- 167404},
	annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{van_ai-powered_2025,
	title = {{AI}-{Powered} {University} {Admission} {Counseling}: {A} {Use} {Case} of {Large} {Language} {Models} in {Student} {Guidance}},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014599067&doi=10.1109%2FTLT.2025.3604096&partnerID=40&md5=a6380190c36f482ff72d2e3097905e09},
	doi = {10.1109/TLT.2025.3604096},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Van, Nguyen Nang Hung and Do, Phuc Hao and Hoang, Vannam and Nguyen, Truc Thi Kim and Pham, Minh Tuan},
	year = {2025},
	note = {Type: Article},
	keywords = {Accuracy, AI in education, Artificial intelligence, Artificial intelligence (AI), educational chatbots, Employee welfare, Engineering profession, Large language models, large language models (LLMs), Mathematical models, Measurement, natural language processing (NLP), Retrieval augmented generation, retrieval-augmented generation (RAG), Scalability, student confidence, student counseling, university admission, university admission counseling, Vectors},
	pages = {856 -- 868},
	annote = {Cited by: 0},
}

@inproceedings{godwin_hybrid_2025,
	title = {A {Hybrid} {Framework} for {Domain}-{Specific} {Knowledge} {Integration} in {Large} {Language} {Models}: {A} {Comprehensive} {Survey}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013835573&doi=10.1109%2FECAI65401.2025.11095523&partnerID=40&md5=7f2da44dc648e18b5f0f4ce87792c170},
	doi = {10.1109/ECAI65401.2025.11095523},
	author = {Godwin, Jerrick and Athuraliya, Banuka},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Accuracy, Distance measurement, Fine-tuning, Indexing, Large language models, Large Language Models, Retrieval augmented generation, Retrieval Augmented Generation, Size measurement, Social networking (online), Surveys, Training, Tuning},
	annote = {Cited by: 0},
}

@article{nawara_comprehensive_2025,
	title = {A {Comprehensive} {Survey} on {LLM}-{Powered} {Recommender} {Systems}: {From} {Discriminative}, {Generative} to {Multi}-{Modal} {Paradigms}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013777704&doi=10.1109%2FACCESS.2025.3599832&partnerID=40&md5=bd905e8d007348328dfdb5c69a80ee42},
	doi = {10.1109/ACCESS.2025.3599832},
	journal = {IEEE Access},
	author = {Nawara, Dina and Kashef, Rasha F.},
	year = {2025},
	note = {Type: Article},
	keywords = {Accuracy, Adaptation models, Automobiles, Collaborative filtering, discriminative learning, Electronic commerce, evaluation metrics, Filtering, Large language models (LLMs), LLM-as-a-judge, Motion pictures, multi-modal paradigms, Real-time systems, recommendation systems, Recommender systems, retrieval-augmented generation (RAG), Surveys},
	pages = {145772 -- 145798},
	annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{pan_survey_2025,
	title = {A {Survey} on {Retrieval}-{Augmented} {Generation} in {Applications} of {Education} and {Teaching}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013538039&doi=10.1109%2FCSTE64638.2025.11092120&partnerID=40&md5=ff859225dc1ca43268a5bfbeaf451707},
	doi = {10.1109/CSTE64638.2025.11092120},
	author = {Pan, Feng and Zhou, Qiyun and Guo, Weitong and Yang, Hongwu},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Ecosystems, Education, Educational application, Ethics, Generative AI, Information retrieval, Intelligence education, Large language model, Large language models, Navigation, Retrieval augmented generation, Retrieval-augmented generation, Surveys, Vocational training},
	pages = {803 -- 807},
	annote = {Cited by: 0},
}

@article{shokrnezhad_autonomous_2025,
	title = {An {Autonomous} {Network} {Orchestration} {Framework} {Integrating} {Large} {Language} {Models} with {Continual} {Reinforcement} {Learning}},
	volume = {63},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012171994&doi=10.1109%2FMCOM.001.2400526&partnerID=40&md5=f3630c257c66822189e5b2a845e04151},
	doi = {10.1109/MCOM.001.2400526},
	number = {8},
	journal = {IEEE Communications Magazine},
	author = {Shokrnezhad, Masoud and Taleb, Tarik},
	year = {2025},
	note = {Type: Article},
	keywords = {Adaptation models, Cognition, Complexity theory, Continuing education, Contrastive learning, Few shot learning, Generators, Large language models, Planning, Reinforcement learning},
	pages = {78 -- 84},
	annote = {Cited by: 0},
}

@inproceedings{yao_adapting_2025,
	title = {Adapting {Large} {Language} {Models} for {Healthcare} with an {Enhanced} {Retrieval}-{Augmented} {Generation} {Framework}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009108389&doi=10.1109%2FICAACE65325.2025.11019949&partnerID=40&md5=4b6b2cc456a3bacd4448cafd5db27d6c},
	doi = {10.1109/ICAACE65325.2025.11019949},
	author = {Yao, Jiasheng and Guo, Yanliang and Yu, Qing},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Adaptation models, Benchmark testing, Cognition, Fine-tuning, Heuristic algorithms, Knowledge based systems, Large language models, Large Language models, Medical services, Real-time systems, Reliability, Retrieval Argumented Generation, Retrieval augmented generation},
	pages = {2601 -- 2606},
	annote = {Cited by: 0},
}

@inproceedings{bhardwaj_ai-powered_2025,
	title = {{AI}-{Powered} {Retrieval}-{Augmented} {Generation} {Framework} with {Large} {Language} {Models} for {Enhanced} {Public} {Health} {Response}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007746310&doi=10.1109%2FInCACCT65424.2025.11011436&partnerID=40&md5=4e52da20341f926eac33f6e40bd16d08},
	doi = {10.1109/InCACCT65424.2025.11011436},
	author = {Bhardwaj, Shivam and Joshi, Rakesh Chandra and Tiwari, Vansh and Říha, Kamil and Sikora, Pavel and Dutta, Malay Kishore},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Accuracy, Databases, Healthcare data management, Large language models, Large Language Models (LLMs), Pipelines, Protocols, Public health response, Public healthcare, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Standards, Surveillance, Vector database, Vectors},
	pages = {616 -- 621},
	annote = {Cited by: 0},
}

@inproceedings{omoush_advancing_2025,
	title = {Advancing {Arabic} {Medical} {Question} {Answering} {Systems} with {RAG} and {LLMs} {Integration}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007296992&doi=10.1109%2FICTCS65341.2025.10989446&partnerID=40&md5=dc7fc546f08f01f57be12c41aacdd4fb},
	doi = {10.1109/ICTCS65341.2025.10989446},
	author = {Omoush, Ebtehal H. and Ghnemat, Rawan},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Accuracy, Arabic language, Biomedical equipment, fine-tuning technique, JAIS LLM, Large language models, Market research, Medical services, Question answering (information retrieval), Retrieval augmented generation, Retrieve augmented generation},
	pages = {511 -- 516},
	annote = {Cited by: 0},
}

@inproceedings{hemmat_adaptive_2025,
	title = {Adaptive {Chunking} for {VideoRAG} {Pipelines} with a {Newly} {Gathered} {Bilingual} {Educational} {Dataset}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005152886&doi=10.1109%2FCSICC65765.2025.10967455&partnerID=40&md5=2189c0dea0a701b296be330be1ac2c8b},
	doi = {10.1109/CSICC65765.2025.10967455},
	author = {Hemmat, Arshia and Vadaei, Kianoosh and Shirian, Melika and Heydari, Mohammad Hassan and Fatemi, Afsaneh},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Academic Question Answering, Context modeling, Datasets Preparation, Education, Large language models, Measurement, Multilingual, Pipelines, Question answering (information retrieval), Retrieval augmented generation, Standards, Video QA, Visualization},
	annote = {Cited by: 0},
}

@article{du_retrieval-augmented_2025,
	title = {A {Retrieval}-{Augmented} {Multiagent} {System} for {Financial} {Sentiment} {Analysis}},
	volume = {40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003267000&doi=10.1109%2FMIS.2025.3544912&partnerID=40&md5=d26235da5b2965bd1e3fda0d197977db},
	doi = {10.1109/MIS.2025.3544912},
	number = {2},
	journal = {IEEE Intelligent Systems},
	author = {Du, Kelvin and Zhao, Yazhi and Mao, Rui and Xing, Frank Z. and Cambria, Erik},
	year = {2025},
	note = {Type: Article},
	keywords = {Benchmark testing, Generators, Intelligent systems, Large language models, Multi-agent systems, Oral communication, Problem-solving, Retrieval augmented generation, Sentiment analysis, Transformers},
	pages = {15 -- 22},
	annote = {Cited by: 0},
}

@inproceedings{lee_novel_2025,
	title = {A {Novel} {Approach} to {Generative} {Al}-based {Optimized} {Code} {Generation} for {Semiconductor} {Equipment} {Interfaces}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002271657&doi=10.23919%2FICACT63878.2025.10936654&partnerID=40&md5=050f77fb8f8d3cc2bca78b609f2fde82},
	doi = {10.23919/ICACT63878.2025.10936654},
	booktitle = {International {Conference} on {Advanced} {Communication} {Technology}, {ICACT}},
	author = {Lee, Hyeoksoo and Yang, Minyeol and Jeong, Jongpil},
	year = {2025},
	note = {Type: Conference paper},
	keywords = {Code Generation, Codes, Databases, Documentation, Electronics industry, Generative AI, Large language models, LLM (Large Language Model), Prompt, Protocols, RAG (Retrieval Augmented Generation), Retrieval augmented generation, Smart manufacturing, Standards},
	pages = {395 -- 400},
	annote = {Cited by: 0},
}

@inproceedings{kong_retrieval-augmented_2024,
	title = {A {Retrieval}-{Augmented} {Generation} method based on {Intent} {Analysis} and {Ranking} {Fusion}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000206329&doi=10.1109%2FDSIT61374.2024.10881002&partnerID=40&md5=d8dc7607c8ba62266ed08c6ea91f0511},
	doi = {10.1109/DSIT61374.2024.10881002},
	author = {Kong, Lingfei and Wang, Gang and Liu, Naiwei and Bai, Tuoya and He, Jingheng},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, Analytical models, Data models, Hallucinations, Information filters, Intent Analysis, Large Language Model, Noise, Noise measurement, Optimization, Query Rewriting, Reliability, Retrieval augmented generation, Retrieval-Augemented Generation, Semantics},
	annote = {Cited by: 0},
}

@inproceedings{singh_adversarial_2024,
	title = {Adversarial {Training} of {Retrieval} {Augmented} {Generation} to {Generate} {Believable} {Fake} {News}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218057781&doi=10.1109%2FBigData62323.2024.10825933&partnerID=40&md5=f5fb314bd3e3b17e77d8e34282b11b2a},
	doi = {10.1109/BigData62323.2024.10825933},
	author = {Singh, Sonali and Namin, Akbar Siami},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Context modeling, Fake news, Fake News, GPT-Neo, Large language Model (LLM), Large language models, Natural language processing, Pipelines, Retrieval augmented generation, Retrieval Augmented Generation (RAG), RoBERTa, Social networking (online), Software development management, Training, Transformers},
	pages = {3589 -- 3598},
	annote = {Cited by: 0},
}

@inproceedings{turaga_information_2024,
	title = {An {Information} {Reliability} {Framework} for {Detecting} {Misinformation} based on {Large} {Language} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218057494&doi=10.1109%2FBigData62323.2024.10826052&partnerID=40&md5=e43190350e9f4213aef13c2fc9f48007},
	doi = {10.1109/BigData62323.2024.10826052},
	author = {Turaga, Venkata Sai Prathyush and Namin, Akbar Siami},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, Computational modeling, Dark Web, Dark-BERT, Data models, Decision making, Fake news, Information Reliability, Large language models, LLaMA, Misinformation, RAG, Real-time systems, Reliability, Training},
	pages = {3599 -- 3608},
	annote = {Cited by: 1},
}

@inproceedings{huang_adapting_2024,
	title = {Adapting {Large} {Language} {Models} for {Biomedicine} though {Retrieval}-{Augmented} {Generation} with {Documents} {Scoring}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217281022&doi=10.1109%2FBIBM62325.2024.10822725&partnerID=40&md5=451f25d8d87e311fc8d2fe2c7c827ac6},
	doi = {10.1109/BIBM62325.2024.10822725},
	author = {Huang, Yinkui and Gao, Tianrun and Zhang, Jiangjiang and Liu, Xiaohong and Wang, Guangyu},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Adaptation models, Benchmark testing, Biological system modeling, Costs, Data acquisition, Data models, Decision making, Large language models, Large Language Models, Retrieval augmented generation, Retrieval-Augmented Generation, score documents, Standards},
	pages = {5770 -- 5775},
	annote = {Cited by: 0},
}

@inproceedings{mahalakshmi_real-time_2024,
	title = {A {Real}-time {Medical} {Report} {Analysis} and {AI}-{Powered} {Diagnosis}: {A} {Cloud}-{Based} {Solution} {forImproved} {Patient} {Care}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208824844&doi=10.1109%2FICAIT61638.2024.10690733&partnerID=40&md5=89ff878de9b5b42871b259ac7cb19cb3},
	doi = {10.1109/ICAIT61638.2024.10690733},
	author = {Mahalakshmi, M. and Bharadwaj, Shardul and Bhuyan, Aklanta Niraz},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, AI-powered diagnosis, Computational modeling, ETL (Extract Load Transform), Large language models, Large language models (LLM), Medical services, Natural Language Pro- cessing (NLP), Natural language processing, Pipelines, Real-time medical in- sights, Transforms},
	annote = {Cited by: 1},
}

@inproceedings{mukhlis_novel_2024,
	title = {A {Novel} {CAD} {Framework} with {Visual} and {Textual} {Interpretability}: {Multimodal} {Insights} for {Predicting} {Respiratory} {Diseases}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207866263&doi=10.1109%2FIDAP64064.2024.10710824&partnerID=40&md5=25808cfdcc920e29a3daf90a340730d3},
	doi = {10.1109/IDAP64064.2024.10710824},
	author = {Mukhlis, Raza and Saleem, Saied and Kwon, Hyunwook and Hussain, Jamil and Aydin, Ahmet Arif and Al-antari, Mugahed A.},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, Comprehensive CAD system, Data models, Heating systems, Large language model (LLM), Location awareness, Medical diagnostic imaging, Meteors, Pulmonary diseases, Retrieval Augmented Generation (RAG), Solid modeling, Text embedding, visual and textual interpretability, Visualization, X-ray imaging},
	annote = {Cited by: 1},
}

@inproceedings{guo_knowledge_2024,
	title = {A {Knowledge} {Graph} and {Large} {Language} {Model}-{Based} {Framework} for {Depression} {Detection}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004583435&doi=10.1109%2FICICML63543.2024.10958051&partnerID=40&md5=3d02fe761d8092d35df57ad16e89c70a},
	doi = {10.1109/ICICML63543.2024.10958051},
	author = {Guo, Youwei and Guo, Yanrong},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, Computational modeling, Data models, Depression, depression detection, Employee welfare, knowledge graph, Knowledge graphs, large language model, Large language models, Mental health, Retrieval augmented generation, retrieval-augmented generation, Training},
	pages = {670 -- 673},
	annote = {Cited by: 0},
}

@inproceedings{saeid_agentfusion_2024,
	title = {{AgentFusion}: {A} {Multi}-{Agent} {Approach} to {Accurate} {Text} {Generation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001863799&doi=10.1109%2FICECER62944.2024.10920460&partnerID=40&md5=a534e8e66860a0ac315d41a45556db81},
	doi = {10.1109/ICECER62944.2024.10920460},
	author = {Saeid, Yasser and Kopinski, Thomas},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, Accuracy Assessment, Autonomous Agents, Benchmark testing, Chunk Size Optimization, Computational modeling, Data models, Domain-Specific Language Models, Embedding Optimization, Finance, Finance Data Processing, Foot-ball Data Analysis, German Language Processing, GPT-4 Omni Benchmark, Inductors, Large language models, Large Language Models (LLMs), Llama Model, Multi-Agent Systems, Natural language processing, Natural Language Processing (NLP), Optimization, Performance Evaluation, Query Optimization, Reactor Decommissioning, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Self-Regulating Systems, Semantic Retrieval, Text Chunking},
	annote = {Cited by: 0},
}

@inproceedings{jeong_4bit-quantization_2024,
	title = {4bit-{Quantization} in {Vector}-{Embedding} for {RAG}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000935913&doi=10.1109%2FICMLA61862.2024.00156&partnerID=40&md5=1a4c976e971bd9ee72552e0dab177d87},
	doi = {10.1109/ICMLA61862.2024.00156},
	author = {Jeong, Taehee},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Accuracy, Databases, Degradation, Embedding vectors, High-dimensional vectors, Large language models, Machine learning, Memory management, Memory storage, Quantization, Quantization (signal), Retrieval augmented generation, Retrieval-augmented generation, Training data, Vector database, Vector search, Vectors},
	pages = {1037 -- 1042},
	annote = {Cited by: 0},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Search} {Results}},
	url = {https://ieeexplore.proxyucr.elogim.com/search/searchresult.jsp?action=search&newsearch=true&matchBoolean=true&queryText=(%22All%20Metadata%22:%22retrieval-augmented%20generation%22%20OR%20%22All%20Metadata%22:%22RAG%22)%20AND%20(%22All%20Metadata%22:%22large%20language%20model%22%20OR%20%22All%20Metadata%22:%22LLM%22%20OR%20%22All%20Metadata%22:%22LLMs%22%20OR%20%22All%20Metadata%22:%22generative%20AI%22%20OR%20%22All%20Metadata%22:%22ChatGPT%22)%20AND%20(%22All%20Metadata%22:%22trust%22%20OR%20%22All%20Metadata%22:%22confidence%22%20OR%20%22All%20Metadata%22:%22credibility%22%20OR%20%22All%20Metadata%22:%22user%20perception%22%20OR%20%22All%20Metadata%22:%22perceived%20reliability%22%20OR%20%22All%20Metadata%22:%22overtrust%22%20OR%20%22All%20Metadata%22:%22calibration%22%20OR%20%22All%20Metadata%22:%22hallucination%22%20OR%20%22All%20Metadata%22:%22hallucinations%22%20OR%20%22All%20Metadata%22:%22factuality%22%20OR%20%22All%20Metadata%22:%22factual%20accuracy%22%20OR%20%22All%20Metadata%22:%22faithfulness%22%20OR%20%22All%20Metadata%22:%22citation%22%20OR%20%22All%20Metadata%22:%22citations%22%20OR%20%22All%20Metadata%22:%22reference%22%20OR%20%22All%20Metadata%22:%22source%20attribution%22%20OR%20%22All%20Metadata%22:%22provenance%22%20OR%20%22All%20Metadata%22:%22grounding%22%20OR%20%22All%20Metadata%22:%22explainable%20artificial%20intelligence%22%20OR%20%22All%20Metadata%22:%22explainable%20AI%22%20OR%20%22All%20Metadata%22:%22XAI%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22interpretable%20AI%22%20OR%20%22All%20Metadata%22:%22interpretable%20artificial%20intelligence%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22AI%20explainability%22%20OR%20%22All%20Metadata%22:%22AI%20interpretability%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22transparent%20AI%22%20OR%20%22All%20Metadata%22:%22AI%20transparency%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22model%20interpretability%22%20OR%20%22All%20Metadata%22:%22post-hoc%20explanation%22%20OR%20%22All%20Metadata%22:%22feature%20attribution%22)&ranges=2020_2025_Year},
	urldate = {2025-10-26},
	file = {IEEE Xplore Search Results:C\:\\Users\\Marco\\Zotero\\storage\\VQJXQ254\\searchresult.html:text/html},
}

@inproceedings{yu_engineering_2025,
	title = {Engineering {Critical} {Analysis} {Software} {Services}: a {Graph}-{Rag} and {Self}-{Learning} {Large} {Language} {Model} {Agent} {Services} {Approach}},
	doi = {10.1109/SOSE67019.2025.00005},
	abstract = {This paper presents Graph-RAG and Self-learning LLM-based Agent Services Framework for structured reasoning and knowledge-driven analysis. The proposed approach integrates graph-enhanced retrieval mechanisms with self-learning Large Language Models (LLMs) to improve critical analysis and domain-specific decision-making. The framework is evaluated using Air Accidents Investigation Branch (AAIB) Publications Reports, which provide structured, investigative narratives aimed at preventing future aviation incidents rather than assigning blame. By leveraging graph-based knowledge learning, the framework enhances causal reasoning, multimodal response generation, and retrieval accuracy, demonstrating its capability to support structured problem analysis based on real-world investigative experiences. Experimental results show significant improvements in hallucination mitigation, retrieval precision, and real-time performance when compared to standard Retrieval-Augmented Generation (RAG) models. The findings highlight the potential of graph-augmented self-learning LLMs in transforming automated analytical workflows, paving the way for enhanced visual knowledge exploration and structured decisionsupport systems.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	author = {Yu, Hong Qing and Scanlon, Brian and Reiff-Marganiec, Stephan},
	month = jul,
	year = {2025},
	note = {ISSN: 2642-6587},
	keywords = {Aviation Safety Analysis, Causal Reasoning, Cognition, Graph-RAG, Knowledge Graphs, Large language models, Prevention and mitigation, Real-time systems, Retrieval augmented generation, Safety, Self-learning LLMs, Service-oriented systems engineering, ServiceOriented AI, Software, Standards, Visualization},
	pages = {1--7},
}

@inproceedings{wang_research_2024,
	title = {Research on {Exchange} {Rate} {Prediction} {Driven} by {Multi}-{Source} {Heterogeneous} {Data}: {Based} on {Retrieval} {Augmented} {Generation} and {Explainable} {Machine} {Learning} {Models}},
	doi = {10.1109/CAIT64506.2024.10963106},
	abstract = {With the accelerating pace of global economic integration, exchange rate fluctuations have been influenced by various uncertain factors. The information explosion has further complicated the factors influencing exchange rate predictions. To fully utilize multi-source heterogeneous data and further optimize exchange rate prediction performance, this paper constructs an exchange rate prediction method based on Retrieval Augmented Generation (RAG) and Explainable Artificial Intelligence models. During the data processing stage, this paper first uses RAG technology to enhance the situational learning ability of Large Language Models (LLMs). By combining prompts and local sentiment databases, LLMs can perform real-time and accurate sentiment analysis on news headlines. Secondly, transfer learning is used to optimize the Inception(v3) model for sentiment analysis of news images, thereby more efficiently analyzing the sentiment variables contained in texts and images, effectively enriching the sources of multi-source heterogeneous data. Extensive experiments have shown that the multi-source heterogeneous dataset constructed in this paper can effectively improve the accuracy of exchange rate predictions and improve the predictive performance of various models across different time steps.},
	booktitle = {2024 5th {International} {Conference} on {Computers} and {Artificial} {Intelligence} {Technology} ({CAIT})},
	author = {Wang, Bocheng and Lu, Yushen and Chen, Long and Li, Jianqing and Qu, Liangzhou},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Biological system modeling, Computational modeling, Data models, exchange rate prediction, Exchange rates, multi-source heterogeneous data, Predictive models, Real-time systems, retrieval augmented generation, Retrieval augmented generation, Sentiment analysis, SHAP, Transfer learning},
	pages = {566--572},
}

@inproceedings{hasegawa_rag_2024,
	title = {{RAG} {Certainty}: {Quantifying} the {Certainty} of {Context}-{Based} {Responses} by {LLMs}},
	doi = {10.1109/ICMLA61862.2024.00133},
	abstract = {Large language models (LLMs) have recently been employed for a wide variety of purposes. Retrieval-augmented generation (RAG), in which an LLM generates a response based on context relevant to the prompt, is often used to enable the LLM to adapt to specialized domains. However, sentences generated by a generative LLM may contain incorrect information, known as “hallucinations.” The challenge in identifying hallucinations within the RAG framework involves evaluating the certainty of both context retrieval and LLM outputs. In this paper, we propose a metric called RAG certainty to quantify the certainty of LLM outputs within a RAG framework. The proposed metric is calculated based on certainty scores from both information retrieval and response generation. Experimental results demonstrate that the proposed metric effectively reflects the certainty of information retrieval in a RAG framework. We further validated the proposed metric through a case study that assesses the predicted Common Vulnerability Scoring Sys-tem (CVSS) scores for cybersecurity vulnerabilities and found that errors are mitigated according to the proposed metric.},
	booktitle = {2024 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Hasegawa, Kento and Hidano, Seira and Fukushima, Kazuhide},
	month = dec,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Accuracy, certainty, Computer security, CVSS, Information retrieval, large language model, Large language models, Machine learning, Measurement, Predictive models, Retrieval augmented generation, retrieval-augmented generation, security},
	pages = {912--917},
}

@article{alabbasi_teleoracle_2025,
	title = {{TeleOracle}: {Fine}-{Tuned} {Retrieval}-{Augmented} {Generation} {With} {Long}-{Context} {Support} for {Networks}},
	volume = {12},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2025.3553161},
	abstract = {The telecommunications industry’s rapid evolution demands intelligent systems capable of managing complex networks and adapting to emerging technologies. While large language models (LLMs) show promise in addressing these challenges, their deployment in telecom environments faces significant constraints due to edge device limitations and inconsistent documentation. To bridge this gap, we present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG) system built on the Phi-2 small language model (SLM). To improve context retrieval, TeleOracle employs a two-stage retriever that incorporates semantic chunking and hybrid key-word and semantic search. Additionally, we expand the context window during inference to enhance the model’s performance on open-ended queries. We also employ low-rank adaption for efficient fine-tuning. A thorough analysis of the model’s performance indicates that our RAG framework is effective in aligning Phi-2 to the telecom domain in a downstream question and answer (QnA) task, achieving a 30\% improvement in accuracy over the base Phi-2 model, reaching an overall accuracy of 81.20\%. Notably, we show that our model not only performs on par with the much larger LLMs but also achieves a higher faithfulness score, indicating higher adherence to the retrieved context.},
	number = {10},
	journal = {IEEE Internet of Things Journal},
	author = {Alabbasi, Nouf and Erak, Omar and Alhussein, Omar and Lotfi, Ismail and Muhaidat, Sami and Debbah, Mérouane},
	month = may,
	year = {2025},
	keywords = {3GPP, 6G networks, Accuracy, Adaptation models, AGI, Automation, Benchmark testing, Computational modeling, Context modeling, large language model (LLM), low-rank Adaptation (LoRA), Retrieval augmented generation, retrieval-augmented generation (RAG), Semantics, Telecommunications},
	pages = {13170--13182},
}

@inproceedings{wu_multirag_2025,
	title = {{MultiRAG}: {A} {Knowledge}-{Guided} {Framework} for {Mitigating} {Hallucination} in {Multi}-{Source} {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/ICDE65448.2025.00230},
	abstract = {Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. Our code is available in https://github.com/wuwenlong123/MultiRAG.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Wu, Wenlong and Wang, Haofen and Li, Bohan and Huang, Peixuan and Zhao, Xinzhe and Liang, Lei},
	month = may,
	year = {2025},
	note = {ISSN: 2375-026X},
	keywords = {Codes, Cognition, Data aggregation, Data engineering, Hallucination Mitigation, Knowledge graphs, Knowledge Graphs, Large language models, Large Language Models, Multi-source Retrieval, Prevention and mitigation, Reliability, Retrieval augmented generation, Retrieval Augmented Generation, Technological innovation},
	pages = {3070--3083},
}

@inproceedings{somasundharam_llm_2025,
	title = {{LLM} {Powered} {News} {Research} {Tool} for {Insight} {Extraction}},
	doi = {10.1109/ICCMC65190.2025.11140911},
	abstract = {The exponential growth of digital news content has created an urgent demand for intelligent systems capable of synthesizing, comparing, and extracting actionable insights from diverse sources in real-time. This paper presents a Large Language Model (LLM)-powered research tool that enables users to input multiple online news article URLs and pose questions to extract concise, accurate, and source-aware answers. The system leverages Retrieval-Augmented Generation (RAG) principles by combining Hugging Face’s transformer-based embeddings, FAISS for efficient semantic vector search, and LangChain to orchestrate the pipeline. To preprocess dynamic web content, we employ Unstructured.io loaders, while response generation is handled by the Groq-accelerated LLaMA-3 model. The tool provides explainable, context-aware answers with traceable sources, supporting journalistic, academic, and research use cases. Experimental evaluation confirms the system’s effectiveness, achieving an average response latency of 0.32 seconds in optimized configurations while maintaining high relevance and factual alignment. Our framework offers a highly adaptable and scalable approach to real-time insight extraction over heterogeneous, web-based news content.},
	booktitle = {2025 8th {International} {Conference} on {Computing} {Methodologies} and {Communication} ({ICCMC})},
	author = {Somasundharam, Lalitha and Vardhan, Chinthala Harsha and Bhimeswara Akshay Varma, Sagi and Swamy, Koyya Manjunadha},
	month = jul,
	year = {2025},
	keywords = {Document Chunking, FAISS, Groq, HuggingFace Embeddings, Intelligent systems, LangChain, Large language models, Large Language Models (LLMs), LLaMA, Load modeling, Multi-Document Question Answering, News Analytics, News Summarization, Pipelines, Question answering (information retrieval), Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantic search, Semantic Search, Source Attribution, Streamlit, Transformers, Unstructured.io, Vectors},
	pages = {675--682},
}

@inproceedings{mohd_comparative_2024,
	title = {A {Comparative} {Evaluation} of {Retrieval}-{Augmented} {Generation} {For} {Arabic} {Documents}},
	doi = {10.1109/ACIT62805.2024.10877197},
	abstract = {Generative AI creates new content using AI models (i.e., LLMs) trained on large datasets. However, LLMs lack specific sources and can produce inaccurate or misleading information. Retrieval-augmented generation (RAG) improves accuracy by combining LLMs with a content store, providing evidence for responses and making the model more adaptable to changing information. This paper introduces a comparative evaluation of various generative AI models—LLama 3, LLama 3.1, Gemma, Gemma 2, Phi 3, Phi 3.5, and Phi 3.5 Mini based on their performance in answering questions derived from Arabic and English documents using the RAG locally. The models were evaluated based on their ability to answer questions of varying difficulty levels. Questions were categorized as easy, moderate, or complex. The models’ performance was measured in terms of two key metrics: response time to answer each question and the accuracy of their responses (or quality of the answer). Using RAG method, the results show that LLaMA 3.1 consistently outperformed other models in both Arabic and English languages, whereas Gemma 2 demonstrated decent results. Additionally, response time can be misleading indicators of model performance because less accurate responses require less effort and are generated more rapidly than highly accurate ones.},
	booktitle = {2024 25th {International} {Arab} {Conference} on {Information} {Technology} ({ACIT})},
	author = {Mohd, Bassam J. and Ahmad Yousef, Khalil M. and Abu Ghalyon, Salah G.},
	month = dec,
	year = {2024},
	note = {ISSN: 2831-4948},
	keywords = {Accuracy, Adaptation models, Artificial Intelligence, Computational modeling, Focusing, Generative AI, Generative Models, Information technology, large language models, Machine Learning, Natural Language Processing, Neural Networks, Retrieval augmented generation, Retrieval-augmented generation, Testing, Time factors, Time measurement},
	pages = {1--6},
}

@inproceedings{rani_augmenting_2024,
	title = {Augmenting {Code} {Sequencing} with {Retrieval}-{Augmented} {Generation} ({RAG}) for {Context}-{Aware} {Code} {Synthesis}},
	doi = {10.1109/SSITCON62437.2024.10796587},
	abstract = {The growing demand for efficient code generation has driven research into improving Large Language Models (LLMs). This project presents a novel system designed to enhance code generation by leveraging Retrieval-Augmented Generation (RAG), Grounding techniques, and Prompt Parameters. RAG integrates external knowledge to enrich code outputs, while Grounding methods improve the model’s ability to interpret language with real-world context. Prompt Parameters offer flexibility, enabling customized outputs based on user preferences. These methods were implemented and tested on various code generation tasks, resulting in contextually relevant and accurate outputs. The proposed system streamlines software development workflows, reduces errors, and fosters better collaboration between developers and machine-assisted coding tools. Ultimately, this approach represents a significant advancement in automating code generation from natural language descriptions.},
	booktitle = {2024 {First} {International} {Conference} on {Software}, {Systems} and {Information} {Technology} ({SSITCON})},
	author = {Rani, S Jansi and Deepika, S G and Devdharshini, D and Ravindran, Harini},
	month = oct,
	year = {2024},
	keywords = {Accuracy, code generation, Codes, Computational modeling, Context modeling, grounding, Grounding, language models, Large language models, natural language processing, Natural languages, prompt parameters, Retrieval augmented generation, retrieval-augmented generation, Software systems, Training data},
	pages = {1--7},
}

@inproceedings{krishnaraj_document_2025,
	title = {Document {Query} {Response} {Using} {Microsoft}-{Phi}-3-{Mini}-{4K}},
	doi = {10.1109/ESCI63694.2025.10988177},
	abstract = {The role of automated mechanisms such as chatbots has improved in today’s environment of rapid technological progress which is majorly attributed to the growing use of Artificial Intelligence (AI) based solutions. The ability of such systems to interact with questions concerning PDFs can completely transform support and after sales services in the automotive field where time and resources can be saved. Within this context, this work analyses the application of Large Language Models (LLMs) in the development of RAG based applications to create a smarter and confidentiality-driven chatbot to preserve the privacy of individuals and companies alike.},
	booktitle = {2025 {International} {Conference} on {Emerging} {Smart} {Computing} and {Informatics} ({ESCI})},
	author = {Krishnaraj, M. and Isaacraj, S. Eben and Soorya, V. Haris Jai},
	month = mar,
	year = {2025},
	note = {ISSN: 2996-1815},
	keywords = {Automotive engineering, Chatbots, Companies, embeddings, Informatics, Large language models, Large Language Models (LLM), Privacy, Retrieval augmented generation, Retrieval Augmented Generation(RAG), Transforms},
	pages = {1--5},
}

@inproceedings{rorseth_towards_2024,
	title = {Towards {Explainability} in {Retrieval}-{Augmented} {LLMs}},
	doi = {10.1109/ICDE60146.2024.00466},
	abstract = {In an era where artificial intelligence (AI) is re-shaping countless aspects of society, we present a forward-looking perspective for enhancing the explainability of large language models (LLMs), with a particular focus on the retrieval-augmented generation (RAG) prompting technique. We motivate the urgency for developing techniques to explain LLM decision-making behaviour, especially as these models are deployed in critical sectors. Central to this effort is RAGE, our novel explain-ability tool that can trace the provenance of an LLM's answer back to external knowledge sources provided via RAG. RAGE builds upon established explainability techniques to recover citations for LLM answers, identify context biases, and mine answer rules. Through our novel explainability formulations and practical use cases, we chart a course toward more transparent and trustworthy AI technologies.},
	booktitle = {2024 {IEEE} 40th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Rorseth, Joel and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw},
	month = may,
	year = {2024},
	note = {ISSN: 2375-026X},
	keywords = {Data engineering, Decision making, Explainable AI, Information retrieval, Large language models, Large Language Models, Perturbation methods, Retrieval-Augmented Generation, Sports},
	pages = {5669--5670},
}

@inproceedings{hayashi_metadata-based_2024,
	title = {Metadata-based {Data} {Exploration} with {Retrieval}-{Augmented} {Generation} for {Large} {Language} {Models}},
	doi = {10.1109/BigData62323.2024.10826055},
	abstract = {Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Hayashi, Teruaki and Sakaji, Hiroki and Dai, Jiayi and Goebel, Randy},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {data exploration, Databases, dataset search, Decision making, Estimation, Explainable AI, large language model, Large language models, Metadata, Retrieval augmented generation, retrieval-augmented generation, Semantics, Soft sensors, Vectors},
	pages = {6574--6583},
}

@inproceedings{nethassanai_mari_2025,
	title = {Mari: {Automating} {Responses} to {Thai} {Legal} {Debt} {Queries} {Using} {Generative} {AI} and {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ECTI-CON64996.2025.11101123},
	abstract = {Over the years, there have been lots of reports that point out the problems of Thailand’s high personal debt levels. With limited public knowledge of legal debt rights, these problems result in right violation between the creditors and debtors. Mari is designed to address such issues. It is an AI-powered system that offers automated legal consultations pertaining to the Thai Debt Collection Act of 2015. Main focus is on aiding creditors, debtors, or any individual, to understand their rights which contribute a boon of addressing information gaps, and enhancing accessibility to legal knowledge without requiring costly consultations. Mari is a website that uses a large language model (LLM) with integrated Retrieval-Augmented Generation (RAG) to ensure the precise responses for the required scope and semantics of the concerned legal statute. In order to achieve the effectiveness of these responses, we considered several LLM models, namely OpenThaiGPT, Typhoon, and SambaNova under the evaluation based on a set of human-generated multiple-choice and open-end questions. The comparison score showed that SambaNova’s LLM, with English prompt engineering, was the best for this case. Lastly the semantic difference, representing the level of hallucination effect, of the final model is measured by the BERT score with satisfaction of 0.726.},
	booktitle = {2025 22nd {International} {Conference} on {Electrical} {Engineering}/{Electronics}, {Computer}, {Telecommunications} and {Information} {Technology} ({ECTI}-{CON})},
	author = {Nethassanai, Thitipa and Seehabong, Nichakon and Roteaim, Saranrat and Mongkolnam, Pornchai and Watanapa, Bunthit},
	month = may,
	year = {2025},
	note = {ISSN: 2837-6471},
	keywords = {chatbot, Chatbots, Computational modeling, Information technology, Large language models, Law, legal, LLM, Prompt engineering, RAG, Retrieval augmented generation, Semantics, Telecommunications, Thai Debt Collection Act, Tropical cyclones},
	pages = {1--6},
}

@inproceedings{shavaki_knowledge_2024,
	title = {Knowledge {Graph} {Based} {Retrieval}-{Augmented} {Generation} for {Multi}-{Hop} {Question} {Answering} {Enhancement}},
	doi = {10.1109/IKT65497.2024.10892619},
	abstract = {Multi-hop question answering (QA), which requires integrating information from multiple sources, poses significant challenges in natural language processing. Existing methods often struggle with effective retrieval across documents, leading to incomplete or inaccurate answers. Building upon Graph-based Retrieval-Augmented Generation (Graph RAG), we enhance multi-hop QA by leveraging structured knowledge graphs. Specifically, we construct individual knowledge graphs for each document, where entities are represented as nodes and the relationships between them as edges enriched with contextual properties. These individual graphs are then seamlessly integrated into a comprehensive, unified graph that captures cross-document relationships. Our method improves retrieval by utilizing vector embeddings of these graph relations, enabling more effective multi-hop reasoning across the interconnected data. To evaluate our approach, we assembled a dataset of 500 documents paired with 296 multi-hop questions requiring cross-document information retrieval. Our contributions include developing a novel graph-based retrieval mechanism that leverages vector embeddings of graph relations within the Graph RAG framework, and assembling a comprehensive dataset for multi-hop QA. Comparative experiments show that our enhanced Graph RAG method significantly outperforms the baseline in factual accuracy and semantic similarity, as measured by the RAGAS framework. Additionally, an LLM-based evaluator highlights our method's superior performance in answer comprehensiveness, empowerment, and directness.11The source code and dataset are available at: https://github.com/AmiriShavaki/KG-based-RAG-for-Multi-hop-QA},
	booktitle = {2024 15th {International} {Conference} on {Information} and {Knowledge} {Technology} ({IKT})},
	author = {Shavaki, Mahdi Amiri and Omrani, Pouria and Toosi, Ramin and Akhaee, Mohammad Ali},
	month = dec,
	year = {2024},
	note = {ISSN: 2476-2180},
	keywords = {Accuracy, Cognition, Databases, Generative AI, Graph RAG, Information retrieval, Knowledge graphs, LLM, Multi-hop QA, NLP, Question answering (information retrieval), RAG, Retrieval augmented generation, Semantics, Source coding, Vectors},
	pages = {78--84},
}

@inproceedings{pan_raglog_2024,
	title = {{RAGLog}: {Log} {Anomaly} {Detection} using {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/WFPST58552.2024.00034},
	abstract = {The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.},
	booktitle = {2024 {IEEE} {World} {Forum} on {Public} {Safety} {Technology} ({WFPST})},
	author = {Pan, Jonathan and Liang, Wong Swee and Yidi, Yuan},
	month = may,
	year = {2024},
	keywords = {Databases, Digital forensics, Fault diagnosis, Large Language Model, Large language models, Log analysis, Machine learning, Pipelines, Retrieval Augmented Generation, Vectors},
	pages = {169--174},
}

@inproceedings{gijre_enhancing_2024,
	title = {Enhancing {Question}-{Answering} with {Knowledge} {Graph} {Retrieval} and {Generation} using {LLMs}},
	doi = {10.1109/ICAIQSA64000.2024.10882212},
	abstract = {This paper describes a novel technique to improving Large Language Models (LLMs) for document analysis that employs knowledge graphs and retrieval-augmented generation (RAG). We are working on constructing a chatbot system that can handle and analyze large documents from a variety of fields. Our approach addresses basic LLM issues including context maintenance and hallucination avoidance. The system combines document chunking, vector embedding, and similarity search with graph-based knowledge representation. Users can upload large papers and answer questions accurately. We show that integrating standard information retrieval approaches with graph-based storage and LLM capabilities improves context awareness and response accuracy across a wide range of document genres. This strategy is especially promising for complicated publications such as financial reports.},
	booktitle = {2024 {International} {Conference} on {Artificial} {Intelligence} and {Quantum} {Computation}-{Based} {Sensor} {Application} ({ICAIQSA})},
	author = {Gijre, Supraj and Agrawal, Rishi and Laddha, Priyash and Keswani, Gunjan},
	month = dec,
	year = {2024},
	keywords = {Information retrieval, Knowledge Graph, Knowledge graphs, LangChain, Large language models, Large Language Models, Maintenance, Natural Language Processing, Neo4j, Quantum computing, Retrieval augmented generation, Retrieval Augmented Generation, Soft sensors, Standards, Text analysis, Vectors},
	pages = {1--6},
}

@inproceedings{wan_facilitating_2025,
	title = {Facilitating {Design} for {Additive} {Manufacturing} with {KG}-based {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICE/ITMC65658.2025.11106597},
	abstract = {Additive manufacturing (AM), or 3D printing, enables the production of complex geometries and highly customized components. However, design for AM (DfAM) requires specialised and comprehensive knowledge of process constraints, material behaviours, and performance parameters. While knowledge graphs (KGs) have been utilized to organize and integrate DfAM knowledge, they do not understand natural language and have limited reasoning capabilities, which make them less accessible to non-experts and ineffective in handling complex and context-dependent design queries. Large language models (LLMs) offer powerful language processing and generalizability but suffer from hallucinations when handling specialized domains. To address these limitations, this study proposes a KG-based retrieval-augmented generation (RAG) approach to develop a domain-specific question-answering (Q\&A) in DfAM. By integrating structured knowledge from a DfAM KG with the strengths of LLMs, the proposed approach improves response accuracy and relevance. Comparative experiments evaluated LLMs with non-RAG and KG-based RAG using generic and domain-specific metrics. Results demonstrated that KG-based RAG enhances information retrieval and response quality, reduces hallucinations, and ensures alignment with domain knowledge.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Engineering}, {Technology}, and {Innovation} ({ICE}/{ITMC})},
	author = {Wan, Yuwei and Liu, Ying and Zammit, Joseph Paul and Chen, Zheyuan and Li, Li and Francalanza, Emmanuel},
	month = jun,
	year = {2025},
	note = {ISSN: 2693-8855},
	keywords = {Accuracy, Additive manufacturing, Design for additive manufacturing, Knowledge graph, Knowledge graphs, Large language model, Large language models, Pipelines, Production, Prompt engineering, Retrieval augmented generation, Retrieval-augmented generation, Stakeholders, Technological innovation, Three-dimensional printing},
	pages = {1--8},
}

@inproceedings{gangavarapu_evaluating_2025,
	title = {Evaluating {Accuracy} in {Large} {Language} {Models}: {Benchmarking} {Corrective} {Rag} {Vs}. {Naive} {Retrieval} {Augmented} {Generation} {Approach}},
	doi = {10.1109/ICAD65464.2025.11114027},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as a promising approach to mitigate the limitations of Large Language Models (LLMs) in generating factually accurate and consistent text. The main focus of this technical survey is on correct answers as the key performance indicator (KPI) for comparing two well-known RAG methods: Naive RAG and Corrective retrieval augmented Generation (CRAG). Naive RAG exhibits a strong dependence on the relevance of retrieved documents, resulting in suboptimal performance when retrieval quality is compromised. CRAG, on the other hand, adds new features to improve robustness and adaptability, such as a retrieval evaluator, large-scale web searches, and a decompose-then-recompose algorithm. We introduce the Comprehensive RAG Benchmark (CRAG), which encompasses a diverse set of question-answer pairs spanning multiple domains, categories, entity popularities, and temporal dynamics, to facilitate a comprehensive evaluation of RAG models' performance in generating correct answers. Experiments show that adding RAG to LLMs makes a big difference in how many correct answers you get, with CRAG consistently beating Naive RAG. Nevertheless, RAG models, including CRAG, demonstrate lower answer correctness when confronted with questions pertaining to highly dynamic, less popular, or more complex facts. These results make it clear that more research and development is needed to make RAG models more reliable and able to give correct answers in a wider range of situations.},
	booktitle = {2025 {IEEE} {International} {Conference} on {AI} and {Data} {Analytics} ({ICAD})},
	author = {Gangavarapu, Rajendra and Srinivasan, Aswath Ram Adayapalam and Moparthi, Venkata},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Benchmark testing, Computer architecture, Corrective Retrieval Augmented Generation (CRAG), Correctness, Data models, Key performance indicator, Language Models, Large language models, Modality, Optimization, Retrieval augmented generation, Training, Web search},
	pages = {1--7},
}

@inproceedings{elfayoumi_knowledge_2025,
	title = {Knowledge {Augmented} {Significant} {Language} {Model}-{Based} {Chatbot} for {Explainable} {Diabetes} {Mellitus} {Prediction}},
	doi = {10.1109/IMCOM64595.2025.10857525},
	abstract = {This study proposes an innovative diabetes prediction chatbot that utilizes large language models (LLMs) to determine the likelihood of diabetes based on specific patient inputs. Unlike conventional machine learning models and in addition to providing precise, individualized, robust prediction of diabetes augmented by the percentage of its confidence, this chatbot provides detailed text-based explanations for its individual predictions and enhances user's understanding by retrieving and presenting similar cases using case-based reasoning techniques. Utilizing key health indicators such as hypertension status, heart disease presence, smoking history, BMI, HbA1c level, and blood glucose levels, the chatbot not only predicts the presence of diabetes but also educates users about the underlying reasons for each prediction. The system's explanation module promotes transparency and trust in the predictive process. The proposed architecture integrates the retrieval augmented generation (RAG) technique with prompt engineering across multiple LLMs, including Llama 3.1, GPT- 3.5, and Gemma2. This approach augments the chatbot's responses with contextually relevant data from similar past cases, thereby enhancing both relevance and accuracy. Furthermore, RAG minimizes LLM hallucinations while enhancing them with up-to-date, precise, personalized medical information. The objective of this chatbot is to offer healthcare professionals and patients a dependable, educational instrument for early detection of diabetes supported by transparent AI principles. This will have a positive effect on preventative healthcare measures.},
	booktitle = {2025 19th {International} {Conference} on {Ubiquitous} {Information} {Management} and {Communication} ({IMCOM})},
	author = {Elfayoumi, Mazen and AbouElazm, Mohamed and Mohamed, Omar and Abuhmed, Tamer and El-Sappagh, Shaker},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Chatbot, Chatbots, Clinical Decision Support Systems, Computational modeling, Diabetes, Diabetes Prediction, Explainable AI, Large language models, Large Language Models, Medical diagnostic imaging, Medical services, Predictive models, Retrieval augmented generation, Retrieval Augmented Generation, Technological innovation},
	pages = {1--8},
}

@inproceedings{i_vilar_accessible_2025,
	title = {Accessible and {Reliable} {AI} {Coding} {Tutors}: {Augmenting} {Large} {Language} {Models} with {Retrieval}-{Augmented} {Generation} for {Java} {Programming}},
	doi = {10.1109/EDUCON62633.2025.11016497},
	abstract = {This paper addresses the challenge of improving the reliability and accuracy of Large Language Models (LLMs) for assisting students in learning Java programming, a critical component of object-oriented computer science courses. While LLMs have shown promise in generating code, they often produce incorrect or unreliable outputs, which can hinder the learning process. To mitigate these issues, we propose a novel augmentation framework that integrates Retrieval-Augmented Generation (RAG), enabling LLMs to retrieve best-practice Java code examples from external sources to enhance the accuracy of generated solutions. We evaluate this framework using 250 Java coding tasks, covering a range of difficulties. Our findings show that the RAG-augmented model, when applied to a lightweight LLM (Google DeepMind's Gemma), outperformed baseline model by 19 \% for mid and high-difficulty problems. In particular, Augmented Gemma generated accepted code for 11 problems where no other model could provide a valid solution. This suggests that retrieving external best-practice examples is critical in addressing complex coding challenges. These results highlight the potential of lightweight, accessible models enhanced with RAG to provide reliable AI coding assistance in educational settings, facilitating both accurate problem-solving and reinforcement of best coding practices.},
	booktitle = {2025 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	author = {i Vilar, Guiu Puigcercos and Rashid, Parvez and Tonekaboni, Navid Hashemi},
	month = apr,
	year = {2025},
	note = {ISSN: 2165-9567},
	keywords = {Accuracy, Artificial Intelligence, Codes, Coding Assistant, Encoding, Java, Java programming language, Large language models, Large Language Models (LLMs), Object oriented modeling, Open-source models, Problem-solving, Programming profession, Reliability engineering, Retrieval augmented generation, Retrieval Augmented Generation (RAG)},
	pages = {1--10},
}

@inproceedings{tjokro_enhancing_2024,
	title = {Enhancing {Data} {Traceability}: {A} {Knowledge} {Graph} {Approach} with {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ISRITI64779.2024.10963652},
	abstract = {This study explores the challenges of data traceability within Company ABC’s Master Data Management (MDM) system, where handling data-related requests has become increasingly complex due to inefficient manual processes. Despite MDM’s implementation, business growth has led to the accumulation of documents and difficulties in tracking request lineage, causing bottlenecks in daily operations. The study proposes a solution that integrates knowledge graphs with Retrieval-Augmented Generation (RAG), inspired by LinkedIn’s question-answering system, to improve data management and traceability. The knowledge graph is constructed by manually parsing spreadsheet data related to requests, allowing for more efficient data integration and retrieval. The study compares three approaches—graph-based, vector-based, and hybrid—in terms of answer correctness and context retrieval. The graph-based approach yielded the best performance, improving answer correctness by 21.09\% over the vector-based method and by 19.34\% over the hybrid method. While the hybrid method achieved the highest context precision at 83.49\%, it struggled with context recall, retrieving relevant context only 69.09\% of the time. The study also found that the vector-based method’s retrieval of irrelevant context caused the language model to produce misleading answers, even when accompanied by more relevant graph-based context. In conclusion, integrating RAG with knowledge graphs significantly enhances answer accuracy and context retrieval precision. Future research should focus on refining context-weighting techniques between the vector and graph-based methods and on addressing the issue of irrelevant context to further improve the overall effectiveness of the model in data traceability and information retrieval tasks.},
	booktitle = {2024 7th {International} {Seminar} on {Research} of {Information} {Technology} and {Intelligent} {Systems} ({ISRITI})},
	author = {Tjokro, Vincencius Christiano and Ady Sanjaya, Samuel},
	month = dec,
	year = {2024},
	note = {ISSN: 2832-1456},
	keywords = {Context modeling, Data Traceability, Information technology, Intelligent systems, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Manuals, RAG, Refining, Retrieval augmented generation, Retrieval-Augmented Generation, Seminars, Vector Retrieval, Vectors},
	pages = {473--478},
}

@inproceedings{roychowdhury_eratta_2024,
	title = {{ERATTA}: {Extreme} {RAG} for enterprise-{Table} {To} {Answers} with {Large} {Language} {Models}},
	doi = {10.1109/BigData62323.2024.10825910},
	abstract = {Large language models (LLMs) with retrieval augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. Although RAG implemented with AI agents (agentic-RAG) has been recently popularized, its suffers from unstable cost and unreliable performances for Enterprise-level data-practices. Most existing use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user-query routing, data-retrieval and custom prompting for question-answering capabilities from Enterprise-level data tables on sustainability. The source tables here are highly fluctuating and large in size storing carbon footprint, energy and water usage at buildings in regional levels globally and the proposed framework enables structured responses in under 10 seconds per query. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve {\textgreater}90\% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Roychowdhury, Sohini and Krema, Marko and Mahammad, Anvar and Moore, Brian and Mukherjee, Arijit and Prakashchandra, Punit},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {authentication, hallucination, Large language models, Measurement, Message authentication, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Routing, Scalability, Social networking (online), Soft sensors, sustainability, Sustainable development, TextBison, Unicorn, Water resources},
	pages = {4605--4610},
}

@inproceedings{hamzic_evaluation_2024,
	title = {Evaluation and {Comparison} of {Open}-{Source} {LLMs} {Using} {Natural} {Language} {Generation} {Quality} {Metrics}},
	doi = {10.1109/BigData62323.2024.10825576},
	abstract = {The rapid advancement of Large Language Models (LLMs) has transformed natural language processing, yet comprehensive evaluation methods are necessary to ensure their reliability, particularly in Retrieval-Augmented Generation (RAG) tasks. This study aims to evaluate and compare the performance of open-source LLMs by introducing a rigorous evaluation framework. We benchmark 20 LLMs using a combination of established metrics such as BLEU, ROUGE, BERTScore, along with and a novel metric, RAGAS. The models were tested across two distinct datasets to assess their text generation quality. Our findings reveal that models like nous-hermes-2-solar-10.7b and mistral-7b-instruct-v0.1 consistently excel in tasks requiring strict instruction adherence and effective use of large contexts, while other models show areas for improvement. This research contributes to the field by offering a comprehensive evaluation framework that aids in selecting the most suitable LLMs for complex RAG applications, with implications for future developments in natural language processing and big data analysis.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Hamzic, Dzenan and Wurzenberger, Markus and Skopik, Florian and Landauer, Max and Rauber, Andreas},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Benchmark testing, Big Data, Context modeling, Data models, Large Language Model, Large language models, LLM Benchmarking, LLM Evaluation, Measurement, Natural language generation, Natural Language Generation Evaluation, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation},
	pages = {5342--5351},
}

@article{praneeth_optimization_2025,
	title = {Optimization of {Customer} {Feedback} {Summarization} {Using} {Large} {Language} {Models} ({LLM}) and {Advanced} {Retrieval}-{Augmented} {Generation}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3588337},
	abstract = {Customer feedback, often shared through online reviews, plays a crucial role in shaping business strategies. However, the overwhelming volume of such reviews poses two major challenges: valuable insights often go unnoticed, and manual analysis introduces human bias. To address this, we propose a system that leverages large language models (LLMs) integrated with the LangChain framework to answer natural language queries over customer reviews. A synthetic dataset was created to resemble food delivery reviews typically seen on the Play Store and was stored in a vector database. On receiving a user query, relevant reviews are retrieved using advanced Retrieval-Augmented Generation (RAG) techniques, namely Hierarchical Chunk Retrieval and RAG Fusion that are further refined using the Declarative Self-improving Python (DSPy) framework to generate accurate, grounded responses. The system was evaluated using LLaMA-3-8B-InstructLite, GPT-3.5-Turbo, and Gemini-1.5-Pro, and compared against two non-LLM baselines: BM25 and a fine-tuned BERT model. Results show that our LLM-based pipeline outperforms by an average 15\% in semantic and factual accuracy. Component-level analysis showed that enhanced retrieval strategies showed aggregate improvements of 4.9\% in semantic relevance, 12.1\% in lexical coverage, and 9.9\% in factual consistency over traditional RAG. Further integration of DSPy led to an additional 10.8\% boost in linguistic fluency and a 9.0\% gain in factual alignment. Among the evaluated models, Gemini-1.5-Pro combined with RAG Fusion and DSPy produced the most fluent and factually accurate responses, demonstrating the effectiveness of combining hybrid retrieval with LLM-driven reasoning for query-based feedback response system.},
	journal = {IEEE Access},
	author = {Praneeth, Buchepalli and {Mohana} and Nattem, Eshitha Chowdary and Jetti, Kamala and Kavyashree, B. K. and Rakshitha, D. and Ramakanth Kumar, P. and Sreelakshmi, K.},
	year = {2025},
	keywords = {Accuracy, BERT, BM25, Cognition, Customer reviews, Databases, DSPy, Gemini-1.5-Pro, GPT-3.5-Turbo, hierarchical chunk retrieval, LangChain, Large language models, Llama-3-8B-Instruct-Lite, LLM, Question answering (information retrieval), RAG, RAG fusion, Retrieval augmented generation, Reviews, Semantics, Surveys, Vectors},
	pages = {124319--124332},
}

@inproceedings{anaguchi_reasoning_2024,
	title = {Reasoning and {Justification} {System} for {Domestic} {Hazardous} {Behaviors} {Based} on {Knowledge} {Graph} of {Daily} {Activities} and {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/CANDAR64496.2024.00010},
	abstract = {Accidents among people over 65 years of age predominantly occur within residential settings, making the maintenance of a safe home environment a crucial social issue. To address this issue, previous research has developed systems that construct Knowledge Graphs (KG) based on simulations of daily household activities, and studies have been conducted on detecting hazardous behaviors using such KG analysis. In this current study, we propose a system capable of presenting the reason and justification for the detected domestic hazardous behaviors. Our system will first generates the reason for the detected behavior using a Large Language Model (LLM). To ensure the accuracy, reliability and reproducibility of the LLM output, the system will provides reliable sources to support the output. We employed Retrieval-Augmented Generation (RAG) to search for sentences similar to the reason generated by the LLM within reliable, authoritative documents describing domestic accident cases and their causes and these will be presented as the evidence alongside the search engine results to the users. Consequently, a knowledge graph (KG) of domestic hazardous behavior is developed based on evidence ontology. Finally, to evaluate the ability of our proposed system in appropriately generating reasons for domestic hazardous behaviors and the adequacy of the justifications provided, the output was rated using LLMs and human volunteers. The rating results showed a significant correlation between LLMs and human evaluation, indicating that the proposed system can provide sufficient reasons and justifications for domestic hazardous behaviors at residential setting.},
	booktitle = {2024 {Twelfth} {International} {Symposium} on {Computing} and {Networking} ({CANDAR})},
	author = {Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
	month = nov,
	year = {2024},
	note = {ISSN: 2379-1896},
	keywords = {Accidents, Behavioral sciences, Correlation, Explainable AI, Knowledge Graph, Knowledge graphs, Large Language Model, Ontologies, Optimization, Reproducibility of results, Retrieval augmented generation, Retrieval-Augmented Generation, Search engines, Sensors},
	pages = {11--20},
}

@inproceedings{vungarala_tpu-gen_2025,
	title = {{TPU}-{Gen}: {LLM}-{Driven} {Custom} {Tensor} {Processing} {Unit} {Generator}},
	doi = {10.1109/ICLAD65226.2025.00010},
	abstract = {The increasing complexity and scale of Deep Neural Networks (DNNs) necessitate specialized tensor accelerators, such as Tensor Processing Units (TPUs), to meet various computational and energy efficiency requirements. Nevertheless, designing optimal TPU remains challenging due to the high domain expertise level, considerable manual design time, and lack of high-quality, domain-specific datasets. This paper introduces TPU-Gen, the first Large Language Model (LLM) based framework designed to automate the exact and approximate TPU generation process, focusing on systolic array architectures. TPU-Gen is supported with a meticulously curated, comprehensive, and open-source dataset that covers a wide range of spatial array designs and approximate multiply-and-accumulate units, enabling design reuse, adaptation, and customization for different DNN workloads. The proposed framework leverages Retrieval-Augmented Generation (RAG) as an effective solution for a data-scarce hardware domain in building LLMs, addressing the most intriguing issue, hallucinations. TPU-Gen transforms high-level architectural specifications into optimized low-level implementations through an effective hardware generation pipeline. Our extensive experimental evaluations demonstrate superior performance, power, and area efficiency, with an average reduction in area and power of 92\% and 96\% from the manual optimization reference values. These results set new standards for driving advancements in next-generation design automation tools powered by LLMs.1},
	booktitle = {2025 {IEEE} {International} {Conference} on {LLM}-{Aided} {Design} ({ICLAD})},
	author = {Vungarala, Deepak and Elbtity, Mohammed Essa and Pandit, Kartik and Syed, Sumiya and Alam, Sakila and Ghosh, Arnob and Zand, Ramtin and Angizi, Shaahin},
	month = jun,
	year = {2025},
	keywords = {accelerators, Artificial neural networks, Hardware, large language model, Manuals, Optimization, Pipelines, Retrieval augmented generation, retrieval-augmented generation, Standards, Systolic arrays, tensor processing unit, Tensors, Transforms},
	pages = {1--8},
}

@inproceedings{sun_irrational_2025,
	title = {The {Irrational} {LLM}: {Implementing} {Cognitive} {Agents} with {Weighted} {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICCCN65249.2025.11134012},
	abstract = {This paper advances research on social networks, extended reality, and the metaverse by bringing together innovations from two different communities – AI and cognitive science – to develop LLM-based agents with not only fluent responses but also realistic opinion dynamics that capture a variety of human biases, imperfections, and general departures from rationality. This avenue of investigation can empower applications from social simulation of human opinions in geopolitical hotspots to realistic non-player character interactions in metaverse games. Recent advances in AI have made remarkable progress toward general intelligence with the introduction of large language models (LLMs). They also enabled grounding LLM responses in specialized information stored externally using retrieval-augmented generation (RAG). In a separate line of research, studies on human cognition have produced cognitive architectures that emulate human departures from rationality, such as biases and imperfections, which are crucial to understanding a wide range of social phenomena and human preferences. A critical mechanism in cognitive architectures is the modulation of retrieval weights from (human) memory; we are biased in what we remember. Combining RAG with cognitive model-inspired computation of information retrieval weights, we develop the Irrational LLM – one that weighs information retrieval in RAG systems according to cognitive models, thereby accurately emulating human opinion formation. We implement the novel human cognition-inspired RAG framework (CogRAG) and use it to emulate option developments on different sides of a conflict regarding debated issues. Responses generated by CogRAG (on posts withheld from training data) show close correspondence with real responses posted on social media, suggesting the viability of this approach in approximating biased human opinions. We hope this study paves the way to new directions in AI, social networks, metaverse computing, and human-in-the-loop modeling that better represent diverse human opinions in geopolitical, entertainment, and socio-technical contexts.},
	booktitle = {2025 34th {International} {Conference} on {Computer} {Communications} and {Networks} ({ICCCN})},
	author = {Sun, Dachun and Lyu, You and Li, Jinning and Liu, Xinyi and Kara, Denizhan and Lebiere, Christian and Abdelzaher, Tarek},
	month = aug,
	year = {2025},
	note = {ISSN: 2637-9430},
	keywords = {Bounded Rationality, Cognitive Architecture, Computational modeling, Computer architecture, Large language models, Large Language Models, Metaverse, Predictive models, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Robustness, Social networking (online), Technological innovation, Training data},
	pages = {1--9},
}

@inproceedings{dutta_ai_2024,
	title = {{AI} {Legal} {Assistant} for {IPC}},
	doi = {10.1109/CSITSS64042.2024.10817061},
	abstract = {The legal framework is a critical component of societal structure, yet its complexity often leaves individuals struggling to understand their rights and obligations. This paper introduces an advanced NLP-based chatbot designed to enhance legal accessibility and comprehension, focusing on the Indian Penal Code (IPC). The proposed system integrates Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to provide precise and contextually relevant legal information. The chatbot utilizes NLP algorithms for interpreting legal texts and generating user-friendly responses, thus facilitating a better understanding of legal articles and statutes. Streamlit is employed to create an interactive user interface, ensuring a seamless experience for users seeking legal advice. The implementation of this system aims to bridge the gap between complex legal language and public understanding, improve the efficiency of accessing legal information, and support individuals and small businesses in navigating legal challenges. This approach is anticipated to significantly enhance legal awareness and accessibility, contributing to a more informed and legally savvy society.},
	booktitle = {2024 8th {International} {Conference} on {Computational} {System} and {Information} {Technology} for {Sustainable} {Solutions} ({CSITSS})},
	author = {Dutta, Ayush and Sarma, Kaushik Kumar and P., Yamini Sahukar},
	month = nov,
	year = {2024},
	note = {ISSN: 2767-1097},
	keywords = {Business, Chatbot Technology, Chatbots, Codes, Indian Penal Code, Large language models, Large Language Models, Law, Legal Information Systems, Natural Language Processing, Navigation, NLP Algorithms, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors, Streamlit, User interfaces},
	pages = {1--5},
}

@inproceedings{hamid_can_2025,
	title = {Can {Large} {Language} {Models} {Perform} {Retrieval}-{Augmented} {Generation} as {Multi}-{Hop} {Reasoning} {Over} {Knowledge} {Graphs}?},
	doi = {10.1109/MIUCC66482.2025.11196859},
	abstract = {Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding their outputs in external knowledge. However, conventional chunk-based retrieval methods are limited in handling complex multi-hop questions due to their lack of structural reasoning and semantic alignment. In this work, we propose a novel pipeline where an LLM performs multi-hop subgraph retrieval over a knowledge graph (KG), followed by hallucination detection using triplet-level graph alignment. The final answer is generated by conditioning the LLM on both the user query and the verified subgraph. To construct our benchmark, we automatically generate 500 diverse multi-hop questions from 10 real-world websites and construct their corresponding KGs using GPT-4o. Experimental results show that our method significantly outperforms traditional chunking-based RAG in answer accuracy (84.29\% vs. 68.77\%). Furthermore, we introduce a fine-tuning procedure using Group Relative Policy Optimization (GRPO) guided by FactAlign rewards, which further improves answer accuracy to 87.62\% while reducing hallucination by over 8 percentage points. Our approach demonstrates a principled and interpretable alternative to retrieval that leverages the structure and semantics of knowledge graphs, offering stronger factual consistency and reasoning capabilities.},
	booktitle = {2025 {International} {Mobile}, {Intelligent}, and {Ubiquitous} {Computing} {Conference} ({MIUCC})},
	author = {Hamid, Mohamed Rashad Abdel and El-Regaily, Salsabil Amin and Aref, Mostafa Mahmoud},
	month = sep,
	year = {2025},
	keywords = {Accuracy, Benchmark testing, Cognition, Factual QA, GRPO, Hallucination Detection, Knowledge graphs, Knowledge Graphs, Large language models, Large Language Models, Multi-hop Reasoning, Optimization, Pipelines, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Subgraph Retrieval, Ubiquitous computing},
	pages = {360--365},
}

@inproceedings{wang_retrieval-augmented_2025,
	title = {Retrieval-{Augmented} {Generation} for {Software} {Requirement}-{Based} {Test} {Case} {Generation}},
	doi = {10.1109/QRS65678.2025.00022},
	abstract = {Testers often need to manually write black-box test cases based on software artifacts such as requirement documents. In agile development, this process is often time-consuming and is further complicated by frequent requirement changes, leading to continuous maintenance overhead. Automating this process is therefore essential. Given the strong natural language understanding and generation capabilities of large language models (LLMs), combined with Retrieval-Augmented Generation (RAG), we propose a RAG-based framework for automated test case generation. Before generation, we embed software artifacts to construct a vectorbased knowledge database. At runtime, software requirements are used as queries to retrieve relevant context, which is integrated into a prompt and passed to the LLM for test case generation. This approach addresses several shortcomings of LLMs, including limited context length, attention dilution over large inputs, and the tendency to hallucinate or over-look key domain-specific constraints. By providing query-specific external knowledge, RAG enhances both accuracy and efficiency. We deploy the framework with different models locally and conduct experiments on two open-source datasets. Compared with the manually written benchmark test cases, our method achieves full requirement coverage with fewer test cases, improved efficiency, reduced error potential, and realized better readability.},
	booktitle = {2025 25th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Wang, Zhiyao and Guo, Xiujing and Tsuchiya, Tatsuhiro},
	month = jul,
	year = {2025},
	note = {ISSN: 2693-9177},
	keywords = {Closed box, Databases, Large Language Model, Large language models, Maintenance, Natural language processing, Retrieval augmented generation, Retrieval-Augmented Generation, Runtime, Security, Software quality, Software reliability, Software Requirement, Test Case Generation},
	pages = {108--119},
}

@inproceedings{arora_revolutionizing_2024,
	title = {Revolutionizing {Third} {Party} {Risk} {Management} {Using} {Generative} {AI} and {RAG}},
	doi = {10.1109/ICDCC62744.2024.10961910},
	abstract = {This integration of Generative AI and Retrieval Augmented Generation (RAG) will open new frontiers in Third Party Risk Management (TPRM) by making the creation of both risk assessments and compliance checks more accurate, efficient, and scalable. Quite naturally, TPRM is one area wherein organizations must engage in processes to make sure risks emanating from vendors and partners are duly mitigated. Traditionally, the TPRM process has been manual and time-consuming. With the functionality of Generative AI, autonomy in creating relevant content and automation in compliance significantly reduce the labor of these processes. With RAG integrated, the model improves the quality of AI responses through real-time grounding in authoritative data sources, reducing the risk of providing outdated or incorrect information. This dual approach will make sure organizations manage a growing number of third-party relationships with greater precision and timeliness. Indeed, the proposed AI-driven framework will minimize human error and reduce operational costs, besides being scalable to adapt to dynamic regulatory changes-thus offering a competitive advantage in third-party risk management.},
	booktitle = {2024 {First} {International} {Conference} on {Data}, {Computation} and {Communication} ({ICDCC})},
	author = {Arora, Shrey and Ramteke, Vidyavati},
	month = nov,
	year = {2024},
	keywords = {Automation, Compliance Automation, Data models, Generative AI, Grounding, Manuals, Organizations, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, Risk Assessment, Risk management, Soft sensors, Third Party Risk Management (TPRM)},
	pages = {16--22},
}

@inproceedings{zubkova_sugar_2025,
	title = {{SUGAR}: {Leveraging} {Contextual} {Confidence} for {Smarter} {Retrieval}},
	doi = {10.1109/ICASSP49660.2025.10890064},
	abstract = {Bearing in mind the limited parametric knowledge of Large Language Models (LLMs), retrieval-augmented generation (RAG) which supplies them with the relevant external knowledge has served as an approach to mitigate the issue of hallucinations to a certain extent. However, uniformly retrieving supporting context makes response generation source-inefficient, as triggering the retriever is not always necessary, or even inaccurate, when a model gets distracted by noisy retrieved content and produces an unhelpful answer. Motivated by these issues, we introduce Semantic Uncertainty Guided Adaptive Retrieval (SUGAR), where we leverage context-based entropy to actively decide whether to retrieve and to further determine between single-step and multi-step retrieval. Our empirical results show that selective retrieval guided by semantic uncertainty estimation improves the performance across diverse question answering tasks, as well as achieves a more efficient inference.},
	booktitle = {{ICASSP} 2025 - 2025 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zubkova, Hanna and Park, Ji-Hoon and Lee, Seong-Whan},
	month = apr,
	year = {2025},
	note = {ISSN: 2379-190X},
	keywords = {Entropy, large language models, Large language models, Noise measurement, question answering, Question answering (information retrieval), retrieval augmented generation, Retrieval augmented generation, Semantics, Signal processing, Speech processing, Sugar, Uncertainty, uncertainty estimation},
	pages = {1--5},
}

@inproceedings{zhang_meditrir_2025,
	title = {{MediTriR}: {A} {Triple}-{Driven} {Approach} to {Retrieval}-{Augmented} {Generation} for {Medical} {Question} {Answering} {Tasks}},
	doi = {10.1109/ICSC64641.2025.00032},
	abstract = {Advances in large language models have driven progress in medical question-answering systems, but challenges remain in accuracy and relevance, especially in complex medical settings. To address this problem, we introduce MediTriR. This approach combines knowledge graph triples with a retrieval-augmented generation framework to enhance the performance of large language models in medical multiple-choice question-answering tasks. Our approach transforms the retrieved medical knowledge into structured triples and uses these triples for accurate information retrieval. Furthermore, we dynamically generate new triples based on the properties of triples to enrich the model's reasoning ability. The integration of retrieval-augmented generation enables more accurate and contextual answers by leveraging external medical triples. MediTriR is evaluated against pure large language model approaches using the MedMCQA and Medical QA datasets, showing better performance in terms of accuracy and relevance. These results highlight the potential of our approach to improve medical decision support and education.},
	booktitle = {2025 19th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Zhang, Hongzhi and Shafiq, M. Omair},
	month = feb,
	year = {2025},
	note = {ISSN: 2472-9671},
	keywords = {Accuracy, Cognition, Data visualization, Knowledge graphs, Knowledge Graphs, Large Language Model, Large language models, Medical question-answering tasks, Natural Language Processing, Reliability, Retrieval augmented generation, Retrieval-augmented generation, Technological innovation, Transforms, Triples, Triples (Data structure)},
	pages = {187--194},
}

@inproceedings{he_can_2025,
	title = {Can {RAG} {Calibrate} {LLMs} for {Information} {Extraction} {Tasks}?},
	doi = {10.1109/CCWC62904.2025.10903733},
	abstract = {Information extraction (IE) is a crucial process in natural language processing (NLP) that involves automatically retrieving structured information from semi-structured and un-structured text sources. Text sources are semi-structured and unstructured documents, for example, W-2 forms, paystubs, mortgage documents, etc. Due to the recent bloom of LLMs, the accuracy of many NLP-based information extraction systems has improved. Another key attribute of a good information extraction system is certainty. The user must know how certain the system is regarding its extraction. Using LLM suffers from the calibration problem as most LLMs use a decoder-only architecture that produces logits conditioned on previous tokens. The confidence score computed by applying softmax on these logits is mostly off from being calibration. In this paper, we provide a generalizable framework to properly calibrate an LLM-based information extraction system with novel context rejection training tasks to teach LLM to ignore less useful examples. It can be easily extended to other tasks other than information extraction. Our method results in a 17.25\% improvement in F1-score and a 46.49\% improvement in certainty score over the baseline. It also outperforms the state-of-the-art ChatGPT model by a large margin.},
	booktitle = {2025 {IEEE} 15th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {He, JiangLong and Kumar, Deepak and Rasamsetty, Pratyusha},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Calibration, Constrastive Learning, Context modeling, Information Extraction, Information retrieval, Knowledge based systems, Large language models, Loans and mortgages, Measurement, Predictive models, Prompt engineering, Retrieval augmented generation, Retrieval-Augmented Generation, Robustness, Training},
	pages = {00414--00420},
}

@inproceedings{shelley_flora-rag_2025,
	title = {Flora-{RAG}: {Enhancing} {Conversational} {AI} with {Retrieval} {Augmented} {Generation} for {Floriculture}},
	doi = {10.1109/ICETM63734.2025.11051979},
	abstract = {Anthophiles are people who love flowers and want to acquire every inch of information they can get on them. This paper presents study on development of a multi-modal RAGenhanced conversational AI with a database full of Floriculture. This conversational AI multimodal is built in order to explore the preparing, embedding, and storing of both text and image data with the KBD.AI vector database. This vector database is specifically used as a retrieval tool within this RAG pipeline for both textual and visual data, making this conversational AI a multimodal. This paper elucidates the methodology used and the characteristics of the proposed RAG multimodal, which is highly domain-specific. The evaluation of joint training of the RAG’s retrieval and generation components for the domain knowledge task in Open Domain Question Answering is done. With numerous test cases giving accurate and exceptional test results, this study successfully demonstrates the effectiveness of the proposed RAG-enhanced conversational AI having floriculture dataset, while achieving a factual accuracy. This states that the model shows robustness, generalizability and efficiency of the proposed framework for image and text retrieval in domain of floriculture, contributing towards the advancement of agricultural technologies and question answering precision.},
	booktitle = {2025 {International} {Conference} on {Engineering}, {Technology} \& {Management} ({ICETM})},
	author = {Shelley, Sayuri and Kaur, Prabhjeet and Aggarwal, Garima and Kaushal, Abhishek and Dutta, Malay Kishore},
	month = may,
	year = {2025},
	keywords = {Accuracy, Conversational artificial intelligence, Floriculture, Gemini, ImageBind, KDB.AI, Large Language Model, Large language models, Multimodal, Question answering (information retrieval), Retrieval augmented generation, Retrieval Augmented Generation, Training, Transformers, Vector database, Vectors, Visual databases, Visualization},
	pages = {i--vi},
}

@inproceedings{li_knowledge_2025,
	title = {Knowledge {Graph} {Completion} using {RAG} and {Improved} {Structural} {Information}},
	doi = {10.1109/ECIS65594.2025.11087034},
	abstract = {Semantic and structural information are essential in Knowledge graph completion. We propose KRIS (Knowledge Graph Completion using Retrieval-Augmented Generation and Improved Structural Information), a method based on Retrieval-Augmented Generation (RAG) with enhanced structural information. KRIS uses semantic embeddings and RAG to enrich triple semantic information, reducing the hallucination problem, especially for domain-specific knowledge graphs. To address the weakness of semantic-embedding-based methods in leveraging structural information, we conduct joint training via supervised contrastive learning and structural information learning. Our model effectively utilizes both types of information. Experiments on public datasets show that KRIS significantly outperforms baseline models in prediction accuracy.},
	booktitle = {2025 {IEEE} 2nd {International} {Conference} on {Electronics}, {Communications} and {Intelligent} {Science} ({ECIS})},
	author = {Li, Bing and Mao, Zhifang and Yan, Rui and Ling, Aojia and Hu, Qin and Zeng, Qiang},
	month = may,
	year = {2025},
	keywords = {Accuracy, Contrastive learning, Knowledge Completion, Knowledge Graph, Knowledge graphs, Large Language Model, Natural Language Processing, Retrieval augmented generation, Sampling methods, Semantics, Stability analysis, Training, Transforms, Vectors},
	pages = {1--5},
}

@inproceedings{garcia_df-rag_2025,
	title = {{DF}-{RAG}: {A} {Dual} {Federated} {Retrieval}-{Augmented} {Generation} {Framework} for {Collaborative} {Medical} {AI}},
	abstract = {This paper introduces Dual Federated Retrieval-Augmented Generation (DF-RAG), a framework addressing privacy, interpretability, and reliability challenges in AI-driven healthcare. Large Language Models (LLMs) typically depend on centralized training and static data, causing inaccuracies ("hallucinations") and privacy risks. DF-RAG mitigates these issues through Federated Fine-Tuning (FFT) and Federated Knowledge Graphs (FKGs). FFT enables secure collaborative model refinement across institutions using encrypted updates, ensuring compliance with HIPAA and GDPR. Concurrently, FKGs offer real-time, validated medical knowledge retrieval, reducing inaccuracies and enhancing interpretability. Initial evaluations suggest DF-RAG improves diagnostic accuracy, scalability, and patient privacy, promising significant advances in clinical decision-making and personalized medicine.},
	booktitle = {2025 {IEEE}/{ACM} {Conference} on {Connected} {Health}: {Applications}, {Systems} and {Engineering} {Technologies} ({CHASE})},
	author = {Garcia, Julian and Hahn, Andrew and Zajac, Michal and Gong, Jiaqi},
	month = jun,
	year = {2025},
	note = {ISSN: 2832-2975},
	keywords = {Accuracy, Collaboration, Decision making, federated learning, knowledge graphs, Knowledge graphs, large language models, Medical diagnostic imaging, Privacy, privacy-preserving, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Scalability, Training},
	pages = {418--423},
}

@inproceedings{neha_exploring_2025,
	title = {Exploring {AI} {Text} {Generation}, {Retrieval}-{Augmented} {Generation}, and {Detection} {Technologies}: a {Comprehensive} {Overview}},
	doi = {10.1109/CCWC62904.2025.10903902},
	abstract = {The rapid development of Artificial Intelligence (AI) has led to the creation of powerful text generation models, such as large language models (LLMs), which are widely used for di-verse applications. However, concerns surrounding AI-generated content, including issues of originality, bias, misinformation, and accountability, have become increasingly prominent. This paper offers a comprehensive overview of AI text generators (AITGs), focusing on their evolution, capabilities, and ethical implications. This paper also introduces Retrieval-Augmented Generation (RAG), a recent approach that improves the contextual relevance and accuracy of text generation by integrating dynamic infor-mation retrieval. RAG addresses key limitations of traditional models, including their reliance on static knowledge and potential inaccuracies in handling real-world data. Additionally, the paper reviews detection tools that help differentiate AI-generated text from human-written content and discusses the ethical challenges these technologies pose. The paper explores future directions for improving detection accuracy, supporting ethical AI development, and increasing accessibility. The paper contributes to a more responsible and reliable use of AI in content creation through these discussions.},
	booktitle = {2025 {IEEE} 15th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Neha, Fnu and Bhati, Deepshikha and Shukla, Deepak Kumar and Guercio, Angela and Ward, Ben},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Artificial intelligence, Artificial Intelligence, Ethics, Fake news, Focusing, Generators, Large language models, Large Language Models (LLMs), Natural Language Un-derstanding, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Reviews, Text Detector, Text Generator},
	pages = {00633--00639},
}

@inproceedings{sung_new_2024,
	title = {A {New} {Pipeline} for {Generating} {Instruction} {Dataset} via {RAG} and {Self} {Fine}-{Tuning}},
	doi = {10.1109/COMPSAC61105.2024.00371},
	abstract = {With the rapid development of large language models (LLMs) in recent years, there has been an increasing demand for domain-specific Agents that can cater to the unique needs of enterprises and organizations. Unlike general models, which strive for broad coverage, these specialized Agents rely on focused datasets tailored to their intended applications. This research proposes a pipeline that leverages the power of LLMs and the Retrieval-Augmented Generation (RAG) related framework to construct high-quality instruction datasets for finetuning on specific domains using custom document collections. By ingesting domain-specific documents, the pipeline generates relevant and contextually appropriate instructions, thus effectively creating a comprehensive dataset for fine-tuning LLMs on the target domain. This approach overcomes the limitations of traditional dataset creation methods, which often rely on manual curation or web-scraping techniques that may introduce noise and irrelevant data. Notably, our pipeline offers a dynamic solution that can quickly adapt to updates or modifications in the domainspecific document collection, eliminating the need for complete retraining. Additionally, it addresses the challenge of data scarcity by enabling the generation of instruction datasets from a limited set of initial documents, rendering it suitable for unpopular or specialized domains where comprehensive datasets are scarce. As a case study, we apply this approach to the domain of psychiatry, a field requiring specialized knowledge and sensitive handling of patient information. The resulting fine-tuned LLM demonstrates showcases the viability of the proposed approach and underscores its potential for widespread adoption across various industries and domains where tailored, accurate, and contextually relevant language models are indispensable.},
	booktitle = {2024 {IEEE} 48th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Sung, Chih-Wei and Lee, Yu-Kai and Tsai, Yin-Te},
	month = jul,
	year = {2024},
	note = {ISSN: 2836-3795},
	keywords = {Accuracy, Computational modeling, Ethics, Instruction Tuning, Large Language Model, Organizations, Pipelines, Psychiatry, Retrieval-Augmented Generation, Technological innovation},
	pages = {2308--2312},
}

@inproceedings{s_graph_2025,
	title = {Graph {Based} {Retrieval}-{Augmented} {Generation} for {Personalized} {Dietary} {Guidance} with {LLMs}},
	doi = {10.1109/SCEECS64059.2025.10940424},
	abstract = {This paper presents a personalized diet recommendation system that leverages a graph database and retrieval-augmented generation (RAG) with large language models (LLMs) to provide accurate and tailored nutritional plans. The system focuses on vegetarian Indian dishes and analyzes individual health metrics such as physical activity levels, weight age and height to generate customized diet plans. A Neo4j graph database, built from scraped recipe data, stores dish names, ingredients, and nutritional values, enabling precise information retrieval via LLMs. Additionally, the system provides nutritional details and overall health assessments of the dishes. To enhance overall health, the system also suggests basic workout plans based on users' physical metrics and dietary recommendations. This approach aims to provide a low-cost, accessible alternative to traditional diet consultations while mitigating hallucinations in AI outputs through RAG and structured data.},
	booktitle = {2025 {IEEE} {International} {Students}' {Conference} on {Electrical}, {Electronics} and {Computer} {Science} ({SCEECS})},
	author = {S, Akilesh and Sekar, Rajeev and R, Tulasi Raman and R, Suganya},
	month = jan,
	year = {2025},
	note = {ISSN: 2688-0288},
	keywords = {Chatbots, Databases, Diet Recommendation, Focusing, GPTs, Healthcare, Information retrieval, Large language models, Large Language Models, Measurement, Medical services, Neo4j, NLP, OpenAI, Recommender systems, Retrieval augmented generation, Retrieval-Augmented Generation, Schedules},
	pages = {1--6},
}

@inproceedings{lin_domain_2024,
	title = {Domain {Adaption} and {Unified} {Knowledge} {Base} {Motivate} {Better} {Retrieval} {Models} in {Dialog} {Systems} {With} {RAG}},
	doi = {10.1109/SLT61566.2024.10832316},
	abstract = {Retrieval augmented generation (RAG) has emerged as a paradigm to address problems like hallucination in dialog systems based on large language model (LLM). Retrieval model is a key component in RAG framework for recalling relevant information. This paper describes our solution for FutureDial-RAG Challenge Track 1. We identify two primary challenges in this track: domain specificity and heterogeneity of knowledge base. To address the two challenges, we first adopt continual pre-training of a pre-trained retrieval model on both labeled and unlabeled data for domain adaption. Subsequently, we modify and expand the knowledge base, ensuring that each piece of knowledge is uniformly structured in a question-answer (QA) format. Finally, we construct negative samples based on the labeled data and the unified knowledge base, and fine-tune the retrieval model using contrastive learning. Our solution achieves a score of 2.023 on the dev set, which significantly outperforms the baseline.},
	booktitle = {2024 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Lin, Huadong and Chen, Yirong and Tao, Wenyu and Chen, Mingyu and Xu, Xiangmin and Xing, Xiaofen},
	month = dec,
	year = {2024},
	keywords = {Adaptation models, Conferences, Contrastive learning, Data models, Dialog System, Domain Adaption, Knowledge based systems, Large language models, RAG, Retrieval augmented generation, Unified Knowledge Base},
	pages = {1047--1052},
}

@inproceedings{xu_design_2025,
	title = {Design {Process} for {Retrieval} {Augmented} {Generation} {Systems}},
	doi = {10.1109/ICSA-C65153.2025.00072},
	abstract = {Generative AI (GenAI) refers to artificial intelligence systems capable of developing and creating new content such as text, images, audio, or videos based on learned patterns from existing data. Integrating GenAI deeply into more complex software systems poses challenges such as hallucination, out-dated knowledge, non-removable knowledge, and non-traceable reasoning process. Retrieval-Augmented Generation (RAG) has emerged as a promising solution for enabling GenAI systems to incorporate knowledge from external sources. However, there is a lack of systematic guidance on designing a RAG system and identifying critical decision points. This paper presents a design process for a RAG system, including a suitability analysis based on the requirements of specific scenarios and the inherent characteristics of RAG. It outlines the key steps to design a RAG system and discusses critical design decisions. Insights are provided on the impact of design alternatives on the quality attributes of GenAI systems and their tradeoffs. Finally, a feasibility study examines an LLM-based chatbot designed for Q\&A in taxation scenarios.},
	booktitle = {2025 {IEEE} 22nd {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Xu, Xiwei and Zhang, Dawen and Zhang, Wenjie and Lu, Qinghua and Zhu, Liming},
	month = mar,
	year = {2025},
	note = {ISSN: 2768-4288},
	keywords = {Chatbots, Cognition, Design Process, Financial industry, GenAI, Generative AI, RAG, Retrieval augmented generation, Software architecture, Software systems, Systematics, Trade-offs, Videos},
	pages = {482--487},
}

@inproceedings{omrani_hybrid_2024,
	title = {Hybrid {Retrieval}-{Augmented} {Generation} {Approach} for {LLMs} {Query} {Response} {Enhancement}},
	doi = {10.1109/ICWR61162.2024.10533345},
	abstract = {In the domain of Natural Language Processing (NLP), the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) represents a significant advancement towards enhancing the depth and relevance of model-generated responses. This paper introduces a novel hybrid RAG framework that synergizes the Sentence-Window and Parent-Child methodologies with an innovative re-ranking mechanism, aimed at optimizing the query response capabilities of LLMs. By leveraging external knowledge sources more effectively, the proposed method enriches LLM outputs with greater accuracy, relevance, and information fidelity. We subject our hybrid model to rigorous evaluation against benchmark datasets and metrics, demonstrating its superior performance over existing state-of-the-art RAG techniques. The results highlight our method’s enhanced ability to generate responses that are not only contextually appropriate but also demonstrate a high degree of faithfulness to the source material, thereby setting a new standard for query response enhancement in LLMs. Our study underscores the potential of hybrid RAG models in refining the interaction between LLMs and external knowledge, paving the way for future research in the field of NLP.},
	booktitle = {2024 10th {International} {Conference} on {Web} {Research} ({ICWR})},
	author = {Omrani, Pouria and Hosseini, Alireza and Hooshanfar, Kiana and Ebrahimian, Zahra and Toosi, Ramin and Ali Akhaee, Mohammad},
	month = apr,
	year = {2024},
	note = {ISSN: 2837-8296},
	keywords = {Benchmark testing, Generative AI, Hybrid power systems, Knowledge engineering, LLM, Measurement, Natural language processing, NLP, Refining, Retrieval Augmented Generation (RAG), Training},
	pages = {22--26},
}

@inproceedings{amarnath_intelligent_2024,
	title = {An {Intelligent} {Retrieval} {Augmented} {Generation} {Chatbot} for {Contextually}-{Aware} {Conversations} to {Guide} {High} {School} {Students}},
	doi = {10.1109/ICSES63445.2024.10762977},
	abstract = {The growth of Large Language Models (LLM) has showcased both their potential and limitations in domain- specific applications. A significant limitation is their tendency to confidently present false information, known as hallucination, which compromises the integrity of their outputs. Retrieval Augmented Generation (RAG) systems mitigate this by enhancing the Large Language Models' ability to retrieve accurate, source-specific knowledge. Many American students begin preparing for college applications in middle school, seeking guidance on planning their high school years to achieve personal goals. The diversity in students' interests, skills, and circumstances often results in an overwhelming amount of information and advice. This research introduces a novel RAG chatbot designed to provide personalized assistance to high school students using their course guide as the primary source document. By integrating a high school course guide with a RAG system, the chatbot delivers relevant suggestions, helping students navigate their educational paths with greater confidence and precision. This system aims to minimize the risk of hallucination by ensuring that responses are grounded in the accurate and relevant information contained within the course guide. Consequently, it addresses the crucial need for personalized educational planning, enabling students to make informed decisions that align with their unique aspirations and circumstances. This research highlights the transformative potential of RAG systems in educational contexts, offering a reliable tool to support students in their academic and personal growth.},
	booktitle = {2024 4th {International} {Conference} on {Sustainable} {Expert} {Systems} ({ICSES})},
	author = {Amarnath, Neeraj Singh and Nagarajan, Rajganesh},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Answer Relevancy, Chatbots, Context Precision, Context Recall, Context Retrieval Pipeline, Faithfulness, Fake news, Ground Truths, Hallucination, Large language models, Navigation, Oral communication, Planning, Real-time systems, Reliability, Retrieval Augmented Generation (RAG), Semantic Similarity Search, Transforms, Vectorstore},
	pages = {1393--1398},
}

@inproceedings{ju_task-classifying_2025,
	title = {Task-{Classifying} {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/ICAIBD64986.2025.11082008},
	abstract = {Hallucination phenomena are inevitably present in large language models (LLMs) for generation tasks. Retrieval Augmented Generation (RAG) technology alleviates this issue to some extent by incorporating external retrieved documents. However, existing Retrieval-Augmented Generation (RAG) methods still face limitations in terms of task adaptability and the richness of retrieval inputs. To address these issues, this paper proposes an innovative RAG framework that enhances both task classification and high-quality document retrieval, aiming to improve the accuracy and robustness of generated results. Specifically, we design a task identification module capable of automatically categorizing user inputs into three types of tasks: information retrieval, text generation, and general question answering. This classification then guides the subsequent processing flow. At the retrieval stage, we introduce a HyDE-inspired hypothetical document generation strategy, which constructs task-relevant hypothetical answers to improve the alignment between retrieved documents and user intent. To improve the quality of generation, this article introduces a task adaptive reflection and generation mechanism, implementing differentiated reflection processes and generation strategies for different task types, thereby improving the accuracy and completeness of the model’s response in multi task scenarios. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance across multiple natural language processing tasks, particularly excelling in complex question answering and open-domain information retrieval scenarios.},
	booktitle = {2025 8th {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	author = {Ju, Jingyuan and Zang, Peng and Zhong, Hao and Wang, Lingfeng},
	month = may,
	year = {2025},
	note = {ISSN: 2769-3554},
	keywords = {Accuracy, Adaptation models, HyDE-based Retrieval, Information retrieval, Large language models, Multitasking, Question answering (information retrieval), Reflection, Reliability theory, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Robustness, Task - classifying framework},
	pages = {122--127},
}

@inproceedings{lou_cognitive_2025,
	title = {A {Cognitive} {Digital} {Twin} for {Industry} 5.0 {Based} on a {Large} {Language} {Model} {Agent}},
	doi = {10.1109/ICHMS65439.2025.11154278},
	abstract = {While digital twins have made significant strides in creating digital replicas of physical manufacturing systems, their cognitive capabilities remain inadequate to address the dynamic complexities in manufacturing, including process variations, environmental fluctuations, and human interactions. These limitations hinder their applicability in Industry 5.0, which emphasizes Self-X cognitive capabilities. This work proposes an improved five-dimensional framework for developing a cognitive digital twin (CDT) that adopts a Large Language Model (LLM) agent at its core. The LLM agent enhances domain-specific tasksolving capabilities through retrieval-augmented generation (RAG) and in-context learning. RAG compensates for the general LLM's limitations by utilizing external tool libraries and industrial knowledge graphs to establish context awareness, retrieve domain-specific knowledge, and convert human commands into sequential task plans via function calls. Incontext learning further enables the LLM agent to learn specific tasks based on contextual examples without retraining. It empowers CDT to address domain-specific challenges with efficiency, flexibility, and cost-effectiveness. The effectiveness of the proposed CDT is demonstrated in a lab-scale manufacturing unit, highlighting its ability to perform valid task planning and handle dynamic incidents, paving the way for more resilient manufacturing systems aligned with Industry 5.0 objectives.},
	booktitle = {2025 {IEEE} 5th {International} {Conference} on {Human}-{Machine} {Systems} ({ICHMS})},
	author = {Lou, Shanhe and Tan, Runjia and Zhou, Mengchu and Lv, Chen},
	month = may,
	year = {2025},
	keywords = {Cognitive digital twin, Context awareness, Digital twins, Fifth Industrial Revolution, Industrial knowledge graph, Knowledge graphs, Large language model agent, Large language models, Manufacturing systems, Planning, Real-time systems, Retrieval augmented generation, Retrieval-augmented generation, Simultaneous localization and mapping, Task planning},
	pages = {223--228},
}

@inproceedings{shah_building_2024,
	title = {Building {Generative} {AI} {Chatbot} {Using} {Oracle} {Cloud} {Infrastructure}},
	doi = {10.1109/UEMCON62879.2024.10754774},
	abstract = {Artificial intelligence (AI) and Generative AI (GenAI) are revolutionizing technology and businesses at an extraordinary speed, providing numerous benefits that can position a company at the forefront of a swiftly changing market. The adoption of Generative AI will strengthen and enhance the role of chatbots in organizations and provide a more efficient and versatile user experience. Successfully integrating these technologies into enterprise systems requires a reliable, scalable, and secure infrastructure. Oracle Cloud Infrastructure (OCI) emerges as a powerful platform designed and crafted to facilitate the building and deployment of Generative AI applications. OCI’s comprehensive suite of AI and machine learning tools like OCI Generative AI service and its exceptional high-performance computing capabilities offers an optimal environment for developing cutting-edge Generative AI solutions. This paper explores the inner workings of Large Language Models (LLMs), OCI Generative AI service and focuses on building Chatbot using pre-trained and custom LLM models with the goal of allowing end users to gain insights seamlessly and efficiently. By presenting a comprehensive guide to leverage OCI for Generative AI, this paper aims to serve as a valuable resource for developers and organizations that are looking to harness cloud-based solutions for innovative AI-driven applications.},
	booktitle = {2024 {IEEE} 15th {Annual} {Ubiquitous} {Computing}, {Electronics} \& {Mobile} {Communication} {Conference} ({UEMCON})},
	author = {Shah, Jay Ashok and Iyer, Nisha Ramesh},
	month = oct,
	year = {2024},
	keywords = {Artificial Intelligence, Buildings, Chatbots, Generative AI, Large language models, Large Language Models, Machine learning, Mobile communication, OCI Generative AI Service, Oracle Cloud Infrastructure, Regulation, Reliability, Retrieval Augmented Generation, Security, User experience},
	pages = {79--84},
}

@inproceedings{barron_domain-specific_2024,
	title = {Domain-{Specific} {Retrieval}-{Augmented} {Generation} {Using} {Vector} {Stores}, {Knowledge} {Graphs}, and {Tensor} {Factorization}},
	doi = {10.1109/ICMLA61862.2024.00258},
	abstract = {Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.},
	booktitle = {2024 {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Barron, Ryan C. and Grantcharov, Vesselin and Wanna, Selma and Eren, Maksim E. and Bhattarai, Manish and Solovyev, Nicholas and Tompkins, George and Nicholas, Charles and Rasmussen, Kim Ø. and Matuszek, Cynthia and Alexandrov, Boian S.},
	month = dec,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Accuracy, Agents, Artificial Intelligence, Knowledge Graph, Knowledge graphs, Malware, Natural Language Processing, Non-Negative Tensor Factorization, Ontologies, Question answering (information retrieval), Reliability, Retrieval augmented generation, Retrieval Augmented Generation, Tensors, Topic Modeling, Tuning, Vectors},
	pages = {1669--1676},
}

@inproceedings{sundar_revolutionizing_2024,
	title = {Revolutionizing {Assessment}: {AI}-{Powered} {Evaluation} with {RAG} and {LLM} {Technologies}},
	doi = {10.1109/ICSSAS64001.2024.10760285},
	abstract = {The world of Artificial Intelligence is rapidly evolving after the introduction of GEN AI. Artificial Intelligence is being adopted in many fields and to automate complex works which frees up humans to something better. In the field of education, Artificial Intelligence is used in various ways, to tailor the content to meet the needs of individual students, one to one tutoring experience, as teaching assistant, in planning, content development. The scope AI and its subfield can be leveraged further, and it is possible to use it for exam assessment, use the data from Student Management System to generate some meaningful insight on Students behavior, discipline, attendance etc. Natural Language Processing, a subset of Artificial Intelligence can be combined with other AI subsets like Large Language Model to facilitate the Teacher, Parents and Students interaction, Parents can use chatBots to receive candid feedback of their kid’s performance, to generate relative performance improvement compared to previous cycle, suggestion for improvement. All it takes is to connect the Artificial Intelligence subsets with some reference system like Students database. This research analysis is to evaluate student’s performance and provide feedback factoring based on four main areas like exam answer sheet, practicals, attendance and assignments. This research provides technical implementation and guidance to implement the solution using the Retrieval Augmented Generation and Large Language Model, and an architectural overview of combining data from various input modes like student information from Student Management System, text book, digital answer sheet and embedding them in the Vector database.},
	booktitle = {2024 2nd {International} {Conference} on {Self} {Sustainable} {Artificial} {Intelligence} {Systems} ({ICSSAS})},
	author = {Sundar, Koushik and Manohar, Eashaan and Vijay, K and Prakash, Sajay},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Artificial intelligence, Assessment, Context modeling, Databases, Education, Exam evaluation, GEN Artificial Intelligence, Hybrid power systems, Large Language Model, Large language models, Planning, Reliability, Retrieval Augmented Generation, Student Management System (SMS), Vectors},
	pages = {43--48},
}

@inproceedings{sharma_dynamic_2024,
	title = {Dynamic {Retriever} {Selection} in {RAG} {Systems}: {An} {RL} {Approach} to {User}-{Centric} {NLP}},
	doi = {10.1109/ICECER62944.2024.10920369},
	abstract = {This paper investigates a novel use of Reinforcement Learning (RL) to dynamically choose the best retriever in Retrieval-Augmented Generation (RAG) systems with the goal of improving the performance of natural language processing tasks. RAG systems combine retrieval techniques with pre-trained language models to produce responses that are accurate within their context, but a static retriever selection system results in inefficiencies, in a dynamically changing environment where user preferences and the document corpus evolves. We attempt to solve this problem by proposing an RL-based solution to enable dynamic retriever selection based on document context and user feedback. The reinforcement learning agent is able to adjust to user preferences and a changing document corpus by utilizing Q-Learning. The methodology covers issue formulation, agent architecture and training procedures. Our experiments validate our RL-based approach's performance characterized by metrics like user satisfaction and response accuracy. The discussion highlights the strengths and limitations of the Reinforcement Learning approach and suggests future research directions. This experiment underlines the potential of adaptive mechanisms in NLP, showcasing RL's capability to revolutionize RAG applications by creating more responsive and user-tailored NLP systems. (Abstract)},
	booktitle = {2024 {International} {Conference} on {Electrical} and {Computer} {Engineering} {Researches} ({ICECER})},
	author = {Sharma, Parth and Mohammad, Aman Kaif},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Computer architecture, Context modeling, Costs, Engines, GenAI, LLM, Measurement, Natural language processing, Q-learning, RAG, Retrieval augmented generation, RL, Training},
	pages = {1--6},
}

@inproceedings{liu_transrag_2025,
	title = {{TransRAG}: {A} {Novel} {Hybrid} {Retrieval} {Augmented} {Generation} {Framework} for {Public} {Transportation} {Application}},
	doi = {10.1109/CCDC65474.2025.11090835},
	abstract = {Large Language Models (LLMs) has significant potential for applications in urban transportation management. However, it faces substantial challenges due to the lack of comprehensive and accurate domain-specific data and knowledge. In this paper, we propose a hybrid Retrieval Augmented Generation framework (TransRAG) specifically designed for the public transportation domain. The framework integrates embeddingbased retrieval with tree and graph structures to enhance both document indexing and question-answering performance. Experimental results on a public transportation dataset demonstrate that TransRAG outperforms existing RAG-based models in retrieval accuracy and user satisfaction, achieving an average improvement of 38.2\%. These findings indicate that TransRAG offers a promising solution for integrating LLMs into urban transportation management systems.},
	booktitle = {2025 37th {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Liu, Yiqian and Yin, Zexia and Wang, Jing and Li, Kang},
	month = may,
	year = {2025},
	note = {ISSN: 1948-9447},
	keywords = {Accuracy, Hybrid power systems, Indexing, Large Language Model, Large language models, Public transportation, Public Transportation, Question answering (information retrieval), Real-time systems, Retrieval augmented generation, Retrieval Augmented Generation, Schedules, Videos},
	pages = {66--71},
}

@inproceedings{pinna_ai-driven_2025,
	title = {{AI}-{Driven} {Differential} {Diagnosis}: {Leveraging} {RAG} and {LLMs} in {Intelligent} {Healthcare} {Systems}},
	doi = {10.1109/IE64880.2025.11130125},
	abstract = {Artificial Intelligence is playing an increasingly prominent role in healthcare. For this integration to continue evolving effectively, professionals in the field, including doctors, psychologists, and nurses, must trust the technology. In the domain of Large Language Models, this trust requires transparency in the sources of information, ensuring they are verifiable and reviewed. This paper presents an intelligent diagnostic assistant designed to support differential diagnosis and disease comparison, leveraging an LLM enriched with the Retrieval-Augmented Generation technique to address trust-related challenges. The knowledge base for our system is PubMed, a biomedical article aggregator, which supplies relevant articles on the considered disorders, as well as evidence supporting or refuting potential links between them. Retrieval-Augmented Generation empowers the model to incorporate external knowledge tailored to the specific question while providing citations for its statements. These citations not only enhance the trustworthiness of the answers but also enable practitioners to explore the referenced articles in greater detail. We developed a prototype of our system, and conducted an evaluation through questionnaires administered to 12 professional psychologists. Results show that our system compares favorably with state of the art Large Language Models in providing the relevant information to distinguish between two diseases, and that the provided details are useful for supporting differential diagnosis.},
	booktitle = {2025 21st {International} {Conference} on {Intelligent} {Environments} ({IE})},
	author = {Pinna, Simone and Massa, Silvia M. and Riboni, Daniele},
	month = jun,
	year = {2025},
	note = {ISSN: 2472-7571},
	keywords = {Biomedical Knowledge Integration, Differential diagnosis, Diseases, Human computer interaction, Human-Computer Interaction in Medicine, Knowledge based systems, Large language models, Medical diagnostic imaging, Medical services, Pervasive Healthcare, Prototypes, Psychology, Retrieval augmented generation, Retrieval-Augmented Generation, Trust and Transparency in AI},
	pages = {1--8},
}

@inproceedings{manuel_tampubolon_retrieval_2025,
	title = {Retrieval {Augmented} {Generation} with {Synergizing} {Reasoning} and {Acting} {Prompt} {Engineering} for {Indonesian} {Open}-{Domain} {Question} {Answering}},
	doi = {10.1109/ICoDSA67155.2025.11157319},
	abstract = {Large Language Models (LLMs) offer promising solutions for various downstream tasks. However, hallucination, limited reasoning capabilities, and high resource consumption remain significant challenges. Retrieval-Augmented Generation (RAG) addresses these issues by providing factual context from external sources. A notable application of RAG is in open-domain question answering, which can be implemented in multiple languages, including Bahasa Indonesia. This study investigates RAG in terms of both its retrieval and generation components. A grid search was conducted to compare sparse and dense retrieval methods. SahabatAI-Gemma-9B and its base model, Gemma-2-9B, are utilized as generative backbones. Additionally, the Synergizing Reasoning and Acting (ReAct) method is explored as a prompt engineering technique to enhance the reasoning capabilities of LLMs. The evaluation compares standard prompting with the ReAct approach. The experimental results show that BM25 is the most effective retriever among the tested methods. SahabatAI-Gemma-9B does not significantly outperform its base model. However, incorporating ReAct improves the recall of METEOR and BERTScore by approximately 2\% and enables the generation of more relevant answers through structured reasoning. ReAct facilitates a step-by-step reasoning process, allowing the model to locate specific entities more effectively and to determine when to answer or express uncertainty, thus enhancing control over its responses.},
	booktitle = {2025 {International} {Conference} on {Data} {Science} and {Its} {Applications} ({ICoDSA})},
	author = {Manuel Tampubolon, Andrew Lomaksan and Anggraini, Ratih Nur Esti and Hidayati, Shintami Chusnul},
	month = jul,
	year = {2025},
	keywords = {bahasa, Cognition, LLM, Measurement, Meteors, Process control, Prompt engineering, question answering, Question answering (information retrieval), ReAct, Retrieval augmented generation, retrieval-augmented generation, Social networking (online), Standards, Uncertainty},
	pages = {333--338},
}

@inproceedings{sonkar_dynamic_2025,
	title = {Dynamic {Query} {Handling} with {RAG} {Fusion} for {PDF}-{Based} {Knowledge} {Retrieval} {Systems}},
	doi = {10.1109/OTCON65728.2025.11070378},
	abstract = {This study explores advancements in Retrieval-Augmented Generation (RAG) systems tailored for PDF-based question answering. By leveraging domain-specific PDFs as input data, the proposed framework incorporates advanced retrieval techniques, including multi-query generation and RAG fusion, to enhance generated responses' relevance and contextual accuracy. Utilizing LangChain to orchestrate interactions between the language model and vector database (Chroma DB), the system effectively processes, embeds, and retrieves information from large-scale PDF collections. Chroma DB ensures efficient storage and similarity searches for vectorized document embeddings. The integration of advanced retrieval strategies, coupled with the dynamic query-handling capabilities of LangChain, significantly improves precision, reduces hallucinations, and enriches response quality for knowledge-intensive tasks. Preliminary results demonstrate the effectiveness of this approach in delivering timely and contextually accurate answers, showcasing its potential in addressing the challenges of traditional LLMs.},
	booktitle = {2025 4th {OPJU} {International} {Technology} {Conference} ({OTCON}) on {Smart} {Computing} for {Innovation} and {Advancement} in {Industry} 5.0},
	author = {Sonkar, Aakash and Singh, Sumeet Pratap and Sahu, Komendra and Sahu, Aayush and Mishra, Sachin},
	month = apr,
	year = {2025},
	keywords = {Accuracy, chatbot, Databases, generative ai, LangChain, LLMs, prompt template, Real-time systems, Retrieval augmented generation, Retrieval-Augmented generation (RAG), Scalability, stream lit, Streaming media, Technological innovation, Transformers, vector database, vector embedding, Vectors, Visualization},
	pages = {1--6},
}

@inproceedings{yang_ecosmartguide_2024,
	title = {{EcoSmartGuide}: {Language} {Learning} {Model} and {Retrieval}-{Augmented} {Generation}-{Based} {Platform} for {Streamlined} {Environmental}, {Social}, and {Governance} {Information} {Access} and {Report} {Generation}},
	doi = {10.1109/ECBIOS61468.2024.10885500},
	abstract = {EcoSmartGuide leverages the Language Learning Model (LLM) and Retrieval-Augmented Generation-based (RAG) architectures to streamline Environmental, Social, and Governance (ESG) reporting, automating data aggregation and analysis. It shows a citation accuracy of 95\% in reflecting ESG metrics and a coverage rate of 76.23\% in the representation of ESG performance. Stakeholder feedback indicates high user satisfaction and improved decision-making processes. In a case study, EcoSmartGuide's effectiveness is demonstrated in identifying overlooked risks and enhancing decisions. The platform showcases AI's transformative potential in sustainability reporting and decision-making efficiently and accurately. Future research is needed to prioritize longitudinal studies, technological advancements, and diverse industry applications to ensure broader applicability and relevance.},
	booktitle = {2024 {IEEE} 6th {Eurasia} {Conference} on {Biomedical} {Engineering}, {Healthcare} and {Sustainability} ({ECBIOS})},
	author = {Yang, Jiun-Yi and Chi, Ren-he and Wu, Chia-Chun and Chen, Li-Ju and Lin, Wei-Ming and Hu, Hsiang-Wei and Cheng, Hui-Ru},
	month = jun,
	year = {2024},
	keywords = {Accuracy, AI, Artificial intelligence, Decision making, ESG reporting, Industries, intelligence research, LLM, Measurement, Medical services, RAG, Stakeholders, Sustainable development, Technological innovation, Transforms},
	pages = {343--347},
}

@inproceedings{rui_chatcl_2025,
	title = {{ChaTCL}: {LLM}-{Based} {Multi}-{Agent} {RAG} {Framework} for {TCL} {Script} {Generation}},
	doi = {10.1109/ISEDA65950.2025.11101624},
	abstract = {Manually generating Tool Command Language (TCL) scripts is time-consuming and error-prone. Although large language models (LLMs) show promise in automating TCL script generation, they struggle with complex EDA tasks. They often fail to meet practical requirements and face compatibility issues across multiple tools. To address these limitations, we introduce ChaTCL, a multi-agent Retrieval-Augmented Generation (RAG) framework based on LLMs. Our contributions include: (1) creating fine-tuned datasets (NondomainT and DomainT) for TCL-specific LLMs, (2) designing a TCL-specific RAG framework with one-to- one retriever-database mapping to reduce hallucinations, and (3) implementing a multi-agent system that interprets user prompts and autonomously selects agents for script generation. Evaluated using TCLEval, ChaTCL outperforms models like ChatGPT-4.0- o and OpenAI o1-preview in generating accurate scripts across diverse tools and design stages.},
	booktitle = {2025 {International} {Symposium} of {Electronics} {Design} {Automation} ({ISEDA})},
	author = {Rui, Yibo and Li, Yuanhang and Wang, Rui and Chen, Ruiqi and Zhu, Yanxiang and Di, Zhixiong and Wang, Xi and Ling, Ming},
	month = may,
	year = {2025},
	keywords = {Accuracy, Command languages, Design automation, EDA, Faces, Large language models, LLM, multi-agent, Multi-agent systems, Numerical models, prompt, retrieval augmented generation, Retrieval augmented generation, TCL script},
	pages = {736--742},
}

@inproceedings{jain_mitigating_2025,
	title = {On {Mitigating} {Code} {LLM} {Hallucinations} with {API} {Documentation}},
	doi = {10.1109/ICSE-SEIP66354.2025.00027},
	abstract = {In this study, we address the issue of API hallucinations in various software engineering contexts. We introduce CloudAPIBench, a new benchmark designed to measure API hallucination occurrences. CloudAPIBench also provides annotations for frequencies of API occurrences in the public domain, allowing us to study API hallucinations at various frequency levels. Our findings reveal that Code LLMs struggle with low frequency APIs: for e.g., GPT-4o achieves only 38.58 \% valid low frequency API invocations. We demonstrate that Documentation Augmented Generation (DAG) significantly improves performance for low frequency APIs (increase to 47.94 \% with DAG) but negatively impacts high frequency APIs when using sub-optimal retrievers (a 39.02 \% absolute drop). To mitigate this, we propose to intelligently trigger DAG where we check against an API index or leverage Code LLMs' confidence scores to retrieve only when needed. We demonstrate that our proposed methods enhance the balance between low and high frequency API performance, resulting in more reliable API invocations (8.20\% absolute improvement on CloudAPIBench for GPT-4o).},
	booktitle = {2025 {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Jain, Nihal and Kwiatkowski, Robert and Ray, Baishakhi and Ramanathan, Murali Krishna and Kumar, Varun},
	month = apr,
	year = {2025},
	note = {ISSN: 2832-7659},
	keywords = {Annotations, API invocations, Benchmark testing, code generation, Codes, Documentation, Frequency-domain analysis, hallucinations, High frequency, Indexes, retrieval augmented generation, Retrieval augmented generation, Software engineering, Software reliability},
	pages = {237--248},
}

@inproceedings{hung_dark_2024,
	title = {Dark {Watchdog}: {A} {Novel} {RAG}-{Driven} {System} for {Real}-{Time} {Detection} and {Analysis} of {Data} {Leaks} on {Dark} {Web} {Forums}},
	doi = {10.1109/ACSACW65225.2024.00010},
	abstract = {Personal data breaches have become increasingly common, making the dark web a key marketplace for trading stolen information, often without the immediate awareness of affected organizations. To address this challenge, we introduce Dark Watchdog, a novel system that actively monitors dark web forums and employs a specially fine-tuned BERT classification model to categorize transaction posts into five distinct types of breaches with high accuracy. Dark Watchdog uniquely integrates retrieval-augmented generation (RAG) to efficiently vectorize and analyze dark web data, allowing cyber security analysts to access the latest intelligence on data leaks while preserving privacy by minimizing data exposure to large language models (LLMs). This approach not only improves detection precision but also optimizes computational resources by reducing token usage. Dark Watchdog offers an innovative and practical solution for real-time dark web monitoring, enabling timely insights into ongoing data leak incidents and enhancing the overall effectiveness of cyber security efforts.},
	booktitle = {2024 {Annual} {Computer} {Security} {Applications} {Conference} {Workshops} ({ACSAC} {Workshops})},
	author = {Hung, Shing-Li and Chen, Chung-Kuan and Furumoto, Keisuke and Takahashi, Takeshi and Sun, Hung-Min},
	month = dec,
	year = {2024},
	keywords = {BERT, Classification, Computer crime, Conferences, cyber security, Dark web, Dark Web, Data privacy, Large language model, Large language models, LLM, Monitoring, Organizations, Privacy, RAG, Real-time systems, Retrieval augmented generation, Retrieval-augmented generation, Tor},
	pages = {11--19},
}

@inproceedings{cai_2nd_2024,
	title = {The 2nd {Futuredial} {Challenge}: {Dialog} {Systems} {With} {Retrieval} {Augmented} {Generation} ({Futuredial}-{RAG})},
	doi = {10.1109/SLT61566.2024.10832299},
	abstract = {Recently, increasing research interests have focused on retrieval augmented generation (RAG) to mitigate hallucination for large language models (LLMs). Following this trend, we launch the FutureDial-RAG challenge at SLT 2024, which aims at promoting the study of RAG for dialog systems. The challenge builds upon the MobileCS2 dataset, a real-life customer service datasets with nearly 3000 high-quality dialogs containing annotations for knowledge base query and corresponding results. Over the dataset, we define two tasks, track 1 for knowledge retrieval and track 2 for response generation, which are core research questions in dialog systems with RAG. We build baseline systems for the two tracks and design metrics to measure whether the systems can perform accurate retrieval and generate informative and coherent response. The baseline results show that it is very challenging to perform well on the two tasks, which encourages the participating teams and the community to study how to make better use of RAG for real-life dialog systems.},
	booktitle = {2024 {IEEE} {Spoken} {Language} {Technology} {Workshop} ({SLT})},
	author = {Cai, Yucheng and Chen, Si and Wu, Yuxuan and Huang, Yi and Feng, Junlan and Ou, Zhijian},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Annotations, Atmospheric measurements, Conferences, Customer services, Dialog systems, Knowledge based systems, Large language models, Market research, Particle measurements, Retrieval augmented generation},
	pages = {1091--1098},
}

@inproceedings{k_comparative_2025,
	title = {Comparative {Analysis} of {Advanced} {RAG} {Techniques} using {Mahabharata}},
	doi = {10.1109/ICCRD64588.2025.10962837},
	abstract = {Retrieval Augmented Generation (RAG) is a hot topic and one of the most compute-effective context-extending techniques in the LLM industry. Unfortunately, RAG also faces certain challenges, which make it vulnerable to mistakes. To overcome these challenges, recent studies have come up with numerous methods, which are collectively referred to as advanced RAG. In our study, we test out the effectiveness of individual Advanced RAG methods and compare them when used with a variety of LLMs. We perform this comparative study using the Mahabharata, the Indian Epic, as the external knowledge base. Examining how Advanced RAG techniques can be used in practical applications is this paper’s primary contribution. We then test the efficacy of the techniques using answer-independent evaluation metrics. We conclude by tabulating the performance metrics recorded and provide insights on the results obtained.},
	booktitle = {2025 {IEEE} 17th {International} {Conference} on {Computer} {Research} and {Development} ({ICCRD})},
	author = {K, Ganesh Vaidyanathan and S, Varun M and Raolji, Shauryadeepsinh Gajendrasinh and Das, Bhaskarjyoti},
	month = jan,
	year = {2025},
	keywords = {Advanced RAG, Comparative Analysis, Computer architecture, Faces, Industries, Knowledge based systems, LLMs, Mahabharata, Measurement, Propulsion, Research and development, Retrieval augmented generation, Retrieval Augmented Generation, Routing, Semantics},
	pages = {216--227},
}

@inproceedings{hidayaturrahman_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Social} {Media} {Content} {Creation} with {Sentence} {Window} and {Auto}-{Merging} {Retrieval}},
	doi = {10.1109/ICIMCIS63449.2024.10956838},
	abstract = {This study investigates the efficacy of Retrieval-Augmented Generation (RAG) in generating content for social media platforms. It explores two innovative techniques: sentence window retrieval and auto-merging retrieval. Our study aims to enhance the relevance and credibility of generated content in dynamic and contextually rich environments. Through our extensive trials, we have found that incorporating phrase windows greatly enhances the contextual relevance and groundedness of the generated material. Increasing the size of sentence windows has a significant impact on both elements, but it does not consistently enhance the relevance of the answers. Alternatively, when it comes to auto-merging retrieval, adjusting the size of text chunks used in the process reveals that smaller chunk sizes tend to yield more precise and well-substantiated information. In addition, using a smaller set of chunk types leads to greater context relevance. These findings suggest that there exists a trade-off between the level of detail in the acquired information and its practicality within a specific context. This work is an important addition to the broader field of natural language generation and retrieval. It offers practical insights for efficiently implementing advanced AI models on social media platforms in real-world scenarios.},
	booktitle = {2024 {International} {Conference} on {Informatics}, {Multimedia}, {Cyber} and {Information} {System} ({ICIMCIS})},
	author = {{Hidayaturrahman} and Prawira, Indra},
	month = nov,
	year = {2024},
	note = {ISSN: 2837-5203},
	keywords = {Auto-Merging Retrieval, Informatics, LLM, Multimedia systems, Natural language generation, RAG, Retrieval augmented generation, Sentence Window Retrieval, Social networking (online)},
	pages = {608--613},
}

@inproceedings{fayyazi_advancing_2024,
	title = {Advancing {TTP} {Analysis}: {Harnessing} the {Power} of {Large} {Language} {Models} with {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/ACSACW65225.2024.00036},
	abstract = {Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT\&CK framework can be challenging for cybersecurity practitioners due to presumed expertise and complex dependencies. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. It is, however, unclear how LLMs can be used in an efficient and proper way to provide accurate responses for critical domains such as cybersecurity. This leads us to investigate how to better use two types of LLMs: small-scale encoder-only (e.g., RoBERTa) and large-scale decoder-only (e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended purposes (i.e., tactics) of a cyberattack procedure. This work studies and compares the uses of Supervised Fine-Tuning (SFT) of encoder-only LLMs vs. Retrieval Augmented Generation (RAG) for decoder-only LLMs (without fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with relevant contexts for each cyberattack procedure. Our studies show decoder-only LLMs with RAG achieves better performance than encoder-only models with SFT, particularly when directly relevant context is extracted by RAG. The decoder-only results could suffer low ‘Precision’ while achieving high ‘Recall’, indicating the hallucinations typically occur during decoding phase. Our findings further highlight a counter-intuitive observation that more generic prompts tend to yield better predictions of cyberattack tactics than those that are more specifically tailored. 1},
	booktitle = {2024 {Annual} {Computer} {Security} {Applications} {Conference} {Workshops} ({ACSAC} {Workshops})},
	author = {Fayyazi, Reza and Taghdimi, Rozhina and Yang, Shanchieh Jay},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Conferences, Context modeling, Cyberattack, Decoding, Large language models, LLM, MITRE ATT\&CK, RAG, Retrieval augmented generation, Surges, TTP},
	pages = {255--261},
}

@inproceedings{azher_futuregen_2025,
	title = {{FutureGen}: {A} {RAG}-based {Approach} to {Generate} the {Future} {Work} of {Scientific} {Article}},
	doi = {10.1109/eScience65000.2025.00087},
	abstract = {The Future Work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from a scientific article. To enrich the generation process with broader insights and reduce the chance of missing important research directions, we use context from related papers using RAG. We experimented with various Large Language Models (LLMs) integrated into Retrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism to enhance the quality of the generated content and introduce an LLM-as-a-judge framework for robust evaluation, assessing key aspects such as novelty, hallucination, and feasibility. Our results demonstrate that the RAG-based approach using GPT-4o mini, combined with an LLM feedback mechanism, outperforms other methods based on both qualitative and quantitative evaluations. Moreover, we conduct a human evaluation to assess the LLM as an extractor, generator, and feedback provider.},
	booktitle = {2025 {IEEE} {International} {Conference} on {eScience} ({eScience})},
	author = {Azher, Ibrahim Al and Jannat Mokarrama, Miftahul and Guo, Zhishuai and Choudhury, Sagnik Ray and Alhoori, Hamed},
	month = sep,
	year = {2025},
	note = {ISSN: 2325-3703},
	keywords = {Collaboration, Databases, Future Work, Generators, Large language models, LLM, LLM as a Judge, Measurement, Retrieval augmented generation, Retrieval Augmented Generation, Text Generation, Vectors},
	pages = {427--438},
}

@inproceedings{tamascelli_leveraging_2025,
	title = {Leveraging {Large} {Language} {Models} for {Robust} {Maintenance} {Rule} {Extraction} in {Industrial} {Settings}},
	doi = {10.1109/ETFA65518.2025.11205682},
	abstract = {Maintenance of medium voltage switchgears is a complex task that requires extensive expertise. However, a substantial portion of this knowledge is often difficult for the next generation of field technicians to access and apply effectively. For instance, maintenance reports – which document system issues and corrective actions – represent a valuable knowledge source yet frequently underutilized due to their unstructured and sparse nature. In this context, recent advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques have been explored to analyze historical maintenance reports and provide real-time guidance to field technicians. However, chatbot-based interactions remain constrained by the stochastic nature of responses, challenges related to domain and industry specificity, and risks of hallucination. This study introduces a novel approach that leverages LLMs to systematically extract structured, machine-readable rules – i.e., sequences of actions used to resolve specific issues – directly from maintenance reports. A human-in-the-loop approach is proposed, where a human expert validates each extracted rule, mitigating inaccuracies and enhancing quality control. These validated rules are eventually stored in a dedicated rule corpus and later deployed in a deterministic rule engine, ensuring reliability and consistency. An industrial case study evaluates the effectiveness of this approach by comparing LLM-generated rules with those manually curated by domain experts. By shifting from conventional chatbot-like interaction to a more human-centric structured rule extraction, our methodology enhances reproducibility, reliability, and accessibility in industrial maintenance knowledge management. This approach addresses hallucinations and increases robustness of LLM applications in industrial AI settings.},
	booktitle = {2025 {IEEE} 30th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Tamascelli, Nicola and Bhattacharya, Nilavra and Song, Chen and Borrison, Reuben and Gitzel, Ralf},
	month = sep,
	year = {2025},
	note = {ISSN: 1946-0759},
	keywords = {Engines, Human in the loop, knowledge extraction, Large Language Model, Large language models, LLM, Maintenance, maintenance reports, maintenance rule extraction, operator support, RAG, Real-time systems, Reproducibility of results, Retrieval augmented generation, Retrieval Augmented Generation, Robustness, Stochastic processes, Switches},
	pages = {1--7},
}

@article{al-qatf_rag4ds_2025,
	title = {{RAG4DS}: {Retrieval}-{Augmented} {Generation} for {Data} {Spaces}—{A} {Unified} {Lifecycle}, {Challenges}, and {Opportunities}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3545387},
	abstract = {Retrieval-Augmented Generation (RAG) has gained significant attention from many researchers as an effective solution to address the hallucination issue of Foundational Models (FMs), particularly Large Language Models (LLMs). Although the RAG framework is considered a successful approach for enhancing LLMs by providing a suitable retrieval mechanism to obtain appropriate external knowledge, it still has limitations in acquiring high-quality knowledge from diverse data sources. The complementary integration of RAG and data spaces is proposed to exploit RAG’s capabilities within data spaces. Data spaces provide RAG with the ability to obtain diverse and high quality data sources from several data providers under secure data-sharing mechanisms and direct data exchange negotiations. At the same time, RAG enhances the support services of data spaces. In this paper, we present a high-level architecture for RAG data space models (RAG-DSMs) with a unified lifecycle for RAG and data spaces, highlight the possible challenges of the proposed integration while presenting potential opportunities. Moreover, we present two use cases for leveraging RAG-DSMs in the mobility and health domains.},
	journal = {IEEE Access},
	author = {Al-Qatf, Majjed and Haque, Rafiqul and Alsamhi, Saeed Hamood and Buosi, Samuele and Razzaq, Muhammad Asif and Timilsina, Mohan and Hawbani, Ammar and Curry, Edward},
	year = {2025},
	keywords = {Artificial intelligence, Data models, Data privacy, data spaces, Distributed databases, Foundation models, foundational models, Frequency modulation, lifecycle, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Soft sensors, Sustainable development, Technological innovation},
	pages = {39510--39522},
}

@article{wahidur_legal_2025,
	title = {Legal {Query} {RAG}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3542125},
	abstract = {Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence (AI) for various core tasks. However, these technologies remain in their early stages and face challenges such as understanding complex legal reasoning, managing biased data, ensuring transparency, and avoiding misleading responses, commonly referred to as hallucinations. To address these limitations, this paper introduces Legal Query RAG (LQ-RAG), a novel Retrieval-Augmented Generation framework with a recursive feedback mechanism specifically designed to overcome the critical shortcomings of standard RAG implementations in legal applications. The proposed framework incorporates four key components: a custom evaluation agent, a specialized response generation model, a prompt engineering agent, and a fine-tuned legal embedding LLM. Together, these components effectively minimize hallucinations, improve domain-specific accuracy, and deliver precise, high-quality responses for complex queries. Experimental results demonstrate that the fine-tuned embedding LLM achieves a 13\% improvement in Hit Rate and a 15\% improvement in Mean Reciprocal Rank (MRR). Comparisons with general LLMs reveal a 24\% performance gain when using the Hybrid Fine-Tuned Generative LLM (HFM), the specialized response generation model integrated into the LQ-RAG framework. Furthermore, LQ-RAG achieves a 23\% improvement in relevance score over naive configurations and a 14\% improvement over RAG with Fine-Tuned LLMs (FTM). These findings underscore the potential of domain-specific fine-tuned LLMs, combined with advanced RAG modules and feedback mechanisms, to significantly enhance the reliability and performance of AI in legal practice. The reliance of this study on a proprietary model as the evaluation agent, combined with the lack of feedback from human experts, highlights the need for improvement. Future efforts should focus on developing a specialized legal evaluation agent and enhancing its performance by incorporating feedback from domain experts.},
	journal = {IEEE Access},
	author = {Wahidur, Rahman S. M. and Kim, Sumin and Choi, Haeung and Bhatti, David S. and Lee, Heung-No},
	year = {2025},
	keywords = {Accuracy, Adaptation models, Hybrid power systems, information retrieval, Law, legal query, LLM agent, Mathematical models, Reliability, Retrieval augmented generation, Retrieval-augmented generation, Semantics, Training, Tuning},
	pages = {36978--36994},
}

@inproceedings{mikulic_integrating_2025,
	title = {Integrating {External} {Knowledge} with {LLMs}: {A} {Systematic} {Review} of {RAG} {Approaches}},
	doi = {10.1109/MIPRO65660.2025.11131735},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, exhibiting exceptional in-context learning abilities without requiring extensive fine-tuning. However, these models often suffer from significant limitations, including hallucinations where fabricated or incorrect information is presented as fact, and the need for constant retraining in order to effectively integrate new knowledge. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these challenges by combining LLMs with information retrieval (IR) techniques. By leveraging external knowledge bases, RAG pipelines retrieve and incorporate relevant information dynamically, enhancing the factual accuracy and adaptability of LLMs. This paper provides an overview of the state-of-the-art methods for implementing RAG in LLMs, examining key components of the RAG pipeline, including data preparation, retrieval, reranking, and post-retrieval techniques. We aim to highlight how these components collectively enable LLMs to achieve greater reliability and flexibility, paving the way for more robust AI applications.},
	booktitle = {2025 {MIPRO} 48th {ICT} and {Electronics} {Convention}},
	author = {Mikulić, Ivan and Vlaić, Marin and Delač, Goran and Šilić, Marin and Vladimir, Klemo},
	month = jun,
	year = {2025},
	note = {ISSN: 1847-3938},
	keywords = {Accuracy, IR, Knowledge based systems, Large language models, Libraries, LLM, Natural language processing, Pipelines, RAG, Reliability, Retrieval, Retrieval augmented generation, Systematic literature review, Training},
	pages = {93--98},
}

@inproceedings{tevissen_towards_2024,
	title = {Towards {Retrieval} {Augmented} {Generation} over {Large} {Video} {Libraries}},
	doi = {10.1109/HSI61632.2024.10613524},
	abstract = {Video content creators need efficient tools to repurpose content, a task that often requires complex manual or automated searches. Crafting a new video from large video libraries remains a challenge. In this paper we introduce the task of Video Library Question Answering (VLQA) through an interoperable architecture that applies Retrieval Augmented Generation (RAG) to video libraries. We propose a system that uses large language models (LLMs) to generate search queries, retrieving relevant video moments indexed by speech and visual metadata. An answer generation module then integrates user queries with this metadata to produce responses with specific video timestamps. This approach shows promise in multimedia content retrieval, and AI-assisted video content creation.},
	booktitle = {2024 16th {International} {Conference} on {Human} {System} {Interaction} ({HSI})},
	author = {Tevissen, Yannis and Guetari, Khalil and Petitpont, Frédéric},
	month = jul,
	year = {2024},
	note = {ISSN: 2158-2254},
	keywords = {Benchmark testing, Large language models, Manuals, Metadata, Multimedia systems, retrieval augmented generation, Streaming media, video library question answering, video retrieval, Visualization},
	pages = {1--4},
}

@inproceedings{ahmed_codeqa_2024,
	title = {{CodeQA}: {Advanced} {Programming} {Question}-{Answering} {Using} {LLM} {Agent} and {RAG}},
	doi = {10.1109/NILES63360.2024.10753267},
	abstract = {In light of this complex information for programming environments, this paper explores how effectiveness in Large Language Models and concepts from Retrieval-Augmented Generation can be used to augment reduced hallucinated question-answering systems for programming environments. The present scenario of transformer-based models, though a big player in natural language processing, doesn't seem to have adaptability and context sensitivity, which is, in fact, a very important feature for a specialist domain. Our study, by integrating a sophisticated LLM model within the RAG framework, further improves the precision, effectiveness of retrieval, and sensitivity of the context of responses. The model based on LLM improves over this transformer-based model by scoring higher in accuracy and context awareness, supported by its pretraining on a massive corpus and dynamic document retrieval functionality. This result underlines the possibility of great improvement in QAS performance, handling very complex and specialized queries, through the integration of LLMs with the RAG systems, and thereby helps in indicating promising areas for further research in optimizing these methodologies across different domains.},
	booktitle = {2024 6th {Novel} {Intelligent} and {Leading} {Emerging} {Sciences} {Conference} ({NILES})},
	author = {Ahmed, Mohamed and Dorrah, Mostafa and Ashraf, Ahmed and Adel, Yousef and Elatrozy, Abdelrahman and Mohamed, Bahaa Eldin and Gomaa, Wael},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Adaptation models, Context awareness, Context modeling, Large language models, LLM, Natural language processing, Programming, Programming environments, Python, QAS, RAG, Sensitivity, Transformers},
	pages = {494--499},
}

@inproceedings{swapnil_automation_2024,
	title = {From {Automation} to {Innovation}: {Harnessing} {The} {Synergy} of {Generative} {AI} and {Retrieval}-{Augmented} {Generation} {For} {Citizen}-centric {Governance}},
	doi = {10.23919/ITUK62727.2024.10772932},
	abstract = {Presented study explores the transformative potential of Generative Artificial Intelligence (AI) and RetrievalAugmented Generation (RAG) in India’s e-governance ecosystem. Employing a multi-faceted analytical approach, the research uncovers insights into AI’s growth dynamics, perceptions, and impact on service delivery and citizen engagement. The findings emphasize the need for robust security measures, ethical guidelines, and context-specific deployment strategies. By presenting a framework for sustainable AI-driven knowledge management practices, the study offers actionable recommendations to policymakers and practitioners, fostering public trust and paving the way for an efficient, citizen-centric governance model in India. This innovative research contributes significantly to the growing body of knowledge on AI-driven governance, positioning India at the forefront of this transformative journey.},
	booktitle = {2024 {ITU} {Kaleidoscope}: {Innovation} and {Digital} {Transformation} for a {Sustainable} {World} ({ITU} {K})},
	author = {Swapnil, Morande and Garima, Sogani and Shashank, Shah},
	month = oct,
	year = {2024},
	keywords = {Artificial intelligence, Citizen-Centric Governance, E-Governance, Ecosystems, Ethics, Generative AI, Government, ITU, Predictive models, Retrieval-Augmented Generation, Security, Standards, Technological innovation},
	pages = {1--8},
}

@inproceedings{irtiza_llm-sentry_2024,
	title = {{LLM}-{Sentry}: {A} {Model}-{Agnostic} {Human}-in-the-{Loop} {Framework} for {Securing} {Large} {Language} {Models}},
	doi = {10.1109/TPS-ISA62245.2024.00036},
	abstract = {LLM-Sentry represents a novel black-box defense strategy to safeguard Large Language Models (LLMs) against jailbreaking attacks. A key advantage of our approach is its model-agnostic nature, as it does not rely on specific information about the model’s architecture or parameters, thereby enabling its application to any commercial or open-source language models. Additionally, LLM-Sentry does not require retraining when new jailbreak attacks are discovered; a simple update to the knowledge base equips LLM-Sentry to defend against new threats. The widespread adoption of LLMs is attributed to their high-quality responses and user-friendly nature. However, these models are susceptible to manipulation by malicious actors exploiting vulnerabilities to generate harmful or compromised content. Recent research has identified various jailbreaking methods that exploit vulnerabilities in LLM security measures. Given the increasing complexity of jailbreaking techniques and the ambiguous nature of LLM safeguards, it is imperative to develop unique defense strategies that can seamlessly integrate into existing security frameworks and make them robust.Our work comprehensively analyzes various commercial LLMs to assess their vulnerability to sophisticated, multilingual jailbreaking prompts. We propose a defensive approach that combines a Zero-shot language classifier with the Retrieval Augmented Generation (RAG) technique to screen and filter potentially harmful input prompts before they are processed by the language model for response generation. We adopt a human-in-the-loop approach to gather a dataset comprising harmful and safe prompts, which serves as a knowledge base for the RAG retriever module to extract relevant context. Our investigation includes successful jailbreaking attempts on prominent commercial LLMs like Gemini, Mistral 7B, and ChatGPT, wherein we successfully bypass existing security measures and elicit compromised responses. We conduct a thorough evaluation of our approach against various baseline methods to validate its resilience and superiority against such attacks empirically. Our approach achieves an attack detection accuracy of 97\%, surpassing all other methods in our comparative analysis.},
	booktitle = {2024 {IEEE} 6th {International} {Conference} on {Trust}, {Privacy} and {Security} in {Intelligent} {Systems}, and {Applications} ({TPS}-{ISA})},
	author = {Irtiza, Saquib and Akbar, Khandakar Ashrafi and Yasmeen, Arowa and Khan, Latifur and Daescu, Ovidiu and Thuraisingham, Bhavani},
	month = oct,
	year = {2024},
	keywords = {Defense, Gemini, GPT, Human in the loop, Human-In-The-Loop, Intelligent systems, Jailbreaking, Knowledge based systems, Large Language Model, Large language models, Mistral, Model Agnostic, Multilingual, Privacy, Resilience, Retrieval augmented generation, Retrieval Augmented Generation, Robustness, Security, Zero-Shot Classification},
	pages = {245--254},
}

@inproceedings{varshney_ragnet_2025,
	title = {{RAGNet}: {Enhancing} {Conversational} {AI} in {Medical} {Domain}},
	doi = {10.1109/ICETM63734.2025.11051911},
	abstract = {An Enhanced version of the RAG framework is proposed to improve the Groundedness of Medical Question Answering systems. It achieves higher retrieval effectiveness and response generation due to advanced techniques, such as few-shot prompting, multi-hop retrieval, and query optimization. In this study, the comparison models used are the Naive and Advanced RAG models for large language models on medQA dataset. Application in the field of practice would highlight this topic in medical applications where precision matters and the Enhanced Modular RAG provided one of the most reliable answers to challenging medical questions.},
	booktitle = {2025 {International} {Conference} on {Engineering}, {Technology} \& {Management} ({ICETM})},
	author = {Varshney, Nihir and Mantoo, Saksham and Wadhwa, Madhav and Aatif, Mohammad and Bhardwaj, Shweta},
	month = may,
	year = {2025},
	keywords = {Accuracy, Advanced RAG, Enhanced Modular RAG, Ensemble, Few-Shot Prompting, Large language models, Large Language Models (LLMs), Medical services, Modular RAG, Multi-Hop Retrieval, Naïve RAG, Optimization, Query processing, Question answering (information retrieval), Reliability, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Technological innovation, Training data},
	pages = {1--7},
}

@inproceedings{li_generating_2025,
	title = {Generating {Is} {Believing}: {Membership} {Inference} {Attacks} against {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICASSP49660.2025.10889013},
	abstract = {Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG’s external database, with the aim of determining whether a given sample is part of the RAG’s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S2MIA, a Membership Inference Attack that utilizes the Semantic Similarity between a given sample and the content generated by the RAG system. With our proposed S2MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experimental results demonstrate that S2MIA outperforms five existing MIAs, even when the system is protected by three representative defenses.},
	booktitle = {{ICASSP} 2025 - 2025 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang},
	month = apr,
	year = {2025},
	note = {ISSN: 2379-190X},
	keywords = {Data privacy, Databases, Focusing, Large language models, Large Language Models, Membership Inference Attacks, Privacy, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Signal processing, Solids, Speech processing},
	pages = {1--5},
}

@inproceedings{kumari_innovative_2024,
	title = {Innovative {Corporate} {Query} {Handling} with {Domain}-{Specific} {RAG}},
	doi = {10.1109/OCIT65031.2024.00058},
	abstract = {Large Language Models (LLMs) have significantly impacted Natural Language Processing, but face limitations in domain-specific query response generation. To overcome these issues, this work proposes a novel method that combines LLMs with Retrieval Augmented Generation (RAG). We have designed a hybrid model to improve the accuracy of responses to domain-specific questions by integrating data from minutes of meetings from an external knowledge store. It involves preprocessing meeting data, developing a retrieval mechanism using similarity search, and implementing a generation model based on the LLaMA-2 13B architecture. We evaluated our model using BLEU and ROUGE scores, demonstrating improved performance in closed-domain question answering. The BLEU score of 0.56037 and ROUGE scores (ROUGE-1: 0.7211, ROUGE-2: 0.6217, ROUGE-L: 0.7211) indicate the model's proficiency in generating accurate responses while reducing hallucinations.},
	booktitle = {2024 {OITS} {International} {Conference} on {Information} {Technology} ({OCIT})},
	author = {Kumari, Simran and Chakraborty, Udit Kr. and Nath, Basab and Kumar, Avinash and Srivastava, Pratham and Vivek, Namsani},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Closed Domain, Data models, Faces, Generators, GPT3.5, Hybrid power systems, Information technology, Large language models, LLM, Measurement, Question answering (information retrieval), RAG, Retrieval augmented generation},
	pages = {291--296},
}

@inproceedings{sanjani_performance_2025,
	title = {Performance {Analysis} of {LLM} {Models} with {RAG} and {Fine}-{Tuning} {T5} for {Chatbot} {Optimization} in {Call} {Centers}},
	doi = {10.1109/ICoCSETI63724.2025.11018908},
	abstract = {This research investigates the comparative performance of various Large Language Models (LLMs) to determine the most suitable model for XYZ Company, a call center organization aiming to integrate a chatbot solution. Chatbots significantly improve call center operations by streamlining interactions, reducing the workload on human agents, and enhancing the overall customer experience. Unlike many chatbot datasets that are document-based or contain contextual fields, the dataset of XYZ Company is composed of question-answer pairs. This unique data structure requires a customized approach to model selection. Experiments were conducted on multiple models, with RAG using Llama3 emerging as the top performer. Results indicate a BLEU score of 18\%, ROUGE-1 of 46\%, ROUGE-2 of 27\%, ROUGE-L of 36\%, BERTScore Precision of 89\%, Recall of 91\%, F1 of 90\%, and METEOR score of 41\%. These metrics underscore RAG with Llama3 to deliver accurate, efficient responses, supporting the goal of XYZ Company to implement an effective, real-world chatbot solution.},
	booktitle = {2025 {International} {Conference} on {Computer} {Sciences}, {Engineering}, and {Technology} {Innovation} ({ICoCSETI})},
	author = {Sanjani, Lukman Arif and Sarno, Riyanarto and Sungkono, Kelly Rossa and Haryono, Agus Tri and Septiyanto, Abdullah Faqih and Sunaryono, Dwi},
	month = jan,
	year = {2025},
	keywords = {Adaptation models, Call Center, Chatbot, Chatbots, Companies, Fine-tuning, Large Language Model, Large language models, Medical services, Meteors, Optimization, Performance analysis, Retrieval augmented generation, Retrieval-Augmented Generation, Technological innovation},
	pages = {152--157},
}

@inproceedings{sun_scrag_2025,
	title = {{SCRAG}: {Social} {Computing}-{Based} {Retrieval} {Augmented} {Generation} for {Community} {Response} {Forecasting} in {Social} {Media} {Environments}},
	doi = {10.1109/SMARTCOMP65954.2025.00062},
	abstract = {This paper introduces SCRAG, a prediction frame-work inspired by social computing, designed to forecast community responses to real or hypothetical social media posts. SCRAG can be used by public relations specialists (e.g., to craft messaging in ways that avoid unintended misinterpretations) or public figures and influencers (e.g., to anticipate social responses), among other applications related to public sentiment prediction, crisis management, and social what-if analysis. While large language models (LLMs) have achieved remarkable success in generating coherent and contextually rich text, their reliance on static training data and susceptibility to hallucinations limit their effectiveness at response forecasting in dynamic social media environments. SCRAG overcomes these challenges by integrating LLMs with a Retrieval-Augmented Generation (RAG) technique rooted in social computing. Specifically, our framework retrieves (i) historical responses from the target community to capture their ideological, semantic, and emotional makeup, and (ii) external knowledge from sources such as news articles to inject time-sensitive context. This information is then jointly used to forecast the responses of the target community to new posts or narratives. Extensive experiments across six scenarios on the X platform (formerly Twitter), tested with various embedding models and LLMs, demonstrate over 10\% improvements on average in key evaluation metrics. A concrete example further shows its effectiveness in capturing diverse ideologies and nuances. Our work provides a social computing tool for applications where accurate and concrete insights into community responses are crucial.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Sun, Dachun and Lyu, You and Li, Jinning and Chen, Yizhuo and Wang, Tianshi and Kimura, Tomoyoshi and Abdelzaher, Tarek},
	month = jun,
	year = {2025},
	note = {ISSN: 2693-8340},
	keywords = {Accuracy, Computational modeling, Forecasting, Ideological Embedding, Large Language Model, Large language models, Retrieval augmented generation, Retrieval-Augmented Generation, Robustness, Semantics, Social computing, Social Computing, Social Media Response Forecasting, Social networking (online), Social Networks, Training data},
	pages = {170--177},
}

@inproceedings{jeon_rag-based_2024,
	title = {{RAG}-based {Cyber} {Threat} {Tracing} {Graph} {Modeling} {Method}},
	doi = {10.1109/TrustCom63139.2024.00098},
	abstract = {Cyber-attacks have become increasingly complex and sophisticated, occurring frequently through various paths. Numerous attack techniques, such as ransomware, advanced persistent threat (APT) attacks, and viruses facilitated by generative artificial intelligence (GAI), are threatening systems. Attackers often remain hidden for extended periods, using networks to infiltrate and spread within internal systems, aiming to steal sensitive information. These stealthy lateral movements are difficult to detect and defend against, necessitating a graph-based approach to effectively track and analyze threats. In this paper, we propose a retrieval-augmented generation (RAG) based threat behavior tracing method that derives more meaningful results through the information retrieval functions in the large language model (LLM). We present a suitable graph model for threat tracing by comparing label property graph (LPG) and resource description framework (RDF) as graph methods for knowledge extraction in RAG.},
	booktitle = {2024 {IEEE} 23rd {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Jeon, Jong-Hee and Koo, Jahoon and Kim, Young-Gab},
	month = dec,
	year = {2024},
	note = {ISSN: 2324-9013},
	keywords = {Computational modeling, cyber threat tracing, graph modeling, Information retrieval, Interoperability, label property graph (LPG), Large language models, Privacy, Ransomware, Resource description framework, resource description framework (RDF), Retrieval augmented generation, retrieval-augmented generation (RAG), Standardization, Tracking},
	pages = {608--615},
}

@inproceedings{ananthanarayanan_generating_2025,
	title = {Generating {Medical} {Diagnostic} {Scenarios} with {LLM}-{Based} {Reinforcement} {Learning} {Feedback}: {Dataset} {Release} and {Methodology}},
	doi = {10.1109/ISEC64801.2025.11147251},
	abstract = {Sample medical scenarios play a crucial role in training healthcare professionals by providing structured cases to develop diagnostic reasoning and clinical decision-making skills. However, access to diverse and inclusive sample diagnostic cases remains challenging due to the limited representation of specific conditions and populations in medical education materials, and existing cases are often not equitable due to a lack of representation of minority groups. In this paper, we present a new dataset of medical diagnostic scenarios generated using a combination of reinforcement learning from artificial intelligence feedback and retrieval augment generation techniques. Despite the dataset’s limited size, it offers a unique resource for advancing medical education, particularly in regions with scarce training materials while also emphasizing inclusivity by incorporating a higher representation of people of color and women. Then, we discuss the data generation process, the dataset structure, and potential applications in medical training programs. This work aims to contribute to the development of accessible, high-quality, and inclusive educational tools in the medical field.},
	booktitle = {2025 {IEEE} {Integrated} {STEM} {Education} {Conference} ({ISEC})},
	author = {Ananthanarayanan, Aniruth},
	month = mar,
	year = {2025},
	note = {ISSN: 2473-7623},
	keywords = {Ethics, Image color analysis, large language models, Large language models, Medical diagnosis, Medical diagnostic imaging, Medical education, Refining, Reinforcement learning, reinforcement learning from AI feedback, Retrieval augmented generation, retrieval-augmented generation, Training, Transforms},
	pages = {1--5},
}

@inproceedings{jiang_exploring_2025,
	title = {Exploring {Large} {Language} {Models} for {Text}-to-{SQL} {Error} {Correction} with {LoRA} {Fine}-{Tuning}},
	doi = {10.1109/ICAACE65325.2025.11018984},
	abstract = {The advent of Large Language Models (LLMs) has significantly advanced Text-to-SQL tasks in recent years. However, these models still face considerable challenges when applied in practice, such as hallucinations inherent in LLMs and the rigid syntactic structure of SQL, which often lead to the generation of erroneous SQL queries. Existing approaches typically rely on older models or simplistic LLM prompts for direct error correction, which is a relatively basic strategy. In this paper, we explore the performance of large models in the Text-to-SQL error correction task, focusing on the combination of LoRA fine-tuning and retrieval-augmented methods. Experimental results demonstrate that our approach achieves state-of-the-art (SOTA) performance on the Splash benchmark, outperforming existing methods and providing more accurate error corrections for SQL generation. Notably, despite using a 7B-parameter model, our method surpasses the performance of much larger 72B-scale models, highlighting the efficiency and effectiveness of our approach.},
	booktitle = {2025 8th {International} {Conference} on {Advanced} {Algorithms} and {Control} {Engineering} ({ICAACE})},
	author = {Jiang, Guangxin and Ma, Chao},
	month = mar,
	year = {2025},
	keywords = {Adaptation models, Codes, Error correction, Error Correction, Faces, Focusing, Large language models, LLMs, LoRA, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Structured Query Language, Syntactics, Text-to-SQL},
	pages = {2670--2674},
}

@article{chondamrongkul_addressing_2025,
	title = {Addressing {Technical} {Challenges} in {Large} {Language} {Model}-{Driven} {Educational} {Software} {System}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3531380},
	abstract = {The integration of large language models (LLMs) into educational systems poses significant challenges across several key attributes, including integration, explainability, testability, and scalability. These challenges arise from the complexity of coordinating system components, difficulty interpreting LLM decision-making processes, and the need for reliable, consistent model outputs in varied educational scenarios. Additionally, ensuring scalability requires robust autoscaling mechanisms and suitable architecture design to handle fluctuating workloads. This paper tackles these challenges by proposing tactics to improve system integration, enhance explainability through metadata and an algorithm process, ensure response consistency via regression testing, and facilitate efficient autoscaling through an event-driven microservice architecture. The evaluation results highlight the effectiveness of these tactics, confirming both functional consistency and robust system performance under varying loads.},
	journal = {IEEE Access},
	author = {Chondamrongkul, Nacha and Hristov, Georgi and Temdee, Punnarumol},
	year = {2025},
	keywords = {Accuracy, Artificial intelligence, Cognition, Decision making, Education, educational application, generative AI, large language model, LLM, Reliability, Retrieval augmented generation, Scalability, Software systems, Testing, Vectors},
	pages = {12846--12858},
}

@inproceedings{shi_improving_2025,
	title = {Improving {LLM}-{Powered} {EDA} {Assistants} with {RAFT}},
	doi = {10.1109/ICLAD65226.2025.00022},
	abstract = {Electronic design engineers often struggle to efficiently access relevant information for tasks like design verification and technology development. While large language models (LLMs) can enhance productivity as conversational agents, pre-trained open-source LLMs lack domain-specific knowledge for Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG) context, LLMs rely on external context but may still produce inaccurate responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but acquiring labeled question/answer (Q/A) data in EDA is difficult. To address this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our results show that RAFT with synthetic data significantly boosts LLM performance for RAG-based EDA tasks. We also investigate the impact of using real user questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data generation. Additionally, we implement secure access control to ensure sensitive information is only accessible to authorized personnel. Finally, we assess the risk of data leakage and unintended memorization during fine-tuning with synthetic data, providing practical insights.},
	booktitle = {2025 {IEEE} {International} {Conference} on {LLM}-{Aided} {Design} ({ICLAD})},
	author = {Shi, Luyao and Kazda, Michael and Schmitter, Charles and Gupta, Hemlata},
	month = jun,
	year = {2025},
	keywords = {Access control, Design automation, EDA, fine-tuning, Large language models, LLM, Personnel, Productivity, RAFT, RAG, Retrieval augmented generation, Synthetic data},
	pages = {9--15},
}

@inproceedings{trang_pham_vigpt_2025,
	title = {{ViGPT} {Researcher}: {An} {AI}-{Powered} {Online} {Research} {Assistant} for {Comprehensive} and {In}-{Depth} {Analysis}},
	doi = {10.1109/ATiGB66719.2025.11142107},
	abstract = {The rapid advancement of artificial intelligence (AI) has significantly transformed the landscape of online research. Large language models (LLMs) have shown great potential in assisting researchers; however, they also present challenges such as hallucinations, bias, and limited contextual depth. This paper introduces ViGPT Researcher, an AI-powered research assistant designed to enhance online research by integrating Retrieval Augmented Generation (RAG) and Plan-and-Solve Prompting (PS Prompting). By leveraging these two techniques, ViGPT Researcher improves information retrieval, minimizes hallucination, and provides users with comprehensive, well-structured research reports. The system is implemented as a web-based application, offering an intuitive user interface and supporting real-time research workflows. This study evaluates the effectiveness of ViGPT Researcher in comparison with existing AI research assistants such as ChatGPT and Gemini. The findings suggest that our approach significantly enhances accuracy, relevance, and user experience in AI-assisted research.},
	booktitle = {2025 10th {International} {Conference} on {Applying} {New} {Technology} in {Green} {Buildings} ({ATiGB})},
	author = {Trang Pham, Thi Thu and Pham, Phuc Hoang and Pham, Van Hoang and Tran, Hieu Minh and Duong, Tan Nghia},
	month = jul,
	year = {2025},
	keywords = {Accuracy, generative pretrained transformer, Information retrieval, Large language models, Law, natural languge processing, Real-time systems, Retrieval augmented generation, Retrieval-augmented generation, Robustness, Transformers, User experience, User interfaces},
	pages = {199--204},
}

@book{farris_notitle_2025,
	isbn = {978-1-63343-708-1},
	url = {https://ieeexplore.proxyucr.elogim.com/document/11079740},
	abstract = {Learn how large language models like GPT and Gemini work under the hood in plain English. How Large Language Models Work translates years of expert research on Large Language Models into a readable, focused introduction to working with these amazing systems. It explains clearly how LLMs function, introduces the optimization techniques to fine-tune them, and shows how to create pipelines and processes to ensure your AI applications are efficient and error-free. In How Large Language Models Work you will learn how to: Test and evaluate LLMs Use human feedback, supervised fine-tuning, and Retrieval Augmented Generation (RAG) Reducing the risk of bad outputs, high-stakes errors, and automation bias Human-computer interaction systems Combine LLMs with traditional ML How Large Language Models Work is authored by top machine learning researchers at Booz Allen Hamilton, including researcher Stella Biderman, Director of AI/ML Research Drew Farris, and Director of Emerging AI Edward Raff. They lay out how LLM and GPT technology works in plain language that’s accessible and engaging for all.},
	publisher = {Manning},
	author = {Farris, Drew and Raff, Edward and Biderman, Stella},
	year = {2025},
	note = {Publication Title: How Large Language Models Work},
	keywords = {AI book, artificial intelligence, ChatGPT, Gemini, GPT, large language models, LLM application, machine learning, natural language processing, prompt engineering, retrieval augmented generation, RLHF, supervised fine-tuning, tokenization, transformers},
}

@inproceedings{abu-arqoub_design_2025,
	title = {Design and {Implementation} of a {Comprehensive} {RAG}-{Driven} {Dashboard} {Within} the {ILO} {System} for {Data} {Visualization} and {Query} {Support}: {A} {Case} {Study} of the {University} of {Petra}},
	doi = {10.1109/ICCIAA65327.2025.11013578},
	abstract = {This article presents a comprehensive study on designing and implementing a dashboard for enhanced data visualization and query support at the University of Petra. The system leverages retrieval-augmented generation (RAG) and large language models (LLMs) to support diverse document types, including curricula, course descriptions, and program outcomes, alongside intended learning outcome (ILO)-related files. Our implementation demonstrates significant improvements in data accessibility and query response times while maintaining high accuracy in information retrieval and visualization. Through extensive evaluation, we show that this innovative approach transforms data management processes in higher education by enabling natural language interactions with educational data systems, building upon established business intelligence frameworks while introducing advanced AI capabilities.},
	booktitle = {2025 1st {International} {Conference} on {Computational} {Intelligence} {Approaches} and {Applications} ({ICCIAA})},
	author = {Abu-Arqoub, Mohammad and Alkarim Banna, Abed and El-Khalili, Nuha and Al-Shaikh Hasan, Mohammad},
	month = apr,
	year = {2025},
	keywords = {Business intelligence, Business Intelligence, Data systems, Data visualization, Data Visualization, Educational technology, Educational Technology, ILO System, Information retrieval, Large language models, Learning Analytics, Natural languages, RAG, Retrieval augmented generation, Time factors, Transforms},
	pages = {1--5},
}

@inproceedings{soularidis_llm-assisted_2024,
	title = {{LLM}-{Assisted} {Generation} of {SWRL} {Rules} from {Natural} {Language}},
	doi = {10.1109/AIxDKE63520.2024.00008},
	abstract = {Recently, Large Language Models (LLMs) have attracted great attention due to their remarkable performance in human-like text generation and reasoning skills (although their memory and hallucination problems still remain key issues to tackle more efficiently). LLMs have been applied to various application domains, including Knowledge Graph (KG) generation, question and answering over KGs and text-to-SPARQL translation. In this work, we investigate the capabilities of LLMs in text-to-SWRL translation, i.e., translation of Natural Language (NL) rules into Semantic Web Rule Language (SWRL) rules, put in the context of an industrial Ontology Engineering (OE) environment called GLUON, presenting our first experimental results. The aim of this work is to identify the level of automation that is adequate for the LLM to generate well-formed SWRL rules, towards the development of an LLM-based framework, as a plugin to the GLUON OE environment. In this direction we leverage and combine the reasoning capabilities of GPT-4o model, the Retrieval-Augmented Generation (RAG) technology, and prompt engineering. We employ quantitative and qualitative metrics to evaluate the generated SWRL rules, focusing on the correct syntax and the level of human intervention.},
	booktitle = {2024 {International} {Conference} on {AI} x {Data} and {Knowledge} {Engineering} ({AIxDKE})},
	author = {Soularidis, Andreas and Kotis, Konstantinos and Lamolle, Myriam and Mejdoul, Zakaria and Lortal, Gaëlle and Vouros, George},
	month = dec,
	year = {2024},
	note = {ISSN: 2831-7203},
	keywords = {Automation, Cognition, Large language models, Large Language Models (LLM), Memory management, Ontologies, Ontology Engineering, Prompt engineering, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantic Web, SWRL, Syntactics, Translation},
	pages = {7--12},
}

@inproceedings{shrimali_large_2025,
	title = {Large {Language} {Model}-{Based} {Time}-{Series} {Data} {Summarization} {Using} a {Novel} {Multi}-{Tiered} {Retrieval} {Pipeline}},
	doi = {10.1109/HPSC66065.2025.00012},
	abstract = {Financial time-series data summarization is a critical task for analysts and decision-makers; yet current approaches rely on labor-intensive manual interpretation or bespoke reporting scripts. In this research, a novel multi-tier retrieval pipeline that leverages large language models (LLMs) to automatically generate concise and accurate summaries of financial data is proposed. By integrating traditional templated methods with retrieval-augmented prompts (using domain-specific examples to guide the LLM) the challenge of balancing linguistic fluency with numerical precision is addressed. The pipeline is evaluated on a knowledge base constructed from daily OHLCV data of 20 large-cap companies, with additional testing on a mid-cap ticker not present in the original dataset (COF). Over 15 independent runs were conducted; these experimental results demonstrate that while direct summarization captures general trends, this templated and refined multi-tier retrieval pipeline ensures factual correctness and achieves the best overall performance in terms of ROUGE, BERTScore, and numeric fidelity. This pipeline has the potential to transform time-series analysis by reducing manual workload and improving the reliability of automated financial data summarization.},
	booktitle = {2025 {IEEE} 11th {International} {Conference} on {High} {Performance} and {Smart} {Computing} ({HPSC})},
	author = {Shrimali, Samyak},
	month = may,
	year = {2025},
	keywords = {Accuracy, Data models, Financial Analytics, Large language models, Large Language Models (LLMs), Manuals, Market research, Numerical models, Pipelines, Prompt engineering, Prompt Engineering, Retrieval-Augmented Generation, Stock Time-Series Analysis, Time series analysis, Time Series Data Retrieval, Time-Series Data Summarization, Transforms},
	pages = {7--12},
}

@inproceedings{neeser_wireless_2025,
	title = {Wireless {Knowledge} {Grounding} in {Smaller} {Llms} {Using} {Retrieval} {Augmented} {Generation} and {Fine}-{Tuning}},
	doi = {10.1109/ICC52391.2025.11160863},
	abstract = {While LLMs have shown tremendous progress in broad natural language processing (NLP) tasks, their application to solving domain-specific NLP problems (e.g., wireless) has been understudied. Existing research on LLMs for the wireless domain is mostly limited to using retrieval-augmented generation or model fine-tuning for wireless question-and-answering ({\textbackslash}mathrmQ / {\textbackslash}mathrmA) tasks using bigger models such as GPT, Mixtral, and Llama. In contrast to state-of-the-art models, in this paper, a smaller LLM such as Phi2 is evaluated on wireless Q/A tasks. A combination of RAG, model fine-tuning and hyper-parameter optimization is proposed to improve the accuracy of Phi 2 for wireless Q/A tasks. Experimental results demonstrate the proposed method's superior performance compared to vanilla LLMs and state-of-the-art LLMs with either fine-tuning or RAG. Comparing the results to larger models like Llama and Phi3 suggests that transitioning towards bigger models and relying on their extensive knowledge retrieval capabilities may not be the best approach for deploying LLMs on edge devices. This is because edge devices have limited computing and storage capabilities and are expected to operate in real-time for future wireless applications. Instead, the focus should be on improving the grounding of LLMs in wireless concepts using smaller models such as Phi2.},
	booktitle = {{ICC} 2025 - {IEEE} {International} {Conference} on {Communications}},
	author = {Neeser, Andrew and Thomas, Christo Kurisummoottil and Xu, Shengzhe and Ramakrishnan, Naren and Saad, Walid},
	month = jun,
	year = {2025},
	note = {ISSN: 1938-1883},
	keywords = {Accuracy, Adaptation models, Benchmark testing, Computational modeling, Data models, Hyperparameter optimization, Natural language processing, Retrieval augmented generation, Space exploration, Wireless communication},
	pages = {1572--1577},
}

@inproceedings{craig_whats_2024,
	title = {What’s the data say? {An} {LLM}-based system for interrogating experimental data},
	doi = {10.1109/BIBM62325.2024.10821725},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, issues such as hallucinations, implicit extrapolations, and inappropriate interpretations currently limit their wide adoption in scientific research. Retrieval-augmented Generation (RAG) has emerged as a promising technique to enhance the accuracy and reliability of LLM outputs. RAG integrates external information with LLM-generated content, ensuring more dependable results and enabling the inclusion of proprietary or domain-specific knowledge unavailable during the initial LLM training. In this work, we present an extensible LLM RAG architecture designed to answer complex scientific questions that can reference user-provided experimental data. The system uses domain-specific knowledge (RAGdom) to respond to general inquiries, supporting answers with citations from authoritative sources. These responses can be recursively integrated to provide user directed context for experiments (RAGexp), allowing subsequent questions to utilize both general and experiment-specific information. Our system can be easily extended with user-defined domain-specific modules for functionally interrogating quantitative experimental results (RAGfun). We demonstrate its application in analyzing, summarizing, interpreting, and assisting with synthesizing results from a gene expression comparison of two phenotypes. The RAGdom component encompasses biological pathways, genes, and disease-related knowledge. The RAGexp component is derived from user queries of differentially expressed gene (DEG) and pathway enrichment analysis (PEA) results. Additionally, RAGfun modules are designed to interface with DEG and PEA data, enabling quantitative experimental results to be integrated seamlessly into user questions. Unlike traditional LLM outputs, our system’s results are free of hallucinations and include paragraph-level citations to the supporting literature. We compared the quality of our system’s output with those generated by LLM-only models such as GPT-3, GPT-4, Llama2, PaLM2, and BioGPT, and found our approach to be superior.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Craig, Douglas B. and Drăghici, Sorin},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {Accuracy, Biological system modeling, differential gene analysis, large language models, pathway enrichment analysis, Phenotypes, Planning, Question answering (information retrieval), Reliability, Retrieval augmented generation, retrieval-augmented generation, scientific experiment analysis, Semantics, Tagging, Training},
	pages = {1457--1462},
}

@inproceedings{yi_maem_2025,
	title = {{MAEM}: {A} {Multi}-{Aspect} {Extraction} {Model} for {Enhanced} {Embedding} in {RAG}},
	doi = {10.1109/ICASSP49660.2025.10889603},
	abstract = {Retrieval Augmented Generation can effectively reduce hallucinations in LLMs during question-answering, with embedding models directly influencing its performance. While current embedding models improve encoding through large-scale training, they often overlook the potential or explicit multi-aspect information within the text. To address this, we propose the Multi-Aspect Extraction Model (MAEM), an eigen decomposition-based approach that extracts and integrates text aspects into a unified vector for enhanced retrieval, and introduce a regularization loss function to assist in training. We utilized LLMs to create the Policy-Corpus dataset and validated the model on both Policy-Corpus and FiQA. Incorporating MAEM and regularization into GTEbase improved NDCG@10 by 3.8 points on FiQA and 4.56 points on Policy-Corpus. Respectively, achieving results comparable to larger models using a smaller parameter model.},
	booktitle = {{ICASSP} 2025 - 2025 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yi, Ningyuan and Liu, Chen and Wang, Yue and Yu, Jianjun},
	month = apr,
	year = {2025},
	note = {ISSN: 2379-190X},
	keywords = {Eigen Decomposition, Encoding, Feature extraction, Information filters, Logic gates, Matrix decomposition, Multi-Aspect Extraction, Retrieval augmented generation, Retrieval Augmented Generation, Signal processing, Speech processing, Text Embedding, Training, Vectors},
	pages = {1--5},
}

@inproceedings{wang_ms-rag_2025,
	title = {{MS}-{RAG}: {A} {Multimodal} {Retrieval}-{Augmented} {Framework} for {Digital} {Twins} with {Knowledge} {Graph} {Reasoning}},
	doi = {10.1109/ICETIS66286.2025.11144098},
	abstract = {Traditional Retrieval-Augmented Generation (RAG) frameworks struggle to effectively handle multi-source heterogeneous data in digital twin scenarios, leading to insufficient accuracy in component relationship analysis and fault reasoning. This paper proposes a Multi-Stage Retrieval-Augmented Framework (MS-RAG), which establishes a knowledge base through a multimodal dataset fusing real-time equipment parameters and industry specifications. It models “fault-symptom-maintenance” causal relationships using a knowledge graph and generates enhanced contexts with causal chains via vector retrieval and knowledge graph logical reasoning. Experiments show that the method significantly improves fault diagnosis accuracy and reduces hallucination rates, providing an efficient solution for complex fault diagnosis in digital twins.},
	booktitle = {2025 10th {International} {Conference} on {Electronic} {Technology} and {Information} {Science} ({ICETIS})},
	author = {Wang, Hongda and Qi, Miao and Wang, Zheng and Yang, Qiliang},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Cognition, Context modeling, Data integration, Digital Twin Fault Diagnosis, Digital twins, Fault diagnosis, Knowledge Graph (KG), Knowledge graphs, Large Language Models (LLMs), Multimodal Data Fusion, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantics, Vectors},
	pages = {307--313},
}

@inproceedings{jacobs_leveraging_2024,
	title = {Leveraging {Lecture} {Content} for {Improved} {Feedback}: {Explorations} with {GPT}-4 and {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/CSEET62301.2024.10663001},
	abstract = {This paper presents the use of Retrieval Augmented Generation (RAG) to improve the feedback generated by Large Language Models for programming tasks. For this purpose, corresponding lecture recordings were transcribed and made available to the Large Language Model GPT-4 as external knowledge source together with timestamps as metainformation by using RAG. The purpose of this is to prevent hallucinations and to enforce the use of the technical terms and phrases from the lecture. In an exercise platform developed to solve programming problems for an introductory programming lecture, students can request feedback on their solutions generated by GPT-4. For this task GPT-4 receives the students' code solution, the compiler output, the result of unit tests and the relevant passages from the lecture notes available through the use of RAG as additional context. The feedback generated by GPT-4 should guide students to solve problems independently and link to the lecture content, using the time stamps of the transcript as meta-information. In this way, the corresponding lecture videos can be viewed immediately at the corresponding positions. For the evaluation, students worked with the tool in a workshop and decided for each feedback whether it should be extended by RAG or not. First results based on a questionnaire and the collected usage data show that the use of RAG can improve feedback generation and is preferred by students in some situations. Due to the slower speed of feedback generation, the benefits are situation dependent.},
	booktitle = {2024 36th {International} {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	author = {Jacobs, Sven and Jaschke, Steffen},
	month = jul,
	year = {2024},
	note = {ISSN: 2377-570X},
	keywords = {Codes, Conferences, Feedback, GPT-4, Large language models, Large language Models, Programming Education, Programming profession, Recording, Retrieval Augmented Generation, Videos},
	pages = {1--5},
}

@inproceedings{feng_secllm-intent-driven_2025,
	title = {{SecLLM}-{Intent}-{Driven} {Orchestration} of {Heterogeneous} {Security} {Devices} via {MCP} {Function} {Calling} and {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/CISAT66811.2025.11181750},
	abstract = {Heterogeneous security infrastructures-network-detection, endpoint-detection, threat-intelligence and ticketing systems-remain siloed, impeding real-time collaboration and prolonging incident response. We introduce SecLLM, an intent-driven orchestration framework that couples a 32-billionparameter large-language model with (i) Model-Context-Protocol (MCP) function calling for deterministic, auditable control-plane actions and (ii) retrieval-augmented generation (RAG) for dynamic knowledge grounding. Given a natural-language goal such as "quarantine lateral-movement hosts and patch vulnerable SMB services", SecLLM parses the intent into a typed task-graph, enriches each node with live device capabilities drawn from a 50 M-artifact vector store, then safely executes the plan across NDR, EDR and SOAR devices while reflecting on intermediate results. In a 72-hour replay of the publicly available CIC-IDS2018 network trace plus host logs, SecLLM cuts mean time-to-containment by 80\% (38 → 7.4 min) and reduces false-positive escalations by 42\% relative to a production SOAR baseline. Ablations show that removing RAG slows containment by 19\%, while disabling MCP yields unsafe, non-deterministic actions. These results demonstrate that intent-aligned LLMs, when grounded via RAG and constrained by strongly typed schemas, can safely automate cross-device security operations at scale.},
	booktitle = {2025 8th {International} {Conference} on {Computer} {Information} {Science} and {Application} {Technology} ({CISAT})},
	author = {Feng, Guocong and Pan, Yuan and Zhang, Chunmei and Huang, Kaitian},
	month = jul,
	year = {2025},
	keywords = {Collaboration, Computer security, Function Calling, Grounding, Information science, Large language models, Large Language Models, MCP, Production, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, Security Orchestration, SOAR, Vectors},
	pages = {225--229},
}

@inproceedings{sambare_survey_2025,
	title = {Survey of {Interactive} {Document} {Query} {Systems} {Using} {RAG} and {LangChain}},
	doi = {10.1109/ICOCT64433.2025.11118753},
	abstract = {This research is to build a chatbot based on Retrieval-Augmented Generation (RAG) to produce the right answers. Rag-mini-bioasq Dataset from HuggingFace is used for ground truth answers, generated answers (LLM), retrieved contexts (ChromaDB, Nomic-embed-text) . For the retrieval orchestration, the system uses LangChain, ChromaDB for storage of vector, and Deepseek r1 1.5B for linearity between long context queries up to 128,000 tokens. Chain of Thought (CoT) and Chain of Notes are techniques that increase multi-hop query resolution with reduced hallucinations and improved accuracy.},
	booktitle = {2025 {International} {Conference} on {Computing} {Technologies} ({ICOCT})},
	author = {Sambare, Govinda B. and Ambala, Srinivas and Kadam, Ganesh and Sonawane, Vedant},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Chatbots, ChromaDB, Custom RAG dataset, Deepseek r1 1.5B, LangChain, Large language models, Linearity, Long Context Large Language Models (LLMs), Prompt engineering, Prompt Engineering, Query Decomposition, RAGAS Framework, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Servers, Surveys, Vectors},
	pages = {1--7},
}

@inproceedings{mailach_themes_2025,
	title = {Themes of {Building} {LLM}-{Based} {Applications} for {Production}: {A} {Practitioner}'s {View}},
	doi = {10.1109/CAIN66642.2025.00011},
	abstract = {Background: Large language models (LLMs) have become a paramount interest of researchers and practitioners alike, yet a comprehensive overview of key considerations for those developing LLM-based systems is lacking. This study addresses this gap by collecting and mapping the topics practitioners discuss online, offering practical insights into where priorities lie in developing LLM-based applications. Method: We collected 189 videos from 2022 to 2024 by practitioners actively developing such systems and discussing various aspects they encounter during development and deployment of LLMs in production. We analyzed the transcripts using BERTopic, then manually sorted and merged the generated topics into themes, leading to a total of 20 topics in 8 themes. Results: The most prevalent topics fall within the theme Design \& Architecture, with a strong focus on retrieval-augmented generation (RAG) systems. Other frequently discussed topics include model capabilities and enhancement techniques (e.g., finetuning, prompt engineering), infrastructure and tooling, and risks and ethical challenges. Implications: Our results highlight current discussions and challenges in deploying LLMs in production. This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications. We further highlight topics of interest for academics where further research is needed.},
	booktitle = {2025 {IEEE}/{ACM} 4th {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Mailach, Alina and Simon, Sebastian and Dorn, Johannes and Siegmund, Norbert},
	month = apr,
	year = {2025},
	keywords = {agents, Computer architecture, evaluation, fine-tuning, Large language models, LLM in production, Manuals, Production, prompt engineering, Prompt engineering, RAG, Retrieval augmented generation, retrieval-augmented generation, Software engineering, Systematics, Tuning, Videos},
	pages = {18--30},
}

@inproceedings{ping_hdlcore_2025,
	title = {{HDLCoRe}: {A} {Training}-{Free} {Framework} for {Mitigating} {Hallucinations} in {LLM}-{Generated} {HDL}},
	doi = {10.1109/ICLAD65226.2025.00034},
	abstract = {Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, when applied to hardware description languages (HDL), these models exhibit significant limitations due to data scarcity, resulting in hallucinations and incorrect code generation. To address these challenges, we propose HDLCoRe, a training-free framework that enhances LLMs’ HDL generation capabilities through prompt engineering techniques and retrieval-augmented generation (RAG). Our approach consists of two main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting technique with self-verification that classifies tasks by complexity and type, incorporates domain-specific knowledge, and guides LLMs through step-by-step self-simulation for error correction; and (2) a two-stage heterogeneous RAG system that addresses formatting inconsistencies through key component extraction and efficiently retrieves relevant HDL examples through sequential filtering and re-ranking. HDLCoRe eliminates the need for model fine-tuning while substantially improving LLMs’ HDL generation capabilities. Experimental results demonstrate that our framework achieves superior performance on the RTLLM2.0 benchmark, significantly reducing hallucinations and improving both syntactic and functional correctness.},
	booktitle = {2025 {IEEE} {International} {Conference} on {LLM}-{Aided} {Design} ({ICLAD})},
	author = {Ping, Heng and Li, Shixuan and Zhang, Peiyu and Cheng, Anzhe and Duan, Shukai and Kanakaris, Nikos and Xiao, Xiongye and Yang, Wei and Nazarian, Shahin and Irimia, Andrei and Bogdan, Paul},
	month = jun,
	year = {2025},
	keywords = {Codes, Data models, Error correction, Filtering, Hardware, Hardware Design Language, Hardware design languages, Large language models, Large Language Models, Prompt engineering, Prompt Engineering, Retrieval augmented generation, Retrivel-Augmented Generation, Syntactics},
	pages = {108--116},
}

@inproceedings{zhou_enabling_2025,
	title = {Enabling {Interactive} {AI} in {Industry} 5.0 with {RAG}-{Enhanced} {GenAI} {Chatbots}},
	doi = {10.1109/ICE/ITMC65658.2025.11106634},
	abstract = {Industry 5.0 advances sustainable development through human-machine collaboration and personalised manufacturing. The increase in intelligent industrial equipment creates data scalability challenges for human workers who face difficulties in making decisions based on relevant data sources. Advanced interactive AI systems, capable of integrating diverse data sources and delivering real-time, context-aware insights, present promising solutions to the challenges of the industrial environment. This research introduces a retrieval-augmented generation (RAG)-enhanced Generative artificial intelligence (GenAI) chatbot to address these challenges. The system integrates a variety of information sources, including government reports, news websites, academic studies, and industry reports. This industry 5.0 chatbot aims to offer users extensive knowledge of the industrial sector through a Question-and-Answer interface. It provides relevant and accurate information through intuitive, context-aware interactions to reduce cognitive load for users, which improves decision-making efficiency and user experience. Through experimental evaluation, the RAG-enhanced GenAI chatbot significantly improves accuracy, relevance and user satisfaction, outperforming models like ChatGPT-4o. This system presents an innovative practical solution to tackle Industry 5.0 core issues particularly in enhancing human-machine collaboration and decision-making efficiency. This research contributes to the theoretical and practical development of RAG-enhanced AI systems, laying a foundation for future investigations of industrial AI interaction.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Engineering}, {Technology}, and {Innovation} ({ICE}/{ITMC})},
	author = {Zhou, Tianyu and Wan, Yuwei and Liu, Ying and Kumar, Maneesh},
	month = jun,
	year = {2025},
	note = {ISSN: 2693-8855},
	keywords = {Accuracy, Chatbots, Cognitive load, Cognitive Load Reduction, Collaboration, Decision making, Fifth Industrial Revolution, Generative AI, Human-Machine Collaboration, Human-machine systems, Industry 5.0, Interactive AI, Knowledge Retrieval, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors},
	pages = {1--10},
}

@book{kimothi_notitle_2025,
	isbn = {978-1-63343-585-8},
	url = {https://ieeexplore.proxyucr.elogim.com/document/11079738},
	abstract = {Everything you need to know about Retrieval Augmented Generation in one human-friendly guide. Retrieval Augmented Generation—or RAG—enhances an LLM’s available data by adding context from an external knowledge base, so it can answer accurately about proprietary content, recent information, and even live conversations. RAG is powerful, and with A Simple Guide to Retrieval Augmented Generation, it’s also easy to understand and implement! In A Simple Guide to Retrieval Augmented Generation you’ll learn: The components of a RAG system How to create a RAG knowledge base The indexing and generation pipeline Evaluating a RAG system Advanced RAG strategies RAG tools, technologies, and frameworks A Simple Guide to Retrieval Augmented Generation gives an easy, yet comprehensive, introduction to RAG for AI beginners. You’ll go from basic RAG that uses indexing and generation pipelines, to modular RAG and multimodal data from images, spreadsheets, and more.},
	publisher = {Manning},
	author = {Kimothi, Abhinav},
	year = {2025},
	note = {Publication Title: A Simple Guide to Retrieval Augmented Generation},
	keywords = {AI hallucination reduction, AI pipelines, generative AI, knowledge base, LangChain, large language models, LLMs, OpenAI, prompt engineering, Python AI, RAG evaluation, RAG guide, RAG tools, retrieval augmented generation, Transformers, vector databases},
}

@inproceedings{rachmat_fine-tuning_2024,
	title = {Fine-{Tuning} {Large} {Language} {Model} ({LLM}) to {Answer} {Basic} {Questions} for {Prospective} {New} {Students} at {Syiah} {Kuala} {University} {Using} the {Retrieval}-{Augmented} {Generation} ({RAG}) {Method}},
	doi = {10.1109/ICIC64337.2024.10956296},
	abstract = {USK Mistral 7B is a large language model designed to answer basic admission questions at Universitas Syiah Kuala (USK). The model was fine-tuned using the open-source model of Mistral 7B using collected data from admissions and lectures at the university. The QLoRA and RAG techniques were used to train the model and retrieve relevant information from external data sources. The results were evaluated using the ROUGE score. Responses were generated with a score of {\textgreater}0.5 on ten out of 46 questions with the RAG method, and testing with the fine-tuning method was carried out on 20 questions and resulted in responses with a score of 1.0 from all questions asked. The performance of USK Mistral 7B shows its potential as an effective tool in helping students querying information about admission and lectures at USK.},
	booktitle = {2024 {Ninth} {International} {Conference} on {Informatics} and {Computing} ({ICIC})},
	author = {Rachmat, Hary and Riza, Hammam and Abidin, Taufik Fuadi},
	month = oct,
	year = {2024},
	keywords = {Computational modeling, Data models, Fine-tuning, Generative AI, Informatics, Large Language Model, Large language models, Measurement, RAG, Retrieval augmented generation, Soft sensors, Testing, Training},
	pages = {1--5},
}

@inproceedings{mulakala_evaluation_2025,
	title = {Evaluation of {LLMs}’ {Reasoning} for {Retrieval} {Augmented} {Generation} ({RAG}) {Systems}},
	doi = {10.1109/WorldSUAS66815.2025.11199211},
	abstract = {Large Language Models (LLMs) have shown remarkable capabilities in various Natural Language Processing Tasks, but are usually suffered by hallucinations which refers to generating factually incorrect content. Retrieval-Augmented Generation (RAG) has shown itself to be a promising approach for minimizing such hallucinations in by accessing external data sources. However, the impact of LLM’s reasoning capabilities such as Chain-of-thought (CoT) prompting in RAG systems is unclear. Three different LLMs, o1-mini, o3-mini, and gpt-4o, with the same RAG pipeline are evaluated on the same PubMed dataset across three different performance metrics Mean Reciprocal Rank (MRR), Faithfulness, and Semantic Answer Similarity (SAS). All three models scored a perfect MRR score, meaning good document retrieval but the reasoning models performed better than non-reasoning baseline in faithfulness and SAS. O1-mini model recorded the optimal somatic quality versus roundedness tradeoff. The outcome indicates that it is possible to improve the quality and credibility of produced answers by using the reasoning-capable LLMs for test generation in RAG systems.},
	booktitle = {2025 {World} {Skills} {Conference} on {Universal} {Data} {Analytics} and {Sciences} ({WorldSUAS})},
	author = {Mulakala, Benarji and Saini, Madan Lal and Bhukya, Vamsi and {Siddhartha} and Chand, Neela Prem},
	month = aug,
	year = {2025},
	keywords = {Cognition, Faithfulness, Hallucination, Large language models, Large Language Models, Performance metrics, Pipelines, Question answering (information retrieval), Reasoning, Retrieval augmented generation, Retrieval-Augmented Generation, Semantic Answer Similarity, Semantics, Soft sensors, Synthetic aperture sonar, Test pattern generators},
	pages = {1--5},
}

@inproceedings{liu_comprehensive_2024,
	title = {Comprehensive {Evaluation} of {AI} {Hallucination} and {Novel} {UV}-{Oriented} {Framework} toward {Safe} and {Trustworthy} {AI}},
	doi = {10.1109/UV63228.2024.11189137},
	abstract = {this work, we present a comprehensive evaluation of hallucination phenomena in LLMs and multimodal systems. First, we propose a structured taxonomy encompassing factuality-based, faithfulness-based, logical-based, and emergent hybrid forms, extending to multimodal-specific risks such as cross-modal inconsistencies, visual overinterpretation, and modality dominance effects. Second, we conduct a mechanistic analysis that traces hallucinations across the full model lifecycle—data-level origins (knowledge gaps, misinformation, annotation noise), training-induced mechanisms (distributional mimicry, reward bias, alignment forgetting), and inference-time vulnerabilities (confidence miscalibration, decoding failures, prompt-induced errors). This perspective reveals how independent mechanisms interact to produce cascade effects, amplifying initial flaws into elaborate but unreliable narratives. Third, we critically assess existing detection and evaluation approaches, highlighting limitations of current benchmarks, taxonomic ambiguities, and the lack of mechanism-aware evaluation protocols. We argue that future evaluation must integrate both surface-level manifestations and their generative causes to achieve more robust measurement.Beyond evaluation, we survey mitigation strategies organized into mechanism-based, phase-based, and hybrid approaches. These range from lightweight prompt engineering and decoding constraints to resource-augmented methods such as retrieval-augmented generation and knowledge graph integration, as well as higher-level frameworks for controllability and uncertainty calibration. We analyze the trade-offs among effectiveness, scalability, interpretability, and creative freedom, emphasizing the importance of context-specific tolerances: hallucinations that are unacceptable in medicine or finance may be tolerable, or even beneficial, in creative applications.Building on these insights, we propose a novel UV-oriented framework for safe and trustworthy AI, inspired by the Universal Village vision of harmonizing human, technological, and environmental systems. In this framework, hallucination is conceptualized not only as an isolated model error but as a systemic vulnerability in information flow and decision-making loops. We design a multi-level dynamic system integrating sensing, communication, decision-making, action, and evaluation, supported by closed feedback loops and user-specific hallucination tolerance levels. This architecture enables adaptive mitigation through mechanism-informed strategies, retrieval and verification integration, and consensus mechanisms, ensuring resilience across diverse application contexts.Our contributions are threefold: (1) we present the most comprehensive taxonomy of hallucination to date, linking manifestations with mechanistic drivers; (2) we unify detection, evaluation, and mitigation strategies into a coherent survey that highlights both current progress and pressing gaps; and (3) we introduce a UV-oriented framework that reframes hallucination control as part of a broader, feedback-driven ecosystem for reliable AI. We conclude by outlining open challenges, including classification ambiguities, multimodal alignment risks, deployment-specific vulnerabilities, and the need to reconcile reliability with creativity. Addressing these challenges is essential to advancing from powerful yet fallible generative models toward AI systems that are not only safe and responsible but also aligned with human values and societal needs.},
	booktitle = {2024 7th {International} {Conference} on {Universal} {Village} ({UV})},
	author = {Liu, Zhenyao and Kou, Jieren and Zhang, Wuyang and Gu, Chuqiao and Fang, Xinyi and Huang, Zhenqian and Yuan, Hao and Li, Hanxia and Lu, Xiuyuan and Yin, Aobing and Liu, Chang and Zhao, Chenjun and Lin, Wenjie and Zhang, Lifeng and Wang, Zhongda and Huang, Haoyang and Duan, Xiaoman and Fang, Yajun},
	month = oct,
	year = {2024},
	keywords = {AI hallucination, alignment bias, Artificial intelligence, benchmark construction, Calibration, confidence miscalibration, creative AI, Decoding, decoding strategies, dynamic feedback loops, emergent hybrid hallucinations, evaluation metrics, explainability, factuality, faithfulness, Feedback loop, hallucination detection, hallucination mitigation, hallucination taxonomy, human–AI interaction, Large language models, large language models (LLMs), logical consistency, mechanism-aware evaluation, multimodal alignment, multimodal large language models (MLLMs), Prevention and mitigation, reinforcement learning from human feedback (RLHF), responsible AI, Retrieval augmented generation, retrieval-augmented generation (RAG), robustness, safe and reliable AI, Surveys, Taxonomy, trustworthiness, Uncertainty, uncertainty calibration, Universal Village (UV) framework},
	pages = {1--136},
}

@inproceedings{yang_timerag_2025,
	title = {{TimeRAG}: {Boosting} {LLM} {Time} {Series} {Forecasting} via {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICASSP49660.2025.10889933},
	abstract = {Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97\% on average.},
	booktitle = {{ICASSP} 2025 - 2025 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yang, Silin and Wang, Dong and Zheng, Haoqi and Jin, Ruochun},
	month = apr,
	year = {2025},
	note = {ISSN: 2379-190X},
	keywords = {Accuracy, Dynamic Time Warping (DTW), Forecasting, Knowledge based systems, Large Language Model(LLM), Predictive models, Retrieval augmented generation, Retrieval-Augmented Generation(RAG), Signal processing, Speech processing, Time measurement, Time series analysis, Time Series Forecasting, Training},
	pages = {1--5},
}

@inproceedings{rani_enhance_2024,
	title = {To {Enhance} {Graph}-{Based} {Retrieval}-{Augmented} {Generation} ({RAG}) with {Robust} {Retrieval} {Techniques}},
	doi = {10.1109/ICOSST64562.2024.10871140},
	abstract = {Large language models have demonstrated exceptional performance in multiple domains. However, practical deployment in the healthcare sector has distinctive challenges. These challenges include hallucination, inconsistency, explainability, reasoning, authenticity, and validity of information sources. Hallucinations in LLM often emerge due to unstructured and obsolete training data and the incompetence to upgrade the model data post-training. Retrieval-augmented generation (RAG) integration with LLM decision-making helps access real-time information from external resources. However, further improvements are needed to improve accurate response generation. A knowledge Graph is a structured data comprising nodes as entities and edges as relationships. When integrated with RAG, Knowledge Graph-based retrieval offers better contextu-ally relevant responses, traceability, and explainability of generated responses than RAG alone. This study proposes a novel knowledge graph-based RAG framework with a refined retrieval pipeline, robust chunking mechanism, and source traceability for enhanced diabetes-focused LLM. The retrieval pipeline integrates three robust retrieval strategies: keyword, graph, and vector. To ensure the authenticity of responses, a knowledge base focusing on diabetes is designed from validated sources. This verified knowledge base is preprocessed and converted to a knowledge graph to design A graph-based RAG pipeline. The empirical results demonstrate effective performance in diabetes-focused LLM, achieving a Rouge 1 score of 82.19\%.},
	booktitle = {2024 18th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Rani, Maneeha and Mishra, Bhupesh Kumar and Thakker, Dhavalkumar and Khan, Mohammad Nouman},
	month = dec,
	year = {2024},
	note = {ISSN: 2770-8225},
	keywords = {Accuracy, Cognition, Diabetes, Graph-based Retrieval-augmented generation, Healthcare, Knowledge based systems, Knowledge graph, Knowledge graphs, Large language model, Pipelines, Real-time systems, Retrieval augmented generation, Retrieval-augmented generation, Training data, Vectors},
	pages = {1--6},
}

@inproceedings{zhang_cara-rag_2025,
	title = {{CARA}-{RAG}: {Civil} {Aviation} {Regulation} and {Authority} {Retrieval}-{Augmented} {Generation} {System}},
	doi = {10.1109/ICIMCT.2025.00039},
	abstract = {The civil aviation regulatory system is huge and complex, and the existing Question Answering (QA) system is difficult to balance the retrieval efficiency and generation quality. This paper introduces CARA-RAG, which improves the reliability and authoritativeness of regulation QA by combining customized corpus construction with Retrieval-Augmented Generation (RAG), and reinforced by security verification mechanism. A domain-specific evaluation dataset was built, combining automatic generation and expert verification to ensure data quality. Experimental results show that, CARA-RAG has better relevance, accuracy, Completeness and Faithfulness than the LLM-only generation and naïve RAG models. The system also integrates a lightweight WebUI to realize real-time interaction and visual display, providing a complete and scalable solution for civil aviation regulation QA system.},
	booktitle = {2025 {International} {Conference} on {Information} {Management} and {Computing} {Technology} ({ICIMCT})},
	author = {Zhang, Zhonghao and Yang, Xiaochen},
	month = may,
	year = {2025},
	keywords = {Accuracy, Civil Aviation Regulation, Industries, Large Language Models (LLMs), Real-time systems, Regulation, Reliability, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Security, System performance, Testing, Visualization},
	pages = {161--164},
}

@inproceedings{kukreja_performance_2025,
	title = {Performance {Enhancement} of {Agentic} {Retrieval} {Augmented} {Generation} {Using} {Relevance} {Generative} {Answering}},
	doi = {10.1109/ICAIE64856.2025.11158296},
	abstract = {The aim of this research paper is to present a novel approach of using Relevance Generative Answering (RGA) in the trending field of Agentic Retrieval Augmented Generation (RAG). The paradigm shift in the RAG system by the introduction of Agentic RAG has opened a new research paradigm. The major issue of hallucination is overcome with the use of a traditional RAG system with some limitations like accuracy and relevance, lack of reasoning, the lost in the middle problem, etc. The Agentic RAG system attempts to address a few of these limitations. However, interpreting results based on the user's intent remains a significant area of research. This research aimed to understand user intent by introducing relevance detection block in the proposed architecture. Different performance metrics like precision, recall, F1 score, relevance, latency are used to validate the proposed approach. The results presented in this research reveal that the performance of the proposed system is much more relevant compared to agentic RAG system. For context and intent specific applications proposed framework suits well.},
	booktitle = {2025 5th {International} {Conference} on {Artificial} {Intelligence} and {Education} ({ICAIE})},
	author = {Kukreja, Sanjay and Kumar, Tarun and Bharate, Vishal and Gadwe, Sweta and Dasgupta, Abhijit and Guha, Debashis},
	month = may,
	year = {2025},
	keywords = {Agentic Retrieval Augmented Generation, Artificial Intelligence, Cognition, Complexity theory, Data integrity, Education, Generative AI, Large Language Model, Large language models, Measurement, Natural language processing, Natural Language Processing, Performance metrics, Relevance Generative Answering, Retrieval augmented generation},
	pages = {465--469},
}

@inproceedings{shi_ask-eda_2024,
	title = {Ask-{EDA}: {A} {Design} {Assistant} {Empowered} by {LLM}, {Hybrid} {RAG} and {Abbreviation} {De}-hallucination},
	doi = {10.1109/LAD62341.2024.10691824},
	abstract = {Electronic design engineers are challenged to find relevant information efficiently for a myriad of tasks within design construction, verification and technology development. Large language models (LLM) have the potential to help improve productivity by serving as conversational agents that effectively function as subject-matter experts. In this paper we demonstrate Ask-EDA, a chat agent designed to serve as a 24×7 expert available to provide guidance to design engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation (RAG) and abbreviation de-hallucination (ADH) techniques to deliver more relevant and accurate responses. We curated three evaluation datasets, namely q2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct aspect: general design question answering, design command handling and abbreviation resolution. We demonstrated that hybrid RAG offers over a 40\% improvement in Recall on the q2a-100 dataset and over a 60\% improvement on the cmds-100 dataset compared to not using RAG, while ADH yields over a 70\% enhancement in Recall on the abbr-100 dataset. The evaluation results show that Ask-EDA can effectively respond to design-related inquiries.},
	booktitle = {2024 {IEEE} {LLM} {Aided} {Design} {Workshop} ({LAD})},
	author = {Shi, Luyao and Kazda, Michael and Sears, Bradley and Shropshire, Nick and Puri, Ruchir},
	month = jun,
	year = {2024},
	keywords = {Accuracy, chatbot, Chatbots, Conferences, de-hallucination, design assistant, EDA, Electric potential, Hybrid power systems, hybrid search, Large language models, LLM, Productivity, Question answering (information retrieval), RAG, retrieval augmented generation},
	pages = {1--5},
}

@inproceedings{lin_athena_2025,
	title = {Athena: {A} {GenAI}-{Powered} {Programming} {Tutor} {Based} on {Open}-{Source} {LLM}},
	doi = {10.1109/ICCT-Pacific63901.2025.11012850},
	abstract = {With the rapid growth of generative artificial intelligence (GenAI), it is important to find ways to utilize them for academic advantage. GenAI tools embody immense potential in providing personalized feedback to students any time anywhere, and hence can provide a reliable helping hand to teachers who often experience burnout in large classes and are burdened with administrative tasks. While current GenAI tools like ChatGPT are helpful, they occasionally offer misinformation - a phenomenon known as hallucination, undermine critical thinking by providing direct answers to questions, and, as paid services, can further widen the digital divide. Against the backdrop of these problems, this research introduces Athena, a GenAI programming mentor based on an open-source large language model (LLM), constructed to guide programming learners to think critically and provide reliable information leveraging retrieval augmented generation. Its impact on learning outcomes was measured by feedback from programming students. Most students have given a positive response, saying that their motivation to keep learning and their confidence in their abilities have increased. These results imply that having a reliable AI mentor that can guide students at all times can have a positive impact in self-directed learning process.},
	booktitle = {2025 1st {International} {Conference} on {Consumer} {Technology} ({ICCT}-{Pacific})},
	author = {Lin, Yadanar and Ferdous Khan, M. Fahim and Sakamura, Ken},
	month = mar,
	year = {2025},
	keywords = {Chatbots, Digital divide, Education, educational chatbot, Fake news, Generative AI, Generative artificial intelligence (GenAI), Hands, large language model (LLM), Large language models, programming education, Programming profession, Reliability, Retrieval augmented generation, retrieval augmented generation (RAG)},
	pages = {1--4},
}

@inproceedings{xiong_research_2025,
	title = {Research on {Document} {Layout} {Detection} and {Description} {Method} for {Retrieval}-{Augmented} {Generation} of {Large} {Language} {Model}},
	doi = {10.1109/ICRCA64997.2025.11011072},
	abstract = {Improve the accuracy of Retrieval-Augmented Generation (RAG) for Large Language Model (LLM), this paper proposes a document layout detection and description network model (Doc-LDNet) for RAG. Doc-LDNet can accomplish two tasks: one is to conduct document layout detection which can accurately identifying the positions and boundaries of different regions in the document, such as text blocks and tables. The other is to describe the images within the document by converting the image content into textual descriptions. Applying Doc-LDNet as a document parsing tool in RAG can effectively enrich the results of document parsing. The experimental results show that applying the Doc-LDNet in RAG can effectively improve the accuracy of LLM in professional domain knowledge question-answering.},
	booktitle = {2025 9th {International} {Conference} on {Robotics}, {Control} and {Automation} ({ICRCA})},
	author = {Xiong, Simiao and Ouyang, Caixiao},
	month = mar,
	year = {2025},
	keywords = {Accuracy, Automation, document detection, image description, Large language models, Layout, LLM, RAG, Retrieval augmented generation, Robots, Semantics},
	pages = {367--370},
}

@inproceedings{ugur_guided_2025,
	title = {Guided {Decoding} and {Its} {Critical} {Role} in {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/SIU66497.2025.11111950},
	abstract = {The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.},
	booktitle = {2025 33rd {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Uğur, Özgür and Yılmaz, Musa and Şavirdi, Esra and Ezerceli, Özay and El Huseyni, Mahmut and Taş, Selva and Bayraktar, Reyhan},
	month = jun,
	year = {2025},
	note = {ISSN: 2165-0608},
	keywords = {Accuracy, Automata, context-free grammar, Decoding, finite-state machines, Grammar, guided decoding, hallucination reduction, large language models, Large language models, lm format enforcer, outlines, Reliability theory, Retrieval augmented generation, retrieval-augmented generation, Signal processing, structured output, Usability, xgrammar},
	pages = {1--4},
}

@inproceedings{min_verification_2025,
	title = {Verification and {Validation} of {LLM}-{RAG} for {Industrial} {Automation}},
	doi = {10.1109/AITest66680.2025.00012},
	abstract = {Large Language Models (LLMs) have emerged as transformative tools in industrial automation, supporting decision-making from device-level control to enterprise-level coordination. When combined with Retrieval-Augmented Generation (RAG), these systems promise context-aware, domain-specific reasoning. However, their integration into mission-critical pipelines introduces considerable challenges to reliability, robustness, and continuous validation, particularly within Development and Operations (DevOps) and Continuous Integration/Continuous Delivery (CI/CD) environments. This work presents a comprehensive Verification and Validation (V\&V) framework designed to address these challenges holistically across the AI system lifecycle. At its core, the framework introduces a novel Continuum-Based Failure Classification (CBFC) model that redefines validation from a binary pass/fail paradigm to a graded assessment of correctness, consistency, and uncertainty. This assessment spans key dimensions such as retrieval relevance, factual accuracy, coherence, and alignment with user intent. The CBFC model integrates embedding-based similarity measures, logical entailment checks, and quality-oriented metrics (e.g., FactScore, ROUGE) to uncover failure modes often overlooked by conventional evaluation methods. The framework emphasizes reliability and adaptability through structured testing, latency optimization, proactive model retraining, and iterative feedback loops. It is demonstrated in an industrial Root-Cause Analyzer application, where it significantly improves system performance, interpretability, and trustworthiness. By advancing toward continuous, evidence-based evaluation, this approach enables the resilient deployment of retrieval-augmented large language model systems in dynamic, real-world industrial environments.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Artificial} {Intelligence} {Testing} ({AITest})},
	author = {Min, Ziran and Budnik, Christof J.},
	month = jul,
	year = {2025},
	note = {ISSN: 2835-3560},
	keywords = {Adaptation models, Automation, CI/CD, DevOps, Industrial Automation, Large language models, Large Language Models (LLMs), Pipelines, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Robustness, System performance, Testing, Uncertainty},
	pages = {50--53},
}

@inproceedings{phukon_localized_2024,
	title = {Localized {Open}-{Source} {LLM} {Aware} {Retrieval} {Augmented} {Generation} of {Legal} {Documents}: {A} {Case} {Study} on {Indian} {Constitution} and {Penal} {Code}},
	doi = {10.1109/BITCON63716.2024.10985396},
	abstract = {Large Language Models (LLMs) are advanced artificial intelligence systems which are designed to generate human-like language based on their training data. Retrieval- Augmented Generation (RAG) is a framework that enhances the response of LLM by incorporating external information sources, leading to more accurate, context-relevant responses. However, LLMs commonly struggle with contextual relevance, especially in particular fields such as legal documentation, where specific local laws are crucial. Furthermore, there is limited research on the use of RAG technology in the legal area, particularly for producing and analyzing legal documents. This study develops a chatbot using RAG to answer questions on the Indian Constitution (IC) and the Indian Penal Code (IPC), with a Streamlit-based interface to display legal queries and their generated responses. This study analyzes the evaluation of various LLMs integrated with RAG including (including (i) Llama3 8b, (ii) Mistral 7b, (iii) Gemma2 7b, (iv) Phi3 3.8b, and (v) Qwen2 7b and it evaluates their performance in terms of relevance, faithfulness, context recall, and context precision.},
	booktitle = {2024 {International} {BIT} {Conference} ({BITCON})},
	author = {Phukon, Pratiksha and Lokhar, Yogesh and Ray, Partha Pratim},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Chatbots, Codes, Documentation, IEEE Constitution, Large Language Model, Large language models, Law, Legal Documents, Localized Interface, Question-Answer Visualizer, Retrieval augmented generation, Retrieval-Augmented Generation, Training data},
	pages = {1--6},
}

@inproceedings{putri_simplification_2024,
	title = {Simplification of {Embedding} {Process} in {Retrieval} {Augmented} {Generation} for {Optimizing} {Question} {Answering} {Chatbot} {Model}},
	doi = {10.1109/COMNETSAT63286.2024.10862926},
	abstract = {In the current digital era, interaction through chatbots has become commonplace due to their ability to serve multiple users simultaneously. One of the widely used types of chatbots today is the Question Answering Chatbot (QAC). In this study, a QAC model is built using a Large Language Model (LLM) and the Retrieval Augmented Generation (RAG) framework. Compared to conventional generation models, RAG offers several advantages, such as strong scalability, easy data acquisition, and low training costs. The RAG process involves Indexing and Retrieval Generation stages. This research focuses on simplifying the embedding process in the indexing stage for efficient context management and response quality. This study utilizes GPT-3.5-Turbo as the LLM. The results obtained using GPT-3.5-Turbo indicate that the use of the Answer Only dataset and Prompt 2 provides the best response quality, with consecutive Faithfulness and Answer Relevancy metric values of 0.9167 and 0.9512, respectively. Subsequently, an experiment is conducted using the latest LLM, namely GPT-4o (omni), with Prompt 2 and the Question Only dataset to compare the two models. GPT-4o produces the best response quality with a Faithfulness metric value of {\textbackslash}mathbf0 . 9 5 5 1. Meanwhile, the best Answer Relevancy score is achieved by GPT-3.5-Turbo with a value of 0.9512.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Communication}, {Networks} and {Satellite} ({COMNETSAT})},
	author = {Putri, Mindi Richia and Husodo, Ario Yudo and Irmawati, Budi},
	month = nov,
	year = {2024},
	note = {ISSN: 2689-8004},
	keywords = {Chatbot, Chatbots, Costs, Embedding, Indexing, Large Language Model, Large language models, Measurement, Prompt, Question answering (information retrieval), Retrieval augmented generation, Retrieval Augmented Generation, Satellites, Scalability, Training},
	pages = {665--670},
}

@inproceedings{mestre_ragnar_2024,
	title = {{RAGNAR}: {Retrieval}-{Augmented} {Generation} using {Networked} and {Advanced} {Relational} {Data}},
	doi = {10.1109/ISAS64331.2024.10845598},
	abstract = {The technological evolution carried out in recent years has enabled significant developments in various areas of Artificial Intelligence (AI), such as Generative AI. Large Language Models (LLMs) are becoming increasingly complex, allowing for better results and enhancing their real-world applicability. However, these models still face issues such as hallucination or outdated information. This last one occurs due to the temporal gap between the training process and the model’s use. A Retrieval-Augmented Generation (RAG) architecture can address these issues since the information source used is not involved in the training phase, which also facilitates the reuse of models for different applications. One of the challenges of RAG is its applicability when the data source is a relational database, becoming even more challenging as the database size and complexity increase. This article proposes a potential architecture and approach for solving this problem and implementing a RAG architecture using a relational database as the data source.},
	booktitle = {2024 8th {International} {Symposium} on {Innovative} {Approaches} in {Smart} {Technologies} ({ISAS})},
	author = {Mestre, António and Marques, Ruben and Fernandes, Alexander and Silva, Bruno},
	month = dec,
	year = {2024},
	keywords = {Complexity theory, Faces, Generative AI, Large language models, LLM, RAG, Relational Database, Relational databases, Retrieval augmented generation, Soft sensors, Training},
	pages = {1--6},
}

@inproceedings{ayala_task_2025,
	title = {Task {Decomposition} and {RAG} as {Design} {Patterns} for {LLM}-{Based} {Systems}},
	doi = {10.1109/CAIN66642.2025.00049},
	abstract = {AI technologies are moving rapidly from research to production. Compared to traditional AI-based software, systems employing Large Language Models (LLMs) are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across LLM-based applications. While Task Decomposition and Retrieval-Augmented Generation (RAG) are well-known techniques, their formalization as design patterns for LLM-based systems benefits AI practitioners. These techniques should be considered not only from a scientific perspective, but also from the standpoint of desired software quality attributes such as safety and modularity. This will help bridge the gap between AI and software engineering as those fine-tuning or prompting LLMs will be aware of the impact that modern techniques have on the overall system.},
	booktitle = {2025 {IEEE}/{ACM} 4th {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Ayala, Orlando Marquez},
	month = apr,
	year = {2025},
	keywords = {Artificial intelligence, Best practices, design patterns, generative ai, Generative AI, Large language models, llms, Production, rag, Retrieval augmented generation, Safety, Software engineering, Software quality, Software systems, task decomposition},
	pages = {279--280},
}

@book{bourne_notitle_2024,
	isbn = {978-1-83588-791-2},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10769240},
	abstract = {Leverage cutting-edge generative AI techniques such as RAG to realize the potential of your data and drive innovation as well as gain strategic advantageKey FeaturesOptimize data retrieval and generation using vector databasesBoost decision-making and automate workflows with AI agentsOvercome common challenges in implementing real-world RAG systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative AI is helping organizations tap into their data in new ways, with retrieval-augmented generation (RAG) combining the strengths of large language models (LLMs) with internal data for more intelligent and relevant AI applications. The author harnesses his decade of ML experience in this book to equip you with the strategic insights and technical expertise needed when using RAG to drive transformative outcomes. The book explores RAG’s role in enhancing organizational operations by blending theoretical foundations with practical techniques. You’ll work with detailed coding examples using tools such as LangChain and Chroma’s vector database to gain hands-on experience in integrating RAG into AI systems. The chapters contain real-world case studies and sample applications that highlight RAG’s diverse use cases, from search engines to chatbots. You’ll learn proven methods for managing vector databases, optimizing data retrieval, effective prompt engineering, and quantitatively evaluating performance. The book also takes you through advanced integrations of RAG with cutting-edge AI agents and emerging non-LLM technologies. By the end of this book, you’ll be able to successfully deploy RAG in business settings, address common challenges, and push the boundaries of what’s possible with this revolutionary AI technique.What you will learnUnderstand RAG principles and their significance in generative AIIntegrate LLMs with internal data for enhanced operationsMaster vectorization, vector databases, and vector search techniquesDevelop skills in prompt engineering specific to RAG and design for precise AI responsesFamiliarize yourself with AI agents' roles in facilitating sophisticated RAG applicationsOvercome scalability, data quality, and integration issuesDiscover strategies for optimizing data retrieval and AI interpretabilityWho this book is forThis book is for AI researchers, data scientists, software developers, and business analysts looking to leverage RAG and generative AI to enhance data retrieval, improve AI accuracy, and drive innovation. It is particularly suited for anyone with a foundational understanding of AI who seeks practical, hands-on learning. The book offers real-world coding examples and strategies for implementing RAG effectively, making it accessible to both technical and non-technical audiences. A basic understanding of Python and Jupyter Notebooks is required.},
	publisher = {Packt Publishing},
	author = {Bourne, Keith and Es, Shahul},
	year = {2024},
	note = {Publication Title: Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG},
}

@inproceedings{zhao_research_2025,
	title = {Research on {Intelligent} {Question} {Answering} {System} for {Electric} {Power} {Domain} {Based} on {LLMs} and {RAG}},
	doi = {10.1109/CAIBDA65784.2025.11183015},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in conversation, reasoning, and knowledge retention. However, they still face challenges in handling knowledge-intensive tasks within the power domain, including insufficient factual accuracy, difficulties in knowledge updating, and a scarcity of high-quality domain-specific datasets. In order to address these challenges, this paper aims to design specifically a system for knowledge-intensive question answering tasks in the electric power domain. The proposed system incorporates an enhanced Retrieval-Augmented Generation (RAG) strategy that integrates a hybrid retrieval strategy and a finetuned model to provide more efficient knowledge acquisition and updating capabilities. Additionally, the system incorporates LLMbased prompt engineering techniques to enhance the coherence of responses to questions. Experimental results show that the question answering system effectively improves the accuracy of retrieving and answering power-related knowledge and reduces hallucinations. Furthermore, ablation experiments indicate that RAG module has the greatest impact on system performance, with the overall accuracy decreasing by 18.6 \% without this component.},
	booktitle = {2025 5th {International} {Conference} on {Artificial} {Intelligence}, {Big} {Data} and {Algorithms} ({CAIBDA})},
	author = {Zhao, Tao and Yang, Xiaojing and Gao, Jun and Pan, Haiyan},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Adaptation models, LLM, Model Fine-tuning, Power industry, Prompt engineering, Prompt Engineering, Question answering (information retrieval), Question Answering System, RAG, Real-time systems, Retrieval augmented generation, System performance, Technological innovation, Vectors},
	pages = {389--393},
}

@inproceedings{chan_optimizing_2025,
	title = {Optimizing {Confidence} {Scoring} in {RAG}-{Based} {LLM} {Chatbots} for {Technical} {Support} {Services}: {A} {Prompt} {Engineering} {Approach}},
	doi = {10.1109/ISCAIE64985.2025.11081158},
	abstract = {The global shortage of skilled personnel in technical support has spurred significant interest in AI-driven solutions, particularly Large Language Model (LLM)-based customer service chatbots. However, a critical challenge in deploying these systems lies in addressing AI hallucination, wherein models generate responses that are plausible yet factually incorrect. This study investigates a prompt engineering approach to enhance confidence estimation and mitigate AI hallucinations in LLM chatbots. Three distinct prompt strategies—Basic, Advanced, and Combo prompts–are systematically evaluated to improve response reliability. Given that LLMs inherently lack the ability to explicitly express uncertainty (e.g., by stating “I don't know”), a structured confidence scoring mechanism is employed to refine accuracy and reduce the Expected Calibration Error (ECE). Experimental results reveal that Basic prompts achieve an accuracy of 69.33\% (ECE: 23.33), Advanced prompts improve accuracy to 75.33\% (ECE: 14.87), and Combo prompts further enhance accuracy to 81.33\% while reducing ECE to 8.4. These findings underscore the efficacy of prompt engineering in mitigating AI hallucinations and advancing the performance of LLM chatbots in real-world customer support applications.},
	booktitle = {2025 {IEEE} 15th {Symposium} on {Computer} {Applications} \& {Industrial} {Electronics} ({ISCAIE})},
	author = {Chan, Wing Tung and Hung, Kevin and Ho, Raymond and Man-Tat Man, Gary},
	month = may,
	year = {2025},
	note = {ISSN: 2836-4317},
	keywords = {Accuracy, AI Hallucination, Chatbots, Confidence Score, Customer Support, Estimation, Industrial electronics, Large language models, LLM Chatbot, Personnel, Prompt engineering, Prompt Engineering, Refining, Reliability engineering, Uncertainty},
	pages = {540--545},
}

@inproceedings{guan_external_2024,
	title = {External {Knowledge} {Is} {Not} {Always} {Needed}: {An} {Adaptive} {Retrieval} {Augmented} {Generation} {Method}},
	doi = {10.1109/AIIM64537.2024.10934284},
	abstract = {Retrieval augmented generation (RAG), by integrating external knowledge with large language models (LLMs), has become a common practice to alleviate LLMs’ hallucination problem. The performance of RAG, however, depends on the capability of the adopted information retriever to a large extent. Specifically, a good information retriever can manage to obtain the most useful information from the external knowledge, which will then enhance the quality of generated content from LLMs and vice versa. Since the vanilla RAG will always leverage the retrieved information to assist with LLMs’ content generation despite of its usefulness, when combined with a retriever of limited capacity, therefore, the vanilla RAG will not benefit LLMs’ content generation sometimes but introduce additional noise, having a negative impact on the final performance. To address this problem, this paper proposes a novel adaptive RAG method which first uses LLMs to determine the usefulness of the retrieved information and then let LLMs to generate content on their own without the retrieved information if it is useless. Experimental results on four datasets demonstrate the effectiveness of the proposed adaptive RAG method with information retrievers of various capabilities, improving the performance of the vanilla RAG by an obvious margin.},
	booktitle = {2024 4th {International} {Symposium} on {Artificial} {Intelligence} and {Intelligent} {Manufacturing} ({AIIM})},
	author = {Guan, Wenbo and Lu, Jiyu and Feng, Qinyu and Li, Xiaoqian and Zhou, Jun},
	month = dec,
	year = {2024},
	keywords = {adaptive RAG method, hallucination, large language model, Large language models, Manufacturing, Noise, Retrieval augmented generation},
	pages = {882--886},
}

@inproceedings{kashyap_rag-em_2025,
	title = {{RAG}-{EM}: {Retrieval}-{Augmented} {Generation} for {Electromagnetic} {System} {Design}},
	doi = {10.1109/EMCSIPI52291.2025.11169861},
	abstract = {As large language models (LLMs) gain broader adoption, incorporating domain knowledge via proprietary data is of the utmost importance. However, data such as platform design guides (PDGs) have heavy restrictions in the design flow, meaning that the data must remain on-premise. Thus, using existing language models over the Internet is infeasible without compromising the data. This paper examines using local LLMs with domain knowledge, especially PDGs, with relevant design information and custom tool interfaces. In order to provide the LLM with domain-relevant knowledge, we use modified retrieval augmented generation (RAG). For domain-relevant knowledge, we provide the LLM with numerous PDGs and Python scripts that enable the control of an electromagnetic tool for PCB design. We demonstrate the ability of the model to perform domain-specific question/answering (QA) and generate code for manipulating 3D EM structures such as striplines and vias. We show the advantage of the on-prem RAG-based approach over finetuning by introducing new tasks/instructions.},
	booktitle = {2025 {IEEE} {International} {Symposium} on {Electromagnetic} {Compatibility}, {Signal} \& {Power} {Integrity} ({EMC}+{SIPI})},
	author = {Kashyap, Priyank and Rouf, Nirjhor and Choi, Yongjin and Franzon, Paul and Cheng, Chris},
	month = aug,
	year = {2025},
	note = {ISSN: 2158-1118},
	keywords = {Codes, Electromagnetic, Electromagnetics, Large language models, LLM, Microstrip, Natural languages, RAG, Retrieval augmented generation, Scattering parameters, Solid modeling, Stripline, Three-dimensional displays, Via},
	pages = {158--163},
}

@article{hindi_enhancing_2025,
	title = {Enhancing the {Precision} and {Interpretability} of {Retrieval}-{Augmented} {Generation} ({RAG}) in {Legal} {Technology}: {A} {Survey}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3550145},
	abstract = {Retrieval-Augmented Generation (RAG) is a promising solution that can enhance the capabilities of large language model (LLM) applications in critical domains, including legal technology, by retrieving knowledge from external databases. Implementing RAG pipelines requires careful attention to the techniques and methods implemented in the different stages of the RAG process. However, robust RAG can enhance LLM generation with faithfulness and few hallucinations in responses. In this paper, we discuss the application of RAG in the legal domain. First, we present an overview of the main RAG methods, stages, techniques, and applications in the legal domain. We then briefly discuss the different information retrieval models, processes, and applied methods in current legal RAG solutions. Then, we explain the different quantitative and qualitative evaluation metrics. We also describe several emerging datasets and benchmarks. We then discuss and assess the ethical and privacy considerations for legal RAG and summarize various challenges, and propose a challenge scale based on RAG failure points and control over external knowledge. Finally, we provide insights into promising future research to leverage RAG efficiently and effectively in the legal field.},
	journal = {IEEE Access},
	author = {Hindi, Mahd and Mohammed, Linda and Maaz, Ommama and Alwarafy, Abdulmalik},
	year = {2025},
	keywords = {Accuracy, Complexity theory, Generators, Information retrieval, Knowledge graphs, large language model (LLM), Large language models, Law, legal technology, Pipelines, prompt engineering, Retrieval augmented generation, retrieval-augmented generation (RAG), Surveys, Transformers},
	pages = {46171--46189},
}

@inproceedings{hachicha_recommendation_2024,
	title = {Recommendation {Module} {Based} on {Web} {Scraping} and {LLM} {Models} with the {RAG} {Technique}},
	doi = {10.1109/AIR63653.2024.00025},
	abstract = {In this research, we investigate the implementation of Retrieval-Augmented Generation (RAG) technology to recommend suitable engineering internships to individuals who are looking for an internship based on their CVs by improving the search for correspondence between candidates and employers, ensuring that both parties find the best adequacy. This innovative approach leverages web scraping, data preprocessing and advanced machine learning techniques including vector databases designed to handle high-dimentional vector data allowing for searches based on the proximity or similarity of vectors, and embedding methods used to convert high-dimentional data into vector representations that can be stored in these vector databases. By analyzing and understanding the content of each CV and internship offer, our methodology aims to deliver personalized and relevant internship opportunities, enhancing candidate-employer matching.},
	booktitle = {2024 {Artificial} {Intelligence} {Revolutions} ({AIR})},
	author = {Hachicha, Mohamed and Omezine, Mohamed Aziz and Zghal, Mohamed Khalil and Riahi, Mohamed Hedi and Ncib, Lotfi},
	month = oct,
	year = {2024},
	keywords = {CVs, Data models, Data preprocessing, Databases, LLM, Machine learning, machine learning techniques, recom-mend internships, Recommender systems, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Scalability, Soft sensors, Testing, Vectors, web scraping},
	pages = {68--73},
}

@inproceedings{chen_adaptive_2025,
	title = {Adaptive {Retrieval}-{Augmented} {Embodied} {Intelligence} {RAG} {Reasoning} {Framework}},
	doi = {10.1109/SEAI65851.2025.11108876},
	abstract = {Large Language Models (LLMs) have demonstrated significant potential in task reasoning for embodied intelligence. However, when confronted with unfamiliar scenarios, these models frequently generate fabricated or erroneous reasoning, a phenomenon known as “hallucination”, which limits their applicability in real-world tasks. In response to this challenge, this paper proposes an embodied task reasoning method based on adaptive Retrieval-Augmented Generation (RAG) technology. Firstly, we store real-world environmental information in a vector knowledge graph, facilitating graph retrieval and reasoning generation. Secondly, when graph retrieval information is insufficient, we employ an adaptive web retriever constructed using LLMs and web search interfaces, which dynamically enhances reasoning generation through a “retrieve-reason-evaluate” loop framework. Finally, new long-term knowledge is extracted and stored in the knowledge graph via an active memory module, improving retrieval efficiency for subsequent similar tasks. Experimental results demonstrate that this method effectively reduces the hallucination phenomenon during the reasoning process, optimizes the task reasoning workflow, and provides a scalable framework for reliable knowledge acquisition and dynamic reasoning in embodied intelligence.},
	booktitle = {2025 {IEEE} 5th {International} {Conference} on {Software} {Engineering} and {Artificial} {Intelligence} ({SEAI})},
	author = {Chen, Mengmeng and Bai, Yuanyuan and Fang, Xiaoyong and Yi, Pengfei},
	month = jun,
	year = {2025},
	keywords = {Adaptive systems, Cognition, Embodied Intelligence, Hallucination Phenomenon, Knowledge graphs, Large language model, Large language models, Memory modules, Real-time systems, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Vectors, Web search},
	pages = {230--234},
}

@inproceedings{fahmi_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Pharmacopoeia}: {Application} and {Evaluation}},
	doi = {10.1109/AIxDKE63520.2024.00032},
	abstract = {Pharmacopoeia documents are used in pharmaceutical companies as references for ensure safe handling of chemical compounds. However, manual review of these documents can be time-consuming and prone to human error. To assist users in working with these documents, we designed PharmChat, a retrieval-augmented generation (RAG)-based system which enables efficient question-and-answer interactions with multiple short pharmacopoeia documents using natural language. In contrast to other conventional RAG systems, PharmChat converts original pharmacopoeia into separate documents to avoid context mixing during document chunking. PharmChat leverages embedding-based similarity searches between the documents as knowledge-base and user queries using an open-source vector database, ChromaDB. Upon receiving a user query, the most relevant chunks are retrieved and provided as context for a large language model (LLM) to be used as reference to minimize hallucination. Experimental results with real pharmacopoeia short documents demonstrate that our PharmChat system provides quick response times and achieves a BERT-F1 score of 76\%.},
	booktitle = {2024 {International} {Conference} on {AI} x {Data} and {Knowledge} {Engineering} ({AIxDKE})},
	author = {Fahmi, Rizaldi and Cheon, Soojin and Park, Yesol and Kwon, Joonho},
	month = dec,
	year = {2024},
	note = {ISSN: 2831-7203},
	keywords = {Databases, document search, Knowledge engineering, large language model, Large language models, Manuals, Natural languages, Pharmaceuticals, Retrieval augmented generation, retrieval-augmented generation, Reviews, Time factors, Vectors},
	pages = {127--128},
}

@article{collini_context-aware_2025,
	title = {Context-{Aware} {Retrieval} {Augmented} {Generation} {Using} {Similarity} {Validation} to {Handle} {Context} {Inconsistencies} in {Large} {Language} {Models}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3614553},
	abstract = {Large Language Models (LLMs) have transformed natural language processing by offering human-like responses. However, issues such as incorrect information (hallucinations) and errors in specific subject areas remain, especially in Retrieval Augmented Generation (RAG) systems. This study introduces a Context-Aware Retrieval Augmented Generation (CA-RAG), which simplifies the process by removing the need to separately find relevant chunks of a document. Instead, after dividing the document into chunks, both the question and chunks are directly given to the LLMs to produce answers. The method then focuses on improving the answers through additional post-processing, aiming to reduce errors and make the answers more relevant to the question. To evaluate the effectiveness of CA-RAG, two scenarios have been designed. Scenario 1 involved experiments using widely adopted and recognized benchmark datasets, such as TriviaQA, Natural Questions, AmbigQA and Stanford Question Answering Dataset (SQuAD). In this context, the proposed CA-RAG method, combined with similarity measure (either cosine similarity or dot product) between generated answers and chunks, achieved the highest F1-score in TriviaQA and AmbigQA. Scenario 2 tested CA-RAG robustness using a custom dataset comprising domain-specific and unstructured documents. Results from automated and manual evaluations revealed that CA-RAG with post-processing consistently outperformed traditional RAG. These findings highlight the critical role of post-processing techniques and similarity measures in improving the accuracy and relevance of generated answers. CA-RAG demonstrates strong potential as a reliable and versatile solution for Retrieval Augmented Generation tasks across diverse datasets and domains.},
	journal = {IEEE Access},
	author = {Collini, Enrico and Indra Kurniadi, Felix and Nesi, Paolo and Pantaleo, Gianni},
	year = {2025},
	keywords = {Accuracy, Context-aware retrieval augmented generation (CA-RAG), cosine similarity, Costs, domain-specific tasks, dot product, Large language models, large language models (LLMs), Retrieval augmented generation, retrieval-augmented generation (RAG), Robustness, Semantics, Strips, Training, Training data, Vectors},
	pages = {170065--170080},
}

@inproceedings{zhao_empirical_2024,
	title = {An {Empirical} {Study} of {Retrieval} {Augmented} {Generation} with {Chain}-of-{Thought}},
	doi = {10.1109/ISCSLP63861.2024.10800207},
	abstract = {Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.},
	booktitle = {2024 {IEEE} 14th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing} ({ISCSLP})},
	author = {Zhao, Yuetong and Cao, Hongyu and Zhao, Xianyu and Ou, Zhijian},
	month = nov,
	year = {2024},
	keywords = {chain-of-thought, Chatbots, Cognition, Data mining, Faces, generative dialogue model, Information retrieval, large language model, Noise, Performance gain, retrieval augmented generation, Retrieval augmented generation, Training},
	pages = {436--440},
}

@inproceedings{bhiwgade_integrating_2025,
	title = {Integrating {Open}-{Source} {LLMs} with {Retrieval}-{Augmented} {Generation} for {Obstetrics} and {Gynecology} {Domain}},
	volume = {1},
	doi = {10.1109/NGISE64126.2025.11085188},
	abstract = {In this research paper, we address the integration of Large Language Models (LLM) with Retrieval Augmented Generation (RAG) to enhance clinical decision support and address patient doubts in the Obstetrics and Gynecology (OBGYN) domain. The research mainly tries to explore on How open source LLM’s can effectively retrieve and generate the relevant responses in the OBGYN domain. The research methodology includes two components: Data Ingestion, which reads the input text data and stores it to a vector database in an embedded format and Data Retriever-Generation, which retrieves the relevant information from the vector database and use it for accurate response generation. The method includes data collection from esteemed medical databases, LLM model selection, integration with RAG and evaluation of the generated outputs. The methodology uses Bio-Mistral 7B fine-tuned LLM with PubMed Bert embeddings. The LLM responses are evaluated using the Ragas framework, context precision and context recall to measure the performance of retrieval system, faithfulness to measure hallucinations and answer relevancy to measure how relevant the answers are to the input query. The evaluated results confirm that the research has improved the accuracy as well as the contextual relevancy of the information in the OBGYN domain. This research provides a robust architecture for integration of Artificial Intelligence for supporting clinical decision making and information retrieval in specialized medical domains. This research can be extended with the upcoming advancements in the field of Artificial Intelligence and Data Science.},
	booktitle = {2025 {International} {Conference} on {Next} {Generation} {Information} {System} {Engineering} ({NGISE})},
	author = {Bhiwgade, Akash Wamanrao and Nagrale, Nilesh},
	month = mar,
	year = {2025},
	keywords = {Accuracy, AI in Healthcare, Clinical Decision Support, Data models, Databases, Gynecology, Information retrieval, Information Retrieval, Knowledge based systems, Large language models, Large Language Models, Machine Learning, Natural Language Processing, OBGYN, Obstetrics, Retrieval augmented generation, Retrieval Augmented Generation, Text Generation, Vectors, Women’s Health},
	pages = {1--5},
}

@inproceedings{bhiwgade_integrating_2025-1,
	title = {Integrating {Open}-{Source} {LLMs} with {Retrieval}-{Augmented} {Generation} for {Obstetrics} and {Gynecology} {Domain}},
	doi = {10.1109/SCEECS64059.2025.10940324},
	abstract = {In this research paper, we address the integration of Large Language Models (LLM) with Retrieval Augmented Generation (RAG) to enhance clinical decision support and address patient doubts in the Obstetrics and Gynecology (OBGYN) domain. The research mainly tries to explore on How open source LLM’s can effectively retrieve and generate the relevant responses in the OBGYN domain. The research methodology includes two components: Data Ingestion, which reads the input text data and stores it to a vector database in an embedded format and Data Retriever-Generation, which retrieves the relevant information from the vector database and use it for accurate response generation. The method includes data collection from esteemed medical databases, LLM model selection, integration with RAG and evaluation of the generated outputs. The methodology uses Bio-Mistral 7B fine-tuned LLM with PubMed Bert embeddings. The LLM responses are evaluated using the Ragas framework, context precision and context recall to measure the performance of retrieval system, faithfulness to measure hallucinations and answer relevancy to measure how relevant the answers are to the input query. The evaluated results confirm that the research has improved the accuracy as well as the contextual relevancy of the information in the OBGYN domain. This research provides a robust architecture for integration of Artificial Intelligence for supporting clinical decision making and information retrieval in specialized medical domains. This research can be extended with the upcoming advancements in the field of Artificial Intelligence and Data Science.},
	booktitle = {2025 {IEEE} {International} {Students}' {Conference} on {Electrical}, {Electronics} and {Computer} {Science} ({SCEECS})},
	author = {Bhiwgade, Akash Wamanrao and Nagrale, Nilesh and Patil Bedekar, Pragati and Sheikh, Sayara Bano},
	month = jan,
	year = {2025},
	note = {ISSN: 2688-0288},
	keywords = {Accuracy, AI in Healthcare, Clinical Decision Support, Computational modeling, Data models, Databases, Information retrieval, Information Retrieval, Knowledge based systems, Large language models, Large Language Models, Machine Learning, Natural Language Processing, OBGYN, Obstetrics, Retrieval augmented generation, Retrieval Augmented Generation, Text Generation, Vectors, Women’s Health},
	pages = {1--5},
}

@article{hu_icca-rag_2025,
	title = {{ICCA}-{RAG}: {Intelligent} {Customs} {Clearance} {Assistant} {Using} {Retrieval}-{Augmented} {Generation} ({RAG})},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3544408},
	abstract = {Document processing and query generation tasks in customs declaration scenarios face key challenges such as the complexity of multimodal data, adaptability to dynamic regulations, and ambiguity in query semantics. This study proposes a Retrieval-Augmented Generation system (ICCA-RAG) that addresses the core issues of processing complex customs documents and dynamically generating queries through multimodal document parsing, sparse-dense hybrid storage, and context-driven large language model generation. In terms of multimodal document parsing, the system supports comprehensive parsing of PDFs, images, tables, and text, which are uniformly transformed into semantic vectors and keyword indices for hybrid storage. By combining the retrieval and generation modules, the ICCA-RAG system achieves significant improvements in contextual relevance and generation accuracy. Compared to traditional methods, the ICCA-RAG system demonstrates a 20.1\% increase in answer correctness, a 15.3\% increase in answer relevancy, and an 18.7\% increase in the faithfulness of generated content, with outstanding performance in noisy query scenarios. The research findings validate the ICCA-RAG system’s advancement and applicability in handling complex document processing and professional domain question-answering tasks, while also providing a transferable technical framework for other fields, such as law and healthcare.},
	journal = {IEEE Access},
	author = {Hu, Rong and Liu, Sen and Qi, Panpan and Liu, Jingyi and Li, Fengyuan},
	year = {2025},
	keywords = {Accuracy, Codes, Companies, Complexity theory, Customs declaration assistance, dynamic regulation adaptation, Inspection, intelligent question-answering system, large language model (LLM), Large language models, multimodal document parsing, Retrieval augmented generation, retrieval-augmented generation (RAG), semantic retrieval, Semantics, Tariffs, Vectors},
	pages = {39711--39726},
}

@inproceedings{zheng_incorporating_2025,
	title = {Incorporating {RAG} for {Factual} {Hallucination} {Detection} {Modeling} in {Intelligent} {Educational} system},
	doi = {10.1109/CSTE64638.2025.11092000},
	abstract = {Factual hallucination is one of the key factors affecting learning effectiveness and knowledge construction in intelligent education system, and reducing factual hallucination can help develop learners' critical thinking and accurate knowledge acquisition. How to effectively detect factual hallucination in intelligent education system is a challenging issue in educational technology research. Based on this, this study proposed a factual hallucination detection model that incorporates knowledge retrieval enhancement and fine-tuning of large language model. First, an educational knowledge vector base is constructed from the collected knowledge datasets in the educational domain; then, a factual illusion detection model is constructed by fusing the retrieval augmented generation(RAG ) technique and Low-Rank Adaptation of Large Language Models(LoRA) fine-tuning method to optimize the large language model. We conducted experiments on a real dataset in the education domain, and the results showed that this experimental model outperforms the existing models in terms of precision, recall, and F1 score for factual hallucination detection based on knowledge retrieval enhancement and model fine-tuning. Therefore, the experimental model in this paper is able to effectively detect factitious hallucination in intelligent education system, which lays the foundation for providing high-quality educational content.},
	booktitle = {2025 7th {International} {Conference} on {Computer} {Science} and {Technologies} in {Education} ({CSTE})},
	author = {Zheng, Lei and Ye, Junmin and Zhao, Gang and Luo, Sheng and Nan, Mengting and Xie, Yiliang},
	month = apr,
	year = {2025},
	keywords = {Accuracy, Adaptation models, Computational modeling, Computer science, Educational technology, factual hallucination, intelligent educational system, Knowledge acquisition, Large language models, RAG, Retrieval augmented generation, Vectors},
	pages = {843--847},
}

@book{gheorghiu_notitle_2024,
	isbn = {978-1-80512-440-5},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10540158},
	abstract = {Solve real-world problems easily with artificial intelligence (AI) using the LlamaIndex data framework to enhance your LLM-based Python applications Key FeaturesExamine text chunking effects on RAG workflows and understand security in RAG app developmentDiscover chatbots and agents and learn how to build complex conversation enginesBuild as you learn by applying the knowledge you gain to a hands-on projectBook DescriptionDiscover the immense potential of Generative AI and Large Language Models (LLMs) with this comprehensive guide. Learn to overcome LLM limitations, such as contextual memory constraints, prompt size issues, real-time data gaps, and occasional ‘hallucinations’. Follow practical examples to personalize and launch your LlamaIndex projects, mastering skills in ingesting, indexing, querying, and connecting dynamic knowledge bases. From fundamental LLM concepts to LlamaIndex deployment and customization, this book provides a holistic grasp of LlamaIndex's capabilities and applications. By the end, you'll be able to resolve LLM challenges and build interactive AI-driven applications using best practices in prompt engineering and troubleshooting Generative AI projects.What you will learnUnderstand the LlamaIndex ecosystem and common use casesMaster techniques to ingest and parse data from various sources into LlamaIndexDiscover how to create optimized indexes tailored to your use casesUnderstand how to query LlamaIndex effectively and interpret responsesBuild an end-to-end interactive web application with LlamaIndex, Python, and StreamlitCustomize a LlamaIndex configuration based on your project needsPredict costs and deal with potential privacy issuesDeploy LlamaIndex applications that others can useWho this book is forThis book is for Python developers with basic knowledge of natural language processing (NLP) and LLMs looking to build interactive LLM applications. Experienced developers and conversational AI developers will also benefit from the advanced techniques covered in the book to fully unleash the capabilities of the framework.},
	publisher = {Packt Publishing},
	author = {Gheorghiu, Andrei},
	year = {2024},
	note = {Publication Title: Building Data-Driven Applications with LlamaIndex: A practical guide to retrieval-augmented generation (RAG) to enhance LLM applications},
}

@inproceedings{chen_lire_2024,
	title = {{LiRe}: {Efficient} {Query} {Rewriting} for {Retrieval} {Augmented} {Generation} {Systems}},
	doi = {10.1109/CEI63587.2024.10871553},
	abstract = {With the continuous advancement of information retrieval technologies, query rewriting has become a key technique for enhancing retrieval effectiveness. In the context of the widespread application of large language models and retrieval-augmented generation technologies, high-quality query rewriting plays a crucial role in tasks like open-domain question answering. Conventional Conversational Query Rewriting (CQR) methods often prioritize human-friendly query formulation, which may not consistently produce optimal retriever-friendly results. To address this limitation, we present LiRe, a lightweight and training-free query rewriting system. LiRe employs a retrieval-oriented strategy to refine queries, optimizing them for retrievers. This approach allows for seamless integration into diverse Retrieval-Augmented Generation (RAG) systems. Our approach involves problem categorization, keyword augmentation from contextual information, and integration with the filtering process. LiRe-enhanced queries demonstrate superior performance compared to existing CQR methods, as evidenced by experimental outcomes.},
	booktitle = {2024 4th {International} {Conference} on {Computer} {Science}, {Electronic} {Information} {Engineering} and {Intelligent} {Control} {Technology} ({CEI})},
	author = {Chen, Chengfeng and Huang, Xiaodong and Xu, Yangzhen and Lin, Runfeng and Li, Gangliang and Liu, Shouqiang},
	month = nov,
	year = {2024},
	keywords = {Accuracy, Agent, Computer science, ConQA, CQR, Filtering, Hallucinations, Information retrieval, Intelligent control, Large language models, LLM, Question answering (information retrieval), RAG, Retrieval augmented generation, Robustness},
	pages = {27--31},
}

@inproceedings{ko_rerag_2024,
	title = {{ReRag}: {A} {New} {Architecture} for {Reducing} the {Hallucination} by {Retrieval}- {Augmented} {Generation}},
	doi = {10.1109/UBMK63289.2024.10773428},
	abstract = {As the application areas of Generative AI models become widespread, new problems arise. One of these problems is the “hallucination” observed in Large Language Models (LLM). Recently, Retrieval-Augmented Generation systems (RAG) have been introduced to cope with this problem. In this paper, we propose a new method, called “ReRag for Retrieval Augmented Generation”, to reduce the hallucination effect of LLM responses. This method assumes that a small dataset of queries and their man-made ideal responses are available for a specific application domain. It attempts to optimize the hyperparameters of the vector database in the RAG system by generating close-to-ideal responses concerning this ideal dataset, in which the questions and the ideal answers are provided. ReRag measures the similarity between the generated and ideal responses and then modifies the hyperparameters iteratively to increase the similarity between these two responses. Experimental results show that the ReRag method reduces the hallucination problem and generates relatively close to ideal responses compared to raw models.},
	booktitle = {2024 9th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Ko÷, Robin and Gürkan, Mustafa Kağan and Yarman Vural, Fatoş T.},
	month = oct,
	year = {2024},
	note = {ISSN: 2521-1641},
	keywords = {Computational modeling, Computer architecture, Computer science, Databases, Generative AI, infor-mation retrieval, Large language models, Large Language Models, Retrieval-Augmented Generation systems, Standards, Vectors},
	pages = {961--965},
}

@inproceedings{aboulela_exploring_2025,
	title = {Exploring {RAG} {Solutions} to {Reduce} {Hallucinations} in {LLMs}},
	doi = {10.1109/SysCon64521.2025.11014810},
	abstract = {Large Language Models (LLMs) often face challenges in generating accurate and reliable information, particularly in knowledge-intensive tasks. This limitation, referred to as hallucination, occurs when models produce content that is incorrect, irrelevant, or unsupported by evidence. Retrieval Augmented Generation (RAG) solutions provide a promising approach by integrating relevant external knowledge, enabling models to generate factually grounded responses. This study evaluates the performance of a base LLM model, a fine-tuned DistilBERT model, and two RAG architectures, Naïve RAG and Graph RAG, to study their impact on reducing hallucinations and enhancing contextual understanding. Using subsets of HaluEval, Squad-V2, and TriviaQA benchmark datasets, the base model achieved accuracies of 10.18\%, 12.67\%, and 5.46\% respectively; Naive RAG resulted in 44.56\%, 19.04\%, and 35.32\% accuracies; while the fine-tuned LLM model's accuracies were 72.5\%, 72.31\%, and 88.7\% respectively. Graph RAG resulted in 8.85\% and 15.12\% accuracies using Squad-V2 and TriviaQA, respectively. Our findings show that while fine-tuned LLMs outperform baseline models, incorporating RAG solutions did not result in significant performance improvements, suggesting that the incorporation of external knowledge may not always align with the needs of the task. Experiments demonstrate that Graph RAG handles complex queries by leveraging relationships within structured knowledge graphs. Data organized as a knowledge graph may enable Graph RAG solutions reach their full potential by utilizing their capacity to efficiently retrieve contextually relevant information. Although computing complexity remains a restriction, this study shows that RAG topologies might not consistently enhance LLM reliability in practical situations.},
	booktitle = {2025 {IEEE} {International} systems {Conference} ({SysCon})},
	author = {AboulEla, Samar and Zabihitari, Paria and Ibrahim, Nourhan and Afshar, Majid and Kashef, Rasha},
	month = apr,
	year = {2025},
	note = {ISSN: 2472-9647},
	keywords = {Accuracy, Context modeling, Graph RAG, Hallucination, Knowledge graphs, Large language models, LLM, Memory, Naive RAG, Optimization, RAG solutions, Reliability, Retrieval augmented generation, Scalability, Topology},
	pages = {1--8},
}

@inproceedings{su_leveraging_2025,
	title = {Leveraging {Knowledge} {Graph} and {Large} {Language} {Model} {Synergies} for {Intelligent} {Fault} {Analysis} in {Urban} {Rail} {Transit} {Signaling} {Systems}},
	doi = {10.1109/FASTA65681.2025.11138748},
	abstract = {To address the challenges of insufficient text data utilization and low manual diagnosis efficiency in urban rail transit signaling system fault analysis, particularly the difficulty in handling novel faults under unmanned train operations, this study proposes a dual-engine framework integrating Knowledge Graph (KG) and Large Language Model (LLM). Based on preprocessed fault diagnosis logs from Nanjing Metro's signaling system, a multi-dimensionally annotated dataset was constructed. The CasRel joint extraction model was employed to overcome the limitations of traditional pipeline methods in recognizing nested terms and mitigating error propagation, building a high-density domain-specific KG. Subsequently, a three-tiered question-answering paradigm (fact query, enumeration verification, causal verification) and multi-granularity intent classification system were innovatively integrated to enable semantic retrieval and dynamic reasoning over the KG. To tackle data dependence and logical hallucinations in vertical domain LLMs, a domain-adapted LLM was developed using QLoRA parameter-efficient fine-tuning technology, complemented by a zero-shot autonomous reasoning mechanism based on prompt engineering. By deconstructing signal transmission logic, cross-system association rules, and device temporal relationships, causal chain prompts, analogical inference templates, and dynamic temporal context cues were generated to guide the LLM in performing root cause deduction along domain knowledge-constrained pathways without historical data. Concurrently, the reasoning logic underwent real-time verification leveraging the KG's security protocol rule library to filter domain-violating hypotheses. Graph Retrieval-Augmented Generation (GraphRAG) technology further embedded KG retrieval results and dynamic prompts into the LLM's input, forming a closed-loop “knowledge anchoring, prompt guidance, verification and generation” decision process. Experimental results demonstrate that the framework significantly enhances novel fault diagnosis accuracy and reliability, achieving security-compliant root cause localization in zero-shot scenarios while outperforming traditional methods. It autonomously transforms new fault resolutions into KG nodes, triggering LLM incremental learning to enable continuous evolution of the knowledge system. This research provides an interpretable and scalable diagnostic decision-support paradigm for intelligent rail transit operations, offering technical substantiation for deploying industrial LLMs in safety-critical systems.},
	booktitle = {2025 4th {Conference} on {Fully} {Actuated} {System} {Theory} and {Applications} ({FASTA})},
	author = {Su, Tianfeng and Ma, Chenjing and Chen, Xin and Wang, Xiaohan},
	month = jul,
	year = {2025},
	keywords = {Cognition, Communication system signaling, Fault Analysis, Fault diagnosis, Graph Retrieval-Augmented Generation, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Logic, Prompt engineering, Prompt Engineering, Rails, Retrieval augmented generation, Signaling Systems, Transforms, Urban Rail Transit},
	pages = {1590--1595},
}

@inproceedings{donvir_leveraging_2025,
	title = {Leveraging {RAG} for {Enhanced} {Business} {Intelligence} with {Local} {LLMs}},
	doi = {10.1109/AIIoT65859.2025.11105349},
	abstract = {This research addresses critical limitations in current business intelligence (BI) insights generation. It focuses on real-time data updates, accuracy, data privacy, security and more to build a system that is practically viable for modern-day businesses. Leveraging the proposed solution gives organizations a competitive edge in the fast-paced market. Research leverages emerging Large Language Models (LLMs) to derive relevant actionable insights while eliminating typical hallucinations. The research proposes system architecture that combines Retrieval-Augmented Generation (RAG) technology with local LLMs and real-time data streaming via Kafka to ensure data privacy, factual accuracy, and timely insights. Research presents quantitative and qualitative analysis for insights generation based on RAG vs. Direct LLM request. Moreover, research is executed on two different families of LLMs - thinking models and traditional models for detailed validation. Empirical testing reveals that RAG-based approaches outperform direct LLM queries in response time, CPU efficiency, and factual accuracy for insights generation. By leveraging local LLMs instead of cloud-based solutions, research aims to protect data and intellectual property of organizations.},
	booktitle = {2025 {IEEE} {World} {AI} {IoT} {Congress} ({AIIoT})},
	author = {Donvir, Anujkumarsinh and Yadav, Priti and Panyam, Sriram and Joshi, Ram},
	month = may,
	year = {2025},
	keywords = {Accuracy, Business intelligence, Business Intelligence (BI), ChromaDB, Data privacy, Data Privacy, DeepSeek-R1, Gemma, Hallucination Mitigation, Kafka, Large language models, Large Language Models, Local LLMs, Natural Language Processing, Organizations, Real-time Analytics, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Security, Time factors, Vector Embeddings, Vectors},
	pages = {0655--0661},
}

@inproceedings{xiao_arag_2025,
	title = {{ARAG}: {Analysis} and {Retrieval} {Augmented} {Generation} for {Comprehensive} {Reasoning} over {Socioeconomic} {Data}},
	doi = {10.1109/ICDE65448.2025.00368},
	abstract = {Recent advancements in Large Language Models (LLMs) have significantly impacted the field of question answering systems, particularly with LLM-based data analysis and Retrieval-Augmented Generation (RAG). Yet, applying them independently has limited their effectiveness in scenarios that require a synthesis of both data analysis and contemporary information retrieval. To bridge this gap, we introduce the Analysis and Retrieval Augmented Generation (ARAG) framework, which integrates data analysis with the retrieval of up-to-date information. Based on the framework, we build a system to showcase how ARAG interprets the dynamics of socioeconomic indicators by examining correlated data and retrieving relevant information from news sources. The comparison of ARAG with ChatGPT Search and Perplexity showed that ARAG significantly outperformed them in delivering indepth analytical insights. Moreover, ARAG is observed to have a stronger ability to verify facts and reject misinformation in users' queries, thus reducing LLM's susceptibility to hallucination.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Xiao, Yixiong and Cao, Jingjia and Jiang, Yangxin and Zhou, Jingbo},
	month = may,
	year = {2025},
	note = {ISSN: 2375-026X},
	keywords = {Chatbots, Cognition, data analysis, Data analysis, Data engineering, Fake news, Information retrieval, Large language models, LLM, Question answering (information retrieval), RAG, Retrieval augmented generation, Socioeconomics},
	pages = {4624--4627},
}

@inproceedings{ding_sc-rag-cot_2025,
	title = {{SC}-{RAG}-{CoT}: {An} {Optimization} {Method} for {Robotic} {Instruction} {Generation}},
	doi = {10.1109/MRAI65197.2025.11135574},
	abstract = {Large Language Models (LLMs) based robotic instruction generation and control methods provide flexible and efficient solutions for autonomous robot decision-making with natural language interaction and task generalization. However, traditional LLMs struggle to adapt to the dynamic physical environments and address the ambiguity of natural language instructions. Therefore, a mechanism that integrates physical environment information and guides users to clarify specific needs through human-computer interaction is required. To this end, we propose the SC-RAG-CoT framework, which combines spatially constrained retrieval augmented generation (SC-RAG) and Chain of Thought (CoT) interaction for robotic instruction generation. SC-RAG-CoT employs SC-RAG to retrieve contextual information from spatial knowledge bases (e.g., spatial data, object attributes) to mitigate hallucinations caused by environmental dynamics. Additionally, CoT decomposes ambiguous instructions into multi-step reasoning chains. The reliability of instruction generation is improved through explicit verification of intermediate logic, and users are guided to gradually clarify their needs through interactive mechanisms. Simulation and ablation experiments demonstrate that the proposed SC-RAG-CoT strategy reduces average communication rounds by 2.6 and failure rates by 25.3 \% compared to traditional LLM-based methods in standardized test scenarios.},
	booktitle = {2025 {International} {Conference} on {Mechatronics}, {Robotics}, and {Artificial} {Intelligence} ({MRAI})},
	author = {Ding, Chen and Li, Cheng and Wu, Guiling and Qian, Liang and Liu, Ruifeng and Cai, Xinhao},
	month = jun,
	year = {2025},
	keywords = {chain of thought, Cognition, instruction disambiguation, Knowledge based systems, large language model, Large language models, Natural languages, Optimization methods, retrieval augmented generation, Retrieval augmented generation, robot control, Robot control, Robot sensing systems, Robustness, Spatial databases},
	pages = {403--407},
}

@article{saha_quim-rag_2024,
	title = {{QuIM}-{RAG}: {Advancing} {Retrieval}-{Augmented} {Generation} {With} {Inverted} {Question} {Matching} for {Enhanced} {QA} {Performance}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3513155},
	abstract = {This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics.},
	journal = {IEEE Access},
	author = {Saha, Binita and Saha, Utsha and Zubair Malik, Muhammad},
	year = {2024},
	keywords = {Accuracy, ChatGPT, Computational modeling, Data models, Faces, GPT-3.5-turbo, hallucination mitigation, Indexes, information dilution, Large language models, Large language models (LLMs), Meta-LLaMA3-8B-instruct, Prototypes, Question answering (information retrieval), question answering (QA), Reliability, retrieval-augmented generation (RAG), Vectors},
	pages = {185401--185410},
}

@inproceedings{nishikawa_enhancing_2024,
	title = {Enhancing {Source} {Code} {Comment} {Generation} via {Retrieval}-{Augmented} {Generation} with {Design} {Document} {Term} {Dictionary}},
	doi = {10.1109/APSEC65559.2024.00061},
	abstract = {Effective software development depends on clear code comments for better understanding. We introduce a method for generating automated source-code comments using retrieval-augmented generation (RAG) with a design document term dictionary. This method aligns terms and their meanings from design documents with lines of source code, producing comments that clearly reflect the source-code's intent and functionality. Our evaluations on the open-source software iDempiere show significant improvements: context precision increased by 22\% and faithfulness by 17 \% compared with conventional RAG methods These results confirm our method's validity. Therefore, we plan to explore its application to different software contexts in future work.},
	booktitle = {2024 31st {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Nishikawa, Kazu and Koreki, Genta and Kanuka, Hideyuki},
	month = dec,
	year = {2024},
	note = {ISSN: 2640-0715},
	keywords = {Accuracy, Codes, Dictionaries, Fitting, Large Language Model, Large language models, Open source software, Retrieval augmented generation, Retrieval-Augmented Generation, Software development management, Software engineering, Source coding, Source-code Comment Generation},
	pages = {467--471},
}

@inproceedings{gu_lens_2024,
	title = {{LENS}: {Layers} of {Evaluation} of {Hallucination} in {GenAI} {Systems}},
	doi = {10.1109/UV63228.2024.11189150},
	abstract = {Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate remarkable capabilities but remain vulnerable to hallucinations—producing plausible yet factually incorrect content—with error rates ranging from as low as 1.47\% in clinical applications [1] to as high as 75\% in domain-specific queries [2]. Despite growing attention, existing hallucination evaluation frameworks remain insufficient to meet critical needs. Through a comprehensive survey of over 100 evaluation methods spanning six methodological paradigms (probe-based, adversarial testing, causal intervention, uncertainty-guided, internal state analysis, and online evaluation), we identify fundamental limitations: current approaches fall short of enabling objective model comparison, providing diagnostic insights into failure modes, supporting domain-evolving benchmark construction, and guiding targeted mitigation strategies. Our analysis reveals that evaluations remain fragmented, operating either horizontally—comparing models across tasks and domains—or vertically—probing reasoning chains within single outputs. This fragmentation limits holistic assessment: horizontal evaluations provide breadth but risk superficiality, while vertical assessments deliver depth but lack generalizability. Moreover, we identify five critical gaps: (1) dimensional poverty reducing hallucinations to binary metrics, (2) failure to integrate horizontal breadth with vertical depth, (3) metacognitive blind spots overlooking when models should seek external verification, (4) adaptability crisis from static benchmarks, and (5) transparency deficits providing scores without actionable insights. Beyond surveying the landscape, this paper articulates eight fundamental challenges confronting comprehensive evaluation—from epistemological difficulties in defining ground truth and computational complexity of scaling assessment, to attribution opacity obscuring causal mechanisms and dynamic knowledge evolution rendering benchmarks obsolete. These challenges span multimodal complexity, scale and diversity requirements, adversarial robustness, and human alignment considerations. We present LENS (Layers of Evaluation of Hallucination in GenAI Systems), a unified framework addressing these gaps through hierarchical, tree-based query decomposition. LENS transforms complex evaluation tasks into multi-layered assessment structures via a six-stage pipeline (task formulation, decomposition, tool-augmented execution, structured generation, multi-dimensional scoring, and trace analysis), enabling MRI-like scanning of inference processes to reveal where and why hallucinations originate. The framework introduces four key innovations: (1) Tool Necessity Detection and Selection (TND/TSA) – explicitly evaluating when models should consult external sources versus relying on parametric memory, addressing a fundamental hallucination source. (2) Multi-Dimensional Metrics – assessing degree (accuracy, faithfulness, tool appropriateness), quantity (coverage, completeness), stability (consistency, robustness), and risk (uncertainty quantification) beyond binary detection. (3) User-Centric Benchmark Construction – empowering organizations to design custom evaluations from their evolving knowledge bases while maintaining methodological rigor. (4) Actionable Error Attribution – providing hierarchical decomposition traces with causal attribution, evidence chains, and OpenTelemetry-based reproducibility for transparent auditing. Our systematic taxonomy unifies previously fragmented approaches across evaluation targets (task-specific, modality-based, hallucination-type, domain-specific), dimensions (factuality, faithfulness, consistency, robustness, causal reasoning, interpretability), and methodologies. We introduce unified metrics transcending individual dimensions and present mitigation-aware evaluation strategies integrating RAG, parameter-efficient fine-tuning, knowledge distillation, preference optimization, and temporal intervention approaches. By combining horizontal breadth (across domains and architectures) with vertical depth (into reasoning processes), LENS advances hallucination evaluation from post-hoc error detection to proactive risk assessment. Case studies in medical diagnosis, legal analysis, and financial reasoning demonstrate the framework’s transformative impact, enabling objective model comparison, informed selection, diagnostic insights, domain-evolving benchmarks, and targeted mitigation development—fostering calibrated trust in AI systems deployed in safety-critical applications where accuracy, interpretability, and accountability are indispensable.},
	booktitle = {2024 7th {International} {Conference} on {Universal} {Village} ({UV})},
	author = {Gu, Chuqiao and Zhang, Wuyang and Huang, Zhenqian and Kou, Jieren and Liu, Zhenyao and Zhao, Chenjun and Liu, Chang and Zhang, Lifeng and Lin, Wenjie and Wang, Zhongda and Deng, Jianwei and Xie, Yuhuan and Huang, Guoxin and Zhang, Charles and Lu, Xiuyuan and Wang, Chengming and Zhang, Zejun and Yuan, Hao and Duan, Xiaoman and Fang, Yajun},
	month = oct,
	year = {2024},
	keywords = {Accuracy, actionable error attribution, adversarial robustness, Analytical models, Benchmark testing, causal attribution, causal reasoning capability, Cognition, compositional complexity, comprehensive hallucination evaluation, computational efficiency, cross-domain evaluation, cross-modal inconsistencies, domain-specific benchmarks, domain-specific queries, dynamic knowledge, evidence grounding, evolving benchmarks, factuality assessment, faithfulness evaluation, generative AI evaluation, ground truth ambiguity, hallucination evaluation, hallucination mitigation, hierarchical query decomposition, horizontal vs vertical evaluation, Large language models, LENS framework, Lenses, mechanistic interpretability, metacognitive evaluation, multi-dimensional metrics, multi-layered assessment structures, multimodal hallucination, post-hoc error detection, Prevention and mitigation, proactive risk assessment, reasoning failures, reliability and robustness, retrieval-augmented generation, Robustness, surface-level accuracy, system theory, temporal drift, tool necessity detection, tool selection accuracy, traceable evidence chains, Transforms, transparency and interpretability, tree-of-thoughts, trustworthy AI, Uncertainty, uncertainty quantification, Universal Village, user-centric benchmark construction, vision-language models, visual-textual misalignment},
	pages = {1--85},
}

@inproceedings{heydari_context_2024,
	title = {Context {Awareness} {Gate} for {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/IKT65497.2024.10892659},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to mitigate the limitations of large language models (LLMs) in answering domain-specific questions. Previous research has predominantly focused on improving the accuracy and quality of retrieved data chunks to enhance the overall performance of the generation pipeline. However, despite ongoing advancements, the critical issue of retrieving irrelevant information-which can impair a model's ability to utilize its internal knowledge effectively-has received minimal attention. In this work, we investigate the impact of retrieving irrelevant information in open-domain question answering, highlighting its significant detrimental effect on the quality of LLM outputs. To address this challenge, we propose the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLM's input prompt based on whether the user query necessitates external context retrieval. Additionally, we introduce the Vector Candidates method, a core mathematical component of CAG that is statistical, LLM-independent, and highly scalable. We further examine the distributions of relationships between contexts and questions, presenting a statistical analysis of these distributions. This analysis can be leveraged to enhance the context retrieval process in retrieval-augmented generation (RAG) systems.},
	booktitle = {2024 15th {International} {Conference} on {Information} and {Knowledge} {Technology} ({IKT})},
	author = {Heydari, Mohammad Hassan and Hemmat, Arshia and Naman, Erfan and Fatemi, Afsaneh},
	month = dec,
	year = {2024},
	note = {ISSN: 2476-2180},
	keywords = {Accuracy, Context awareness, Context modeling, Hallucination, Large language models, Large Language Models, Logic gates, Open Domain Question Answering, Pipelines, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation, Statistical analysis, Vectors},
	pages = {260--264},
}

@inproceedings{danuarta_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} ({RAG}) {Large} {Language} {Model} {For} {Educational} {Chatbot}},
	doi = {10.1109/ICIC64337.2024.10957676},
	abstract = {In the digital age, educational chatbots are increasingly essential for providing personalized learning experience. This study explores the potential of the Retrieval-Augmented Generation (RAG) method to enhanced the performance of educational chatbots. By integrating Large Language Models (LLMs) with real-time data retrieval from external sources, the RAG approach improves the accuracy, relevance, and safety of chatbot responses. Tested using an Indonesian elementary school e-book dataset on Physical Education, Sports, and Health (PJOK), the RAG model demonstrated significantly improvements in generating accurate and contextually appropriate answers while minimizing harmful content. The model achieved a mean context precision of 0.69, context recall of 0.70, faithfulness of 0.60, answer relevancy of 0.86, and harmfulness reduction to 0.26. The findings underscore the effectiveness of RAG advancing educational chatbot performance.},
	booktitle = {2024 {Ninth} {International} {Conference} on {Informatics} and {Computing} ({ICIC})},
	author = {Danuarta, Leo and Mawardi, Viny Christanti and Lee, Viciano},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Chatbots, Context modeling, Educational chatbot, Large Language Model, Large language models, Relevance, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Safety, Sports, Training, Uncertainty},
	pages = {1--6},
}

@inproceedings{deng_intelligent_2024,
	title = {Intelligent {Orientation} {Robot} {Based} on {Large} {Language} {Models} and {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICAICE63571.2024.10863858},
	abstract = {To address the issues of hallucination and knowledge security in large language models (LLMs) during Question and Answer processes, this project developed a Q\&A platform integrating Retrieval-Augmented Generation (RAG) with LLMs and enabled communication with intelligent robots. This project, applied to the university orientation scenario, built an orientation knowledge base comprising 11 categories of documents, about 300,000 words in all. By leveraging RAG for precise retrieval and using LLMs to generate logical and traceable answers, the system enhances both the accuracy and reliability of responses. The project also established remote communication with robots through the WebSocket protocol, offering flexible voice and touchscreen interaction, greatly improving the user experience. This platform not only improves the logical consistency of text-based Q\&A but also strengthens the interaction between robots and users, providing a certain technical foundation for future multi-scenario intelligent interactions.},
	booktitle = {2024 5th {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Engineering} ({ICAICE})},
	author = {Deng, Xinying and Yang, Dongju and Zhang, Yuan},
	month = nov,
	year = {2024},
	keywords = {Data collection, Embodied Intelligence, Information security, Intelligent robots, Knowledge based systems, LangChain, Large language models, Large Language Models, Protocols, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Touch sensitive screens, User experience, WebSocket},
	pages = {779--782},
}

@inproceedings{ye_intelligent_2025,
	title = {Intelligent {Tutoring} {Agent} with {Retrieval}-{Augmented} {Generation}: {A} {Case} {Study} of {Quality} {Management} {System} {Course}},
	doi = {10.1109/ICCECE65250.2025.10984799},
	abstract = {The application of large language models (LLMs) in specialized education presents challenges due to misalignment with domain-specific content and the risk of hallucination. This paper proposes an intelligent tutoring agent powered by Retrieval-Augmented Generation (RAG) to overcome these limitations, focusing on a Quality Management System (QMS) course. By integrating a vector-based semantic retrieval mechanism with LLMs, the system dynamically grounds responses in structured course materials, ensuring accuracy and contextual relevance. Additionally, tailored system-level prompts enhance semantic understanding and improve query rephrasing for more effective retrieval. A case study demonstrates the agent's ability to provide precise conceptual explanations, generate practice questions, and facilitate real-world application of theoretical knowledge, significantly improving student engagement and comprehension. Furthermore, a generalization test highlights existing limitations in adapting to broader contexts, identifying key challenges and future research directions in multimodal retrieval and domain adaptation. Our findings underscore the potential of RAG-based architectures to bridge the gap between general-purpose LLMs and domain-specific education, offering a scalable and adaptable solution for technical learning environments.},
	booktitle = {2025 5th {International} {Conference} on {Consumer} {Electronics} and {Computer} {Engineering} ({ICCECE})},
	author = {Ye, Zhenhong},
	month = feb,
	year = {2025},
	keywords = {Accuracy, Large language models, Large Language Models, Mechanical engineering, Quality management, Question answering (information retrieval), Question generation, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Technical drawing, Tutoring Agent, Visualization},
	pages = {189--194},
}

@inproceedings{meng_analysis_2025,
	title = {Analysis of {Text} {Generation} {System} {Design} {Combining} {Retrieval} {Augmented} {Generation} and {Fine}-{Tuning} {Strategy}},
	doi = {10.1109/SGAI64825.2025.11009349},
	abstract = {Text generation technology based on Pre-trained Language Model (PLM) has made significant progress in recent years, but it still faces many challenges such as semantic inconsistency and insufficient computing power. In view of this phenomenon, Retrieval Augmented Generation (RAG) and Fine-tuning strategy is studied, the general framework design for the text generation system of fusion strategy is put forward, several typical systems in specific fields are analyzed. The discussion of the realization and development direction of technology provides a reference for studying fusion strategy, which can improve the quality of text generation system design.},
	booktitle = {2025 2nd {International} {Conference} on {Smart} {Grid} and {Artificial} {Intelligence} ({SGAI})},
	author = {Meng, Qi and Wu, Zhenglong and Zhao, Zhongshi and Lian, Xi'nan},
	month = mar,
	year = {2025},
	keywords = {Analytical models, Computational modeling, efficient fine-tuning, Faces, fusion strategy, large language model, Large language models, retrieval augmented generation, Retrieval augmented generation, Semantics, Smart grids, System analysis and design, system design analysis},
	pages = {204--208},
}

@inproceedings{sng_novel_2024,
	title = {A {Novel} {Approach} to {Eliminating} {Hallucinations} in {Large} {Language} {Model}-{Assisted} {Causal} {Discovery}},
	doi = {10.1109/URTC65039.2024.10937570},
	abstract = {The increasing use of large language models (LLMs) in causal discovery as a substitute for human domain experts highlights the need for optimal model selection. This paper presents the first hallucination survey of popular LLMs for causal discovery. We show that hallucinations exist when using LLMs in causal discovery so the choice of LLM is important. We propose using Retrieval Augmented Generation (RAG) to reduce hallucinations when quality data is available. Additionally, we introduce a novel method employing multiple LLMs with an arbiter in a debate to audit edges in causal graphs, achieving a comparable reduction in hallucinations to RAG.},
	booktitle = {2024 {IEEE} {MIT} {Undergraduate} {Research} {Technology} {Conference} ({URTC})},
	author = {Sng, Grace and Zhang, Yanming and Mueller, Klaus},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Automation, Cause effect analysis, Large language models, Logic, Pipelines, Retrieval augmented generation, Surveys},
	pages = {1--6},
}

@inproceedings{lu_initial_2025,
	title = {Initial {Evaluation} of {Retrieval}-{Augmented} {Generation} {Approaches} in {Spectrum} {Policy} {Research}},
	doi = {10.1109/DySPAN64764.2025.11115912},
	abstract = {Spectrum policy documents, such as notices and comments in Federal Communications Commission (FCC) and the National Telecommunications and Information Administration (NTIA) proceedings, are often lengthy and complex, making them difficult to access and understand. Combining large language models (LLMs) with retrieval-augmented generation (RAG) techniques offers a promising way to address these challenges by improving information retrieval and processing. This paper evaluates the use of RAG in spectrum policy analysis. We establish an open-source knowledge dataset of policy comments that has been cleaned and annotated to work effectively with RAG systems. We also develop a corresponding test dataset of question-answer pairs to evaluate system performance. By benchmarking four RAG-based systems, we show that RAG techniques significantly enhance the ability of LLMs to interpret and respond to complex spectrum policy queries, demonstrating their potential to make these important documents more accessible.},
	booktitle = {2025 {IEEE} {International} {Symposium} on {Dynamic} {Spectrum} {Access} {Networks} ({DySPAN})},
	author = {Lu, Bingyan and Reinking, Caleb and Laneman, J. Nicholas},
	month = may,
	year = {2025},
	note = {ISSN: 2473-070X},
	keywords = {Benchmark testing, Dynamic spectrum access, FCC, Information retrieval, Knowledge graphs, Large language models, LLM, NTIA, RAG, Retrieval augmented generation, spectrum policy, System performance},
	pages = {1--5},
}

@inproceedings{lin_novel_2024,
	title = {Novel {Preprocessing} {Technique} for {Data} {Embedding} in {Engineering} {Code} {Generation} {Using} {Large} {Language} {Model}},
	doi = {10.1109/LAD62341.2024.10691715},
	abstract = {We introduce four principal contributions to augment the capabilities of Large Language Models (LLMs) in generating domain-specific code: (i) leveraging LLM-based data splitting and data renovation techniques to refine the semantic representation within the embedding space; (ii) proposing an effective method for refactoring existing scripts, enabling the generation of new and high-quality scripts with the aid of LLMs; (iii) developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt technique; and (iv) showcasing the efficacy of our data pre-processing approach through a case study using engineering simulation software RedHawk-SC. Our contributions collectively advance the Retrieval-Augmented Generation (RAG) framework, enabling more relevant and precise information retrieval. An arena-style evaluation by 28 domain experts and 182 votes confirms the significant effectiveness of our methods. Notably, our approach achieves up to 1.43 times the improvement in code generation for MapReduce applications compared to the Chain-of-Thought (CoT) technique.},
	booktitle = {2024 {IEEE} {LLM} {Aided} {Design} {Workshop} ({LAD})},
	author = {Lin, Yu-Chen and Kumar, Akhilesh and Chang, Norman and Zhang, Wenliang and Zakir, Muhammad and Apte, Rucha and He, Haiyang and Wang, Chao and Jang, Jyh-Shing Roger},
	month = jun,
	year = {2024},
	keywords = {code generation, Codes, Conferences, data preprocessing, Data preprocessing, data renovation, data splitter, domain-specific, Information retrieval, Knowledge engineering, Large language models, Large language models (LLMs), MapReduce, prompt engineering, Prompt engineering, RedHawk-SC (RH-SC), Retrieval-Augmented Generation (RAG), Semantics, Software},
	pages = {1--5},
}

@inproceedings{zhou_exploring_2024,
	title = {Exploring the {Application} of {Retrieval}-{Augmented} {Generation} {Technology} in {Defense} {Technology} {Intelligence}},
	doi = {10.1109/CSIS-IAC63491.2024.10919351},
	abstract = {Large language models (LLMs) are impeded by issues such as spurious generation, knowledge obsolescence, and domain expertise deficiency, which constrain their efficacy in defense technology intelligence applications. Mitigating these challenges is imperative for enhancing the utility of these models in this domain. Grounded in the core requirements of defense technology intelligence, this paper provides an incisive analysis of the significance of Retrieval-Augmented Generation (RAG) in boosting the performance and applicability of LLMs. The paper explores the primary scenarios and deployment paradigms for RA G in defense technology intelligence. Additionally, it synthesizes the technical obstacles encountered during implementation and elucidates the countermeasures devised to overcome them. Our research demonstrates that RAG technology can significantly enhance the efficiency, precision, relevance, and timeliness of intelligence gathering, enabling LLMs to better meet the demands of defense technology intelligence. These findings highlight the promising potential of RAG in augmenting the capabilities of language models for critical defense applications, paving the way for future advancements in this emerging field.},
	booktitle = {2024 {International} {Annual} {Conference} on {Complex} {Systems} and {Intelligent} {Science} ({CSIS}-{IAC})},
	author = {Zhou, Lijie and Yan, Shoujun and Li, Zhongjie and Ma, Jie},
	month = sep,
	year = {2024},
	keywords = {Aging, Best practices, Boosting, Complex systems, Defense Technology Intelligence, Focusing, Large language models, Large Language Models, National security, Retrieval augmented generation, Retrieval-Augmented Generation, Security, Systematics},
	pages = {664--669},
}

@book{raieli_notitle_2025,
	isbn = {978-1-83508-038-2},
	url = {https://ieeexplore.proxyucr.elogim.com/document/11099035},
	abstract = {Master LLM fundamentals to advanced techniques like RAG, reinforcement learning, and knowledge graphs to build, deploy, and scale intelligent AI agents that reason, retrieve, and act autonomouslyKey FeaturesImplement RAG and knowledge graphs for advanced problem-solvingLeverage innovative approaches like LangChain to create real-world intelligent systemsIntegrate large language models, graph databases, and tool use for next-gen AI solutionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThis AI agents book addresses the challenge of building AI that not only generates text but also grounds its responses in real data and takes action. Authored by AI specialists with deep expertise in drug discovery and systems optimization, this guide empowers you to leverage retrieval-augmented generation (RAG), knowledge graphs, and agent-based architectures to engineer truly intelligent behavior. By combining large language models (LLMs) with up-to-date information retrieval and structured knowledge, you'll create AI agents capable of deeper reasoning and more reliable problem-solving. Inside, you'll find a practical roadmap from concept to implementation. You’ll discover how to connect language models with external data via RAG pipelines for increasing factual accuracy and incorporate knowledge graphs for context-rich reasoning. The chapters will help you build and orchestrate autonomous agents that combine planning, tool use, and knowledge retrieval to achieve complex goals. Concrete Python examples built on popular libraries, along with real-world case studies, reinforce each concept and show you how these techniques come together. By the end of this book, you’ll be well-equipped to build intelligent AI agents that reason, retrieve, and interact dynamically, empowering you to deploy powerful AI solutions across industries.What you will learnLearn how LLMs work, their structure, uses, and limits, and design RAG pipelines to link them to external dataBuild and query knowledge graphs for structured context and factual groundingDevelop AI agents that plan, reason, and use tools to complete tasksIntegrate LLMs with external APIs and databases to incorporate live dataApply techniques to minimize hallucinations and ensure accurate outputsOrchestrate multiple agents to solve complex, multi-step problemsOptimize prompts, memory, and context handling for long-running tasksDeploy and monitor AI agents in production environmentsWho this book is forIf you are a data scientist or researcher who wants to learn how to create and deploy an AI agent to solve limitless tasks, this book is for you. To get the most out of this book, you should have basic knowledge of Python and Gen AI. This book is also excellent for experienced data scientists who want to explore state-of-the-art developments in LLM and LLM-based applications.},
	publisher = {Packt Publishing},
	author = {Raieli, Salvatore and Iuculano, Gabriele},
	year = {2025},
	note = {Publication Title: Building AI Agents with LLMs, RAG, and Knowledge Graphs: A practical guide to autonomous and modern AI agents},
}

@inproceedings{de_castro_human---loop_2024,
	title = {Human-in-the-loop knowledge base upkeep for retrieval augmented generation applications},
	doi = {10.1109/ISM63611.2024.00053},
	abstract = {Retrieval augmented generation (RAG) has been proposed as a way of grounding the large language models (LLMs) with factual and updated sources of information. However, the quality of the chunks in the knowledge base is important. In this work, we propose a human-in-the-loop knowledge base visualization and upkeep system that aims to maintain the quality of the knowledge base by preassigning tags to chunks based on their structural forms, rather than semantics. This allows a human reviewer to quickly identify pieces of chunks that could be considered noise, and easily remove unwanted chunks that will naturally come, due to the process of document chunking in RAG.},
	booktitle = {2024 {International} {Symposium} on {Multimedia} ({ISM})},
	author = {de Castro, Pedro Baptista and Sukeda, Hiroko and Takashige, Soichi},
	month = dec,
	year = {2024},
	keywords = {Grounding, Human in the loop, human-in-the-loop, knowledge base upkeep, Knowledge based systems, Large language models, Noise, retrieval augmented generation, Retrieval augmented generation, Semantics, Visualization},
	pages = {232--233},
}

@inproceedings{curto_building_2025,
	title = {Building {Software} {Functional} {Requirements} {Lists} {Using} {RAG} with {Distinct} {LLMs} in {Multiple} {Interactions}},
	doi = {10.1109/RE63999.2025.00077},
	abstract = {This work explores the automated elicitation of functional software requirements using Large Language Models (LLMs). The proposed approach employs multiple LLMs in conjunction with Retrieval-Augmented Generation (RAG) across iterative refinement rounds. This process enables the generation of candidate requirements from high-level software descriptions, with validation through similarity analysis. The method addresses key challenges in current AI-based elicitation approaches, including lack of iterative refinement, hallucination control, and semantic convergence. Preliminary results demonstrate that multi-round feedback and cross-model integration improve alignment with expert-defined requirements. This study marks the initial phase of a broader doctoral research project titled Agent Family for Software Engineering Teams, which aims to develop a set of intelligent agents to support various stages of the software engineering lifecycle, including requirement elicitation, project management, and software quality, ultimately forming a cohesive and responsible AI ecosystem for software teams.},
	booktitle = {2025 {IEEE} 33rd {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Curto, Hayala Nepomuceno},
	month = sep,
	year = {2025},
	note = {ISSN: 2332-6441},
	keywords = {AI in Software Engineering, Iterative methods, Large language models, Large Language Models, Medical services, Multi-agent systems, Multi-Agent Systems, Project management, Requirements engineering, Requirements Engineering, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Software engineering, Software quality},
	pages = {612--616},
}

@inproceedings{sarnikar_using_2025,
	title = {Using {LLMs} for {Querying} and {Understanding} {Long} {Legislative} {Texts}},
	doi = {10.1109/SCEECS64059.2025.10940780},
	abstract = {With over 2,000 bills being passed in the California State Legislature each year, it can be very confusing for the average citizen to understand what is being approved. This process can be made much easier with recent advancements to large language models (LLMs). Specifically, using Retrieval Augmented Generation (RAG). This research paper will study two different approaches to using LLMs to answer questions about bills passed in our state legislature. The first approach will employ standard RAG protocol to scan and retrieve any information relevant to the user’s prompt and summarize it in an easy-to-read manner. Our second approach expands the context window of the model to consider a wider range of legislature when scanning for keywords entered by the user. By analyzing the performance of each LLM, we can find the best model that guarantees contextually rich answers to any questions surrounding the state legislature.},
	booktitle = {2025 {IEEE} {International} {Students}' {Conference} on {Electrical}, {Electronics} and {Computer} {Science} ({SCEECS})},
	author = {Sarnikar, Arhanth},
	month = jan,
	year = {2025},
	note = {ISSN: 2688-0288},
	keywords = {Analytical models, Computational modeling, Computer science, Context modeling, extended context, large language model, Large language models, legislative texts, Protocols, Retrieval augmented generation, retrieval-augmented generation, Standards},
	pages = {1--5},
}

@inproceedings{obaid_seerahgpt_2024,
	title = {{SeerahGPT}: {Retrieval} {Augmented} {Generation} based {Large} {Language} {Model}},
	doi = {10.1109/ICOSST64562.2024.10871159},
	abstract = {Large Language Models(LLMs) have become widely recognized in recent years for their exceptional performance in language generation capabilities. As a result, an unprecedented rise is seen in its use cases in various domains specifically involving Natural Language Processing(NLP). These models however perform suboptimally when exploited in the field of studies where authenticity of the generated content is a critical aspect. One such domain is usage of LLM for exploration of the life of Prophet Muhammad S.A.W(commonly referred to as Seerah). It is of utmost significance to ensure the authenticity and reliability in the sources used and reported by the LLM due to the sensitive nature of the domain. The contemporary LLMs, however, lack the explainability in their response due to their inherent black-box nature. In our study, we have presented a novel LLM named SeerahGPT that addresses this challenge with the help of retrieval-augmented generation (RAG). This technique enables the model to utilize both parametric and nonparametric memories for generating response of queries. Our model, built on the Llama-2-7b architecture, employs Sentence Transformer embedding to effectively retrieve relevant information. The model's capabilities are augmented by integrating it with a corpus having Islamic texts such as the Quranic translation and Hadith collections, and historical accounts. The model's performance is benchmarked against its base model using both quantitative and qualitative metrics. The comparative analysis with Llama-2-7b revealed that SeerahGPT incorporation with external knowledge sources, provided more authentic and verifiable responses, despite the others exhibiting greater fluency. Performance metrics such as BLEU, ROUGE, and METEOR indicated SeerahGPT's better accuracy and contextual handling. This study paves way for analysis of such sensitive domains in more efficient way that can be utilized in other complex domains such as Islamic theology and Fiqh or legal system.},
	booktitle = {2024 18th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Obaid, Surayya and Bawany, Narmeen Zakaria},
	month = dec,
	year = {2024},
	note = {ISSN: 2770-8225},
	keywords = {Accuracy, Context modeling, Large Language Model, Large language models, Law, Llama, Measurement, Meteors, Natural Language Processing, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Seerah, Transformers, Translation},
	pages = {1--7},
}

@inproceedings{huang_medeeprag_2025,
	title = {{MedeepRAG}: {A} {Retrieval}-{Augmented} {Generation} {System} for {Medical} {Q}\&{A} {Using} {DeepSeek}-{R1}},
	doi = {10.1109/ICETCI64844.2025.11083988},
	abstract = {Large Language Models (LLMs) represent a major breakthrough in Artificial Intelligence (AI), with models like DeepSeek from China demonstrating effectiveness in various natural language processing (NLP) tasks. These models, trained on large datasets, capture the intricate relationships between words in textual data. Retrieval-Augmented Generation (RAG) is a novel approach that combines retrieval-based and generation-based models to enhance text generation quality. However, LLMs have yet to achieve optimal performance in biomedical tasks, where domain-specific expertise is crucial. To address this gap, this paper proposes MedeepRAG, a model built on the distilled version of DeepSeek-R1-Distill-1.5B. MedeepRAG is designed to balance inference power and deployment efficiency with only 1.5 billion parameters. It is trained on the Medical-R1-Distill-Data-Chinese, a high-quality, structurally labeled Chinese medical dataset. Experimental results show that MedeepRAG produces responses with more reliable medical knowledge, supporting the integration of Artificial Intelligent (AI) into the medical field for enhanced Question and Answer (Q\&A) in clinical settings. Future work will focus on clinical validation, multimodal data processing, and improving nested medical concept handling.},
	booktitle = {2025 {IEEE} 5th {International} {Conference} on {Electronic} {Technology}, {Communication} and {Information} ({ICETCI})},
	author = {Huang, Hoying},
	month = may,
	year = {2025},
	keywords = {Biological system modeling, Biomedical tasks, Data processing, DeepSeek, Heuristic algorithms, Large language models, Large Language Models, Medical diagnostic imaging, Natural language processing, Oral communication, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Stability analysis},
	pages = {146--150},
}

@inproceedings{zhang_large_2025,
	title = {A {Large} {Language} {Model} {Question} {Answering} {System} for {Power} {System} {Equipment} {Fault} {Diagnosis} based on {RAGFlow}},
	doi = {10.1109/MRAI65197.2025.11135758},
	abstract = {This paper aims to construct a knowledge graph-enhanced power system fault diagnosis question-answering system based on Retrieval-Augmented Generation (RAG), addressing challenges in power equipment fault diagnosis such as fragmented data sources, over-reliance on expert experience, low diagnostic accuracy, and insufficient intelligent capabilities. By integrating large language models (LLMs) with domain-specific knowledge bases, the proposed system leverages the open-source framework RAGFlow to retrieve relevant information and generate diagnostic recommendations, thereby enhancing the model’s domain expertise and prediction accuracy to deliver an intelligent solution for power system fault diagnosis. Experimental validation demonstrates that the system achieves significant improvements in diagnostic efficiency and accuracy.},
	booktitle = {2025 {International} {Conference} on {Mechatronics}, {Robotics}, and {Artificial} {Intelligence} ({MRAI})},
	author = {Zhang, Qi and Zhang, Zhi},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Fault diagnosis, Knowledge graph, large language model, Large language models, Mechatronics, Power system faults, Predictive models, Question answering (information retrieval), RAG, Retrieval augmented generation, Robots, Soft sensors},
	pages = {359--362},
}

@inproceedings{varma_data_2025,
	title = {From {Data} to {Decisions}: {Enterprise}-{Level} {Domain}-{Specific} {Graph} {Retrieval}-{Augmented} {Generation} {Systems} for {Advanced} {Question} {Answering}},
	doi = {10.1109/BigComp64353.2025.00016},
	abstract = {Retrieval-Augmented Generation (RAG) is aimed at improving the functionality of large language model (LLM) applications by incorporating specific data. This may include searching for relevant materials or files concerning a particular issue or search provided as background information to the LLM. Some prominent architectures like the Vanilla RAG architecture focuses primarily on retrieval of textual data, primarily utilizing vector databases, thus neglecting the structural intricacies of textual data, resulting in a critical gap in the generation process. To address this gap, we have introduced SAGE-QA (Scalable Advanced Graph RAG for Enterprise Question Answering), which significantly enhances both the retrieval and generation processes by emphasizing the importance of topological information for reasoning tasks on a variety of data sources. For our use case, SAGE-QA significantly outperforms current state-of-the-art RAG methods with its novel retrieval mechanism, which effectively mitigates hallucinations, and acts as a sustainable contextually relevant application.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Big} {Data} and {Smart} {Computing} ({BigComp})},
	author = {Varma, Sandeep and Shivam, Shivam and Natarajan, Sarun and Banerjee, Ankita and Roy, Sourodeep},
	month = feb,
	year = {2025},
	note = {ISSN: 2375-9356},
	keywords = {Computational modeling, Computer architecture, Data models, Databases, Knowledge Graph(KG), Large Language Model(LLM), Large language models, Measurement, Question answering (information retrieval), Question Answering(Q\&A), Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Soft sensors, Vectors},
	pages = {41--48},
}

@inproceedings{hecking_architecture_2025,
	title = {An {Architecture} and {Protocol} for {Decentralized} {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/ICSA-C65153.2025.00012},
	abstract = {Retrieval-augmented generation (RAG) has become a widely adopted approach for the integration of knowledge bases and large language models (LLMs). This paper proposes a decentralized software architecture for RAG, where retrieval, augmentation, and generation components are operated independently by distributed entities. This approach addresses challenges like efficient usage of resources when building LLM-based software systems as well as data protection issues in centralized systems. By allowing data providers to implement their own retrieval mechanisms, they fully retain control over data access and the flexibility to use their own data infrastructure. An interaction platform implements communication between client applications, data providers, and model providers. To standardize this process, the paper introduces introduces an architecture and protocol called External Retrieval Interface (ERI), which ensures compatibility of services, enforces data restrictions, and simplifies the development of decentralized RAG systems.},
	booktitle = {2025 {IEEE} 22nd {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Hecking, Tobias and Sommer, Thorsten and Felderer, Michael},
	month = mar,
	year = {2025},
	note = {ISSN: 2768-4288},
	keywords = {Buildings, Computer architecture, Data models, Data protection, Distributed Systems, Knowledge based systems, Large language models, Large Language Models, Protocols, Retrieval augmented generation, Retrieval Augmented Generation, Software architecture, Software systems},
	pages = {31--35},
}

@inproceedings{joldea_multimodal_2025,
	title = {Multimodal {AI} for {Romanian} {University} {Support}: {An} {LLM}, {RAG} and {Voice} {Approach}},
	doi = {10.1109/SACI66288.2025.11030141},
	abstract = {The era of large language models (LLMs) brings forth a new wave of automation to many fields of activity. In this work we employ the AI advancements catalyzed by these LLMs to create a smart university assistant. A chatbot that comes to assist university enrolled students and staff on administrative, legislative and public interest topics. To this end, we develop a platform that combines Large Language Models, Retrieval Augmented Generation, Speech-to-Text and Text-to-Speech technologies to automate accessibility to university-related information. We start from openly available models and resources, adapt and finetune them to our target - Romanian question answering with information retrieval - and then release our solutions publicly at https://github.com/Andrei481/RomanianChatbot.},
	booktitle = {2025 {IEEE} 19th {International} {Symposium} on {Applied} {Computational} {Intelligence} and {Informatics} ({SACI})},
	author = {Joldea, Andrei-Răzvan and Cernăzanu-Glăvan, Diana and Sârbu, Vlad and Bulzan, Andrei-Ştefan},
	month = may,
	year = {2025},
	note = {ISSN: 2765-818X},
	keywords = {Assistant, Chatbot, Chatbots, Computational modeling, Domain-specific fine-tuning, Informatics, Information retrieval, Large language models, LLM, Question answering (information retrieval), RAG, Retrieval augmented generation, Romanian, Software development management, Speech to text, STT, Text to speech, TTS},
	pages = {000189--000194},
}

@inproceedings{agrawal_mindful-rag_2024,
	title = {Mindful-{RAG}: {A} {Study} of {Points} of {Failure} in {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/FLLM63129.2024.10852457},
	abstract = {Large Language Models (LLMs) excel at generating coherent text but often struggle with knowledge-intensive queries, particularly in domain-specific and factual question-answering tasks. Retrieval-augmented generation (RAG) systems have emerged as a promising solution by integrating external knowledge sources, such as structured knowledge graphs (KGs). While KG-based RAG approaches have demonstrated value, current state-of-the-art solutions frequently fall short, failing to deliver accurate and reliable answers even when the necessary factual knowledge is available. In this paper, we present a critical analysis of failure points in existing KG-based RAG methods, identifying eight key areas of concern, including misinterpretation of question context, incorrect relation mapping, and ineffective ambiguity resolution. We argue that these failures primarily stem from design limitations in current KG-RAG systems, such as inadequate attention to discerning user intent and insufficient alignment of retrieved knowledge with the contextual demands of the query. Based on this analysis, we propose a new approach for KG-RAG systems, termed Mindful-RAG, which re-engineers the retrieval process to be more intent-driven and contextually aware. By enhancing reasoning capabilities, improving constraint identification, and addressing the structural limitations of knowledge graphs, we aim to improve the reliability and effectiveness of KG-RAG systems. To validate this approach, we developed a proof-of-concept by integrating the principles of Mindful-RAG into an existing KG-RAG system. The Mindful-RAG approach seeks to deliver more robust, accurate, and contextually aligned AI-driven knowledge retrieval systems, with potential applications in critical domains such as healthcare, legal, research, and scientific discovery, where precision and reliability are paramount.},
	booktitle = {2024 2nd {International} {Conference} on {Foundation} and {Large} {Language} {Models} ({FLLM})},
	author = {Agrawal, Garima and Kumarage, Tharindu and Alghamdi, Zeyad and Liu, Huan},
	month = nov,
	year = {2024},
	keywords = {Accuracy, Cognition, Hallucinations, Knowledge graphs, Knowledge Graphs (KG), Large language models, Law, LLMs, Medical services, Points of Failure, Reliability, Retrieval augmented generation, Retrieval Augmented Generation (RAG)},
	pages = {607--611},
}

@inproceedings{li_deepseek-med-8b_2025,
	title = {{DeepSeek}-{Med}-{8B}: {Medical} {LLM} for {Chinese} {Diagnosis} and {Referral}},
	doi = {10.1109/CISAT66811.2025.11181817},
	abstract = {The uneven distribution of medical resources in China poses significant challenges, especially in rural areas. While large language models (LLMs) offer potential for clinical support, existing systems like GPT-4 and Med-PaLM suffer from hallucinations, English-centric biases, and lack real-time physician integration. We present DeepSeek-Med-8B, a Chinese medical conversational agent based on the DeepSeek-R1-DistillLlama-8B architecture.DeepSeek-Med-8B is trained through: (i) Supervised Fine-Tuning (SFT) on curated Chinese medical corpora; (ii) Reinforcement Learning with AI and Doctor Feedback (RLAIF) for factuality, empathy, and referral quality; and (iii) Retrieval-Augmented Generation (RAG) for real-time grounding in physician databases.Across eight clinical tasks, DeepSeek-Med-8B achieves a top1 mean score of 66.9 on GPT-4o-based benchmarks and a 74\% top-3 doctor match rate, outperforming rule-based baselines. The model runs efficiently on a single RTX 4090 GPU via INT8 quantization.},
	booktitle = {2025 8th {International} {Conference} on {Computer} {Information} {Science} and {Application} {Technology} ({CISAT})},
	author = {Li, Chenxing and Mao, Jiawei and Liu, Bin and Luo, Wei},
	month = jul,
	year = {2025},
	keywords = {Grounding, Healthcare Natural Language Processing, Information science, Large language models, Large Language Models, Medical diagnostic imaging, Medical services, Natural language processing, Quantization (signal), Real-time systems, Reinforcement learning, Reinforcement Learning with AI and Human Feedback, Retrieval augmented generation, Retrieval-Augmented Generation, Supervised Fine-Tuning},
	pages = {240--244},
}

@inproceedings{zhang_dynamic_2024,
	title = {A {Dynamic} {Retrieval}-{Augmented} {Generation} {Framework} for {Border} {Inspection} {Legal} {Question} {Answering}},
	doi = {10.1109/IALP63756.2024.10661194},
	abstract = {Border inspection legal question answering (LQA) is designed for specific legal scenarios, aiming to address legal questions related to border inspections and provide accurate and practical legal guidance to the public and border inspection departments. Current research requires a retrieval process regardless of whether the necessary knowledge already exists, and does not evaluate the legal information in the generated responses, leading to errors and low-quality content. we propose the Dynamic Retrieval-Augmented Generation framework for Border Inspection LQA (DRAG-BILQA), which generates responses first and then dynamically controls retrieval based on confidence scores. In this framework, we innovatively introduce a legal factor recognition module to ensure the legal accuracy of generated answers and improve the quality of the generation. We also present the BorderLegal-QA dataset, filling the gap in border inspection legal datasets. We conducted extensive experiments on both the BorderLegal-QA and the general LQA dataset JEC-QA. Experimental results show that DRAG-BILQA outperforms Sota models in terms of legal factor recognition accuracy and retrieval performance, achieving a METEOR score of 39.5 and improving generation quality. Case analysis also demonstrate that DRAG-BILQA effectively Augmented the legal accuracy of the generated content.},
	booktitle = {2024 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	author = {Zhang, Yanjun and Li, Dapeng and Peng, Gaojun and Guo, Shuang and Dou, Yu and Yi, Ruheng},
	month = aug,
	year = {2024},
	note = {ISSN: 2159-1970},
	keywords = {Accuracy, border inspection LQA, Filling, Inspection, large language model, Law, Meteors, Question answering (information retrieval), retrieval-augmented generation},
	pages = {372--376},
	annote = {Cited by: 1},
}

@inproceedings{joshi_robust_2024,
	title = {Robust {Multi} {Model} {RAG} {Pipeline} {For} {Documents} {Containing} {Text}, {Table} \& {Images}},
	doi = {10.1109/ICAAIC60222.2024.10574972},
	abstract = {RAG (Retrieval Augmented Generation) is generally used for generating results from the existing knowledge-base. RAG refers to finding references (R), Adding references (A) and improving generation(i.e, answers to the question) (G). MultiModel-RAGs are used for generation of results over the documents which contain images and texts. There exists multiple different Multimodel-RAGs but these are not still efficient in generation of the results from the documents which contain relationships between images and texts. This study has proposed the solution to enable effective retrieval and generation of results, which includes the relationship between images and texts. The comparison of proposed Multimodal RAG with four different datasets (i.e., Short-form-type-QA, Long-form-type-QA, MCQ-type-QA, True-False-type-QA) shows the proposed solution improves the effectiveness of the existing Multimodal RAGs. Testing of proposed Multimodal RAG over two different other multimodal LLM i.e, Open-AI \& Gemini helps in deciding whether the proposed solution fits best with LLM in different cases.},
	booktitle = {2024 3rd {International} {Conference} on {Applied} {Artificial} {Intelligence} and {Computing} ({ICAAIC})},
	author = {Joshi, Pankaj and Gupta, Aditya and Kumar, Pankaj and Sisodia, Manas},
	month = jun,
	year = {2024},
	keywords = {Accuracy, Analytical models, Computational modeling, GenAI(Generative AI), Knowledge based systems, LLM(Large Language Models), MuRAG(Multimodal Retrieval Augmented Generation), Pipelines, RAG(Retrieval Augmented Generation), Testing},
	pages = {993--999},
}

@inproceedings{viboonsang_leveraging_2025,
	title = {Leveraging {Large} {Language} {Model} to {Augment} {Cybersecurity} {Analysis} and {Response} in {Thai} {Language}},
	doi = {10.1109/ICMSS64503.2025.00028},
	abstract = {Cybersecurity operations in Thailand face evolving challenges, including a surge in data volume and diverse threats that exceed human analytical capacity. This study explores the potential of Large Language Models (LLMs) to augment Thai-language cybersecurity analysis and response. Using OpenThaiGPT integrated with Retrieval-Augmented Generation (RAG), we developed a generative AI chatbot tailored for cybersecurity inquiries. By leveraging a curated dataset from authoritative sources, the results demonstrate that the RAG-integrated model significantly improves response accuracy and relevance compared to the base model. However, limitations such as dataset size, translation challenges, and hallucination risks highlight areas for further research. Proposed future work includes expanding datasets, refining translation processes, and exploring alternative LLM architectures. This study establishes a foundational framework for deploying LLMs in Thai cybersecurity, contributing to the development of effective AI-driven solutions for cybersecurity landscape in Thailand.},
	booktitle = {2025 9th {International} {Conference} on {Management} {Engineering}, {Software} {Engineering} and {Service} {Sciences} ({ICMSS})},
	author = {Viboonsang, Pranodnard and Kosolsombat, Somkiat and Ratanavilisagul, Chiabwoot},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Chatbots, Computer security, Context modeling, Cybersecurity, Generative AI, Knowledge based systems, Large language models, Large Language Models (LLMs), Refining, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Translation},
	pages = {68--73},
}

@inproceedings{bhatia_optimization_2024,
	title = {Optimization {Techniques} in {Large} {Language} {Models} for {News} {Report} {Generation}},
	doi = {10.1109/EmergIN63207.2024.10961084},
	abstract = {This paper aims to explore optimization techniques in Large Language Models (LLMs) for automated news report generation, utilizing methods such as Retrieval-Augmented Generation (RAG), Zero-Shot Learning, ICL (In Context Learning), and reAct agents. Using these techniques, the system collects real-time information based on the user context and the images from the internet, to improve factual accuracy and enhance reliability. Various LLMs such as Llama 3.1, Llama 3.2, Qwen 2.5, Gemma, LLaVa, Aya were utilized in the system to generate the News Reports. Among these, Llama3.1 demonstrated the best performance with Context Precision as 92\%, Answer Relevancy as 90.84 \% and Faithfulness as 12.5\%, closely followed by Qwen 2.5, which showed a notable improvement in Faithfulness as 43\%. Fine-tuning the models can improve the various metrics such as Context Precision, Recall, Answer Relevancy, BLEU and ROUGE Scores as well as improve the adoption to various writing styles as seen in News Reports.},
	booktitle = {2024 {International} {Conference} on {Emerging} {Technologies} and {Innovation} for {Sustainability} ({EmergIN})},
	author = {Bhatia, Tushar and Gupta, Ashish and Varshney, Dhruv and Mutreja, Tvisha and Kumar, Sumit and Vijh, Surbhi},
	month = dec,
	year = {2024},
	keywords = {Accuracy, AI in Journalism, Content Personalization, Large language models, Large Language Models (LLMs), Natural language processing, Natural Language Processing (NLP), News Automation, News Report Generation, Optimization, Personalization in Content Generation, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Sustainable development, Technological innovation, Text Coherence and Accuracy, Writing, Zero shot learning},
	pages = {497--502},
}

@inproceedings{ahmed_quality_2025,
	title = {Quality {Assurance} for {LLM}-{RAG} {Systems}: {Empirical} {Insights} from {Tourism} {Application} {Testing}},
	doi = {10.1109/ICSTW64639.2025.10962487},
	abstract = {This paper presents a comprehensive framework for testing and evaluating quality characteristics of Large Language Model (LLM) systems enhanced with Retrieval-Augmented Generation (RAG) in tourism applications. Through systematic empirical evaluation of three different LLM variants across multiple parameter configurations, we demonstrate the effectiveness of our testing methodology in assessing both functional correctness and extra-functional properties. Our framework implements 17 distinct metrics that encompass syntactic analysis, semantic evaluation, and behavioral evaluation through LLM judges. The study reveals significant information about how different architectural choices and parameter configurations affect system performance, particularly highlighting the impact of temperature and top-p parameters on response quality. The tests were carried out on a tourism recommendation system for the Varmland region in Sweden, utilizing standard and RAG-enhanced configurations. The results indicate that the newer LLM versions show modest improvements in performance metrics, though the differences are more pronounced in response length and complexity rather than in semantic quality. The research contributes practical insights for implementing robust testing practices in LLM-RAG systems, providing valuable guidance to organizations deploying these architectures in production environments.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	author = {Ahmed, Bestoun S. and Baader, Ludwig Otto and Bayram, Firas and Jagstedt, Siri and Magnusson, Peter},
	month = mar,
	year = {2025},
	note = {ISSN: 2159-4848},
	keywords = {AI Quality Assurance, Extra-Functional Properties, Large language models, Large Language Models (LLM), ML System Testing, Quality assurance, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantics, Software Quality Testing, Standards organizations, Syntactics, System performance, System testing, Systematics, Temperature},
	pages = {200--207},
}

@inproceedings{xu_chat_2024,
	title = {A {Chat} {Bot} for {Enrollment} of {Xi} 'an {Jiaotong}-{Liverpool} {University} {Based} on {RAG}*},
	doi = {10.1109/IWCEAA63616.2024.10823979},
	abstract = {Recent advancements in large language models (LLMs) have established pre-training on extensive textual cor-pora as a foundational methodology. However, in specialised applications such as admissions systems, the focus shifts from general knowledge-based reasoning to ensuring accuracy and relevance in domain-specific responses. This study presents the development of an automated admissions system for Xi'an Jiaotong-Liverpool University, leveraging GLM-4 in conjunction with Retrieval-Augmented Generation (RAG) to handle targeted queries. The implementation of RAG mitigates the occurrence of hallucinations often seen in LLM outputs, thereby enhancing the reliability and alignment of generated responses with real-world data, which is critical for prospective students and their parents. This paper details the construction of the RAG corpus and cue word methodology, and provides an empirical comparison of the efficacy of various major language models with and without RAG integration. The results demonstrate the potential of RAG to significantly improveresponse accuracy in domain-specific tasks, and suggest directions for future research in optimising LLMs for admissions processes.},
	booktitle = {2024 8th {International} {Workshop} on {Control} {Engineering} and {Advanced} {Algorithms} ({IWCEAA})},
	author = {Xu, Liwei and Liu, Jiarui},
	month = nov,
	year = {2024},
	keywords = {Accuracy, Chatbots, Context modeling, Large Language Model (LLM), Large language models, Maintenance, Pollution, Prompt Engineering, Question answering (information retrieval), Retrieval augmented generation, Retriever\_Augmented Generation (RAG), Search engines, Web and internet services, Web search},
	pages = {125--129},
}

@inproceedings{zeeshan_rag_2025,
	title = {{RAG} {Powered} {LLMs} for {QA}: {Evolution}, {Challenges}, {Applications}, and {Future} {Directions}},
	doi = {10.1109/ComTech65062.2025.11034531},
	abstract = {Large language models (LLMs) are evolving to excel in challenging tasks such as text generation, mathematical reasoning, code generation, question answering, text summarization, etc. However, the responses generated by LLMs are prone to hallucinations, out-of-the-date knowledge, and nontransparent and untraceable reasoning. Retrieval augmented generation (RAG) addresses these shortcomings by incorporating external knowledge in the LLM prompt. RAG combines parametric knowledge of LLM with non-parametric knowledge from external databases by efficient retrieval techniques. This comprehensive review paper provides a detailed overview of the significance and emergence of RAG. Moreover, the building blocks of RAG are thoroughly discussed along with the evolution, challenges, and current research trends of deploying RAG-based applications. Finally, we explore the RAG performance enhancement strategies while also underlining the potential future research directions.},
	booktitle = {2025 {International} {Conference} on {Communication} {Technologies} ({ComTech})},
	author = {Zeeshan, Hafiz Muhammad Ali and Faizan, Muhammad and Zia, Usman and Gohar, Abdullah},
	month = apr,
	year = {2025},
	note = {ISSN: 2996-3621},
	keywords = {Codes, Cognition, Communications technology, Databases, Large language models, Large Language Models (LLM), Market research, Question answering (information retrieval), Question Answering (QA), RAG Applications and Challenges, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Reviews, Text summarization},
	pages = {1--6},
}

@inproceedings{mohsin_retrieval_2025,
	title = {Retrieval {Augmented} {Generation} with {Multi}-{Modal} {LLM} {Framework} for {Wireless} {Environments}},
	doi = {10.1109/ICCWorkshops67674.2025.11162457},
	abstract = {Future wireless networks aims to deliver high data rates and lower power consumption while ensuring seamless connectivity, necessitating robust wireless network optimization. To achieve this, wireless network optimization is necessary. Large language models (LLMs) have been deployed for generalized optimization scenarios. To take advantage of generative AI (GAI) models, retrieval augmented generation (RAG) is proposed for multi-sensor wireless environment perception. Utilizing domain-specific prompt engineering, we apply retrieval-augmented generation (RAG) to efficiently harness multimodal data inputs from sensors in a wireless environment. Wireless environment perception is necessary for global LLM optimization tasks. Key pre-processing pipelines including image-to-text conversion, object detection, and distance calculations for multimodal RAG input from multi-sensor data from different devices are proposed in this paper to obtain a unified vector database crucial for optimizing large language models (LLMs) in global wireless tasks. Our evaluation, conducted with OpenAI's GPT and Google's Gemini models, demonstrates an 8\%, 8\%, 10\%, 7\%, and 12\% improvement in relevancy, faithfulness, completeness, similarity, and accuracy, respectively, compared to conventional LLM-based designs. Furthermore, our RAG-based LLM framework with vectorized databases are computationally efficient providing real time convergence under latency constraints. 1},
	booktitle = {2025 {IEEE} {International} {Conference} on {Communications} {Workshops} ({ICC} {Workshops})},
	author = {Mohsin, Muhammad Ahmed and Bilal, Ahsan and Bhattacharya, Sagnik and Cioffi, John M.},
	month = jun,
	year = {2025},
	note = {ISSN: 2694-2941},
	keywords = {6G, Conferences, Databases, Large language models, LLMs, Multi modal LLM, Optimization, Real-time systems, Retrieval augmented generation, Retrieval Augmented Generation, Sensors, Vectors, Wireless Communication, Wireless networks, Wireless sensor networks},
	pages = {184--189},
}

@inproceedings{dong_how_2025,
	title = {How to {Build} an {Adaptive} {AI} {Tutor} for {Any} {Course} {Using} {Knowledge} {Graph}-{Enhanced} {Retrieval}-{Augmented} {Generation} ({KG}-{RAG})},
	doi = {10.1109/ICEIT64364.2025.10975937},
	abstract = {Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems (ITS) presents transformative opportunities for personalized education. However, current implementations face two critical challenges: maintaining factual accuracy and delivering coherent, context-aware instruction. While Retrieval-Augmented Generation (RAG) partially addresses these issues, its reliance on pure semantic similarity limits its effectiveness in educational contexts where conceptual relationships are crucial. This paper introduces Knowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel framework that integrates structured knowledge representation with context-aware retrieval to enable more effective AI tutoring. We present three key contributions: (1) a novel architecture that grounds AI responses in structured domain knowledge, (2) empirical validation through controlled experiments (n=76) demonstrating significant learning improvements (35\% increase in assessment scores, p{\textless}0.001), and (3) a comprehensive implementation framework addressing practical deployment considerations. These results establish KG-RAG as a robust solution for developing adaptable AI tutoring systems across diverse educational contexts.},
	booktitle = {2025 14th {International} {Conference} on {Educational} and {Information} {Technology} ({ICEIT})},
	author = {Dong, Chenxi and Yuan, Yimin and Chen, Kan and Cheng, Shupei and Wen, Chujie},
	month = mar,
	year = {2025},
	keywords = {Adaptation models, Artificial intelligence, Biological system modeling, Education, Generative AI, Information technology, Intelligent Tutoring Systems, Knowledge graphs, Large language model, Large language models, Measurement, Retrieval augmented generation, Retrieval-Augmented generation, Semantics},
	pages = {152--157},
}

@inproceedings{guntupalli_integrating_2024,
	title = {Integrating {Generative} {AI} for {Enhanced} {Automation} in {System} {Design} {Processes}},
	doi = {10.1109/ETFA61755.2024.10710979},
	abstract = {This work delves into the potential of applying Generative AI techniques to the system design phase, aiming to streamline processes and augment human expertise. Gen AI is used to automate the creation of complex design elements and is emerging as a powerful tool for system engineers. Also, with LLM -generated content, evaluating and verifying the correctness of the responses is a challenge. The SE Assistant designed for system engineers intends to create detailed system design documents quickly and accurately along with its evaluation. At the core of the SE Assistant is a sophisticated system that combines the power of GPT-4 with a Multimodal Retrieval Augmented Generation (RAG) pipeline capable of understanding text, images, and tables to provide valuable context. The evaluation uses the strength of strong LLMs in analyzing content based on design-specific criteria. The SE Assistant prototype demonstrates its ability to streamline the system design process, from initial data gathering to the final design output, making it an invaluable tool for system engineers.},
	booktitle = {2024 {IEEE} 29th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Guntupalli, Jayesh and Watanabe, Kentarou},
	month = sep,
	year = {2024},
	note = {ISSN: 1946-0759},
	keywords = {Generative AI, Large Language Model, Manufacturing automation, Pipelines, Prototypes, Retrieval Augmented Generation, Streaming media, System analysis and design, System Design, System Engineering},
	pages = {1--4},
}

@article{dangsungnoen_opg-spell_2025,
	title = {{OPG}-{SPELL}: {Visual} and {Textual} {Explanations} {That} {Influence} {Understanding} of the {AI} {Model} for {Sex} {Prediction} {From} {Orthopantomograms}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3600956},
	abstract = {As AI models become increasingly powerful, the field of Explainable AI (XAI) has gained significant attention. However, the effectiveness of explanations often varies depending on the audience’s domain knowledge. This paper introduces OPG-SPELL (understanding of OrthoPantomoGram’s artificial intelligence prediction through ShaPlEy value and LLms text generation), an approach combining visual and textual explanations to enhance understanding and trust in AI models. OPG-SPELL focuses on an AI model trained to classify sex based on orthopantomograms (panoramic radiographs of human jaw structure). Our method integrates visual explanations using the OPG-SHAP technique with textual explanations generated through Large Language Models’ Retrieval-Augmented Generation. This dual-modality approach aims to bridge the gap between expert and non-expert users, making AI decisions more interpretable across diverse audiences. We conducted a user study with 24 participants to evaluate the influence of both visual and textual explanations on understanding, trust, and willingness to use the XAI tool. Our findings demonstrate the potential of OPG-SPELL to enhance user comprehension and confidence in AI-driven sex prediction from orthopantomograms, contributing to the broader field of intelligent user interfaces and Explainable AI.},
	journal = {IEEE Access},
	author = {Dangsungnoen, Lapatrada and Thongprasant, Kwansawan and Wiratchawa, Kannika and Hirunchavarod, Natthanich and Sributsayakarn, Natnicha and Pornprasertsuk-Damrongsri, Suchaya and Jirarattanasopha, Varangkanar and Piumsomboon, Thammathip and Intharah, Thanapong},
	year = {2025},
	keywords = {Accuracy, Artificial intelligence, Brain modeling, Dentistry, Diagnostic radiography, Explainable AI, Explainable AI (XAI), model trust, model understanding, orthopantomogram, Predictive models, Reliability, Retrieval augmented generation, Visualization},
	pages = {146690--146706},
}

@inproceedings{ran_driver-guide_2025,
	title = {Driver-{Guide}: {A} {Multimodal} {Large} {Language} {Model}-{Based} {Agent} for {Driving} {Scene} {Understanding}},
	doi = {10.23919/CCC64809.2025.11179498},
	abstract = {The research proposes a driving scene understanding agent based on a Multimodal Large Language Model (MLLM) to enhance scene understanding and human-machine interaction capabilities in autonomous driving. A high-quality multimodal driving scene dataset has been constructed, covering tasks such as scene description, question answering, and action suggestions. Two types of multimodal feature mappers (MLP with two hidden layers and Q-Former) were compared, and experiments demonstrated the superior performance of Q-Former in complex scene. Additionally, by incorporating a traffic rules knowledge base and Retrieval-Augmented Generation (RAG), the system effectively reduces hallucination phenomena, ensuring the accuracy and compliance of model responses. Experimental results indicate that the proposed system excels in complex scene semantic understanding and interaction tasks, offering a novel solution to address the limitations of traditional methods in complex driving scene understanding.},
	booktitle = {2025 44th {Chinese} {Control} {Conference} ({CCC})},
	author = {Ran, Yabing and Gao, Bingzhao and Yu, Qiankun},
	month = jul,
	year = {2025},
	note = {ISSN: 1934-1768},
	keywords = {Accuracy, Autonomous Driving, Autonomous vehicles, Human computer interaction, Knowledge based systems, Large language models, Multimodal Large Language Model, Question answering (information retrieval), Retrieval augmented generation, Scene understanding, Semantics, Vehicles},
	pages = {8670--8675},
}

@inproceedings{anjum_halo_2025,
	title = {{HALO}: {Hallucination} {Analysis} and {Learning} {Optimization} to {Empower} {LLMs} with {Retrieval}-{Augmented} {Context} for {Guided} {Clinical} {Decision} {Making}},
	abstract = {Large language models (LLMs) have significantly advanced natural language processing tasks, yet they are susceptible to generating inaccurate or unreliable responses, a phenomenon known as hallucination. In critical domains such as health and medicine, these hallucinations can pose serious risks. This paper introduces HALO, a novel framework designed to enhance the accuracy and reliability of medical question-answering (QA) systems by focusing on the detection and mitigation of hallucinations. Our approach generates multiple variations of a given query using LLMs and retrieves relevant information from external open knowledge bases to enrich the context. We utilize maximum marginal relevance scoring to prioritize the retrieved context, which is then provided to LLMs for answer generation, thereby reducing the risk of hallucinations. The integration of LangChain further streamlines this process, resulting in a notable and robust increase in the accuracy of both open-source and commercial LLMs, such as Llama-3.1 (from 44\% to 66\%) and ChatGPT (from 56\% to 70\%). This framework underscores the critical importance of addressing hallucinations in medical QA systems, ultimately improving clinical decision-making and patient care. The open-source HALO is available at: https://github.com/ResponsibleAILab/HALO.},
	booktitle = {2025 {IEEE}/{ACM} {Conference} on {Connected} {Health}: {Applications}, {Systems} and {Engineering} {Technologies} ({CHASE})},
	author = {Anjum, Sumera and Zhang, Hanzhi and Zhou, Wenjun and Paek, Eun Jin and Zhao, Xiaopeng and Feng, Yunhe},
	month = jun,
	year = {2025},
	note = {ISSN: 2832-2975},
	keywords = {Accuracy, AI Hallucination, Chain of Thought (CoT), Clinical Decision Making, Decision making, Few-shot Question Answering, Large language models, Large Language Models (LLMs), Neurological diseases, Optimization, Prevention and mitigation, Prompt engineering, Question answering (information retrieval), Reliability engineering, Retrieval augmented generation, Retrieval Augmented Generation (RAG)},
	pages = {187--198},
}

@inproceedings{patra_tailored_2024,
	title = {Tailored {Resume} {Generation} {Using} {RAG} with {LLM} as per {Job} {Specifications}},
	doi = {10.1109/PDGC64653.2024.10984036},
	abstract = {The traditional resume being common for each specific job applications which result to rejection for job seekers. These research tailors the resume based on different domain of job applications using Retrieved Augmented Generation (RAG) and LLMs. The automated system starts from resume parsing for information retrieval as per the sections using LLM model like Mixtral, Google Gemma and LLAMA-3. The resume has been further stored into vector databases such as Pinecone and MongoDB with JSON resume further retrieved using re-ranking methods such as BM25 and Cross encoder. The prompt techniques used to construct the model to make them understand better. For providing the memory to the LLM models and refining the tailored resume, the conversational buffer memory is implemented. The efficiency and the performance of the methodology is showcase through the performance evaluation metrics such as BERTscore, RAGAs Metrics and Custom metrics such as Content preservation and Job Alignment. By comparing the cosine similarity for content preservation between LLM and RAG with LLM are 72\% and 89\% respectively. Compared to LLM models using RAG with LLM models generates consistent and relevant tailored resumes. For a better user friendly and seamless UI, the chatbot is developed.},
	booktitle = {2024 {Eighth} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Patra, Komal Prakashchandra and Diwakar, Manoj and Arya, Chandrakala},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-3079},
	keywords = {BERTscore, Computational modeling, Conversational Memory, Internet, Job specification, Large Language Model (LLM), Large language models, LLAMA, Memory, Mixtral, Performance evaluation, Prompt engineering, Prompt Engineering, RAGAS, Refining, Resume Generation, Resumes, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Vectors},
	pages = {848--853},
}

@inproceedings{hussain_mitigating_2025,
	title = {Mitigating {Values} {Debt} in {Generative} {AI}: {Responsible} {Engineering} with {Graph} {RAG}},
	doi = {10.1109/RAIE66699.2025.00006},
	abstract = {Generative AI technologies are rapidly transforming industries such as healthcare, education, and transportation. However, this progress often incurs a Values Debt-ethical and operational deficits due to insufficient ethical considerations during development. This paper examines Values Debt in Gen-erative AI and introduces the Helpful, Honest, Harmless (HHH) framework to align AI systems with human values. In developing GRAISE, a Graph RAG-based chatbot for aviation safety, the HHH framework is applied to integrate ethical practices through-out the development process. This case study demonstrates how the HHH framework addresses ethical challenges and provides reliable contextual information to enhance pilot communication, exemplifying responsible AI engineering. These findings advocate for the broader adoption of ethical AI frameworks across various sectors, promoting trust and integrity in AI applications.},
	booktitle = {2025 {IEEE}/{ACM} {International} {Workshop} on {Responsible} {AI} {Engineering} ({RAIE})},
	author = {Hussain, Waqar},
	month = apr,
	year = {2025},
	keywords = {AI Bias, AI Safety, Ethical AI, Ethics, Generative AI, Hallucination, HHH Framework, Industries, Medical services, Reliability engineering, Responsible AI, Safety, Shape, Symbiosis, Systems engineering and theory, Transportation, Values Debt},
	pages = {9--12},
}

@inproceedings{luo_application_2024,
	title = {Application and evaluation of {RAG} technology in civil aviation policy question answering},
	doi = {10.1109/CNTEIE66268.2024.00032},
	abstract = {Civil aviation policies are crucial for practitioners, but the current method of manually searching for relevant policies is cumbersome and inefficient. Existing question-answering systems based on knowledge graphs or large language models are not ideal due to the lack of high-quality policy question-answering datasets and the inability to update in real-time. To this end, this paper builds a civil aviation policy question-answering system based on Retrieval-Augmented Generation (RAG). The system obtains civil aviation policy-related knowledge from multiple data sources and segments it, uses ZhipuEmbedding-3 and Chroma to create a civil aviation policy text block vector library, combines vector retrieval technology to complete the retrieval of Query text blocks, obtains the context most relevant to the Query, and then combines the prompt word function of the large language model to integrate the Query and context information in real time to generate accurate answers. Finally, we use the Ragas evaluation framework to conduct a comprehensive evaluation of the system. The results show that Qwen-max has the best overall performance in the indicators related to Faithfulness, Answer Relevancy, and Context Recall, which are 0.89, 0.75, and 0.90, respectively. The reliability and effectiveness of the system have been verified.},
	booktitle = {2024 2nd {International} {Conference} on {Computer} {Network} {Technology} and {Electronic} and {Information} {Engineering} ({CNTEIE})},
	author = {Luo, Yinhui and Xu, Wenhao and Zeng, Changchang and Fu, Qiang and Xiang, Ziyu},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Knowledge graphs, LangChain, Large Language Model, Large language models, Prompt Engineering, Question answering (information retrieval), Real-time systems, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors, Usability, Vectors},
	pages = {138--142},
}

@inproceedings{yang_apicoder_2025,
	title = {{APICoder}: {A} {Multi}-{Role} {Large} {Language} {Model} {Framework} for {API} {Service} {Call} {Code} {Generation}},
	doi = {10.1109/ICWS67624.2025.00109},
	abstract = {With the rapid advancement of large language models (LLMs) in code generation, the task of API service invocation code generation faces increasing challenges such as inaccurate context understanding, improper API selection, and outdated data. To address these issues, this paper constructs a high-quality dataset, APIDataset, and fine-tunes the DeepSeek-Coder-V2-Lite-Instruct model based on this dataset. We further integrate a retrieval-augmented generation (RAG) approach to enhance the model's contextual comprehension capabilities. Moreover, we propose APICoder, a multi-role collaborative framework that optimizes different stages of code generation, significantly improving the quality of generated API invocation code. Finally, we develop the APITest benchmark and conduct two rounds of evaluation combining both API-specific and general code generation test sets. Experimental results demonstrate that APICoder achieves superior performance in both API invocation and general code generation tasks, validating its effectiveness and practical potential.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Yang, Conghui and Yu, Lei and Su, Huafeng and Zhou, Xiang},
	month = jul,
	year = {2025},
	note = {ISSN: 2836-3868},
	keywords = {API service call, Benchmark testing, Code generation, Codes, Collaboration, Context modeling, Faces, Large language models, LLMs, Multirole, RAG, Reinforcement learning, Retrieval augmented generation, Web services},
	pages = {849--851},
}

@inproceedings{singh_understanding_2025,
	title = {Understanding and {Mitigating} {Hallucinations} in {Large} {Language} {Models}: {Insights} from a {Systematic} {Literature} {Review}},
	doi = {10.1109/ICMCTC62214.2025.11196493},
	abstract = {The analysis of hallucinations in Large Language Models (LLMs), sequences that are coherent but factually false and semantically conflicting, has implications for the credibility and practical value of such systems. To address this concerning issue, the current paper presents a three-tier systematic review of 30 foundational and state-of-the-art publications, summarizing the existing knowledge on the error’s nature of hallucinations in LLMs and their antecedents and management. The classification of hallucinations is analyzed as intrinsic, extrinsic, compounding, and multimodal errors; focusing on their phenomenon in tasks of questions answering, summarization, and other multimodal cases. Current mitigation strategies are discussed at the data level (e.g., dataset cleaning and ad hoc knowledge injection), at the model level (e.g., fine-tuning and regularization), and at the inference level (e.g., retrieval-augmented generation and prompt engineering). Benchmarks such as HALUEVAL and TruthfulQA are compared to determine the usefulness and drawbacks for measuring the incidence of hallucinations. Some of the recurring issues such as the scarcity of domain-related corpora, adaptability in the LM inspired by scarce resources, and the ethical use of purposeful hallucinations are observed together with the prospects of a combination of approaches and large-scale evaluation frameworks. It is proposed that this review be used as a guide for future research in the domain of LLMs and as a reference that highlights gaps and possible directions for continuing work towards the development of more dependable, domain-adaptive, and ethically consistent LLMs.},
	booktitle = {2025 {International} {Conference} on {Metaverse} and {Current} {Trends} in {Computing} ({ICMCTC})},
	author = {Singh, Ravneet and Singh, Parminder and Malik, Arun and Sukmawan, Dede},
	month = apr,
	year = {2025},
	keywords = {and Large Language Models (LLMs) Benchmarks, Benchmark testing, Ethics, Generative AI, Hallucinations in LLMs, Large language models, Market research, Metaverse, Multimodal Alignment, Prevention and mitigation, Prompt engineering, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Systematic literature review, Trustworthy AI},
	pages = {1--10},
}

@inproceedings{richard_slm-based_2025,
	title = {{SLM}-based {Hybrid} {Retrieval} for {Resource} {Constrained} {Retrieval}-{Augmented} {Generation} on {Open} {Super}-{Large} {Crawled} {Data}},
	doi = {10.1109/ICSP65755.2025.11086925},
	abstract = {Large Language Models (LLMs) have significantly advanced the accessibility of natural language processing across diverse applications. However, susceptibility to hallucinations and reliance on outdated data limit their reliability. Retrieval-Augmented Generation (RAG) systems address this by providing access to external, up-to-date information during inference. Despite this improvement, existing dense, sparse, and hybrid retrieval strategies often struggle to balance semantic relevance and efficiency. In this study, we implement a hybrid retrieval approach that utilizes a small language model (SLM)-based reranking. We compare its performance against traditional hybrid retrieval methods combining FAISS HNSW with BM25 and TF-IDF reranking on a 500,000-document subset from OSCAR, evaluated under realistic compute limits using a single-GPU Colab environment (L4 GPU, 22.5 GB VRAM) to simulate resource-constrained RAG settings. We evaluate retrieval quality (precision, Recall) and efficiency (seconds-per-query, query-per-seconds). Results show that hybrid BM25 (0.05 SPQ, 21.96 QPS) and TF-IDF (0.01 SPQ, 71.17 QPS) methods are more efficient, but the SLM-based hybrid achieves superior retrieval accuracy. It substantially outperforms both traditional approaches in top-5 relevance, achieving a Precision@5 of 0.028 and a Recall of 0.102.},
	booktitle = {2025 10th {International} {Conference} on {Intelligent} {Computing} and {Signal} {Processing} ({ICSP})},
	author = {Richard, Roman M. and Villanueva, Alonica R.},
	month = may,
	year = {2025},
	keywords = {Big Data, Generative AI, Graphics processing units, Hybrid power systems, Information retrieval, Information Retrieval, Large language models, Natural language processing, Reliability, Retrieval augmented generation, Semantics, Signal processing},
	pages = {1157--1160},
}

@book{anthapu_notitle_2025,
	isbn = {978-1-83620-622-4},
	url = {https://ieeexplore.proxyucr.elogim.com/document/11099031},
	abstract = {A comprehensive guide to building cutting-edge generative AI applications using Neo4j's knowledge graphs and vector search capabilitiesKey FeaturesDesign vector search and recommendation systems with LLMs using Neo4j GenAI, Haystack, Spring AI, and LangChain4jApply best practices for graph exploration, modeling, reasoning, and performance optimizationBuild and consume Neo4j knowledge graphs and deploy your GenAI apps to Google CloudPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEmbark on an expert-led journey into building LLM-powered applications using Retrieval-Augmented Generation (RAG) and Neo4j knowledge graphs. Written by Ravindranatha Anthapu, Principal Consultant at Neo4j, and Siddhant Agrawal, a Google Developer Expert in GenAI, this comprehensive guide is your starting point for exploring alternatives to LangChain, covering frameworks such as Haystack, Spring AI, and LangChain4j. As LLMs (large language models) reshape how businesses interact with customers, this book helps you develop intelligent applications using RAG architecture and knowledge graphs, with a strong focus on overcoming one of AI’s most persistent challenges—mitigating hallucinations. You'll learn how to model and construct Neo4j knowledge graphs with Cypher to enhance the accuracy and relevance of LLM responses. Through real-world use cases like vector-powered search and personalized recommendations, the authors help you build hands-on experience with Neo4j GenAI integrations across Haystack and Spring AI. With access to a companion GitHub repository, you’ll work through code-heavy examples to confidently build and deploy GenAI apps on Google Cloud. By the end of this book, you’ll have the skills to ground LLMs with RAG and Neo4j, optimize graph performance, and strategically select the right cloud platform for your GenAI applications.What you will learnDesign, populate, and integrate a Neo4j knowledge graph with RAGModel data for knowledge graphsIntegrate AI-powered search to enhance knowledge explorationMaintain and monitor your AI search application with HaystackUse LangChain4j and Spring AI for recommendations and personalizationSeamlessly deploy your applications to Google Cloud PlatformWho this book is forThis LLM book is for database developers and data scientists who want to leverage knowledge graphs with Neo4j and its vector search capabilities to build intelligent search and recommendation systems. Working knowledge of Python and Java is essential to follow along. Familiarity with Neo4j, the Cypher query language, and fundamental concepts of databases will come in handy.},
	publisher = {Packt Publishing},
	author = {Anthapu, Ravindranatha and Agarwal, Siddhant and Webber, Dr. Jim and Risch, Dr. Julian},
	year = {2025},
	note = {Publication Title: Building Neo4j-Powered Applications with LLMs: Create LLM-driven search and recommendations applications with Haystack, LangChain4j, and Spring AI},
}

@inproceedings{fan_towards_2025,
	title = {Towards {Retrieval}-{Augmented} {Large} {Language} {Models}: {Data} {Management} and {System} {Design}},
	doi = {10.1109/ICDE65448.2025.00341},
	abstract = {Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by integrating external, reliable, and up-to-date knowledge. This addresses critical limitations such as hallucinations and outdated internal information. This tutorial delves into the evolution and frameworks of RAG, emphasizing the pivotal role of data management technologies in optimizing query processing, storage, indexing, and efficiency. It explores how RAG systems can deliver high-quality, context-aware outputs through efficient retrieval and integration, covering key topics such as retrieval-augmented LLM (RA-LLM) architectures, retrieval techniques, learning methodologies, and applications in NLP and domain-specific tasks. Challenges like customized query and generation, real-time retrieval, and trustworthy RAG are discussed alongside future directions and opportunities for innovation. Designed for students, researchers, and industry practitioners with basic artificial intelligence and data engineering knowledge, this tutorial offers practical insights into designing data management-powered RAG systems. It inspires the exploration of novel solutions in this rapidly evolving field.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Fan, Wenqi and Wu, Pangjing and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Qing},
	month = may,
	year = {2025},
	note = {ISSN: 2375-026X},
	keywords = {Data engineering, Data management, Knowledge engineering, Large Language Model, Large language models, Query processing, Real-time systems, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, System analysis and design, Technological innovation, Tutorials},
	pages = {4509--4512},
}

@inproceedings{munir_leveraging_2024,
	title = {Leveraging {Multimodal} {Retrieval}-{Augmented} {Generation} for {Cyber} {Attack} {Detection} in {Transit} {Systems}},
	doi = {10.1109/TPS-ISA62245.2024.00046},
	abstract = {Large Language Models (LLMs) often tend to hallucinate, one of the reasons is due to limitations in their training datasets. These datasets are vast, and the training process is resource-intensive, making LLMs unreliable for generating accurate responses for recent information. To address this issue, Retrieval-Augmented Generation (RAG) uses indexed text chunks from relevant, up-to-date knowledge databases to generate more accurate and current responses. Our project explores the use of RAG in the domain of transit security. Transit security systems include physical objects such as video and audio surveillance, alarms, threat sensors, and infrastructure monitoring sensors, which scan the environment for potential threats and relay this information to the Transit Management Center, Transit Vehicles, Emergency Management Center, etc. We aimed to predict potential cyber threats to these information flows that adversaries might exploit to infiltrate the systems. By utilizing the description of the information flow and other characteristics of the data, we leveraged LLMs with RAG to map possible cyber attack techniques from the MITRE ATT\&CK knowledge-base. As the MITRE ATT\&CK technique database is continuously updated to keep track of the new cyberattack techniques, using RAG enhances our ability to predict how adversaries might target transit security information flows. We analyzed information flows of transit systems from the USDOT public website, manually annotating possible attack techniques to establish a benchmark. Our multimodal RAG model achieved an F-1 score of 40.5\% and a precision of 42.5\%, representing a 73.65\% improvement over the baseline approach. These results demonstrate the effectiveness of integrating LLMs with RAG and incorporating multimodality in predicting cyber threats in transit cybersecurity.},
	booktitle = {2024 {IEEE} 6th {International} {Conference} on {Trust}, {Privacy} and {Security} in {Intelligent} {Systems}, and {Applications} ({TPS}-{ISA})},
	author = {Munir, Muhaimin Bin and Cai, Yuchen and Khan, Latifur and Thuraisingham, Bhavani},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Cyberattack, Databases, Knowledge based systems, Large Language Model, MITRE ATT\&CK, Multimodal RAG, Retrieval augmented generation, Retrieval Augmented Generation, Sensor systems, Sensors, Target tracking, Threat assessment, Training, Transit System, Transportation Security},
	pages = {341--350},
}

@inproceedings{darji_enhancing_2024,
	title = {Enhancing {Financial} {Risk} {Analysis} using {RAG}-based {Large} {Language} {Models}},
	doi = {10.1109/ICACRS62842.2024.10841711},
	abstract = {The rapid development of Generative AI has brought major changes in way of functioning of different sectors throughout the world. Many research work has been done in the field of financial sector to increase the efficiency and reduce the errors due to human intervention. However, the current financial risk analysis relies on manual reviews and conventional machine learning models which repeatedly failing to process financial risk data. This study investigates how Retrieval-Augmented Generation (RAG) approach can help Large Language Models (LLM) to generate risk analysis reports for audit reports which extract detailed information from the audit reports and avoid overlooking of small details, which was a major drawback in the earlier system. This research study covers how Retrieval Augmented Generation (RAG) enhances the performance financial risk analysis of audit reports using different LLMs like GPT-4o, Gemini-1.5-flash, and LlaMa3.1. This research work includes the performance of LLMs beyond multiple metrics, including faithfulness, context precision-recall-relevancy, and answer relevance. The research findings imply that LlaMa3.1 is a great model in terms of faithfulness of the generated report with a score of 78.26\%. In terms of retrieval of the documents and its context, Llama had a very strong performance by getting the score of 79.62\% in context-precision, 78.26\% in context-recall and 86.99\% in context-relevancy. In terms of generated report, the Llama3.1 model have the score of 37.83\% for answer-relevancy and Gemini-1.5-flash have a score of 58.64\% for answer-correctness.},
	booktitle = {2024 3rd {International} {Conference} on {Automation}, {Computing} and {Renewable} {Systems} ({ICACRS})},
	author = {Darji, Abhishek and Kheni, Fenil and Chodvadia, Dhruvil and Goel, Parth and Garg, Dweepna and Patel, Bankim},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Analytical models, Computational modeling, Financial Risk Analysis, Gemini-1.5-flash, Generative AI, GPT-4o, Large language Model (LLM), Large language models, Machine learning, Natural language processing, Ollama, Renewable energy sources, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Reviews, Risk analysis},
	pages = {754--760},
}

@inproceedings{guo_integrating_2025,
	title = {Integrating {Retrieval}-{Augmented} {Generation} ({RAG}) and {Knowledge} {Augmented} {Generation} ({KAG}) {Frameworks} to {Build} {Accurate} {Enterprise} {Question} {Answering} {Systems}},
	volume = {8},
	doi = {10.1109/IAEAC65194.2025.11166604},
	abstract = {With the rapid development of Large Language Models (LLMs), more and more requirements from enterprises for accurate question answering (QA) systems are emerging. LLMs have shown great potential in natural language processing (NLP) but face limitations such as hallucinations and lack of domain-specific knowledge. Retrieval-Augmented Generation (RAG) addresses these issues by retrieving external information to enhance LLMs' outputs, while Knowledge Augmented Generation (KAG) further improves reasoning capabilities through knowledge graphs. We propose five strategies for combining RAG and KAG, each balancing storage, retrieval efficiency, and response accuracy. Experiments using a dataset from a power industry company show that integrating RAG and KAG improves QA system performance, with Strategy 5 achieving the highest accuracy rate of 78\%. Our findings highlight the potential of combining retrieval and knowledge augmentation to enhance enterprise QA systems.},
	booktitle = {2025 {IEEE} 8th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	author = {Guo, Yonghe and Yan, Longchuan and Niu, Jianing and Gao, Dequan and Yuan, Xiaoyu},
	month = aug,
	year = {2025},
	note = {ISSN: 2689-6621},
	keywords = {Accuracy, Companies, Information technology, KAG, Knowledge graphs, Large language models, Large Language Models, Power industry, Question Answering, Question answering (information retrieval), RAG, Resource management, Retrieval augmented generation, System performance},
	pages = {651--657},
}

@inproceedings{hamayat_seebot_2025,
	title = {{SEEBot}: {Leveraging} {Open}-{Source} {LLMs} and {RAG} for {Secure} and {Economical} {Enterprise} {Chatbots}},
	doi = {10.1109/ICAIE64856.2025.11158323},
	abstract = {In this research, we introduce SEEBot, a secure and economical enterprise chatbot designed for local deployment within organizations to address data privacy and security concerns. SEEBot leverages a PrivateGPT architecture, integrating open-source large language models (LLMs) with retrieval-augmented generation (RAG) for enhanced performance. We evaluated three open-source LLMs including: Llama2 7B, Llama2 13B, and Mistral 7B, using a benchmark dataset of 44 questions from the Paul Graham Dataset, assessed through LlamaIndex. Key performance metrics include faithfulness, relevance, and inference time. Additionally, we compare these models with GPT-3.5 Turbo, using two embedding models: (i) open source BAAI General Embedding (BGE), and (ii) paid OpenAI's text-embedding-ada-002. Experimental results indicate that Mistral 7B outperforms Llama2 7B, Llama2 13B, and GPT-3.5 Turbo in faithfulness and relevance, while GPT-3.5 Turbo achieves faster inference times. In addition, the performance of Mistral 7B was nearly same using both the OpenAI's and BAAI's embedding models. Moreover, our cost analysis demonstrates that deploying SEEBot reduces annual expenses by nearly 45\% compared to cloud-based solutions. These findings support SEE-Bot as a viable, cost-effective, and privacy-conscious solution for enterprise chatbot deployment within secure, locally controlled infrastructures.},
	booktitle = {2025 5th {International} {Conference} on {Artificial} {Intelligence} and {Education} ({ICAIE})},
	author = {Hamayat, Faizan and Ejaz, Luqman and Danish, Muhammad and Nazir, Ahsen and Ahadian, Pegah and Ahmad, Rana Fayyaz},
	month = may,
	year = {2025},
	keywords = {AI based Assistant, Biological system modeling, Chatbots, Education, Large language models, Measurement, Open-Source LLMs, Organizations, Paul Graham Dataset, Performance metrics, Private Chatbot, RAG, Retrieval augmented generation, Security, SEEBot, Time factors},
	pages = {147--151},
}

@inproceedings{kong_document_2024,
	title = {Document {Embeddings} {Enhance} {Biomedical} {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/BIBM62325.2024.10822781},
	abstract = {Large language models (LLMs) perform well in many NLP tasks but frequently generate inaccurate information in the biomedical domain, due to hallucination issues. Retrieval-Augmented Generation (RAG) has been introduced to address this issue by integrating external knowledge, enhancing the factual accuracy of outputs. However, naive RAG encounters challenges in effectively utilizing retrieved content, particularly in specialized domains like biomedicine. LLMs often struggle to integrate retrieved content as irrelevant information can interfere with the model’s judgment. Even if relevant documents are retrieved, the model may be unable to accurately comprehend and utilize the domain-specific features due to its inherent knowledge limitations. To overcome these limitations, we propose Document Embeddings Enhanced Biomedical RAG (DEEB-RAG), a framework that incorporates document embeddings along with the original retrieved text. DEEB-RAG uses MedCPT to generate document embeddings and these embeddings are then aligned with the LLM’s semantic space using a two-stage training process on a simple projector. Experimental results on biomedical QA datasets show that DEEB-RAG improves accuracy, with an average performance increase of 2.3\% over naive RAG. This demonstrates DEEB-RAG’s ability to mitigate the challenges of utilizing complex biomedical information, thereby enhancing the reliability and effectiveness of LLMs in biomedical domain.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Kong, Yongle and Yang, Zhihao and Luo, Ling and Ding, Zeyuan and Wang, Lei and Liu, Wei and Zhang, Yin and Xu, Bo and Wang, Jian and Sun, Yuanyuan and Zhao, Zhehuan and Lin, Hongfei},
	month = dec,
	year = {2024},
	note = {ISSN: 2156-1133},
	keywords = {Accuracy, Bioinformatics, Biological system modeling, biomedical question answering, document embedding, large language model, Large language models, Question answering (information retrieval), Refining, Reliability, retrieval augmented generation, Retrieval augmented generation, Semantics, Training},
	pages = {962--967},
}

@inproceedings{yoon_implementation_2025,
	title = {Implementation of {Multi}-{Level} {RAG} {Model} for {Enhanced} {Synergistic} {Vulnerability} {Analysis}},
	doi = {10.1109/ICCT-Pacific63901.2025.11012876},
	abstract = {Traditional vulnerability analysis models often rely on a single data source, which limits their ability to comprehensively detect and analyze vulnerabilities. In this study, we propose a multi-level Retrieval-Augmented Generation (RAG) model that progressively integrates multiple vulnerability databases to overcome these limitations. By incorporating core and peripheral data sources at different levels, our approach aims to enhance the quality of vulnerability analysis. We integrate four major vulnerability databases-National Vulnerability Database, CERT Vulnerability Notes Database, GitHub Advisory Database, and Exploit Database-into a Level 4 RAG model. This integration demonstrates an average performance improvement of 6.4\% compared to a Level 1 model that only refers to the National Vulnerability Database. The evaluation is based on several metrics, including METEOR, BERT Score, Faithfulness, and Answer Relevancy, which collectively highlight the benefits of a multi-source, incremental integration strategy for more effective vulnerability analysis. These results highlight the potential of multi-level RAG models in enhancing vulnerability analysis through the integration of complementary vulnerability data sources.},
	booktitle = {2025 1st {International} {Conference} on {Consumer} {Technology} ({ICCT}-{Pacific})},
	author = {Yoon, Min-Joo and Yoo, Sun-Mo and Park, Jong-Hwa and Park, Ki-Woong},
	month = mar,
	year = {2025},
	keywords = {Analytical models, Common Vulnerabilities and Exposures, Data integration, Data models, Databases, Large Language Model, Measurement, Meteors, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors, Software development management},
	pages = {1--4},
}

@inproceedings{agrawal_llm_2025,
	title = {{LLM} {Interpretability}: {Tracing} {How} {LLMs} {Answer} {Factual} {Queries} and {Math} {Questions}},
	doi = {10.1109/CAI64502.2025.00151},
	abstract = {In this paper, we study how LLMs like GPT store and retrieve facts to answer factual queries. Using a pretrained GPT-2 model, we evaluate factual accuracy on knowledge datasets, math questions, and hand-crafted prompts, employing metrics such as weighted first token accuracy, F1 score for token overlap, perplexity, and BLEU. We leverage interpretability techniques like attention visualization, logit-lens analysis, and causal tracing to identify layers responsible for knowledge retrieval. To enhance the model's factual and mathematical capabilities, we implement prompt engineering, retrieval-augmented generation (RAG), and fine-tuning, comparing their impact on accuracy and fact retrieval mechanisms.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Agrawal, Shweta and Li, He Nan Tony and Lu, Lucas},
	month = may,
	year = {2025},
	keywords = {Accuracy, AI, explainability, interpretability, LLM, Mathematical models, Measurement, Prompt engineering, Retrieval augmented generation, Visualization},
	pages = {1--6},
}

@inproceedings{mao_scrag_2025,
	title = {{scRAG}: an {Efficient} {Retrieval} {Augmented} {Generation} {System} for {scRNA}-seq {Data} {Analysis}},
	doi = {10.1109/ICDE65448.2025.00352},
	abstract = {An average person usually contains more than 10 trillion human cells, and each cell's transcriptome can be profiled by a single-cell RNA sequence (scRNA-seq). The huge volume and high complexity of scRNA-seq data put challenges on fundamental tasks of scRNA-seq data analysis, i.e., cell type identification and new cell type discovery. In this paper, we demonstrate scRAG, which can efficiently remove batch effect in cell-type identification and enable reliable new cell discovery, facilitated by GPU-based scRNA-seq data management and Large Language Models (LLMs). The GPU-based scRNA-seq data management enables high throughput scRNA-seq data retrieval and update, while the LLM utilizes the retrieval results to remove the batch effect and discover novel cells. We demonstrate scRAG for its: (a) interfaces, (b) GPU-based scRNA-seq data management, and (c) applications in batch effect removal and cancer cell discovery.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Mao, Yuren and Zhu, Yifan and Liu, Qing and Liu, Peigen and Yu, Haoran and Gao, Yunjun},
	month = may,
	year = {2025},
	note = {ISSN: 2375-026X},
	keywords = {Cancer, Complexity theory, Data analysis, Data engineering, Data retrieval, Large language models, Reliability, Retrieval augmented generation, Retrieval Augmented Generation, RNA, scRNA-seq Data Analysis, scRNA-seq Data Management, Throughput},
	pages = {4560--4563},
}

@inproceedings{abolhasani_ontokgen_2025,
	title = {{OntoKGen}: {A} {Genuine} {Ontology} and {Knowledge} {Graph} {Generator} {Using} {Large} {Language} {Model}},
	doi = {10.1109/RAMS48127.2025.10935139},
	abstract = {Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.},
	booktitle = {2025 {Annual} {Reliability} and {Maintainability} {Symposium} ({RAMS})},
	author = {Abolhasani, Mohammad Sadeq and Pan, Rong},
	month = jan,
	year = {2025},
	note = {ISSN: 2577-0993},
	keywords = {Generators, Knowledge graphs, Large Language Model, Large language models, Neo4J, Ontologies, Ontology and Knowledge Graph Generator, Pipelines, Prompt engineering, Prompt Engineering, Random access memory, Reliability engineering, Retrieval augmented generation, User interfaces},
	pages = {1--6},
}

@inproceedings{alshammary_rfpg_2024,
	title = {{RFPG}: {Question}-{Answering} from {Low}-{Resource} {Language} ({Arabic}) {Texts} using {Factually} {Aware} {RAG}},
	doi = {10.1109/CIC62241.2024.00023},
	abstract = {Since Large Language Models (LLMs) face several challenges, including hallucinations, the Retrieval-Augmented Generation (RAG) model has been proposed as a solution. While RAG has been widely used for English, it remains underexplored in low-resource languages like Arabic. This study enhances the RAG model by focusing on a specific domain within Arabic, a low-resource language. Our proposed framework, RFPG, addresses question-answering by integrating (a) fact-checking into the retrieval process and (b) customized and innovative prompts. Our model was tested on 123 questions and it was able to answer with an accuracy of 100\% and reference the sources with a precision of 98\%, outperforming both RAG and standard LLMs, including the latest models like GPT-4o, GPT-4o mini, and GPT-4, in Arabic text question-answering.},
	booktitle = {2024 {IEEE} 10th {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Alshammary, Mitha and Uddin, Md Nahiyan and Khan, Latifur},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Artificial Intelligence (AI), Atmospheric modeling, Collaboration, Computational modeling, Faces, Focusing, Generative AI, Hallucination, Internet, Large language models, Large Language Models (LLMs), Low-Resource Languages, Question Answering, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Standards},
	pages = {107--116},
}

@inproceedings{shayaninasab_enhancing_2024,
	title = {Enhancing {Patient} {Intake} {Process} in {Mental} {Health} {Consultations} {Using} {RAG}-{Driven} {Chatbot}},
	doi = {10.1109/ACIIW63320.2024.00053},
	abstract = {In this paper, we develop and evaluate an empathic chatbot, called EmoBot, for a mental healthcare patient in-take scenario. EmoBot employs Retrieval-Augmented Generation (RAG) methods with Large Language Models (LLMs), to provide empathic support and advice, while ensuring the intake process is completed. We evaluate EmoBot's performance both by automated metrics using available datasets and through dialogues with a simulated patient model that mimics varying levels of depression severity. Our evaluations showed good agreement between EmoBot's categorization of depression severity levels and human raters. EmoBot's topic classification system achieved 78.48\% accuracy on the Primate2022 dataset [1] without fine-tuning. EmoBot's responses showed a 0.93 similarity with patient inputs, proving contextual relevance. These findings highlight the effectiveness of RAG-based methods in guiding and providing safety guardrails for LLMs and their potential as supportive tools for health assessment and support.},
	booktitle = {2024 12th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} {Workshops} and {Demos} ({ACIIW})},
	author = {Shayaninasab, Minoo and Zahoor, Maryiam and Yalçin, Özge Nilay},
	month = sep,
	year = {2024},
	keywords = {Chatbots, Conferences, Conversational Agents, Depression, Large language models, Large Language Models (LLM), Measurement, Medical services, Mental health, Mental Health, MIMICs, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Safety},
	pages = {256--264},
}

@inproceedings{barnett_seven_2024,
	title = {Seven {Failure} {Points} {When} {Engineering} a {Retrieval} {Augmented} {Generation} {System}},
	abstract = {Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS• Software and its engineering → Empirical software validation.},
	booktitle = {2024 {IEEE}/{ACM} 3rd {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Barnett, Scott and Kurniawan, Stefanus and Thudumu, Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
	month = apr,
	year = {2024},
	keywords = {Case Study, Chatbots, Education, Information retrieval, RAG, Retrieval Augmented Generation, Robustness, SE4AI, Semantic search, Software, Task analysis},
	pages = {194--199},
}

@article{song_enhancing_2025,
	title = {Enhancing {RAG} {Performance} by {Representing} {Hierarchical} {Nodes} in {Headers} for {Tabular} {Data}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3569872},
	abstract = {Recent developments in retrieval-augmented generation (RAG) techniques have aimed at integrating structured tabular data with external data sources. Nevertheless, because existing approaches have difficulty effectively retrieving and reasoning from diverse and complex table structures, there is a growing demand for innovative methods that surpass traditional retrieval paradigms. This study introduced hierarchical node-based header representation models designed to enhance the processing and integration of external table data, addressing the key limitations of existing RAG techniques when working with structured table sources. By classifying table types designed for the structural complexity and the level of domain knowledge required for their interpretation, this study explores the impact of using embedded information to improve retrieval accuracy. The findings of this study demonstrate that presenting richer and more meaningful information within table headers significantly enhances RAG performance. This improvement is attributed to the utilization of table header representations, which allow for a more precise identification of hierarchical node-based headers during the table retrieval process. Furthermore, this study confirms that vector database achieves greater accuracy when employing a distributed table collection (DTC) structure than a unified context collection (UCC). These results indicate that the structure of the information extracted from text and tables varies considerably. Our experiment results highlight that separating and organizing such information using the DTC structure proved to be an effective strategy for improving the accuracy of table-specific responses.},
	journal = {IEEE Access},
	author = {Song, Minchae},
	year = {2025},
	keywords = {Accuracy, Adaptation models, Cognition, Context modeling, Finance, Generative AI, Hierarchical nodes, Pipelines, Retrieval augmented generation, retrieval-augmented generation (RAG), Soft sensors, table retrieval, Training, vector database},
	pages = {85072--85083},
}

@inproceedings{bharambe_exploring_2025,
	title = {Exploring {Opportunities} and {Addressing} {Challenges} in {Designing} {Knowledge} {Graph}-{Enhanced} {RAG}-{Based} {Chatbots} for {Managing} {Radiation} {Toxicity} in {Cancer} {Care}},
	doi = {10.1109/AI2E64943.2025.10983078},
	abstract = {Chatbots offer essential support for patients undergoing radiation therapy by providing timely advice, tracking symptoms, and suggesting coping strategies for side effects. They act as a bridge between patients and their care teams, ensuring continuous guidance during treatment. This requires accurate, context-aware, and explainable responses, which is challenging when designing chatbots. New features and capabilities have recently been introduced to chatbot functionality via Large Language Models (LLMs) using technologies such as Retrieval-Augmented Generation (RAG), chatbots can retrieve real-time, relevant information, generating contextually relevant interactions. Traditional LLMs and RAG models still suffer from limitations such as generating hallucinated information and difficulty reasoning over complex relationships. These challenges can be addressed with knowledge graphs, which offer structured, interconnected data representations for chatbots to deliver grounded, understandable responses. Designing a chatbot that integrates knowledge graphs with RAG presents significant opportunities and challenges for developing intelligent, patient-centric conversational systems. Knowledge graphs enhance structured reasoning and accuracy, while RAG supports dynamic and personalized interactions. The paper examines architectural considerations, technological possibilities, and practical applications of such systems, especially in healthcare domains like oncology, where precision and reliability are crucial.},
	booktitle = {2025 {International} {Conference} for {Artificial} {Intelligence}, {Applications}, {Innovation} and {Ethics} ({AI2E})},
	author = {Bharambe, Uiwala and Patil, Kaushal and Ingle, Pushkar and Bhangale, Ujwala},
	month = feb,
	year = {2025},
	keywords = {Accuracy, Cancer, Cancer Care, Chatbot, Chatbots, Clinical Decision Support, Cognition, Computer architecture, Data visualization, Ethics, Knowledge graphs, Knowledge Graphs, Large Language Model, Large language models, Patient Monitoring, Retrieval augmented generation, Retrieval Augmented Generation},
	pages = {1--6},
}

@inproceedings{nickel_integrating_2025,
	title = {Integrating {LLMs} with {NetBox} and {Netmiko} for {Vendor}-{Agnostic} {Intent}-{Based} {Networking}},
	doi = {10.1109/NOMS57970.2025.11073741},
	abstract = {The operation of modern network infrastructures presents network operations engineers with considerable challenges, esp. in multi-vendor environments, due to their inhomogeneity and technological diversity. Intent-based networking (IBN) offers a solution to reduce this complexity by allowing users to specify desired network states as intents, which are then translated into network policies and continuously monitored. This paper presents an approach for the translation and implementation of intents based on Retrieval-Augmented Generation (RAG) that is integrated with the network automation tools Netmiko and NetBox. Intents can be used to express and perform changes in these tools, e.g., to automate otherwise tedious and hence error-prone tasks to assist human network administrators. The evaluation shows a 90.2\% success rate for individual phases (Intent Conception, Request Construction, Response Correctness) and 81.2\% end-to-end. To safeguard network availability and integrity, the Large Language Model (LLM) references manufacturer documentation online and prompts operators for confirmation. The implementation is provided as open source. Potential network management and operations use cases benefiting from LLMs and RAG are discussed based on the accuracy, correctness, and practicability of the presented testbed.},
	booktitle = {{NOMS} 2025-2025 {IEEE} {Network} {Operations} and {Management} {Symposium}},
	author = {Nickel, Lucas Immanuel and Hohmann, Lorenz and Stolbov, Nick and Gerstacker, Lukas and Rieger, Sebastian},
	month = may,
	year = {2025},
	note = {ISSN: 2374-9709},
	keywords = {Accuracy, Automation, Complexity theory, Documentation, Intent-based Networking, Large Language Model, Large language models, Monitoring, Network Automation, Network Operations, Nonhomogeneous media, Retrieval augmented generation, Retrieval-Augmented Generation, Translation},
	pages = {1--6},
}

@inproceedings{cheng_rule_2025,
	title = {Rule {Verification} {Method} for {Power} {Operation} {Order} {Instructions} based on {Large} {Language} {Model}},
	doi = {10.1109/AAIEE64965.2025.11100921},
	abstract = {In order to identify the temporary additional rules that may appear for specific operation tasks to prevent errors in substation operation instructions, a rule verification method for power operation order instructions is established based on a large language model (LLM). First, the paper uses the graph isomorphism algorithm to form an error-prevention knowledge graph (EPKG) that integrates the physical structure information and rule information of the power grid, so as to improve the analysis ability of the LLM on the power grid topology; the paper also uses retrieval-augmented generation (RAG) technology, which is used to improve the understanding ability of the LLM on the operation rules, to build a vector database for the temporary additional rules of the operation task. Then, by extracting the argument information of operation instructions and using the argument post-processing method, the operation instruction graph is obtained which can be located on the EPKG to realize the graph-based instruction verification. Furthermore, the corresponding additional rules related to an instruction that can be understood by LLM are extracted from the RAG knowledge base to realize RAG-based instruction verification. Finally, in the case analysis, the operation of a certain device operation instruction is simulated and deduced, and interpretability of the graph-RAG-based rule verification method is demonstrated.},
	booktitle = {2025 {IEEE} {International} {Symposium} on the {Application} of {Artificial} {Intelligence} in {Electrical} {Engineering} ({AAIEE})},
	author = {Cheng, Haoyuan and Wang, Ming and Liu, Yuanlong and Li, Naiyong and Mao, Chenxu and Wang, Wenxin and Liu, Enren and Wang, Kai},
	month = apr,
	year = {2025},
	keywords = {error-prevention knowledge graph (EPKG), Green energy, Knowledge based systems, Knowledge graphs, Large language models, LLM, Load forecasting, Power grids, power operation order, RAG, Retrieval augmented generation, Rule verification method, Substations, Topology, Vectors},
	pages = {892--897},
}

@inproceedings{purba_towards_2025,
	title = {Towards {Automated} and {Explainable} {Threat} {Hunting} with {Generative} {AI}},
	doi = {10.1109/DSN64029.2025.00067},
	abstract = {This paper describes an attempt to automate threat hunting, taking cyber threat intelligence messages as input and generating queries to search logs for attack evidence using a popular query language, Kibana, used in Security Operations Centers (SOC). Our prototype implementation, AIThreatTrack, uses GPT-4 to extract actionable threat intelligence from real-time messages like X and Slack. The core idea is to explain the extracted intelligence in terms of MITRE ATT\&CK TTPs using a knowledge graph containing "is-a" and "part-of" relationships (extracted using GPT-4) with the following benefits:(a) Significantly reduced hallucinations from 47\% (GPT-4) to 1.5\% using two orthogonal ways to cross-check answers. (b) Gaining analysts’ trust with explained results. (c) Using chain-of-knowledge prompting to significantly improve query generation accuracy. This approach supports expanding the scope of the knowledge graph to further improve query generation. Our approach significantly outperforms the Retrieval-Augmented Generation (RAG) approach and chain-of-thought reasoning LLM in reducing hallucinations.},
	booktitle = {2025 55th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Purba, Moumita Das and Chu, Bill and French, Will},
	month = jun,
	year = {2025},
	note = {ISSN: 2158-3927},
	keywords = {Cognition, Cyber threat intelligence, Database languages, Generative AI, GPT-4, Kibana, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, OpenAI o1, Prompt, Prototypes, RAG, Real-time systems, Retrieval augmented generation, Security, Threat hunting},
	pages = {664--677},
}

@inproceedings{khan_enhancing_2024,
	title = {Enhancing {Large} {Language} {Models} for {Telecom} {Networks} {Using} {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/GCWkshp64532.2024.11100791},
	abstract = {This paper presents a comprehensive approach for fine-tuning large language models (LLMs) for domain-specific tasks in the telecommunications field. We utilize a dataset with 1,827 multiple-choice questions (MCQs) from 3GPP standard documents. A publicly available LLM named "Phi-2" is used to answer the MCQs correctly. We develop a Retrieval-Augmented Generation (RAG) pipeline to improve Phi-2 model's performance. The RAG pipeline comprises document segmentation, synthetic question-answer (QA) generation, custom fine-tuning of the embedding model, and incremental fine-tuning of Phi-2. Our experiments show that accuracy greatly increased by combining all the above-mentioned steps in the RAG pipeline. The proposed approach outperforms the baseline Phi-2 model by 45.20\% in terms of accuracy. This study identifies the limitations of instruction fine-tuning in specialized fields and explores the possibility of using sophisticated data processing with fine-tuned models to improve performance even more.},
	booktitle = {2024 {IEEE} {Globecom} {Workshops} ({GC} {Wkshps})},
	author = {Khan, Nasik Sami and Hasan, Md Mahibul and Towhid, Md. Shamim and Basnet, Saroj and Shahriar, Nashid},
	month = dec,
	year = {2024},
	note = {ISSN: 2166-0077},
	keywords = {3GPP Standards, Accuracy, Conferences, Data models, Data processing, embeddings, fine-tuning, large language models, Large language models, LoRA, Pipelines, Retrieval augmented generation, retrieval-augmented generation, Telecom, Telecommunications},
	pages = {1--7},
}

@inproceedings{nezafat_fake_2024,
	title = {Fake {News} {Detection} with {Retrieval} {Augmented} {Generative} {Artificial} {Intelligence}},
	doi = {10.1109/FLLM63129.2024.10852474},
	abstract = {The rapid spread of false information on social media has grown to be a serious problem that influences public opinion and decision-making. Fake news spreads rapidly and extensively, often outpacing efforts to debunk or mitigate its effects. Traditional methods for detecting fake news face numerous challenges, including the necessity for extensive model training and the potential for inherent biases. Although Large Language Models (LLMs) have seen substantial improvements recently, their use in fake news detection poses the risk of producing false or misleading information due to their possible hallucinations. This study presents a new strategy to combat fake news by integrating Mixtral-8x7B, a Sparse Mixture of Experts (SMoE) Large Language Model, with a Retrieval-Augmented Generation (RAG) framework. Our framework employs Google’s search API to retrieve relevant articles in real time, harnessing Mixtral’s sophisticated language processing capabilities and RAG’s ability to access current information dynamically. Initial results are promising, indicating that our approach performs comparably to established fake news detection techniques. Our method operates without the need for extensive model training, offering significant cost savings and contributing to developing more efficient tools for detecting misinformation in the digital era, which will help stop the spread of misleading data more efficiently.},
	booktitle = {2024 2nd {International} {Conference} on {Foundation} and {Large} {Language} {Models} ({FLLM})},
	author = {Nezafat, Mohammad Vatani and Samet, Saeed},
	month = nov,
	year = {2024},
	keywords = {Accuracy, Costs, Data models, Fake news, Fake News Detection, Large Language Model, Large language models, Real-time systems, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Sparse Mixture of Experts, Testing, Training},
	pages = {160--167},
}

@inproceedings{sun_rabbit_2025,
	title = {Rabbit: {Retrieval}-{Augmented} {Generation} {Enables} {Better} {Automatic} {Database} {Knob} {Tuning}},
	doi = {10.1109/ICDE65448.2025.00284},
	abstract = {The large language model (LLM)-based knob tuning method has attracted considerable attention due to its excellent in-context learning ability and generalizability. However, the existing LLM-based tuning methods do not effectively harmonize multi-source external knowledge, leading to missed opportunities for enhanced knob tuning. In light of this, we propose Rabbit, a novel approach that leverages Retrieval-augmented generation to enhance database knob tuning tools, which seamlessly integrates structured historical tuning experience with graph-encoded static knowledge. First, we introduce an experience-driven knob selection strategy, enhanced by dependency-aware external knowledge integration, to systematically select key knobs. Second, we develop a cutting-edge multi-agent knob domain pruning method, which ensures the reduced search space remains compact yet effective. Finally, we leverage the few-shot capabilities of LLMs to act as surrogate models, enabling rapid exploration of the pruned search space, followed by incremental optimization that expands the search space using historical insights. Moreover, we also design an adaptive strategy to transition between these two search spaces, striking an optimal balance between exploration and exploitation. Extensive experiments on well-established bench-marks demonstrate that Rabbit outperforms the state-of-the-art methods in both effectiveness and efficiency, pointing to a new paradigm for this area.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Sun, Wenwen and Pan, Zhicheng and Hu, Zirui and Liu, Yu and Yang, Chengcheng and Zhang, Rong and Zhou, Xuan},
	month = may,
	year = {2025},
	note = {ISSN: 2375-026X},
	keywords = {Bayesian optimization, Data processing, Databases, Indexes, Knob tuning, Large language models, LLM, Optimization, Rabbits, RAG, Retrieval augmented generation, Sparks, Tuners, Tuning},
	pages = {3807--3820},
}

@inproceedings{kumar_development_2025,
	title = {Development of {Interactive} {Assistance} for {Academic} {Preparation} {Using} {Large} {Models} {Language}},
	doi = {10.1109/ICCCIT62592.2025.10928137},
	abstract = {With the increase in capabilities and availability of large language models, it is inevitable that it finds its way into application in a variety of domains. Education domain has seen remarkable improvements in both efficiency and capabilities with the incorporation of innovative and latest technology to improve the experience for both teachers as well as students and when talking about latest technologies it is impossible to not talk about large language models. Their ability to contemplate huge volumes of data as well as generate human-like responses to queries related to data it possesses has made many industries rethink certain aspects of their workflow to integrate this powerful technology to improve the working efficiency. Education sector can benefit from this technology in various aspects. This project aims to integrate the usage of locally run large language models using Ollama and LangChain to build a conversational platform application for the usage by students for assisted learning. The model also utilizes Retrieval Augmented generation techniques for creating an updated and dynamic large language model. To promote transparency stateof-the-art open-source technologies are preferred rather than closed source technologies like OpenAI's ChatGPT.},
	booktitle = {2025 {International} {Conference} on {Computational}, {Communication} and {Information} {Technology} ({ICCCIT})},
	author = {Kumar, P and M, Haresh and V, Hayagreevan},
	month = feb,
	year = {2025},
	keywords = {Chatbots, Computational modeling, Education, Hardware, Industries, Information and communication technology, LangChain, Large language models, LLM, NLP, Ollama, RAG, Retrieval augmented generation, vector embedding, Vectors},
	pages = {265--269},
}

@inproceedings{cummings_rag_2025,
	title = {{RAG} {Pipeline} for {Domain} {Specific} {Applications}: {A} {Case} {Study} in {Disseminating} {Dementia} {Care} {Practices}},
	abstract = {In closed-domain Question Answering (QA), Large Language Models (LLMs) often fail to deliver responses specialized enough for niche subdomains. Broadly trained models may not capture the nuanced terminology and contextual precision required in these fields, which frequently lack domain-specific conversational data and face computational constraints. To address this, we propose a methodology leveraging a Retrieval-Augmented Generation (RAG) framework that integrates data extraction with fine-tuning using domain-specific question-answer pairs. Our approach employs Question-Answer Generation (QAG) to create tailored training datasets, enabling fine-tuned models to incorporate specialized jargon and context while remaining computationally accessible to domain experts. To exemplify this methodology, we demonstrate its application within the medical domain through a case study centered on the creation of a dementia care chat assistant. A significant benefit of this approach lies in its ease of replication across various domains and scalability for integration into diverse user groups, making it a versatile solution for enhancing chat assistants.CCS Concepts• Computing methodologies → Natural language generation; • Human-centered computing → Natural language interfaces.},
	booktitle = {2025 {IEEE}/{ACM} {Conference} on {Connected} {Health}: {Applications}, {Systems} and {Engineering} {Technologies} ({CHASE})},
	author = {Cummings, Aaron and Zhang, Xinyue and Olaniran, Mercy and Akintomide, Modupe},
	month = jun,
	year = {2025},
	note = {ISSN: 2832-2975},
	keywords = {Computational modeling, Context modeling, Dementia, FineTuning, Healthcare, Large Language Model, Large language models, Pipelines, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Retrieval Augmented Generation, Scalability, Terminology, Training},
	pages = {139--143},
}

@inproceedings{lee_llm-based_2025,
	title = {{LLM}-{Based} {Agents} for {Automated} {Confounder} {Discovery} and {Subgroup} {Analysis} in {Causal} {Inference}},
	doi = {10.1109/IRI66576.2025.00055},
	abstract = {Estimating individualized treatment effects from observational data presents a persistent challenge due to unmeasured confounding and structural bias. Causal Machine Learning (causal ML) methods, such as causal trees and doubly robust estimators, provide tools for estimating conditional average treatment effects. These methods have limited effectiveness in complex real-world environments due to the presence of latent confounders or those described in unstructured formats. Moreover, reliance on domain experts for confounder identification and rule interpretation introduces high annotation cost and scalability concerns. In this work, we proposed Large Language Model-based agents for automated confounder discovery and subgroup analysis that integrate agents into the causal ML pipeline to simulate domain expertise. Our framework systematically performs subgroup identification and confounding structure discovery by leveraging the reasoning capabilities of LLM-based agents, which reduces human dependency while preserving interpretability. Experiments on real-world medical datasets show that our proposed approach enhances treatment effect estimation robustness by narrowing confidence intervals and uncovering unrecognized confounding biases. Our findings suggest that LLM-based agents offer a promising path toward scalable, trustworthy, and semantically aware causal inference.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Information} {Reuse} and {Integration} and {Data} {Science} ({IRI})},
	author = {Lee, Po-Han and Lin, Yu-Cheng and Ku, Chan-Tung and Hsu, Chan and Huang, Pei-Cing and Wu, Ping-Hsun and Kang, Yihuang},
	month = aug,
	year = {2025},
	note = {ISSN: 2835-5776},
	keywords = {Agent, Causal Inference, Causal Machine Learning, Iterative methods, Large language models, Large Large Language Model, Machine learning, Maximum likelihood estimation, Pipelines, Retrieval augmented generation, Retrieval Augmented Generation, Robustness, Rough surfaces, Scalability, Standards, Treatment Effect Estimation},
	pages = {265--270},
}

@inproceedings{vonderhaar_surveying_2025,
	title = {Surveying the {RAG} {Attack} {Surface} and {Defenses}: {Protecting} {Sensitive} {Company} {Data}},
	doi = {10.1109/AITest66680.2025.00015},
	abstract = {As the performance of Large Language Models (LLMs) improves, they are quickly becoming a part of daily life for many people. Even industry companies are adopting them as tools to increase employee productivity. However, due to the huge computational requirements to train an LLM, companies need methods to customize their LLMs without training their own or retraining a pre-trained model. Therefore, retrieval-based methods, such as Retrieval-Augmented Generation (RAG) are becoming popular to reduce hallucinations and incorporate application-specific information into LLM generations without fine-tuning or training. However, the integration of a company’s sensitive data into LLM generations using RAG changes the LLM’s attack surface. Current literature lacks a complete view of the LLM’s changing attack surface due to RAG. While many authors have noted specific vulnerabilities, a survey of the attack surface is not present. This work is a survey on the vulnerabilities introduced to LLMs by using RAG and how these vulnerabilities present security risks to industry companies utilizing RAG to incorporate their sensitive data into LLM generations. This work then provides a survey of the state-of-the-art defense methods against these vulnerabilities.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Artificial} {Intelligence} {Testing} ({AITest})},
	author = {Vonderhaar, Lynn and Machado, Daniel and Ochoa, Omar},
	month = jul,
	year = {2025},
	note = {ISSN: 2835-3560},
	keywords = {attack surface, Companies, defense methods, Industries, large language models, Large language models, Prevention and mitigation, Retrieval augmented generation, retrieval-augmented generation, security, Security, Surface treatment, Surveys, Testing, Training},
	pages = {69--76},
}

@inproceedings{ku_enhancing_2025,
	title = {Enhancing {Autonomous} {Ship} {Communication}: {A} {Cost}-{Effective} and {High}-{Accuracy} {LLM} {Framework} {Using} {Decision} {Trees} and {RAG}},
	doi = {10.1109/ICAIIC64266.2025.10920831},
	abstract = {This study introduces a novel architecture designed to enhance the performance and cost-efficiency of Large Language Models (LLMs) in autonomous ship communication systems. Autonomous ships require high accuracy and rapid response, yet their operational constraints-limited computational resources and lack of internet connectivity-pose significant challenges for traditional LLMs. To address these issues, we integrate Retrieval-Augmented Generation (RAG) with Decision Trees. This integration improves the efficiency of RAG's data retrieval and processing, significantly reduces the computational overhead of LLMs, and enhances response accuracy. Evaluations using 200 hours of real-world maritime communication data demonstrate that the proposed system outperforms existing methods in speed and accuracy under resource-constrained conditions. This research advances the practical application and reliability of LLMs in autonomous ship communication, providing a strong foundation for improving automation and ensuring safety in the maritime industry.},
	booktitle = {2025 {International} {Conference} on {Artificial} {Intelligence} in {Information} and {Communication} ({ICAIIC})},
	author = {Ku, Jaebin and Kim, Sanha and Lee, Eunkyu and Zaman, Umar and Kim, Kyungsup},
	month = feb,
	year = {2025},
	note = {ISSN: 2831-6983},
	keywords = {Accuracy, Autonomous ship, Autonomous vehicles, Communication System, Communication systems, Computer architecture, Decision Tree, Decision trees, LLM, Marine vehicles, Maritime communications, RAG, Reliability, Retrieval augmented generation, Safety},
	pages = {0420--0426},
}

@inproceedings{pathak_utilizing_2024,
	title = {Utilizing {Large} {Language} {Models} to {Predict} {ICD}-10 {Diagnosis} {Codes} from {Patient} {Medical} {Records}},
	doi = {10.1109/URTC65039.2024.10937521},
	abstract = {The application of Large Language Models (LLMs) has been a deeply explored topic, but with little focus on utilizing LLMs for predicting ICD-10 patient diagnoses in the medical field. Using LLMs to predict these patient codes has significant potential to streamline medical processes and reduce human burden drastically. In this work, we use GPT-4o to test four prompting techniques: Base GPT-4o, GPT-4o with Chain of Thought (CoT) prompting, GPT-4o with Retrieval-Augmented Generation (RAG), and GPT-4o with both CoT and RAG. Our results show that combining CoT and RAG significantly improve predictive accuracy of GPT-4o in patient diagnosis.},
	booktitle = {2024 {IEEE} {MIT} {Undergraduate} {Research} {Technology} {Conference} ({URTC})},
	author = {Pathak, Rudransh and Vald, Gabriel and Sermet, Yusuf and Demir, Ibrahim},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Chain of Thought Prompting, Codes, Hallucination, Large language models, Large Language Models, Medical diagnosis, Medical diagnostic imaging, Prompt engineering, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Training, Transforms},
	pages = {1--5},
}

@inproceedings{khan_towards_2025,
	title = {Towards {Efficient} {Educational} {Chatbots}: {Benchmarking} {RAG} {Frameworks}},
	doi = {10.1109/ETCC65847.2025.11108323},
	abstract = {Large Language Models (LLMs) have proven immensely beneficial in education by capturing vast amounts of literature-based information, allowing them to generate context without relying on external sources. In this paper, we propose a generative AI-powered GATE question-answering framework (GATE stands for Graduate Aptitude Test in Engineering) that leverages LLMs to explain GATE solutions and support students in their exam preparation. We conducted extensive benchmarking to select the optimal embedding model and LLM, evaluating our framework based on criteria such as latency, faithfulness, and relevance, with additional validation through human evaluation. Our chatbot integrates state-of-the-art embedding models and LLMs to deliver accurate, context-aware responses. Through rigorous experimentation, we identified configurations that balance performance and computational efficiency, ensuring a reliable chatbot to serve students' needs. Additionally, we discuss the challenges faced in data processing and modelling and the implemented solutions. Our research investigates the implementation of Retrieval-Augmented Generation (RAG) for GATE Q/A explanation tasks, and our findings demonstrate significant improvements in retrieval accuracy and response quality. This research offers practical insights for developing effective AI -driven educational tools while highlighting areas for future enhancement in usability and scalability.},
	booktitle = {2025 {International} {Conference} on {Emerging} {Technologies} in {Computing} and {Communication} ({ETCC})},
	author = {Khan, Umar Ali and Khan, Fiza and Khan, Ekram and Hasnain, Mohd Areeb and Moinuddin, Athar Ali},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Benchmark testing, Chatbots, Computational modeling, Evaluation, Generative AI, Information systems, Logic gates, Reliability engineering, Retrieval augmented generation, Retrieval Augmented Generation, Scalability, Usability, User experience},
	pages = {1--6},
}

@inproceedings{mishra_daignostic-_2025,
	title = {{dAIgnostic}- {Optimizing} {Diagnostic} {Process} with {Generative} {AI}},
	doi = {10.1109/AIMV66517.2025.11203542},
	abstract = {Generative Artificial Intelligence (Gen-AI) is dramatically changing the healthcare sector through improved diagnosis and streamlined administrative processes. Gen-AI technologies meet these needs in the face of mounting pressures arising from cost levels, patient expectations, and the pressure for more individualised treatment. It can identify trends by analysing large datasets, such as electronic health records and imaging data, allowing for trend recognition and providing clinical decision support, thereby improving the quality of both patient care and operational efficiency. Despite the promise that Gen-AI presents, there are inherent risks associated with integrating Gen-AI into healthcare. These include issues regarding data privacy, biases in algorithms, and lack of appropriate frameworks for ethical decision-making. Governance structures can ensure compliance with privacy regulations and enhance user trust. Ongoing evaluations are also necessary about Gen-AI and its impact on clinical outcomes to maximize benefits and minimize risks. The end-to-end solutions that would be developed using Gen-AI include automated report generation and facial recognition to reduce administrative burdens and improve patient experiences. This promises to revolutionize clinical practices while requiring mature exploration of the ethics and data protection policies with the deployment of these technologies in healthcare organizations.},
	booktitle = {2025 {International} {Conference} on {Artificial} {Intelligence} and {Machine} {Vision} ({AIMV})},
	author = {Mishra, Deepasikha and Rayasam, Venkat Harsha and Kandukuri, Sai Akshay and Sai, Koneru Pranav and Ashray, N. V. Ravi},
	month = aug,
	year = {2025},
	keywords = {Clinical Decision Support, Diagnostics, Electronic Health Records (EHRs), Electronic medical records, Ethics, Face recognition, Generative AI, Healthcare, Large Language Models (LLMs), Market research, Medical services, Natural language processing, Natural Language Processing (NLP), Regulation, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Streaming media},
	pages = {1--6},
}

@inproceedings{abualhaija_llm-assisted_2025,
	title = {{LLM}-assisted {Extraction} of {Regulatory} {Requirements}: {A} {Case} {Study} on the {GDPR}},
	doi = {10.1109/RE63999.2025.00023},
	abstract = {Modern software systems increasingly rely on personal data. Despite the enforcement of the European General Data Protection Regulation (GDPR) and the growing awareness about privacy and data protection, many individuals’ rights remain unsatisfactorily implemented in software systems. This is partially due to the knowledge gap between legal interpretation and software development.In this paper, we address this gap first by extracting, in close collaboration with legal experts, a list of 108 requirements pertinent to the right of access (ACC) and the right to portability (PRT), two fundamental rights under the GDPR. We further propose the XTRAREG approach, which utilizes large language models (LLMs) and retrieval augmented generation (RAG) to provide automated assistance in extracting privacy requirements from predefined legal sources.Compared to the manually extracted requirements, XTRAREG can automatically generate requirements with an accuracy of 81.8\% for ACC and 85.7\% for PRT. Our empirical evaluation reveals two notable observations: (i) A skewed performance in terms of coverage in the favor of ACC, indicating the significant impact of abundant training data of the LLM, (ii) despite explicit exposure of legal references through RAG, the LLM generates requirements predominantly from the GDPR.},
	booktitle = {2025 {IEEE} 33rd {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Abualhaija, Sallam and Ceci, Marcello and Sannier, Nicolas and Bianculli, Domenico and Lannier, Salomé and Siclari, Martina and Voordeckers, Olivier and Tosza, Stanisław},
	month = sep,
	year = {2025},
	note = {ISSN: 2332-6441},
	keywords = {Accuracy, Collaboration, General Data Protection Regulation, General Data Protection Regulation (GDPR), Large language models, Large Language Models (LLMs), Law, Natural language processing, Natural Language Processing (NLP), Privacy Requirements, Requirements Generation, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Software systems, Temperature distribution, Training data},
	pages = {142--154},
}

@inproceedings{hajaghaie_leveraging_2025,
	title = {Leveraging {Large} {Language} {Models} and {Retrieval}-{Augmented} {Generation} for {Enhanced} {Multi}-{Asset} {Portfolio} {Construction}},
	doi = {10.1109/CiFer64978.2025.10975739},
	abstract = {This study assesses the Large Language Models (LLMs) in creating investment portfolios. We implement a few-shot learning technique, followed by Retrieval Augmented Generation (RAG) enhanced with comprehensive up-to-date financial data, using Meta's latest LLM, Llama 3.1-8b. In the first phase, We assess the models' efficacy using key financial indicators, including total returns, annualized volatility, riskadjusted performance (Sharpe ratio), potential loss estimates (value-at-risk), and their pre-training knowledge with the S\&P 500 Index performance baseline. In the second phase, we enhance the LLM's knowledge base by RAG and the latest historical and statistical metrics (such as earnings per share (EPS), dividends per share (DPS), profit margin, and many more) for each asset from different classes. The study constrains model inputs to specific sets of financial assets, such as equities, exchangetraded funds (ETFs), commodities, cryptocurrencies, and bonds. To evaluate model performance and adaptability, we analyzed across two distinct time frames: (1) within the models' training data cutoff, and (2) from the cutoff date to the present. This approach enables the assessment of model generalization to past and present market conditions. The research quantifies LLMs’ capabilities in financial asset allocation, comparing baseline performance against RAG-augmented strategies. Our results demonstrate that RAG-enhanced LLM significantly outperforms vanilla LLM in portfolio construction across various asset classes. We contemplate that these results could influence AI-driven financial decision-making processes such as automated trading, real-time sentiment analysis, and investment management.},
	booktitle = {2025 {IEEE} {Symposium} on {Computational} {Intelligence} for {Financial} {Engineering} and {Economics} ({CiFer})},
	author = {Hajaghaie, Ahmadreza and Thulasiram, Ruppa K.},
	month = mar,
	year = {2025},
	keywords = {Adaptation models, Analytical models, Asset Allocation, Biological system modeling, Computational Finance, Data models, Generative AI, Investment, Large language models, Large Language Models, Llama, Portfolio management, Portfolios, Resource management, Retrieval augmented generation, Training data},
	pages = {1--7},
}

@inproceedings{hou_llm-based_2024,
	title = {{LLM}-based user requirement analysis and intelligent {Q}\&{A} toward {Chang}’an {Twelve} {Hours}},
	doi = {10.1109/CoST64302.2024.00038},
	abstract = {The new technology of artificial intelligence can bring users a fresh experience and accelerate the rapid development of cultural and tourism integration. In this paper, we use a large language model (LLM) to mine user implicit demands and implement intelligent Q\&A reasoning for the Chang’an Twelve Hours Scenic Area. First, multi-faceted data related to Chang’an Twelve Hours is collected to construct a vector database. Next, the LLM is utilized to mine the implicit requirements of user questions by combining attribute features such as gender and age. Then, the user demand vectors are retrieved and matched with the content vectors in the vector database. Finally, the similar content obtained from user demand and retrieval feedback is used to populate the prompt template and input into the LLM, realizing intelligent Q\&A and reasoning for the Twelve Hours of Chang’an. On this basis, a prototype of an intelligent Q\&A system tailored to the Twelve Hours of Chang’an is constructed. Experimental comparisons with the traditional Retrieval-Augmented Generation (RAG) technique demonstrate that using an LLM enables finer mining of user requirements and generates more accurate answers. The intelligent Q\&A system for Chang’an Twelve Hours developed in this paper effectively addresses users’ customized inquiries, thereby enhancing the user experience and improving the operational efficiency of the scenic spot. Additionally, this system can serve as a reference model for implementation at other cultural tourism attractions.},
	booktitle = {2024 {International} {Conference} on {Culture}-{Oriented} {Science} \& {Technology} ({CoST})},
	author = {Hou, Fuqing and Chen, Yibing and Shi, Xin and Li, Min and Zhao, Xueqing},
	month = aug,
	year = {2024},
	keywords = {Accuracy, Analytical models, Costs, Databases, Large language model, Large language models, Prototypes, Questions and answers, Retrieval augmented generation, User requirements, Vectors},
	pages = {151--155},
}

@inproceedings{kamra_enhancing_2024,
	title = {Enhancing {Document} {Retrieval} {Using} {AI} and {Graph}-{Based} {RAG} {Techniques}},
	doi = {10.1109/C2I663243.2024.10895931},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as a potent method for enhancing the capabilities of large language models (LLMs) by integrating them with external knowledge sources. While traditional RAG models rely heavily on textual similarity for retrieval, often leading to issues like context drift and hallucinations, graph-based RAG offers a more sophisticated approach. By representing documents and their relationships within a graph structure, graph-based RAG enables more context-aware retrieval, reducing hallucinations, and facilitating multi-hop reasoning. This abstract provides an overview of the RAG landscape, contrasting traditional and graph-based approaches, and highlights the advantages of graph-based RAG in addressing the limitations of traditional methods. The application of graph-based RAG to various domains, such as question answering, dialogue systems, and recommendation systems, is also explored. The abstract concludes by emphasizing the potential of graph-based RAG to revolutionize information access and retrieval in diverse AI applications.},
	booktitle = {2024 5th {International} {Conference} on {Communication}, {Computing} \& {Industry} 6.0 ({C2I6})},
	author = {Kamra, Vikas and Gupta, Lakshya and Arora, Dhruv and Yadav, Ashwin Kumar},
	month = dec,
	year = {2024},
	keywords = {Cognition, Context modeling, Document Retrieval, Evaluation, Graph Neural Networks, Graph-Based RAG, Industries, Information processing, Knowledge Graphs, Large language models, Neural networks, Question answering (information retrieval), Recommender systems, Retrieval augmented generation, Surveys},
	pages = {1--7},
}

@inproceedings{pratami_llm-based_2025,
	title = {{LLM}-{Based} {Chatbot} for the {Indonesia} {University} {IT} {Service} {Desk}: {Integrating} {DeepSeek}-v3 {API} and a {RAG} {Approach}},
	doi = {10.1109/ICoDSA67155.2025.11157184},
	abstract = {The growing need for streamlined IT service management in higher education has highlighted the limitations of traditional helpdesk models, which depend heavily on manual processes and human operators. These legacy systems often suffer from slow response times, accumulating service requests, and staff fatigue. To overcome these challenges, this study introduces an intelligent, LLM-powered chatbot that integrates the DeepSeek-v3 API with a Retrieval-Augmented Generation (RAG) strategy tailored to an Indonesian university’s IT service desk environment. Built upon a modular architecture, the proposed solution automates document ingestion with multi-format preprocessing, dynamic chunking, and embedding generation, which are stored in Supabase for efficient top-k contextual retrieval. When a user submits an inquiry, pertinent knowledge segments are retrieved in real time and synthesized into coherent, contextually appropriate responses by the DeepSeek-v3 language model. The system was evaluated through a comprehensive experimental framework, including functional and hardware benchmarking, performance testing, and a standardized User Experience Questionnaire (UEQ). Functional tests confirmed stable, 24/7 operation with over 98\% retrieval accuracy, while performance benchmarking demonstrated an average response time of 2.1 ± 0.2 seconds—a 35\% improvement over a legacy helpdesk baseline. User studies with 26 IT staff and students yielded a high UEQ score of 4.5 ± 0.3 (out of 5), reflecting strong satisfaction across pragmatic (efficiency, dependability) and hedonic (stimulation, novelty) dimensions. In addition, nine prompting strategies were compared via statistical analysis to optimize response quality. These findings demonstrate the viability and scalability of integrating LLMs with institutional IT infrastructures and offer practical guidance for deploying RAG-based chatbots in resource-constrained academic settings. By detailing implementation best practices and comparative evaluations, this work advances AI-driven automation in academic IT support and lays the groundwork for future enhancements in multilingual and low-resource environments.},
	booktitle = {2025 {International} {Conference} on {Data} {Science} and {Its} {Applications} ({ICoDSA})},
	author = {Pratami, Rahmat and Ruhallah, Muhammad Lutfi and Gozali, Alfian Akbar},
	month = jul,
	year = {2025},
	keywords = {Benchmark testing, chatbot, Chatbots, IT helpdesk, large language models, Multilingual, Pragmatics, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Scalability, Statistical analysis, Time factors, User experience},
	pages = {979--984},
}

@inproceedings{j_intelligent_2024,
	title = {Intelligent {Conversations} with {Documents} using {Natural} {Language} {Processing}},
	doi = {10.1109/ARIIA63345.2024.11051881},
	abstract = {This research tries to implement the RAG approach to combine LLMs with other techniques and create a system that can talk directly to the system. The core of this endeavor hinges on the combination of the LLMs ability to generate meaningful responses and the dynamism of the model to recall and incorporate contextual information stored in a corpus of documents during the discussion. This combination tackles lack of latest knowledge and specific information that can be addressed by integrating LLMs with an updated knowledge base which is kept external. The project shows how the interaction of RAG and LLMs is a precondition for the creation of an agent with a broader capacity and greater accuracy for communication in academic and professional fields. When we equip them with these abilities, we open the door to deeper and more context-sensitive interactions as they can directly refer to and retrieve data from specific records and other sources. The system is identified to be beneficial in education institutions where it will assist students/researchers to interact with academic text, research paper and other educational content through conversation hence achieving a simpler, interesting and more effective learning process. In addition, this project requires the establishment of a robust retriever, an integration of this retriever with current LLMs technology, and a set up of an interface that users can communicate with the system via spoken language. The project combines both retrieval techniques with LLMs to use as a basis and conduct research aimed at making AI systems the way they are more interactive and resourceful while handling real-world information.},
	booktitle = {2024 {International} {Conference} on {Augmented} {Reality}, {Intelligent} {Systems}, and {Industrial} {Automation} ({ARIIA})},
	author = {J, Ravichandran and Khalaf, Hameed Hassan and Mohammed, Nada Qasim and Al-Hussein, Rouaida Kadhim A. and Pirani, Safzan and Nithin, Enthala and Pujitha, Kasoju},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Artificial Intelligence, Deep Learning, Knowledge based systems, Large Language Model, Large language models, Measurement, Natural Language Processing, Oral communication, Question answering (information retrieval), Real-time systems, Reliability, Retrieval augmented generation, Retrieval Augmented Generation, Scalability},
	pages = {1--6},
}

@article{agarwal_toward_2025,
	title = {Toward {Inclusive} {Healthcare}: {An} {LLM}-{Based} {Multimodal} {Chatbot} for {Preliminary} {Diagnosis}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3594218},
	abstract = {This paper presents the design and development of a multimodal medical chatbot that leverages Gemini-2.0-Flash Model alongside a novel Retrieval-Augmented Generation (RAG) architecture to support preliminary medical diagnosis and recommendations. The system integrates textual prompt analysis and medical image interpretation, aiming to improve healthcare accessibility, particularly for underserved populations. Focused on data-rich medical conditions, the chatbot generates reliable diagnostic insights based on natural language inputs and/or medical images, requiring minimal user expertise. The proposed RAG-based architecture incorporates a curated medical knowledge base and structured retrieval mechanisms, significantly reducing hallucinations and enhancing response credibility compared to direct Large Language Model (LLM) querying. By demonstrating the efficacy of multimodal reasoning in conjunction with structured retrieval, this work paves the way for more accessible, accurate, and scalable AI-driven health support systems.},
	journal = {IEEE Access},
	author = {Agarwal, Ishita and Sakthivel, V. and Prakash, P.},
	year = {2025},
	keywords = {Accuracy, Artificial intelligence, Chatbots, Gemini, generative AI, large language models, medical chatbot, medical diagnosis, Medical diagnostic imaging, Medical services, Oral communication, Random forests, retrieval augmented generation, Retrieval augmented generation, Training, Vectors},
	pages = {136420--136432},
}

@inproceedings{rachha_llm-enhanced_2024,
	title = {{LLM}-{Enhanced} {Learning} {Environments} for {CS}: {Exploring} {Data} {Structures} and {Algorithms} with {Gurukul}},
	doi = {10.1109/FIE61694.2024.10893211},
	abstract = {In this Innovative Practice full paper, we introduce Gurukul, an innovative coding platform designed to support teaching Data Structures and Algorithm (DSA) course by integrating advanced Large Language Models (LLMs). LLMs have emerged as powerful tools in Computer Science Education (CSEd), offering unparalleled opportunities for enhancing student comprehension and engagement. However, their use in educational settings presents challenges, including tendencies toward hallucination, contextual inaccuracies, and the risk of undermining critical thinking by providing explicit solutions. To address these challenges, and to explore how specialized LLMs can bolster learner engagement, we present Gurukul, a platform featuring dual innovations: Retrieval-Augmented Generation (RAG) and Guardrails. Gurukul offers a hands-on practice feature where students can solve DSA problems within a code editor, supported by a dynamically Guardrailed LLM that prevents the delivery of explicit solutions. Additionally, the platform's study feature utilizes RAG, drawing from OpenDSA as a trusted source, to ensure accurate and contextually relevant information is provided. To assess the platform's effectiveness, we conducted a User Study with students, and a User Expert Review with faculty from a U.S. public state university specializing in DSA courses. Our analysis of student usage patterns and perceptions, along with insights from instructors, reveal that Gurukul positively impacted student engagement and learning in DSA, demonstrating the potential of specialized LLMs to enhance educational outcomes in this field.},
	booktitle = {2024 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Rachha, Ashwin and Seyam, Mohammed},
	month = oct,
	year = {2024},
	note = {ISSN: 2377-634X},
	keywords = {AI in Education, ChatGPT, Codes, Computer science education, Computer Science Education, Data models, Data structures, Education, Encoding, Guardrails, Large language models, Large Language Models, Retrieval augmented generation, Retrieval Augmented Generation, Reviews, Technological innovation},
	pages = {1--9},
}

@inproceedings{lui_gptutor_2024,
	title = {{GPTutor}: {A} {Generative} {AI}-powered {Intelligent} {Tutoring} {System} to {Support} {Interactive} {Learning} with {Knowledge}-{Grounded} {Question} {Answering}},
	doi = {10.1109/AEECA62331.2024.00124},
	abstract = {With increasing popularity of artificial intelligence (AI) in the education industry, intelligent tutoring system (ITS) powered by AI have been widely adopted to optimize the learning experience. However, the relationship between students’ engagement level of Generative AI (GenAI) and their academic performance is still under exploration. Also current popular GenAI products like ChatGPT suffer from the hallucination problem, which includes factuality, faithfulness, and maliciousness issues in the generated answer. This paper presents GPTutor, an ITS leveraging GenAI to support students' learning processes. GPTutor integrates a Retrieval-Augmented Generation (RAG) pipeline to deliver actual and contextually rich answers aligned to student questions and intended learning outcomes (ILO). A pilot evaluation involving undergraduate and postgraduate students assessed the system’s association with user experience, engagement, and academic performance. Results demonstrated that students generally recognize the effectiveness of GPTutor. Some students also provided insightful feedback on the benefits of GPTutor in improving learning efficiency and some limitations to be addressed. Notably, students with higher engagement levels showed significantly better academic performance on the final exam. This study proposed GPTutor to provide an interactive and knowledge-grounded learning experience and showed the strong association between students’ engagement in GPTutor and academic performance.},
	booktitle = {2024 {International} {Conference} on {Advances} in {Electrical} {Engineering} and {Computer} {Applications} ({AEECA})},
	author = {Lui, Richard Wing Cheung and Bai, Haoran and Zhang, Aiden Wen Yi and Chu, Elvin Tsun Him},
	month = aug,
	year = {2024},
	keywords = {Computer applications, Education, Electrical engineering, generative AI, Generative AI, Industries, intelligent tutoring system, interactive learning, knowledge-grounded question-answering, Learning (artificial intelligence), Pipelines, Question answering (information retrieval), Retrieval augmented generation, retrieval-augmented generation, User experience},
	pages = {702--707},
}

@inproceedings{syeda_llm-based_2024,
	title = {{LLM}-based kidney disease diagnostic framework for {Pathologists}},
	doi = {10.1109/EMBC53108.2024.10782599},
	abstract = {Large language models revolutionize the recent paradigm in the medical field and its contributing to various applications, diversified from clinical decision support to information extraction and summarization. The substantial linguistic understanding and contextual awareness allow language models to process and evaluate decision tasks. Concurrently, it addresses the challenges encountered by pathologists in disease diagnosis by adeptly retrieving precise and accurate facts from an external knowledge base. In this paper, we propose a framework which incorporates advanced retrieval augmented generation with prompt engineering techniques, contain prompting levels and structured prompts, which enables the model to extract refine, and customize responses. The model has been equipped with a large corpus of several kidney diseases clinical data which is collected from the vast information sources of kidney diagnostic books. The utilization of varied prompt techniques, exemplified by standard prompts like few-shots and the Reasoning Act (ReAct), manifests notable improvements in disease diagnosis responses. Structured prompts are designed to provide pathologists with specific instructions for formulating questions that effectively enhance the performance of the model. In the evaluation of prompt performance, three key metrics are employed answer relevance, faithfulness, and context relevance. Notably, in the context relevance metric, an optimal performance score of 1.0 was attained indicating perfect alignment with the conversational context.},
	booktitle = {2024 46th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Syeda, Masooma Zehra and Bukhari, Syed Usama Khalid and Hussain, Maqbool and Khan, Wajahat Ali and Shah, Syed Sajid Hussain},
	month = jul,
	year = {2024},
	note = {ISSN: 2694-0604},
	keywords = {Biological system modeling, decision support, Diseases, information retrieval, Kidney, kidney diseases, large language models, Large language models, Measurement, Medical diagnosis, Medical diagnostic imaging, natural language processing, Prompt engineering, retrieval augmented generation, Retrieval augmented generation, Standards},
	pages = {1--4},
}

@article{xu_chattf_2024,
	title = {{ChatTf}: {A} {Knowledge} {Graph}-{Enhanced} {Intelligent} {Q}\&{A} {System} for {Mitigating} {Factuality} {Hallucinations} in {Traditional} {Folklore}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3485877},
	abstract = {Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q\&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q\&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7\% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.},
	journal = {IEEE Access},
	author = {Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
	year = {2024},
	keywords = {Accuracy, Cognition, Cultural differences, Knowledge engineering, Knowledge graph, Knowledge graphs, large language model, Large language models, Ontologies, question answering, Reliability, retrieval-augmented generation, Semantics, traditional folklore, Training},
	pages = {162638--162650},
}

@inproceedings{quynh_nhu_rag-smartvuln_2025,
	title = {{RAG}-{SmartVuln}: {Enhancing} {Smart} {Contract} {Vulnerability} {Detection} via {Retrieval}-{Augmented} {LLMs}},
	doi = {10.1109/MAPR67746.2025.11134018},
	abstract = {The burgeoning adoption of economically incentivized smart contracts faces persistent security vulnerabilities, resulting in significant financial losses due to their immutability post-deployment. This paper presents a novel framework integrating fine-tuned large language models (LLMs) with Retrieval-Augmented Generation (RAG) to enhance the precision and explainability of smart contract vulnerability detection. By fine-tuning an open-source LLM and employing RAG, our model dynamically incorporates domain-specific external knowledge during inference, significantly improving threat identification. On two public benchmarks, SolidiFI-Benchmark and Smart Bugs Curated, our fine-tuned Qwen2.5-Coder-14B model (QC-14B-FT) outperforms zero-shot LLMs (GPT-3.5 with and without RAG) in terms of F1-score. Specifically, QC-14B-FT achieves an F1-score of 0.64 on SolidiFI, surpassing GPT-3.5-RAG by 9\% and GPT-3.5 by 10\%. On Smart Bugs Curated, QC-14B-FT achieves an F1-score of 0.73, outperforming GPT-3.5-RAG by 14\% and GPT-3.5 by 19\%. These results demonstrate the effectiveness of combining RAG with fine-tuning to provide accurate and clear smart contract security assessments.},
	booktitle = {2025 {International} {Conference} on {Multimedia} {Analysis} and {Pattern} {Recognition} ({MAPR})},
	author = {Quynh Nhu, Nguyen Dang and Minh Quan, Le and Van, Thai Hung and Minh Trung, Doan and Duy, Phan The},
	month = aug,
	year = {2025},
	note = {ISSN: 2770-6850},
	keywords = {Accuracy, Benchmark testing, Computer bugs, Faces, Fine-tuning, Large language models, Large Language Models, Pattern recognition, RAG, Retrieval augmented generation, Security, Smart Contract, Smart contracts, Solid modeling, Vulnerabilities Detection},
	pages = {1--6},
}

@inproceedings{panday_puaa_2024,
	title = {{PUAA}: {Personal} {University} {AI} {Assistant} using {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/IEMCON62851.2024.11093120},
	abstract = {A significant improvement in knowledge acquisition has been made by introducing Large Language Models (LLMs) like Chat-GPT and Bard, which provide a more efficient way to acquire information than the labor-intensive procedure of going through various online checkpoints. This new tendency in LLMs makes the widely used rule-based chatbots that colleges use look antiquated and inadequate. To better serve the needs of students looking for information about their universities, this research project suggests incorporating LLM technology into university websites. Our research introduces Personal University AI Assistant (PUAA) with methodology leveraging the RetrievalAugmented Generation (RAG) framework, utilizing the power of the LangChain in combination with cutting-edge LLMs like Mistral-7B, which is made available by the Hugging Face as an open-source option, and Chat GPT 3.5 as the most optimal solution for the problem of fast knowledge acquisition. PUAA improves the students’ experience of accessing university knowledge by providing instant, accurate information while reducing the workload of administrative staff and allowing them to focus on more complex inquiries and tasks. PUAA aims to promote the adoption of artificial intelligence (AI) in educational institutions, demonstrating the viability and benefits of advanced AI tools in enhancing the academic experience and information accessibility for students and staff.},
	booktitle = {2024 {IEEE} 15th {Annual} {Information} {Technology}, {Electronics} and {Mobile} {Communication} {Conference} ({IEMCON})},
	author = {Panday, Sijan and Ghosh, Robin and Rahman, Md Atiqur},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Artificial intelligence, Chatbot, Chatbots, Digital transformation, Education, Knowledge acquisition, LLM, Quality of service, RAG, Retrieval augmented generation, Technological innovation, University AI assistant, User experience},
	pages = {066--073},
}

@inproceedings{saha_special_2025,
	title = {Special {Session}: {ThreatLens}: {LLM}-guided {Threat} {Modeling} and {Test} {Plan} {Generation} for {Hardware} {Security} {Verification}},
	doi = {10.1109/VTS65138.2025.11022932},
	abstract = {Current hardware security verification processes predominantly rely on manual threat modeling and test plan generation, which are labor-intensive, error-prone, and struggle to scale with increasing design complexity and evolving attack methodologies. To address these challenges, we propose ThreatLens, an LLM-driven multi-agent framework that automates security threat modeling and test plan generation for hardware security verification. ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant security knowledge, LLM-powered reasoning for threat assessment, and interactive user feedback to ensure the generation of practical test plans. By automating these processes, the framework reduces the manual verification effort, enhances coverage, and ensures a structured, adaptable approach to security verification. We evaluated our framework on the NEORV32 SoC, demonstrating its capability to automate security verification through structured test plans and validating its effectiveness in real-world scenarios.},
	booktitle = {2025 {IEEE} 43rd {VLSI} {Test} {Symposium} ({VTS})},
	author = {Saha, Dipayan and Al Shaikh, Hasan and Tarek, Shams and Farahmandi, Farimah},
	month = apr,
	year = {2025},
	note = {ISSN: 2375-1053},
	keywords = {Cognition, Complexity theory, Computational efficiency, Computational modeling, Computer architecture, Hardware security, Hardware Security and Trust, LLM, Manuals, Retrieval augmented generation, Security Test Plan Generation, Security Threat Modeling, Threat modeling, Very large scale integration},
	pages = {1--5},
}

@inproceedings{caglayan_structured_2025,
	title = {Structured {Financial} {QA} with {LLMs}: {Fine}-{Tuning} vs. {Code}-{Augmented} {Retrieval}},
	doi = {10.1109/UBMK67458.2025.11207079},
	abstract = {This paper provides a comparative study of two major language model (LLM) strategies (instruction-based fine-tuning and retrieval-augmented generation (RAG)) for corporate financial question answering. A modular AI pipeline is designed where each financial domain (e.g. liquidity, investment, credit analysis) is treated as an independent QA module that operates on structured company-level data. The dataset includes Turkish public company financial statements from 2008 to 2025. The fine-tuned variant based on LLaMA 3 8B is trained on domain-specific prompts in Turkish using LoRA adapters. The RAG-based variant utilizes a vector search engine to retrieve relevant financial pieces. In addition, tabular reasoning is integrated using pandas to provide dynamic, code-based access to structured data and is developed as a more advanced version (ragenh). To evaluate the quality of generated answers, a set of metrics are applied that capture semantic similarity, numerical accuracy, and directional consistency—such as ROUGE-L, BERTScore, and domain-specific number and trend alignment scores. The results obtained from these metrics show that while the fine-tuned models perform well in interpretive and trend-based tasks, ragenh outperforms both baselines in ground truth and opinion-based reasoning. This work provides a scalable framework for building interpretable financial assistants in under-resourced language environments by combining modular QA design, hybrid architectures, and custom evaluation. The findings contribute to developing robust, context-aware LLM applications for financial decision support.},
	booktitle = {2025 10th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Çağlayan, Alperen and Gökçe, Saliha Nur and Ayata, Değer},
	month = sep,
	year = {2025},
	note = {ISSN: 2521-1641},
	keywords = {Cognition, Financial NLP, Financial Question Answering, Fine-Tuning, Large Language Models, LLaMA, LoRA, Market research, Measurement, Numerical models, Pipelines, Question answering (information retrieval), RAG, Retrieval augmented generation, Retrieval-Augmented Generation, Search engines, Semantics, Structured QA, Turkish Financial Data, Vectors},
	pages = {539--544},
}

@inproceedings{pouhela_context-aware_2025,
	title = {Context-{Aware} {Internet}-of-{Things} {Communication} via {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICCWorkshops67674.2025.11162197},
	abstract = {A major challenge to the advancement of the Internet of Things (IoT) as we know it, is related to the need of a context-aware communication framework that can dynamically adapt to environments with constantly changing requirements. The paper at hand presents a potential avenue to address this limitation. The study proposes an innovative approach to enable context-awareness in dynamic IoT settings by leveraging the Natural Language Processing (NLP) capabilities of the Large Language Models (LLMs) in conjunction with the Knowledge Graphs (KGs) within a custom Retrieval-Augmented Generation (RAG) system to enable real-time understand and processing of the Natural Language (NL) instructions. This approach makes the communication system flexible and holds the potential for future innovations in the realm of IoT technologies.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Communications} {Workshops} ({ICC} {Workshops})},
	author = {Pouhela, Franc and Krummacker, Dennis and Schotten, Hans D.},
	month = jun,
	year = {2025},
	note = {ISSN: 2694-2941},
	keywords = {6G, AI, Communication systems, Conferences, Hands, Internet of Things, IoT, Knowledge graphs, Large language models, LLM, Natural language processing, RAG, Real-time systems, Retrieval augmented generation, Technological innovation},
	pages = {505--511},
}

@inproceedings{wu_study_2025,
	title = {A {Study} on {Improving} the {Generation} {Quality} of {Large} {Language} {Model} in {Question}-{Answering} {Systems} {Using} {Multi}-{Population} {Genetic} {Algorithm}},
	doi = {10.1109/ICoAILO66760.2025.11156060},
	abstract = {Existing Retrieval-Augmented Generation (RAG) methods remain insufficient in handling hallucination due to several underlying limitations. First, semantic vector-based retrieval may lead to the loss of critical information from the original text. Second, ambiguous queries can result in the retrieval of unhelpful content. Third, the single-round retrieval strategy cannot simulate the human reasoning process. Therefore, we propose a novel answer reasoning framework that integrates Multi-Population Genetic Algorithms (MPGA) to enhance RAG-based QA systems. The framework first leverages a multi-head self-attention mechanism to capture diverse semantic aspects of the input text. Subsequently, relevant passages retrieved from multiple semantic perspectives are iteratively refined through the optimization process, making them more suitable as contexts for the model. Experimental results demonstrate that our approach effectively exploits semantic diversity across attention heads, leading to responses with improved diversity, completeness, and factual accuracy.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Artificial} {Intelligence} for {Learning} and {Optimization} ({ICoAILO})},
	author = {Wu, Cheng-Han and Liu, Ren-Shiou},
	month = aug,
	year = {2025},
	keywords = {Cognition, Genetic algorithms, multi-population genetic algorithm, Optimization, Process control, prompt engineering, Prompt engineering, question answering, Question answering (information retrieval), Retrieval augmented generation, retrieval-augmented generation, Scalability, Semantics, Vectors},
	pages = {230--236},
}

@inproceedings{azam_cancerbot_2024,
	title = {{CancerBot}: {A} {Retrieval}-{Augmented} {Generation} based {Cancer} {Chatbot} {Using} {Large} {Language} {Models}},
	doi = {10.1109/ICOSST64562.2024.10871155},
	abstract = {Cancer is the second leading cause of death globally, highlighting the urgent need for innovative solutions to support patients and healthcare providers. Remote and digital cancer care interventions, such as chatbots, are more relevant than ever, offering timely information and personalized assistance. With the rise of conversational AI, chatbots have emerged as a potential solution to bridge this gap, offering 24/7 assistance and personalized responses. This paper introduces CancerBot, a Retrieval-Augmented Generation (RAG) system designed to address the need for reliable, contextually relevant cancer-related information. Built on top of a large language model LLaMA-2, and integrated with a vector database, CancerBot retrieves relevant medical literature to generate accurate responses, minimizing the risk of hallucinations. Through synthetic testing and evaluation using metrics such as faithfulness, context relevancy, and response relevancy, our results demonstrate that the RAG-based CancerBot significantly outperforms the standard LLaMA-2 model in addressing cancer-specific queries. This advancement showcases the potential of RAG systems to improve patient care by providing reliable, context-driven information that enhances patient engagement and healthcare decision-making.},
	booktitle = {2024 18th {International} {Conference} on {Open} {Source} {Systems} and {Technologies} ({ICOSST})},
	author = {Azam, Ayesha and Naz, Zubaira and Khan, Muhammad Usman Ghani},
	month = dec,
	year = {2024},
	note = {ISSN: 2770-8225},
	keywords = {Cancer, Chatbot, Chatbots, Databases, Large language models, Llama2, LLM, Medical services, RAG, Reliability engineering, Retrieval augmented generation, Standards, Testing, Vector Databases, Vectors},
	pages = {1--6},
}

@inproceedings{ates_semantic_2025,
	title = {Semantic {Chunking} and {Chain}-{Of}-{Thought} {Reasoning} for {Rag}-{Based} {Document} {Processing}},
	doi = {10.1109/MLSP62443.2025.11204203},
	abstract = {This paper presents a novel two-phase semantic chunking methodology designed to enhance document processing within Retrieval-Augmented Generation (RAG) systems. The proposed approach utilizes Large Language Models (LLMs) and Chain of Thought (CoT) to systematically generate and refine document chunks, while concurrently producing associated metadata, such as hypothetical user queries and contextual tags. By integrating established information retrieval techniques-namely Best Matching 25 (BM25)-with the advanced semantic understanding capabilities of LLMs, the proposed method substantially improves the relevance and quality of retrieved context for Generative Artificial Intelligence (GenAI) applications. Empirical evaluations reveal that this approach yields significant improvements in response accuracy and contextual relevance when compared to traditional chunking techniques. The implementation utilizes the open-source Qwen 2.572 B model for its semantic processing operations, demonstrating how state-of-the-art language models can be effectively deployed in practical RAG systems.},
	booktitle = {2025 {IEEE} 35th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Ateş, Yiğit and Sayar, Alperen and Bozlar, İbrahim Umut and Ertuğrul, Seyit and Arslan, Suayb S.},
	month = aug,
	year = {2025},
	note = {ISSN: 2161-0371},
	keywords = {Conferences, Generative AI, information retrieval, Information retrieval, large language models, Large language models, Machine learning, Metadata, natural language processing, Natural language processing, retrieval augmented generation, Retrieval augmented generation, semantic chunking, Semantics, Signal processing},
	pages = {1--6},
}

@inproceedings{koh_clara_2025,
	title = {Clara: {Context}-{Aware} {RAG}-{LLM} {Framework} for {Anomaly} {Detection} in {Mobile} {Device} {Sensors}},
	doi = {10.1109/MDM65600.2025.00035},
	abstract = {Mobile devices are equipped with various sensors that continuously gather data about user activity and environmental conditions. Detecting anomalies in this sensor data is crucial for both health monitoring applications and technical quality control. This paper presents a novel framework, namely CLARA, that leverages Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) to detect anomalies in mobile device sensor data. Using the ExtraSensory dataset, which contains labeled sensor data from smartphones and smartwatches, we demonstrate how RAG enhances LLM-based anomaly detection by retrieving relevant historical patterns and domain knowledge to provide context-rich analysis. Our framework serves dual purposes: (1) providing health and lifestyle insights to end-users through anomaly detection in their daily activities and (2) offering manufacturers a tool for identifying technical sensor malfunctions during quality control processes. The framework delivers rich, contextual explanations of detected anomalies, making the results actionable for end-users and technical teams, addressing key industry challenges in sensor data analysis.},
	booktitle = {2025 26th {IEEE} {International} {Conference} on {Mobile} {Data} {Management} ({MDM})},
	author = {Koh, Chan Young and DeMedeiros, Kyle and Hendawi, Abdeltawab},
	month = jun,
	year = {2025},
	note = {ISSN: 2375-0324},
	keywords = {Anomaly detection, Anomaly Detection, Data analysis, Health Monitoring, Large language models, Large Language Models, Mobile Sensors, Monitoring, Quality control, Quality Control, Retrieval augmented generation, Retrieval-Augmented Generation, Sensors, Smart phones, Statistical analysis, Vectors},
	pages = {1--8},
}

@article{chen_llm-based_2025,
	title = {{LLM}-{Based} {Semantic} {Communication}: {The} {Way} {From} {Task}-{Originated} to {General}},
	volume = {14},
	issn = {2162-2345},
	doi = {10.1109/LWC.2025.3583053},
	abstract = {Semantic communication represents a paradigm shift in 6G communications, emphasizing the transmission of meaning rather than solely syntactic elements. Despite this advancement, current approaches exhibit limitations in generality, adaptability to dynamic environments, and dependence on static knowledge bases. To address these challenges, we propose a novel Large Language Model-based Semantic Communication (LLM-SemCom). LLM-SemCom incorporates three key innovations: 1) structured semantic triple representation that mitigates LLM hallucinations unlike existing unstructured approaches, 2) knowledge-base-free LLM semantic processing that adapts dynamically without static domain constraints, and 3) Retrieval-Augmented Generation-enhanced personalization that maintains semantic fidelity while enabling user-specific adaptation. Experimental results demonstrate that LLM-SemCom significantly outperforms existing methods, achieving up to a 22.7\% improvement in sentence similarity while maintaining consistent performance across diverse languages and channel conditions.},
	number = {10},
	journal = {IEEE Wireless Communications Letters},
	author = {Chen, Mingkai and Sun, Zhende and He, Xitao and Wang, Lei and Al-Dulaimi, Anwer},
	month = oct,
	year = {2025},
	keywords = {Accuracy, Adaptation models, Data mining, Encoding, hallucination, Knowledge based systems, LLMs, Pipelines, prompt, Retrieval augmented generation, Semantic communication, semantic triples, Semantics, Triples (Data structure)},
	pages = {3029--3033},
}

@inproceedings{gaddameedi_rag_2025,
	title = {{RAG} {HUB} - {A} comprehensive guide to build an {AI} {Architecture}},
	doi = {10.1109/ICCC64910.2025.11077193},
	abstract = {The Retrieval-Augmented Generation (RAG) HUB framework introduces a visionary, streamlined approach to simplify AI development by addressing complex challenges that often dissuade new developers. The unique selling point of RAG HUB lies in its question-driven methodology utilizing the libraries provided by robust frameworks such as LangChain and LlamaIndex. This methodology allows users with varying expertise to create efficient AI architectures that are tailored to their use case with tailored prompts. For instance, developers working on creating chatbots or retrieving data can progressively follow a structured path from broad problem statements like a chatbot with SharePoint-stored data to niche-specific solution-based choices. Thus, it allows for the removal of trial and error, enabling faster and more precise solutions.RAG HUB uses dynamic filtering, decision tree logic, and an extensive repository of validated techniques to concentrate the development process across various AI tasks, from data ingestion to embedding and retrieval. The framework eliminates irrelevant methods as developers navigate through guided queries, ensuring that only the most applicable techniques remain. This segmented strategy proves to be optimal in all ways of implementation adapting itself to real-time the ever-evolving requirements and also in reducing the learning curve and time invested in modeling advanced AI frameworks. Further enhancements include automating the process from problem statement input and real-time user feedback aiming to make RAG HUB more responsive and intuitive, thereby solidifying its role as an imperative tool in the rapidly changing AI realm.},
	booktitle = {2025 6th {International} {Conference} on {Control}, {Communication} and {Computing} ({ICCC})},
	author = {Gaddameedi, Suvardhan Dileep and Aryan, Sudeep},
	month = may,
	year = {2025},
	keywords = {Aerodynamics, Artificial intelligence, Chatbots, Computer architecture, Filtering, Lang Chain, Libraries, LlamaIndex, Logic, Navigation, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation},
	pages = {1--7},
}

@inproceedings{huang_chatdc_2025,
	title = {{ChatDC}: {A} {Multi}-{Expert} {RAG} {Enhanced} {LLM} for {Data} {Center} {Operations}},
	doi = {10.1109/VTC2025-Spring65109.2025.11174324},
	abstract = {Data centers (DC), as the computational core, play a pivotal role in driving the development of various industries. However, their 24/7 operational workload places immense pressure on operations staff, and traditional AI for IT Operations technologies due to their inability to communicate with operations personnel through language, are unable to effectively alleviate this pressure. In contrast, Retrieval-Augmented Generation (RAG), by integrating external knowledge bases with Large Language Models, enables effective communication with operations teams. Nevertheless, the direct application of RAG in DC currently faces various challenges, such as limited text representation, context length limitations, and poor scalability for large-scale databases. In this paper, we propose multi-modal multi-agent framework for DC facility operations. This framework is primarily composed of a manager and multiple specialized experts, which supports providing intuitive multi-modal information to assist in responses and optimizes response quality in terms of both depth and breadth, delivering more comprehensive and precise answers to offer an innovative solution for enhancing DC operations. Finally, we build a DC operations question-answering dataset and conduct comparative testing of the proposed framework using multiple evaluation metrics.},
	booktitle = {2025 {IEEE} 101st {Vehicular} {Technology} {Conference} ({VTC2025}-{Spring})},
	author = {Huang, Jin and Sun, Yu and Dou, Jinhui and Zhou, Haibo},
	month = jun,
	year = {2025},
	note = {ISSN: 2577-2465},
	keywords = {data center, Data centers, Faces, Industries, Large language models, Large Language Models, Measurement, multi-agent, multimodal, operations, Personnel, Retrieval augmented generation, Retrieval-Augmented Generation, Scalability, Testing, Vehicular and wireless technologies},
	pages = {1--6},
}

@inproceedings{hsu_addressing_2024,
	title = {Addressing {LLM} {Challenges}: {A} {Hybrid} {Framework} for {Duplicate} {Question} {Detection}},
	doi = {10.1109/ICAIRC64177.2024.10900148},
	abstract = {Detecting similar or duplicate texts or questions is a critical subtask in information retrieval. Despite the remarkable advancements achieved by large language models (LLMs) in the question answering (QA) domain in recent years, challenges such as unreliable responses caused by LLM hallucinations and outdated information have highlighted the continued importance of information retrieval techniques. The integration of retrieval-augmented generation (RAG) with LLMs has mitigated some of these limitations, enhancing the reliability of applying LLMs to domain-specific knowledge. This paper uses Chinese educational questions as a case study and introduces an innovative framework for duplicate question detection, termed the Balanced Duplicate Question Detector (BDQD). The BDQD incorporates multiple extensible detectors and a machine learning-based balancer. Experimental results reveal that detecting duplicate questions involves an inherent trade-off between precision and recall. Logistic regression proves effective in balancing thresholds across different detectors, establishing a more stable and reliable standard for duplicate question determination. Finally, we propose a hybrid QA framework that considers both cost and efficiency, integrating the lightweight question retrieval architecture developed in this study with RAG and LLMs. This framework offers practical recommendations for building robust and efficient QA systems.},
	booktitle = {2024 4th {International} {Conference} on {Artificial} {Intelligence}, {Robotics}, and {Communication} ({ICAIRC})},
	author = {Hsu, Hao-Hsuan and Huang, Nen-Fu},
	month = dec,
	year = {2024},
	keywords = {Costs, Detectors, Information retrieval, keyword network, Large language models, LLM challenges, Logistic regression, machine learning, Question answering (information retrieval), question answering system, Reliability, Retrieval augmented generation, Robots, Standards},
	pages = {283--287},
}

@inproceedings{s_leveraging_2024,
	title = {Leveraging {Technology} to {Empower} {Millet} {Farmers}: {A} {Retrieval}-{Augmented} {Generation} {Approach} with {Large} {Language} {Models}},
	doi = {10.1109/GCAT62922.2024.10923869},
	abstract = {This research addresses the critical need to bridge the gap between traditional agricultural knowledge and modern findings, particularly focusing on millet cultivation in India. Millet, being a staple grain in the region, holds significant importance for both farmers and agricultural researchers. However, the lack of accessible technology exacerbates the dissemination of crucial information to farmers. To tackle this challenge, our study proposes a comprehensive solution leveraging cutting-edge technologies such as Large Language Models (LLMs), Prompting, and vector database management. The core of our approach centers on the utilization of RAG techniques, which involves scraping and segmenting millet-related data into manageable chunks. These chunks are then transformed into vectors using LLMs and stored in the vector database. Similarity measures such as cosine similarity to obtain relevant vectors in response to questions posed by farmers. Prompting is then employed to provide responses that understood by humans. Crucially, the technology prevents the spread of false information by refusing to respond to questions that are not related to agriculture, thus guaranteeing accuracy.},
	booktitle = {2024 5th {IEEE} {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {S, Anju and Krishnan, Anuja G and G, Veena},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Agriculture, COSINE SIMILARITY, Databases, EMBED-DING, Farming, Focusing, Knowledge based systems, Large language models, LLMS, NLP, Oral communication, PROMPTING, RAG, Retrieval augmented generation, VECTOR DATABASE, Vectors},
	pages = {1--7},
}

@inproceedings{kumawat_sh-rag_2025,
	title = {{SH}-{RAG} (self-healing retrieval augmented generation): a self-correcting {RAG} framework for large language models},
	volume = {2025},
	doi = {10.1049/icp.2025.1292},
	abstract = {Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) have significantly enhanced AI-driven knowledge synthesis by dynamically incorporating external sources. However, these models frequently encounter retrieval inconsistencies, hallucinations, and factual misalignment, leading to unreliable outputs. This paper introduces SH-RAG (Self-Healing Retrieval-Augmented Generation), a novel framework that autonomously detects, corrects, and refines retrieval errors and generated responses. SH-RAG comprises three core components: Self-Verification Module – Implements real-time fact-checking and confidence scoring to assess retrieved content. Retrieval Refinement Engine – Dynamically ranks and filters retrieved documents, ensuring optimal relevance. Self-Feedback Learning Loop – Leverages reinforcement learning to iteratively enhance retrieval strategies and reduce misinformation propagation. We evaluate SH-RAG across multiple domains, including financial analysis, scientific literature retrieval, and real-time AI-driven customer support, benchmarking its performance against conventional RAG approaches. Empirical results demonstrate a significant reduction in hallucination rates, improved retrieval precision, and enhanced response accuracy, establishing SH-RAG as a reliable and adaptive AI framework for knowledge augmentation.},
	booktitle = {Parul {University} {International} {Conference} on {Engineering} and {Technology} 2025 ({PiCET} 2025)},
	author = {Kumawat, Ayush and Jawdekar, Anand and Patsariya, Sanjay and Gupta, Vicky},
	month = may,
	year = {2025},
	pages = {178--185},
}

@inproceedings{bouchekir_hallucination_2025,
	title = {Hallucination {Detection} in {LLMs} via {Beam} {Search} {Sampling} and {Semantic} {Consistency} {Analysis}},
	doi = {10.1109/DSN-W65791.2025.00076},
	abstract = {Large Language Models (LLMs) have revolutionized natural language processing, enabling high-quality content generation. However, they remain prone to hallucinations, instances where generated outputs deviate from factual truth. In this paper, we propose a novel approach that integrates Beam Search Sampling (BSS) with Semantic Consistency Analysis to systematically detect factual hallucinations. Our method leverages BSS to generate multiple candidate answers, capturing the model's confidence distribution across different plausible answers. These answers are then clustered based on semantic similarity, and a Natural Language Inference (NLI) model is applied to assess entailment and contradiction relationships. To quantify hallucinations, we introduce a scoring mechanism that combines token probabilities with semantic similarity metrics. Additionally, for cases where BSS produces a single answer, we incorporate a Chain-of-Verification (CoVe) mechanism to perform self-consistency checks. We evaluate our approach using Llama3.8B on the TruthfulQA dataset, achieving a precision of 0.87 and an AUROC of 0.81 for multi-answer generation. For single-answer verification with CoVe, we report a precision of 0.64 and accuracy of 0.71. Our approach outperforms conventional semantic entrony-based methods.},
	booktitle = {2025 55th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Bouchekir, Radouane and Faghih, Fathiyeh and Beyene, Tewodros A.},
	month = jun,
	year = {2025},
	note = {ISSN: 2325-6664},
	keywords = {Accuracy, Beam Search Sampling, Hallucination Detection, Large language models, Large Language Models (LLMs), Natural language processing, Probability distribution, Refining, Retrieval augmented generation, Robustness, Semantic Consistency, Semantics, Training data, Uncertainty},
	pages = {274--281},
}

@inproceedings{mawardi_indonesian_2025,
	title = {Indonesian {Educational} {Chatbot} for {Sports}, {Health} and {Physical} {Education} using {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICoDSA67155.2025.11157494},
	abstract = {This study evaluates the effect of retrieval embeddings and similarity metrics at the generative component (LLaMA3). No fine-tuning or decoding variation was applied to the generator, ensuring that any performance differences were due to retrieval quality alone. This design enables a focused evaluation of how retrieval configurations influence the final output in a controlled RAG setup, particularly for primary school topics in sports and health (PJOK). We point out that RAG is a technique that helps large language models make fewer mistakes by using outside information, which improves the accuracy of facts and the relevance of the context. The research compares various embedding models and retrieval similarity metrics to evaluate their effectiveness in retrieving relevant information. The baai/bge-m3 and gte-multilingual-base models demonstrate strong performance with 0.98 precision, while the indo-bge-m3 model shows high recall but lower precision. From 5 embeddings, Llama Embedding performs significantly worse than the other models. The dot product has higher recall than Euclidean and cosine similarity metrics but embedding model selection significantly influences overall performance. The study underscores the importance of embedding choice and retrieval quality in improving chatbot responses. Further refinements are suggested, including better filtering, reranking retrieved chunks, and fine-tuning the retriever, or LLaMA, to align with the target language domain, such as Bahasa Indonesia. The results show that even though RAG improves how well things are generated, the quality of what is retrieved is still a major problem, which means we need to keep making improvements for the best results in educational uses.},
	booktitle = {2025 {International} {Conference} on {Data} {Science} and {Its} {Applications} ({ICoDSA})},
	author = {Mawardi, Viny Christanti and Trilaksono, Bambang Riyanto and Purwarianti, Ayu},
	month = jul,
	year = {2025},
	keywords = {Accuracy, Chatbots, educational chatbot, embedding, Filtering, Generators, Large language models, LLM, Measurement, PJOK, RAG, Reliability, Retrieval augmented generation, Semantics, Sports},
	pages = {667--672},
}

@inproceedings{qiu_stella_2024,
	title = {{SteLLA}: {A} {Structured} {Grading} {System} {Using} {LLMs} with {RAG}},
	doi = {10.1109/BigData62323.2024.10825385},
	abstract = {Large Language Models (LLMs) have shown strong general capabilities in many applications. However, how to make them reliable tools for some specific tasks such as automated short answer grading (ASAG) remains a challenge. We present SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval Augmented Generation (RAG) approach is used to empower LLMs specifically on the ASAG task by extracting structured information from the highly relevant and reliable external knowledge based on the instructor-provided reference answer and rubric, b) an LLM performs a structured and question-answering-based evaluation of student answers to provide analytical grades and feedback. A real-world dataset which contains students’ answers in an exam was collected from a college-level Biology course. Experiments show that our proposed system can achieve substantial agreement with the human grader while providing break-down grades and feedback on all the knowledge points examined in the problem. A qualitative and error analysis of the feedback generated by GPT4 shows that GPT4 is good at capturing facts while may prone to inferring too much implication from the given text in the grading task which provides insights into the usage of LLMs in the ASAG system.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Qiu, Hefei and White, Brian and Ding, Ashley and Costa, Reinaldo and Hachem, Ali and Ding, Wei and Chen, Ping},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Big Data, Biology, Data mining, Error analysis, Knowledge based systems, Large language models, LLM-based ASAG system, QA-based evaluation, RAG, Reliability, Retrieval augmented generation, structured evaluation},
	pages = {8154--8163},
}

@inproceedings{sun_fusion-based_2025,
	title = {Fusion-{Based} {Retrieval}-{Augmented} {Generation} for {Complex} {Question} {Answering} with {LLMs}},
	doi = {10.1109/CISAT66811.2025.11181772},
	abstract = {This paper proposes a Retrieval-Augmented Generation (RAG) model that integrates structured and unstructured knowledge. The aim is to enhance the knowledge coverage and generation accuracy of large language models in complex question-answering tasks. The method introduces a dual-channel knowledge retrieval mechanism. One channel targets structured knowledge sources such as knowledge graphs and databases. The other focuses on unstructured textual resources such as documents and paragraphs. A unified knowledge fusion network integrates both types of heterogeneous information into a coherent generation context. The model performs multi-level modeling across key components. These include query representation generation, knowledge retrieval, representation alignment, and fusion expression construction. As a result, the generation stage produces text that is semantically rich and logically consistent. Under low-resource conditions, the method significantly improves the accuracy and linguistic quality of generated outputs. It also shows strong stability and generalization in cross-domain tasks. Systematic experiments were conducted on the proportion of structured knowledge, types of knowledge sources, and fusion strategies. The results demonstrate the effectiveness of the fusion architecture in enhancing knowledge representation in language models. This study provides a methodological foundation and empirical support for building controllable and trustworthy knowledge-enhanced natural language generation systems.},
	booktitle = {2025 8th {International} {Conference} on {Computer} {Information} {Science} and {Application} {Technology} ({CISAT})},
	author = {Sun, Yumeng and Zhang, Renhan and Meng, Renzi and Lian, Lian and Wang, Heyi and Quan, Xuehui},
	month = jul,
	year = {2025},
	keywords = {Accuracy, Buildings, cross-domain question answering, Knowledge fusion, Large language models, Question answering (information retrieval), Retrieval augmented generation, retrieval enhancement generation, Solids, Stability analysis, Standards organizations, structured knowledge, Systematics, Terminology},
	pages = {116--120},
}

@inproceedings{zeng_federated_2024,
	title = {Federated {Recommendation} via {Hybrid} {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/BigData62323.2024.10825302},
	abstract = {Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as incomplete recommendation and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, in the second stage, the results returned by hybrid retrieval are converted into text prompts and fed into GPT for re-ranking. Under GPT-FedRec, the privacy of both local training data and global test data is well protected, as there is no data exchange across any clients or the global server. For test users, GPT-FedRec executes inference only on the global server: given the historical data of a test user, GPT-FedRec performs hybrid retrieval and GPT-based re-ranking, without exposing test data to any other clients. Our proposed hybrid retrieval mechanism and LLM-based re-ranking aim to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. Finally, the RAG nature of GPT-FedRec also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods. Our code is available at https://github.com/huiminzeng/GPT-FedRec.git.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Zeng, Huimin and Yue, Zhenrui and Jiang, Qian and Wang, Dong},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Chatbots, Feature extraction, federated recommendation, Hands, Hybrid power systems, large language models, Large language models, Process mining, Recommender systems, Retrieval augmented generation, sequential recommendation, Servers, Training data},
	pages = {8078--8087},
}

@inproceedings{tang_hivegen_2025,
	title = {{HiVeGen} – {Hierarchical} {LLM}-based {Verilog} {Generation} for {Scalable} {Chip} {Design}},
	doi = {10.1109/ICLAD65226.2025.00031},
	abstract = {With Large Language Models (LLMs) recently demonstrating impressive proficiency in code generation, it is promising to extend their abilities to Hardware Description Language (HDL). However, LLMs tend to generate single HDL code blocks rather than hierarchical structures for hardware designs, leading to hallucinations, particularly in complex designs like Domain-Specific Accelerators (DSAs). To address this, we propose HiVeGen, a hierarchical LLM-based Verilog generation framework that decomposes generation tasks into LLM-manageable hierarchical submodules. HiVeGen further harnesses the advantages of such hierarchical structures by integrating automatic Design Space Exploration (DSE) into hierarchy-aware prompt generation, introducing weight-based retrieval to enhance code reuse, and enabling real-time human-computer interaction to lower error-correction cost, significantly improving the quality of generated designs.},
	booktitle = {2025 {IEEE} {International} {Conference} on {LLM}-{Aided} {Design} ({ICLAD})},
	author = {Tang, Jinwei and Qin, Jiayin and Thorat, Kiran and Zhu-Tian, Chen and Cao, Yu and Zhao, Yang Katie and Ding, Caiwen},
	month = jun,
	year = {2025},
	keywords = {Costs, Domain-Specific Accelerator, Hardware acceleration, Hardware design languages, Hierarchy, Human computer interaction, Large language models, LLM, Parser, Periodic structures, Real-time systems, Redundancy, Retrieval augmented generation, Retrieval-Augmented Generation, Space exploration},
	pages = {30--36},
}

@inproceedings{li_knowledge_2025-1,
	title = {Knowledge {Graph} {Prompting} for {Complex} {Domain} {Question} {Answering}},
	doi = {10.1109/AIoTC66747.2025.11198680},
	abstract = {Large Language Models (LLMs) have achieved widespread success in various Natural Language Processing (NLP) tasks, especially in the Question Answering (QA) task. Despite their impressive performance, they often struggle with issues such as hallucinations and lack of domain knowledge, limiting their ability to accurately reasoning within specialized fields. Knowledge Graphs (KGs), as structured sources of factual knowledge, offer a valuable resource to address these issues. However, existing KG-augmented LLM methods mainly focus on the open-domain multi-hop QA task. Notably, they fail to account for other types of com plex questions, such as comparing and parallel questions. In this paper, we propose a novel method KGPCQA, which introduces a joint fine-tuning task for extracting topic entities and predicting relation paths to enhance domain knowledge understanding of the model. This improves reasoning path retrieval and enables the model to effectively answer do main questions using questions and pruned knowledge. Experiments on the operator field dataset prove that our method outperforms existing advanced approaches, achieving Hit@1 accuracy and F1\_score of 95.2 and 90.8, respectively.},
	booktitle = {2025 4th {International} {Conference} on {Artificial} {Intelligence}, {Internet} of {Things} and {Cloud} {Computing} {Technology} ({AIoTC})},
	author = {Li, Fei and Xu, Yin and Wang, Yanyan and Zhao, Long},
	month = aug,
	year = {2025},
	keywords = {Cognition, Computer architecture, Computers, Internet of Things, Knowledge Graph, Knowledge graphs, Large language models, Limiting, Predictive models, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation},
	pages = {592--596},
}

@inproceedings{li_alleviating_2024,
	title = {Alleviating {Action} {Hallucination} for {LLM}-based {Embodied} {Agents} via {Inner} and {Outer} {Alignment}},
	doi = {10.1109/PRAI62207.2024.10826957},
	abstract = {Large language models (LLMs) have demonstrated impressive potential in empowering embodied agents, fortifying them with task planning and reasoning capabilities closely akin to humans. However, there remain significant challenges in aligning LLM-based embodied agent actions with the executable and safe action space to reduce hallucination. In this paper, we propose a flexible and resource-efficient framework for aligning the action space of LLM-based embodied agents. The framework employs a parameter-efficient fine-tuning method for inner alignment and a retrieval-based generation approach for outer alignment. Specifically, when the inner aligned model generates an action, the outer alignment employs ROUGE to calculate the similarity between the action and all actions within a safe and valid action space, ultimately selecting the action with the highest similarity as the output. For situations with multiple alternative actions, the outer alignment introduces a policy model, which could be either open-source small LLMs or commercial LLMs, to determine the optimal action based on the current context of the agent's task execution. The retrieval-based outer alignment ensures all actions align with an executable action space, alleviating action hallucination for LLM-based agents and significantly improving the controllability and interpretability of their decision-making process. Through extensive experimentation with the LlaMA-2, Bloomz, and OPT models on the ALFWorld benchmark, we validate the effectiveness and adaptability of our framework.},
	booktitle = {2024 7th {International} {Conference} on {Pattern} {Recognition} and {Artificial} {Intelligence} ({PRAI})},
	author = {Li, Kanxue and Zheng, Qi and Zhan, Yibing and Zhang, Chong and Zhang, Tianle and Lin, Xu and Qi, Chongchong and Li, Lusong and Tao, Dapeng},
	month = aug,
	year = {2024},
	keywords = {Aerospace electronics, Benchmark testing, Controllability, Decision making, embodied agent, large language model, Large language models, model alignment, Pattern recognition, Planning, Process control, retrieval-augmented generation, Safety, Stability analysis, task planning},
	pages = {613--621},
}

@inproceedings{topalis_langbo_2025,
	title = {{LangBO}: {A} {Framework} for {Language}-{Guided} {Prior} {Integration} in {Bayesian} {Optimization}},
	doi = {10.1109/ETFA65518.2025.11205749},
	abstract = {The development of high-performance machine learning models has traditionally required extensive expertise, thereby excluding domain experts without a formal AI background. To overcome this barrier, we propose LangBO, a novel framework that systematically integrates domain-specific prior knowledge into the Bayesian Optimization (BO) process via natural language input. By leveraging Large Language Models (LLMs) in combination with Retrieval-Augmented Generation (RAG) and a self-evaluation mechanism, unstructured domain expert knowledge is transformed into a structured Dirichlet prior distribution, thereby guiding the optimization of neural architectures and hyperparameters. Initial experiments on a real-world classification task demonstrate accelerated convergence without compromising final model performance while improving interpretability and sample-efficient AutoML for users lacking machine learning expertise.},
	booktitle = {2025 {IEEE} 30th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Topalis, Philip and Schieseck, Marvin and Gehlhoff, Felix},
	month = sep,
	year = {2025},
	note = {ISSN: 1946-0759},
	keywords = {Automated machine learning, AutoML, Bayes methods, Bayesian Optimization, Convergence, Large language models, Large Language Models, Manufacturing automation, Natural languages, Optimization, Prior Knowledge Integration, Retrieval augmented generation, Retrieval-Augmented Generation, Self-evaluation},
	pages = {1--4},
}

@inproceedings{celik_hybridizing_2025,
	title = {Hybridizing {Large} {Language} {Models} and {Dual}-{Layer} {Retrieval} for {Context}-{Aware} {Movie} {Rating} {Prediction}: {A} {Case} {Study} with {Gemma} 2 ({9B}) and {MovieLens}},
	doi = {10.1109/UBMK67458.2025.11206808},
	abstract = {This study introduces a novel approach to movie rating prediction by fine-tuning the Gemma 2 (9B) Large Language Model (LLM) within a Retrieval-Augmented Generation (RAG) framework. The model is tuned using structured prompts derived from the MovieLens 100K dataset. To enable context-aware predictions, we employ a dual-layer RAG architecture that retrieves structured user history from PostgreSQL and semantic movie metadata from a Qdrant vector database. The proposed system demonstrates that hybridizing LLMs with structured and semantic retrieval mechanisms is an effective strategy for building accurate, context-aware recommender systems.},
	booktitle = {2025 10th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Çelik, Yusuf and Bilge, Alper},
	month = sep,
	year = {2025},
	note = {ISSN: 2521-1641},
	keywords = {Accuracy, Computational modeling, Dual-Layer Retrieval, Fine-Tuning, Gemma 2, History, Large language models, Large Language Models (LLM), Metadata, Motion pictures, Movie Rating Prediction, Predictive models, Recommender systems, Recommender Systems, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantics},
	pages = {28--33},
}

@inproceedings{reeves_robustness_2025,
	title = {Robustness of {Question} {Answering} {Systems} in the {Biomedical} {Domain}: a study of the {BioASQ} dataset},
	doi = {10.1109/ICIT64950.2025.11049250},
	abstract = {Robustness is a critical consideration when integrating artificial intelligence (AI) systems into decision-making processes. This concern is particularly relevant for generative AI systems, which are designed to consistently produce convincing outputs but can be prone to hallucinations, risking overconfidence in their outputs. This paper investigates the performance of fine-tuning and retrieval augmented generation (RAG) under external data quality perturbations, including typographical errors and factual inaccuracies. Results from experiments using the BioASQ Task 12b question answering dataset and PubMed articles showed nuanced trade-offs, with either RAG or fine-tuning performing better for different scenarios. Furthermore, an analysis of LLMs’ self-reported confidence scores indicated a tendency toward overconfidence, particularly in the presence of inconsistent or erroneous context data. A novel mitigation strategy, leveraging an LLM for data quality error correction was evaluated, but the results demonstrated limited effectiveness, highlighting the need for more advanced correction techniques.},
	booktitle = {2025 12th {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Reeves, Andrew and Dong, Hang},
	month = may,
	year = {2025},
	note = {ISSN: 2831-3399},
	keywords = {Data integrity, Generative AI, Information technology, Large language models, Perturbation methods, Predictive models, Prevention and mitigation, Question answering, Question answering (information retrieval), Retrieval augmented generation, Robustness},
	pages = {247--252},
}

@inproceedings{jadon_enhancing_2025,
	title = {Enhancing {Domain}-{Specific} {Retrieval}-{Augmented} {Generation}: {Synthetic} {Data} {Generation} and {Evaluation} using {Reasoning} {Models}},
	doi = {10.1109/IAICT65714.2025.11100564},
	abstract = {Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance.First, we introduce token-aware metrics PrecisionΩ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade offs inherent in technical texts. Second, we develop a reasoning model driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity).Our empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31–42\% (IoU=0.071 vs. baseline 0.053) at recall costs (–18\%), while domain-specific embedding strategies yield 22\% variance in optimal chunk sizing (5–20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14\% mean IoU over alternatives), though no configuration universally dominates financial texts favor larger chunks for risk factor coverage (Recall=0.81@size=20), whereas cybersecurity content benefits from atomic segmentation (PrecisionΩ=0.28@size=5).We open-source this toolkit enabling reproducible optimization of chunking strategies through automated synthetic dataset generation and multi-metric analysis pipelines. This work bridges critical gaps between generic RAG architectures and enterprise requirements for precision-sensitive domains. Our code is available on https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Models.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Industry} 4.0, {Artificial} {Intelligence}, and {Communications} {Technology} ({IAICT})},
	author = {Jadon, Aryan and Patil, Avinash and Kumar, Shashank},
	month = jul,
	year = {2025},
	note = {ISSN: 2834-8249},
	keywords = {Biological system modeling, Chunk Optimization, Cognition, Computer security, Data models, Domain-Specific NLP, Evaluation Metrics, Finance, Measurement, Optimization, Pipelines, Reasoning Models, Retrieval augmented generation, Retrieval-Augmented Generation, Synthetic data, Synthetic Data Generation},
	pages = {445--452},
}

@inproceedings{nurhayati_performance_2024,
	title = {Performance {Evaluation} of a {Chatbot} with {Retrieval} {Augmented} {Generation} and {Generative} {Pre}-trained {Transformer}-4 {Model} for {Taharah} {Domain}},
	doi = {10.1109/ICIC64337.2024.10957519},
	abstract = {The chatbot is one of the developments of artificial intelligence. the chatbots are utilized as conversational agents employing large language models (LLMs). One challenge with LLMs is the occurrence of hallucinations in the generated responses. The author proposes a chatbot developed using retrieval-augmented generation (RAG) technique and the GPT-4 model. The chatbot retrieves additional information stored in a graph database as a source to generate responses to users. The chatbot is designed to answer questions regarding the foundational principles based on the four imams' interpretations found in the book Rahmah Al Ummah Fi Ikhtilaf Al A’immah. Based on testing results, the chatbot demonstrates accuracies, precisions, recalls, and F1-scores of {\textbackslash}mathbf9 0 \%, {\textbackslash}mathbf8 6 . 6 7 \%, {\textbackslash}mathbf1 0 0 \%, and {\textbackslash}mathbf9 2 . 8 6 \%, respectively.},
	booktitle = {2024 {Ninth} {International} {Conference} on {Informatics} and {Computing} ({ICIC})},
	author = {{Nurhayati} and Abdurrohman, Royyan and Hulliyah, Khodijah and Khairani, Dewi},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Chatbot, Chatbots, Databases, GPT-4, Large language models, Performance evaluation, Retrieval augmented generation, Retrieval Augmented Generation, Semantics, Taharah, Testing, Transformers, Vectors},
	pages = {1--6},
}

@inproceedings{abrahamyan_stackrag_2024,
	title = {{StackRAG} {Agent}: {Improving} {Developer} {Answers} with {Retrieval}-{Augmented} {Generation}},
	doi = {10.1109/ICSME58944.2024.00098},
	abstract = {Developers spend much time finding information that is relevant to their questions. Stack Overflow has been the leading resource, and with the advent of Large Language Models (LLMs), generative models such as ChatGPT are used frequently. However, there is a catch in using each one separately. Searching for answers is time-consuming and tedious, as shown by the many tools developed by researchers to address this issue. On the other, using LLMs is not reliable, as they might produce irrelevant or unreliable answers (i.e., hallucination). In this work, we present StackRAG, a retrieval-augmented Multiagent generation tool based on LLMs that combines the two worlds: aggregating the knowledge from SO to enhance the reliability of the generated answers. Initial evaluations show that the generated answers are correct, accurate, relevant, and useful. A description video can be found here11Please note that as the tool requires API keys, we are not able to set up a live demo of StackRAG..},
	booktitle = {2024 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Abrahamyan, Davit and Fard, Fatemeh H.},
	month = oct,
	year = {2024},
	note = {ISSN: 2576-3148},
	keywords = {Accuracy, Chatbots, Large language models, LLM, Multiagent tool, RAG-based tool, Search engines, Software engineering, Software maintenance, Software reliability, Stack Overflow},
	pages = {893--897},
}

@inproceedings{guan_external_2025,
	title = {From {External} {Similarity} to {Internal} {Consistency}: {An} {Enhanced} {Retrieval}-{Based} {Method} for {LLMs}' {Reliable} {Content} {Generation}},
	doi = {10.1109/CSCWD64889.2025.11033484},
	abstract = {Artificial Intelligence Generated Content (AIGC) has emerged as a mainstream research direction with the development of Large Language Models (LLMs). The hallucination of LLMs, however, always interweaves the generated content with outdated or fabricated information, making it hard to be fully trusted and severely hindering LLMs form being widely applied in real-life scenarios. To address this problem, Retrieval Augmented Generation (RAG) has been proposed, which incorporates external knowledge to assist LLMs with content generation and significantly alleviates the hallucination problem. Nonetheless, the vanilla RAG uses similarity as the sole criterion for selecting external knowledge, neglecting the problem of internal inconsistency within the knowledge itself, which may distract LLMs from focusing on the most important information during the content generation process and, therefore, has a negative impact on the generated content's reliability. In this paper, we propose a novel metric, Entropy-based Internal Consistency (EIC), to measure the internal consistency of the external knowledge which is then integrated with similarity to mutually determine the knowledge's importance. Experimental results demonstrate that the proposed metric can provide a more fine-grained signal for external knowledge selection, thereby enhancing the reliability of generated content.},
	booktitle = {2025 28th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	author = {Guan, Wenbo and Liu, Hangchen and Li, Xiaoqian and Zhou, Jun and Yan, Yonghong},
	month = may,
	year = {2025},
	note = {ISSN: 2768-1904},
	keywords = {Entropy-based internal consistency, Federated learning, Focusing, Hallucination, Large language models, Measurement, Reliability engineering, Retrieval augmented generation, Sorting},
	pages = {1399--1405},
}

@inproceedings{russo_europeanlawadvisor_2024,
	title = {{EuropeanLawAdvisor}: an open source search engine for {European} laws},
	doi = {10.1109/BigData62323.2024.10826025},
	abstract = {Legal Artificial Intelligence has emerged as an essential field, focusing on AI technologies that facilitate various legal tasks and alleviate the workload of legal professionals. Despite advancements in Legal Artificial Intelligence, there remains a critical gap in systems that can provide both comprehensive and contextually accurate retrieval tailored to the intricate structure of EU legislation. We propose EuropeanLawAdvisor, an efficient and user-friendly legal information retrieval system designed to deliver tailored responses to legal queries. This system utilizes open-source Large Language Models within a Retrieval-Augmented Generation framework, facilitating precise and relevant information retrieval. The system employs a robust retrieval approach that integrates multi-match, k-nearest neighbors, hybrid methods, and TF-IDF search strategies across both complete documents and segmented text indexes, ensuring comprehensive retrieval for diverse query types. The implementation of the framework has demonstrated significant improvements in the accuracy and relevance of responses to EU legal queries, enhancing both the retrieval of relevant legal documents and the generation of precise responses. We show that EuropeanLawAdvisor, leveraging open-source models like Phi3-mini-3B and LLaMa-3-8B, achieves competitive Faithfulness and Relevance compared to GPT-4-Turbo. The performance gap narrows significantly in zero-shot scenarios, and our approach outperforms GPT-4-Turbo in the percentage of answered questions. We publicly release our code on GitHub: https://github.com/raffaele-russo/EuropeanLawAdvisor.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Russo, Raffaele and Russo, Diego and Orlando, Gian Marco and Romano, Antonio and Riccio, Giuseppe and Gatta, Valerio La and Postiglione, Marco and Moscato, Vincenzo},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Accuracy, Europe, Explainable AI, Generative AI, Large language models, Law, Legislation, Nearest neighbor methods, NLP in Legal Industry, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Search engines, Search problems, Software development management},
	pages = {4751--4756},
}

@inproceedings{ongris_towards_2024,
	title = {Towards an {Open} {NLI} {LLM}-based {System} for {KGs}: {A} {Case} {Study} of {Wikidata}},
	doi = {10.1109/ISRITI64779.2024.10963661},
	abstract = {The rise of large language models (LLMs) has significantly advanced information retrieval, yet challenges like the limitation of knowledge updating ability, lack of openness, and hallucination issues persist. To address these, Retrieval-Augmented Generation (RAG) has been introduced but remains limited in interpretability due to its reliance on vector-based representations. This paper presents a question-answering (QA) system using GraphRAG, a RAG system with knowledge graphs (KGs) as its base. We develop a natural language interface (NLI) for QA over Wikidata, a popular, open, and crowdsourced KG. Our approach employs LLM chaining, i.e., a paradigm that leverages multiple LLM calls sequentially, to generate SPARQL queries, with the aim of creating an open system that ensures transparency and allows direct inspection of its components. Utilizing an experimental research approach, we evaluated the generated SPARQL queries and found that incorporating a broader set of property candidates into the prompts significantly boosts performance, achieving a Jaccard similarity score of 0.7806. These findings demonstrate the system’s effectiveness in SPARQL query generation, highlighting its potential for further development. However, we consider the limitation of the LLM’s context window and the hallucination phenomenon as the major challenges that limit the system’s performance.},
	booktitle = {2024 7th {International} {Seminar} on {Research} of {Information} {Technology} and {Intelligent} {Systems} ({ISRITI})},
	author = {Ongris, Jaycent Gunawan and Tjitrahardja, Eduardus and Darari, Fariz and Ekaputra, Fajar J.},
	month = dec,
	year = {2024},
	note = {ISSN: 2832-1456},
	keywords = {GraphRAG, KG, Knowledge graphs, Large language models, Linked data, LLM, Natural languages, Open systems, Pipelines, RAG, Retrieval augmented generation, Semantic search, Seminars, Technological innovation, Wikidata},
	pages = {44--49},
}

@inproceedings{kusuma_trisurya_2024,
	title = {Trisurya: {A} {Local} {Language}-{Powered} {Omnichannel} {Chatbot} as a {Solution} for the {Enhancement} of {E}-{Govemment} {Quality} and {Accessibility} of {Public} {Services} in {Indonesia}},
	doi = {10.1109/ICIMCIS63449.2024.10956359},
	abstract = {E-government initiatives have evolved in various forms in Indonesia over the past several decades. However, the services remain far from optimal; information sources are still fragmented and inaccessible to people who are not fluent in Indonesian. Thus, Trisurya is introduced as a chatbot capable of providing information from government and public service officials in local languages, in addition to Indonesian. Our system is built on Large Language Models (LLMs), specifically GPT API, enhanced by Retrieval-Augmented Generation (RAG) and leveraged by graph (Neo4j) and relational (PostgreSQL) databases to ensure the accuracy of the information provided. This study also compares two versions of the GPT API, GPT-4 and GPT-4o, to determine which version performs better as the LLM backend of the Trisurya system. While both offer similar performance, GPT-4o is more cost-effective. The evaluation results of Trisurya demonstrate its superiority over other publicly available and widely used chatbots, i.e., the vanilla ChatGPT-4o, ChatGPT-3.5, Gemini-1.0-Pro, and Claude-3-Sonnet. Based on the result, Trisurya demonstrated clear superiority during quantitative and qualitative assessments by generating accurate and high-quality responses in Javanese, Sundanese, and Balinese. This research represents a breakthrough in technological innovation by enhancing the efficiency of public services and revolutionizing the management and dissemination of government information in an inclusive manner through Trisurya's omnichannel chatbot.},
	booktitle = {2024 {International} {Conference} on {Informatics}, {Multimedia}, {Cyber} and {Information} {System} ({ICIMCIS})},
	author = {Kusuma, Arya Raditya and Wongso, Carleano Ravelza and Aldrich, Darren and Darari, Fariz},
	month = nov,
	year = {2024},
	note = {ISSN: 2837-5203},
	keywords = {Accuracy, Chatbots, Database Integration, Electronic government, Informatics, Large language models, Large Language Models (LLM), Local Languages, Multimedia databases, Omnichannel Chatbot, Public Service, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Technological innovation},
	pages = {601--607},
}

@inproceedings{de_araujo_luz_junior_generative_2025,
	title = {Generative {AI} for {Facial} {Expressions} in {3D} {Game} {Characters}: {A} {Retrieval}-{Augmented} {Approach}},
	doi = {10.1109/GAS66647.2025.00007},
	abstract = {This paper examines a Retrieval-Augmented Generation (RAG) approach for generating facial expressions in 3D game characters using artificial intelligence. By integrating large language models within a RAG-based architecture, we developed a proof-of-concept system that animates expressions based on Facial Action Coding System (FACs) action units. Testing demonstrates the potential of RAG-driven animations to create immersive, adaptive experiences with contextually appropriate expressions that enhance perceived emotional responsiveness in Non-Playable Characters, highlighting RAG’s promise for dynamic character interactions and AI-driven personalization in game development.},
	booktitle = {2025 {IEEE}/{ACM} 9th {International} {Workshop} on {Games} and {Software} {Engineering} ({GAS})},
	author = {De Araújo Luz Junior, Jonas and Prado Saldanha Ribeiro, Guadalupe and Pessoa, Rafael Fonseca and Huarastaca Taveira Magalhães, Alberto and Formico Rodrigues, Maria Andréia},
	month = apr,
	year = {2025},
	note = {ISSN: 2996-5187},
	keywords = {3D Character Animation, Animation, Artificial Intelligence, Conferences, Encoding, Facial Expressions, Games, Generative AI, Large language models, Large Language Models, Retrieval augmented generation, Retrieval-Augmented Generation, Software engineering, Testing, Three-dimensional displays},
	pages = {9--16},
}

@inproceedings{liang_retrieval-augmented_2025,
	title = {Retrieval-{Augmented} {Multilingual} {Citation} {Generation}},
	doi = {10.1109/ICASSP49660.2025.10888712},
	abstract = {Retrieval-augmented citation generation (RACG) helps users trust the large language model output by retrieving evidence from reliable sources. However, most current RACG research focuses on single-language tasks, particularly in English, and overlooks the need for cross-lingual evidence retrieval and utilization in real-world applications. To address this issue, we introduce a plug-and-play Retrieval-Augmented Multilingual Citation Generation method (RAMCG) which uses a multilingual retriever to identify relevant evidence from a multilingual knowledge base. The evidence is then combined with the query and processed by a multilingual citation generator. The result is citations that are both accurate and comprehensive. Experiments show that RAMCG outperforms baseline methods in multilingual citation generation and is well-suited for practical use.},
	booktitle = {{ICASSP} 2025 - 2025 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Liang, Xun and Niu, Simin and Zhang, Sensen and Li, Zhiyu and Zhang, Xuan and Wu, Bo and Xiong, Feiyu and Tang, Bo and Wang, Hanyu and Song, Shichao and Wang, Mengwei and Yang, Jiawei},
	month = apr,
	year = {2025},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Applications of Generative AI, Citation Generation, Generative AI, Generators, Knowledge based systems, Large language models, Measurement, Multilingual, Reliability, Retrieval-Augmented Generation, Signal processing, Speech processing},
	pages = {1--5},
}

@inproceedings{zheng_knowledge-based_2025,
	title = {Knowledge-{Based} {Question} {Answering} {System} for {Hydraulic} {Hoist} {Using} {Large} {Language} {Models}},
	doi = {10.1109/CITSC64390.2025.00056},
	abstract = {The hydraulic hoist is an essential electromechanical equipment widely used in water conservancy projects. Due to its system complexity and the diversity of failure modes, traditional knowledge management is inefficient, and it is difficult to support the demand for timely and accurate knowledge application during the operation and maintenance process of the equipment. This paper develops a knowledge-based question answering (QA) system for hydraulic hoists using the Large Language Model (LLM) combined with the Retrieval Augmented Generation (RAG) and LangChain methods. Firstly, a high-quality knowledge base is constructed with unstructured text data such as hydraulic hoist design data, failure cases, and maintenance manuals. Secondly, database retrieval technology is utilized to achieve graph retrieval and vector retrieval. Finally, combining the LLM and RAG methods, the answers are generated by reasoning between the retrieved relevant text segments and the user's questions. To verify the application effect of the QA system, this study employed a dataset comprising common failure types of hydraulic hoists and conducted a systematic QA evaluation and knowledge reasoning process test based on typical failure scenarios. The results show that the system can effectively deal with complex problem scenarios, provide fast and accurate solutions, and enhance the interpretability of the results through additional information, providing a valuable reference for the intelligent maintenance of electromechanical equipment in water conservancy projects.},
	booktitle = {2025 {Asia}-{Europe} {Conference} on {Cybersecurity}, {Internet} of {Things} and {Soft} {Computing} ({CITSC})},
	author = {Zheng, Shufeng and Li, Helin and Wang, Wenjie and Zhang, Fan and Zhao, Huadong},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Cognition, hydraulic hoist, Hydraulic systems, knowledge base, Knowledge based systems, large language modeling, Large language models, Lifting equipment, Maintenance, Question answering (information retrieval), question answering system, retrieval augmented generation, Retrieval augmented generation, Water conservation},
	pages = {272--276},
}

@inproceedings{jervas_context-driven_2025,
	title = {Context-{Driven} {Chatbot} {Development}: {Leveraging} {Zephyr}-7b with {RAG} for {Improved} {Response} {Accuracy}},
	doi = {10.1109/ETIS64005.2025.10960935},
	abstract = {In recent years, the demand for AI-driven conversational agents has increased significantly across various industries. Traditional generative models, while capable of producing human-like responses, often struggle with factual accuracy and context retention. Retrieval Augmented Generation (RAG) presents a novel approach to enhance the performance of AI chatbots by combining the strengths of both retrieval-based and generative models. The paper focuses on the development of an AI-driven chatbot using a RAG framework, integrating the Zephyr-7b model, to serve the needs of the ICT Academy of Kerala (ICTAK). The chatbot is engineered to deliver precise and contextually relevant responses to user queries by integrating advanced language models with structured data extracted from the ICTAK website. The data, which was meticulously scraped and processed, ensures that the chatbot's knowledge base remains current and accurately reflects the ICTAK's services and operations. A critical challenge addressed by this work is the issue of hallucinations in large language models (LLMs), where models may generate seemingly plausible yet incorrect or irrelevant information. To counteract this, the paper employs various chunking methods that enhance the chatbot's capability to retrieve and generate accurate responses. This AI Chatbot incorporates cutting-edge techniques in natural language processing, including document retrieval, context compression, and re-ranking. This multi-faceted approach ensures that the chatbot not only provides accurate responses but also delivers them in a contextually relevant manner.},
	booktitle = {2025 {Emerging} {Technologies} for {Intelligent} {Systems} ({ETIS})},
	author = {Jervas, Sandra and Jacob, Chinnu and Sundar, Sumod and P, Deepasree Varma},
	month = feb,
	year = {2025},
	keywords = {Accuracy, chatbot, Chatbots, Context modeling, Data models, hallucinations, Industries, Information retrieval, LLM, Overfitting, RAG, Retrieval augmented generation, Transformers, Vectors, Zephyr-7b},
	pages = {1--6},
}

@inproceedings{yazid_imaaduddin_multi-dimensional_2025,
	title = {Multi-{Dimensional} {Analysis} of {LLM} {Models} with {Synthetic} {Data} on {Early} {Marriage} {Prevention}},
	doi = {10.1109/ICoAILO66760.2025.11155930},
	abstract = {Early marriage is a serious social issue that impacts the well-being of society, with approximately 12 million girls marrying before the age of 18 each year according to UNICEF data. This study aims to analyze the performance of three Large Language Models (LLMs) - Mistral-7B-Instruct, Llama-3.1-SB-Instruct, and Qwen2.5-7B-Instruct - using synthetic data in the context of early marriage prevention. The models were developed through fine-tuning and Retrieval- Augmented Generation (RAG) to improve text relevance and accuracy, and evaluated using UniEval. The evaluation results showed that Qwen2.5-7B obtained the highest score of 3.893, Llama-3.1-8B showed strength in coherence (up to 0.998) and groundedness (0.999) and Mistral-7B excelled in naturalness improvement after fine-tuning (0.900). This study highlights that the balance between readability, factual accuracy, and conversational appeal is key in improving the output quality of AI assistants for social education on early marriage prevention.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Artificial} {Intelligence} for {Learning} and {Optimization} ({ICoAILO})},
	author = {Yazid Imaaduddin, Ma'isy and Wijaya, Rifki and Kosala, Gamma},
	month = aug,
	year = {2025},
	keywords = {Accuracy, Analytical models, chatBot, child married, Coherence, Context modeling, Data models, fine-tuning, Large language models, large language models (LLM), Prevention and mitigation, Retrieval augmented generation, retrieval-augmented generation (RAG), Synthetic data, Testing, uniEval},
	pages = {359--365},
}

@inproceedings{tvarozek_what_2025,
	title = {What {If} {LLM} {Doesn}’t {Know}},
	doi = {10.1109/KI64036.2025.10916478},
	abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing, demonstrating impressive capabilities across a wide range of tasks. However, there are instances where LLMs fall short, either due to outdated training data or the absence of domain-specific knowledge. This article explores the challenges and solutions when an LLM “doesn’t know” or lacks sufficient information to provide accurate responses. We discuss various strategies to address these gaps, including fine-tuning with additional datasets and integrating Retrieval-Augmented Generation (RAG). RAG, which combines the strengths of LLMs with external knowledge sources, stands out as a powerful solution, enabling real-time retrieval of relevant data during model inference. This hybrid approach ensures that LLMs can provide more up-to-date, contextually accurate, and domain-specific responses by leveraging external databases or knowledge systems. The article highlights the potential of RAG as a scalable, adaptable method for enhancing LLM performance in environments where generalistic models alone may be insufficient, offering practical guidelines for implementation and future research directions in this area.},
	booktitle = {2025 {Cybernetics} \& {Informatics} ({K}\&{I})},
	author = {Tvarožek, Rastislav and Haffner, Oto},
	month = feb,
	year = {2025},
	note = {ISSN: 2767-875X},
	keywords = {Accuracy, Adaptation models, Data models, Knowledge gap, Large language models, LLM, Natural language processing, Prompt engineering, RAG, Real-time systems, Reliability, Retrieval augmented generation, Training data},
	pages = {1--6},
}

@inproceedings{liu_research_2025,
	title = {Research on {Aerospace} {Intelligent} {Question} {Answering} {Model} {Based} on {Large} {Model} {Technology}},
	doi = {10.1109/AINIT65432.2025.11034989},
	abstract = {With the rapid development of aerospace technology in our country, news reports in related fields are increasingly common. However, the aerospace domain is characterized by vast, complex, and highly specialized data, making it difficult for the general public and researchers to obtain the information they need in a short time. To help people more conveniently acquire and understand knowledge in the aerospace field, this paper constructs a high-precision dialogue model by integrating innovative dataset construction methods, model fine-tuning techniques, and retrieval-augmented generation techniques. To address the challenges of manually constructing datasets, the study employed chain-of-thought prompting techniques combined with the GLM-4 large language model to generate high-quality question-and-answer datasets from raw data. To improve the accuracy of the model in text generation tasks in the aerospace field, the LoRA fine-tuning technique was applied to fine-tune and compare the ChatGLM3-6B and ChatGLM2-6B large language models. To resolve the “hallucination” issue that may occur in complex dialogues after model fine-tuning, retrieval-augmented generation techniques were applied to establish a dedicated local knowledge base and optimize retrieval strategies, ultimately increasing the accuracy of the question-and-answer model to 89\%. Experimental results indicate that the proposed construction scheme significantly enhances the accuracy and practicality of the aerospace domain question-and-answer model. Through automated dataset construction, domain-specific model fine-tuning, and retrieval augmentation techniques, an efficient and accurate aerospace information dialogue model has been successfully built.},
	booktitle = {2025 {IEEE} 6th {International} {Seminar} on {Artificial} {Intelligence}, {Networking} and {Information} {Technology} ({AINIT})},
	author = {Liu, Bo and Chen, Fei and Wen, Zhonghua},
	month = apr,
	year = {2025},
	keywords = {Accuracy, Aerospace field, ChatGLM model, Context modeling, Data models, Fine tuning of large models, Large language models, Prompt project, question answering, Question answering (information retrieval), RAG technology, Real-time systems, Retrieval augmented generation, Seminars, Training, Tuning},
	pages = {1582--1586},
}

@inproceedings{abdullah_p4omp_2025,
	title = {{P4OMP}: {Retrieval}-{Augmented} {Prompting} for {OpenMP} {Parallelism} in {Serial} {Code}},
	doi = {10.1109/HPEC67600.2025.11196284},
	abstract = {We present P4OMP, a retrieval-augmented framework for transforming serial C/C++ code into OpenMP-annotated parallel code using large language models (LLMs). To our knowledge, this is the first system to apply retrieval-based prompting for OpenMP pragma correctness without model fine-tuning or compiler instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with structured instructional knowledge from OpenMP tutorials to improve the reliability of prompt-driven code generation. By grounding generation in the retrieved context, P4OMP improves syntactic correctness compared to baseline prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline, GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites. P4OMP achieves 100\% compilation success on all parallelizable cases, while the baseline fails to compile in 20 out of 108 cases. Six cases that rely on non-random-access iterators or thread-unsafe constructs are excluded due to fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP consistently avoids scoping errors, syntactic misuse, and invalid directive combinations that commonly affect baseline-generated code. We further demonstrate strong runtime scaling across seven compute-intensive benchmarks on an HPC cluster. P4OMP offers a robust, modular pipeline that significantly improves the reliability and applicability of LLM-generated OpenMP code.},
	booktitle = {2025 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Abdullah, Wali Mohammad and Kabir, Azmain},
	month = sep,
	year = {2025},
	note = {ISSN: 2643-1971},
	keywords = {Benchmark testing, C++ languages, Code Parallelization, Codes, Grounding, HPC, Large language models, LLM, OpenMP, Pipelines, RAG, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Syntactics, Tutorials},
	pages = {1--6},
}

@inproceedings{lovtsov_automated_2025,
	title = {Automated {Mobile} {Operator} {Customer} {Service} {Using} {Large} {Language} {Models} {Combined} with {RAG} {System}},
	doi = {10.1109/REEPE63962.2025.10971107},
	abstract = {Large Language Models (LLMs) have made a real breakthrough in the field of artificial intelligence and have been rapidly integrated into our daily lives, including the telecom domain. The use of Retrieval Augmented Generation (RAG) reduces the likelihood of generating incorrect or outdated data in LLMs and improves the understanding of the query context and the generation of more relevant answers. The aim of this research is to improve the quality of automated customer service for mobile operator customers by using RAG system in combination with different language models. The paper makes a comprehensive analysis of open-source LLMs, the main methods of model adaptation and pre-training. Conclusions are drawn on the applicability of the analyses in the study. The architecture of the RAG system and the deployment diagram are designed. The main stages of system training are described, system results and performance evaluation for different LLMs are given. Major conclusions are drawn on the achievement of the research objective and further development.},
	booktitle = {2025 7th {International} {Youth} {Conference} on {Radio} {Electronics}, {Electrical} and {Power} {Engineering} ({REEPE})},
	author = {Lovtsov, Vladimir A. and Skvortsova, Maria A.},
	month = apr,
	year = {2025},
	note = {ISSN: 2831-7262},
	keywords = {Accuracy, Adaptation models, Biological system modeling, Customer services, language model adaptation, Large language models, LLM, machine learning, mobile operator, RAG, Retrieval augmented generation, Security, Telecommunications, telecoms, Training, Training data},
	pages = {1--6},
}

@inproceedings{haldar_rubra_2025,
	title = {{RUBRA}: {An} {Agentic} {AI} {System} for {Automatic} {Short} {Answer} {Grading} {Using} {LLMs} and {RAG}},
	doi = {10.1109/CIACON65473.2025.11189329},
	abstract = {The proliferation of deep Learning applications in natural language processing has facilitated automated evaluation of short-answer questions, providing more transparent, interpretable and unbiased evaluation of students’ responses. In this work, we introduce RUBRA, an agentic AI system for Automatic Short Answer Grading (ASAG) using Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques. The system incorporates domain-specific context information retrieved from textbooks and study materials through a RAG framework and a dynamically generated rubric from reference answers provided by human tutors. Then, by aligning student responses with the rubric via semantic similarity scoring, the system computes an explainable and fair grade and provides transparent feedback to the learner. We discuss the architecture, components, and potential effectiveness of the proposed agentic AI system and tested our results over the SCIENTSBANK dataset. It is observed that the proposed approach not only outperformed the existing approaches in prediction accuracy but also significantly improved the autonomy and adaptability of the system.},
	booktitle = {2025 {International} {Conference} on {Computing}, {Intelligence}, and {Application} ({CIACON})},
	author = {Haldar, Subhajit and Sengupta, Souvik and Das, Asit Kumar},
	month = jul,
	year = {2025},
	keywords = {Accuracy, Agentic AI, ASAG, Computer architecture, Context modeling, Deep learning, Intelligent systems, Large language models, LLM, Mathematical models, Natural language processing, NLP, RAG, Retrieval augmented generation, Semantics},
	pages = {1--6},
}

@inproceedings{abtahi_augmenting_2025,
	title = {Augmenting {Large} {Language} {Models} with {Static} {Code} {Analysis} for {Automated} {Code} {Quality} {Improvements}},
	doi = {10.1109/Forge66646.2025.00017},
	abstract = {This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI’s GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations—where the model generates plausible but incorrect outputs—is addressed by a custom-built "Code Comparison App," which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure.},
	booktitle = {2025 {IEEE}/{ACM} {Second} {International} {Conference} on {AI} {Foundation} {Models} and {Software} {Engineering} ({Forge})},
	author = {Abtahi, Seyed Moein and Azim, Akramul},
	month = apr,
	year = {2025},
	keywords = {Automation, Code Comparison, Codes, Costs, GPT-3.5 Turbo, GPT-4o, Issue Detection, Large language models, LLMs, Prompt engineering, Prompt Engineering, RAG, Real-time systems, Retrieval augmented generation, Software Development Automation, Software development management, Software quality, Static analysis, Static Code Analysis},
	pages = {82--92},
}

@inproceedings{jiang_food_2025,
	title = {Food {Safety} {News} {Summarization} {Paradigm} {Based} {On} {Large} {Language} {Models} {And} {RAG}},
	doi = {10.1109/DDCLS66240.2025.11065829},
	abstract = {The issue of food safety is becoming increasingly severe, and generating accurate and understandable news summaries is crucial for raising public awareness and supporting decision-making. However, current summarization methods lack effective integration of domain-specific knowledge and often suffer from issues such as factual errors, omission of key information, or misinterpretations, failing to meet the demand for high-quality news summaries in the food safety domain. This paper proposes a food safety news summarization generation paradigm based on the combination of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). First, the model extracts the "5W1H" (Who, What, When, Where, Why, How) from the news, providing key information for subsequent classification and knowledge retrieval. Then, based on the extracted "5W1H", the model classifies the article and narrows the knowledge retrieval scope, improving retrieval accuracy and ensuring the acquisition of domain-relevant knowledge. Finally, combining the original text, extracted "5W1H", and retrieved knowledge, the model generates the final summary, ensuring the summary accurately reflects the core content of the news and integrates domain knowledge. This paper constructs a knowledge base containing 4500 items of food safety knowledge and applies it to a self-built news data set from domestic authoritative food safety websites. Experimental results show that the proposed method has significant advantages in food safety news summary generation, especially in terms of relevance and information accuracy, performing better than traditional methods. Compared with the reference summary, the BLEU-4 is improved by 2.63\% and ROUGE-L is improved by 1.67\%. This approach is not only applicable to the food safety field but also has potential applications in automatic summary generation for other vertical domains.},
	booktitle = {2025 {IEEE} 14th {Data} {Driven} {Control} and {Learning} {Systems} ({DDCLS})},
	author = {Jiang, Zhiying and Zhang, Zeyu and Ji, Gang and Li, Fang},
	month = may,
	year = {2025},
	note = {ISSN: 2767-9861},
	keywords = {Accuracy, Food safety, Knowledge based systems, Knowledge engineering, Large Language Model, Large language models, Learning systems, Medical services, News Summarization, Prompt engineering, Prompt Engineering, Retrieval augmented generation, Retrieval Augmented Generation, Semantics},
	pages = {1681--1686},
}

@inproceedings{xu_research_2024,
	title = {Research on the {Construction} of {Knowledge} {QA} {System} {Driven} by {Large} {Language} {Model}: {One} {Case} {Study} of {Power} {Transformer} {Domain}},
	doi = {10.1109/DSIT61374.2024.10881483},
	abstract = {The QA system can provide various professionals with fast and accurate knowledge support, significantly improving work efficiency. The introduction of LLMs has further enhanced the accuracy and efficiency of QA systems. A QA system based on LLMs for power transformers can effectively improve the efficiency of fault diagnosis and the resolution of technical issues in the power transformer domain. This study first proposes an automatic dataset construction method for QA based on LLMs, through which a QA dataset of basic knowledge about power transformers is obtained. This dataset then performs low-rank adaptive fine-tuning on the LLM. Subsequently, an external knowledge base covering fundamental knowledge and fault cases of power transformers is built. Finally, with the aid of prompt texts, knowledge responses were generated by RAG in combination with the fine-tuned LLM. Experimental results show that compared with the QA systems driven by general LLMs, the method proposed in this paper generates more concise and professional responses, thus promoting, to some extent, the application of LLMs in the power domain.},
	booktitle = {2024 7th {International} {Conference} on {Data} {Science} and {Information} {Technology} ({DSIT})},
	author = {Xu, Hao and Kang, Zhenyuan and Zhang, Yan and Jin, Zhenqiang and Wang, Mulan and Ge, Linlin and Lu, Haihua},
	month = dec,
	year = {2024},
	keywords = {Accuracy, Adaptation models, Data models, Data science, Fault diagnosis, Information technology, Knowledge based systems, Large Language Model, Large language models, Low-Rank Adaptation Fine-Tuning, Power Transformer, Power transformers, Question-Answering System, Retrieval-Augmented Generation},
	pages = {1--8},
}

@inproceedings{guo_streamlining_2025,
	title = {Streamlining {Research} {Complexities} for {AI} {Agents}: {Charting} {Pathways} to {Innovative} {Idea} {Generation}},
	doi = {10.1109/ACDSA65407.2025.11166476},
	abstract = {The emergence of generative artificial intelligence in language processing has made it possible to automate complex human tasks, such as research. By breaking down these intricate tasks, large language models (LLMs) can gain a deeper understanding of how to tackle problems effectively. Emphasizing hierarchical information and contrasting perspectives is crucial for enhancing LLMs’ contextual awareness, leading to more informed and accurate outcomes. We propose an innovative approach to help LLMs generate novel research ideas and assess their quality across various dimensions. Our case studies and experiments show that simplifying research narratives enables LLMs to generate more concrete, high-quality ideas. We conduct extensive automated experiments on the generated novel ideas, evaluating them against various metrics and demonstrating their effectiveness based on different qualitative factors.},
	booktitle = {2025 {International} {Conference} on {Artificial} {Intelligence}, {Computer}, {Data} {Sciences} and {Applications} ({ACDSA})},
	author = {Guo, Jinghui and Akbar, Khandakar Ashrafi and Khan, Latifur and Thuraisingham, Bhavani and Irtiza, Saquib},
	month = aug,
	year = {2025},
	keywords = {Accuracy, Complexity theory, Generative AI, In-Context Learning, Large language models, Learning (artificial intelligence), LLMs, Measurement, Prompt-Engineering, Retrieval augmented generation, Retrieval-Augmented Generation},
	pages = {1--10},
}

@inproceedings{nazar_nextg-gpt_2025,
	title = {{NextG}-{GPT}: {Leveraging} {GenAI} for {Advancing} {Wireless} {Networks} and {Communication} {Research}},
	doi = {10.1109/ICCCN65249.2025.11133874},
	abstract = {Artificial intelligence (AI) and wireless networking advancements have created new opportunities to enhance network efficiency and performance. In this paper, we introduce Next-Generation GPT (NextG-GPT), an innovative framework that integrates retrieval-augmented generation (RAG) and large language models (LLMs) within the wireless systems’ domain. By leveraging state-of-the-art LLMs alongside a domain-specific knowledge base, NextG-GPT provides context-aware real-time support for researchers, optimizing wireless network operations. Through a comprehensive evaluation of LLMs—including Mistral-7B, Mixtral-8×7B, LLaMa3.1-8B, and LLaMa3.1-70B—we demonstrate significant improvements in answer relevance, contextual accuracy, and overall correctness. In particular, LLaMa3.1-70B achieves a correctness score of 86.2\% and an answer relevancy rating of 90.6\%. By incorporating diverse datasets such as ORAN-13K-Bench, TeleQnA, TSpecLLM, and Spec5G, we improve NextG-GPT’s knowledge base, generating precise and contextually aligned responses. This work establishes a new benchmark in AI-driven support for next-generation wireless network research, paving the way for future innovations in intelligent communication systems.},
	booktitle = {2025 34th {International} {Conference} on {Computer} {Communications} and {Networks} ({ICCCN})},
	author = {Nazar, Ahmad M. and Selim, Mohamed Y. and Qiao, Daji and Zhang, Hongwei},
	month = aug,
	year = {2025},
	note = {ISSN: 2637-9430},
	keywords = {ARA, Benchmark testing, Generative AI, GPT, Knowledge based systems, Large language models, LLM, Next generation networking, RAG, Real-time systems, Retrieval augmented generation, Technological innovation, Telecommunications, Wireless networks},
	pages = {1--9},
}

@inproceedings{medeiros_tailoring_2025,
	title = {Tailoring {RAG} {Strategies} for {Industrial} {Protocols}: {A} {Comparative} {Study} on {PROFIBUS} {Document} {Retrieval} using {Gemma} and {GPT} {Models}},
	doi = {10.1109/ETFA65518.2025.11205618},
	abstract = {The advancement of Industry 4.0 has intensified the demand for intelligent systems that can efficiently access and interpret technical information critical to industrial operations. However, recovering knowledge from extensive and complex technical documentation remains a significant challenge. This study examines the effectiveness of various Retrieval-Augmented Generation (RAG) strategies, combined with different Large Language Models (LLMs), in extracting and generating answers from industrial technical documents. A case study was conducted based on PROFIBUS, a widely adopted digital communication protocol in automation networks, with technical documents organized into categories for engineers and developers. Twenty questions of varying complexity were formulated, and responses were generated using three RAG strategies (Basic, Decomposition, and HyDE) combined with two LLMs (Gemma 3 and GPT-4o-mini). The outputs were compared against reference answers generated by the NotebookLM system and evaluated using automatic metrics, including ROUGE, METEOR, BERTScore, and MATTR. The results indicate that the Decomposition and HyDE strategies achieved superior semantic similarity scores when combined with more capable models. That model’s performance varied depending on the complexity of the document profile. These findings highlight the importance of tailored RAG strategies in enhancing intelligent information retrieval in industrial domains, which supports safer and more efficient operational environments.},
	booktitle = {2025 {IEEE} 30th {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Medeiros, Thaís and Medeiros, Morsinaldo and Andrade, Matheus and Silva, Marianne and Silva, Ivanovitch and Gaffurini, Massimiliano and Brandão, Dennis and Ferrari, Paolo},
	month = sep,
	year = {2025},
	note = {ISSN: 1946-0759},
	keywords = {Complexity theory, Fourth Industrial Revolution, Industrial Automation, Industry 4.0, Large language models, Large Language Models (LLM), Measurement, Meteors, PROFIBUS, Protocols, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Technical Document Retrieval, Testing},
	pages = {1--8},
}

@inproceedings{parthasarathy_engineering_2025,
	title = {Engineering {LLM} {Powered} {Multi}-{Agent} {Framework} for {Autonomous} {CloudOps}},
	doi = {10.1109/CAIN66642.2025.00031},
	abstract = {Cloud Operations (CloudOps) is a rapidly growing field focused on the automated management and optimization of cloud infrastructure which is essential for organizations nav-igating increasingly complex cloud environments. MontyCloud Inc. is one of the major companies in the CloudOps domain that leverages autonomous bots to manage cloud compliance, security, and continuous operations. To make the platform more accessible and effective to the customers, we leveraged the use of GenAl. Developing a GenAl-based solution for autonomous CloudOps for the existing MontyCloud system presented us with various challenges such as i) diverse data sources; ii) orchestration of multiple processes and iii) handling complex workflows to automate routine tasks. To this end, we developed MOYA, a multi-agent framework that leverages GenAI and balances autonomy with the necessary human control. This framework integrates various internal and external systems and is optimized for factors like task orchestration, security, and error mitigation while producing accurate, reliable, and relevant insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our multi-agent system with the help of practitioners as well as using automated checks demonstrate enhanced accuracy, responsiveness, and effectiveness over non-agentic approaches across complex workflows.},
	booktitle = {2025 {IEEE}/{ACM} 4th {International} {Conference} on {AI} {Engineering} – {Software} {Engineering} for {AI} ({CAIN})},
	author = {Parthasarathy, Kannan and Vaidhyanathan, Karthik and Dhar, Rudra and Krishnamachari, Venkat and Kakran, Adyansh and Akshathala, Sreemaee and Arun, Shrikara and Karan, Amey and Muhammed, Basil and Dubey, Sumant and Veerubhotla, Mohan},
	month = apr,
	year = {2025},
	keywords = {Accuracy, AI Engineering, Autonomous CloudOps, Generative AI, LLM, Multi-Agent Framework, Multi-agent systems, Optimization, Prevention and mitigation, Retrieval augmented generation, Security, Soft sensors, Software architecture, Software Architecture, Software reliability},
	pages = {201--211},
}

@inproceedings{singireddi_nnrag_2025,
	title = {{nnRAG} – {Incorporating} {Human}-{Feedback} for {Neural} {Network} {Driven} {Similarity} {Search}: {A} {Preliminary} {Feasibility} {Study}},
	doi = {10.1109/ICHI64645.2025.00036},
	abstract = {Medical data presents unique challenges for large language models (LLMs), especially because any inaccurate or incomplete information may pose a risk of serious harm to the patient. While Retrieval-Augmented Generation (RAG) architectures offer promising solutions for improving the context and accuracy of LLM-generated responses, it is crucial that the retrievals are highly precise. Due to the potential consequences of errors, Human-in-the-Loop (HITL) processes are essential to ensure reliability and relevance in medical data retrieval. In this work, we propose a first-of-its-kind approach for retrieval workflows that incorporates Human Feedback for a Neural Network-Driven Similarity Search within the RAG architecture (nnRAG). This novel approach stores and integrates user feedback-based reward scores directly into the retrieval process to make it adaptive and refine the accuracy of context selection. We conducted a preliminary study to verify the functionality of this approach, initially evaluating it on a non-medical dataset and subsequently testing its feasibility with a medical dataset. The study demonstrated the ability of the nnRAG system to dynamically update its selection from non- or less-relevant sections to highly precise sections, post-training, on both non-medical and medical datasets. The system was able to perform this precise retrieval, even when the top-k was kept to 1, after a single iteration of training. Concluding, this approach offers a unique solution by introducing dynamism into the retrieval workflows, leveraging user feedback and potentially ensuring improved relevance and utility in the context of diverse and evolving user needs.},
	booktitle = {2025 {IEEE} 13th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Singireddi, Siva and Pandey, Sarthak and Pardasani, Rohit and V, Raj Kiran and Awasthi, Navchetan},
	month = jun,
	year = {2025},
	note = {ISSN: 2575-2634},
	keywords = {Accuracy, Adaptive retrieval, Artificial neural networks, Human in the loop, Human-in-the-loop, Informatics, Large language model, Large language models, Medical data, Medical services, Neural Network, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Testing, Training},
	pages = {243--250},
}

@inproceedings{gopalan_leveraging_2024,
	title = {Leveraging {LLMs} for {Context}-{Aware} {Process} {Mining} and {Analysis} on {Multi}-{Source} {Data}},
	doi = {10.1109/CICT64037.2024.10899487},
	abstract = {Organizations are fundamentally based on business processes that allow the streamlined functioning and achievement of goals. These business processes depend on various IT systems and tools that manage their performance. While techniques like process mining enable the analysis of these processes, the presence of multiple sources of contextually linked data makes it difficult to derive valuable insight with ease. This research aimed to establish contextual connections between multiple sources of data and perform information retrieval and analytical tasks using LLMs with a focus on process mining. We introduce a novel approach that utilizes a custom-built Python-based Retrieval Augmented Generation (RAG) pipeline that has been designed to overcome the limitations of existing LLM-based approaches, mainly focusing on handling multiple data sources and process-oriented queries. We have incorporated a unique table and row retrieval mechanism, providing the LLM with rich context and enabling it to reason about data from a process-aware perspective. In conclusion, this paper proves the ability to contextually link process logs with additional process-related data from varied sources to enable LLMs in providing relevant process insights which can be used to plan, monitor, and improve operations all through natural language querying.},
	booktitle = {2024 {IEEE} 8th {International} {Conference} on {Information} and {Communication} {Technology} ({CICT})},
	author = {Gopalan, Sathyanarayanan and V, Vijay and Kalyanaraman, Sreenidhi and Raj, Daniel Joseph and V, Sneha and Thomas, Anto N},
	month = dec,
	year = {2024},
	keywords = {Information and communication technology, Information retrieval, large language models, Linked data, Monitoring, multiple data sources, Natural languages, Organizations, Pipelines, process mining, Process mining, retrieval augmented generation, Retrieval augmented generation, Soft sensors},
	pages = {1--6},
}

@inproceedings{gozukara_rag_2025,
	title = {{RAG} {Based} {Interactive} {Chatbot} for {Video} {Streaming} {Services}},
	doi = {10.1109/UBMK67458.2025.11206833},
	abstract = {The proliferation of content within video streaming services presents a significant challenge for users seeking personalized recommendations and specific information. This research addresses this challenge by developing a Retrieval-Augmented Generation (RAG) chatbotn designed to enhance user experience through conversational AI. The primary contribution of this work is a novel Retrieval-Augmented Generation (RAG) architecture featuring a dual-retrieval system that combines semantic search for descriptive requests and structured queries for fact based inquiries. This approach grounds the Large Language Model (LLM) in a factual knowledge base, mitigating the risk of hallucinations. The system is engineered to handle empty data retrieval scenarios by dynamically relaxing search filters, ensuring a robust user experience. The effectiveness of this RAG approach was validated through a comprehensive set of automated evaluations. The system demonstrates high precision in ranked list retrieval with questions like "Recommend me the top 5 action movies with highest IMDb scores", achieving an average NDCG@k of 0.837. While the chatbot shows strong semantic understanding by achieving 91\% accuracy with contextual clues such as "Which Batman movies are directed by Christopher Nolan?", its performance with more ambiguous, plot-only queries (59.5\% accuracy) indicates clear opportunities for future refinement. These results confirm that the dual-tool architecture successfully combines the flexibility of semantic search with the precision of structured queries, paving the way for more intuitive and efficient content discovery on streaming platforms.},
	booktitle = {2025 10th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Gözükara, Hamza and Patel, Jay and Kara, Erkan and Yıldız, Ayşenur and Köseoğlu, Ozan and Makaroğlu, Didem and Drias, Yassine and Özlem, Şirin and Çakar, Tuna},
	month = sep,
	year = {2025},
	note = {ISSN: 2521-1641},
	keywords = {Accuracy, Chatbots, Computer architecture, embeddings, Filters, interactive assistant, Knowledge based systems, large language models, Large language models, Motion pictures, movie recommendation, retrieval augmented generation, Retrieval augmented generation, Semantic search, User experience},
	pages = {1350--1355},
}

@inproceedings{jeong_lightweight_2025,
	title = {Lightweight {Relevance} {Grader} in {RAG}},
	doi = {10.1109/ICICT64582.2025.00037},
	abstract = {Retrieval-augmented generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents is a challenge. To address this, a secondary model, known as a relevant grader, is used to verify document relevance. To reduce computational requirements, a lightweight small language model can be used as a relevant grader. This work aims to improve the capability of such a model, achieving a significant increase in precision from 0.1038 to 0.7750 using llama-3.2-1b, outperforming llama-3.1-70b and gpt4o-mini. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.},
	booktitle = {2025 8th {International} {Conference} on {Information} and {Computer} {Technologies} ({ICICT})},
	author = {Jeong, Taehee},
	month = mar,
	year = {2025},
	note = {ISSN: 2769-4542},
	keywords = {Accuracy, Computational modeling, Databases, Fine-tuning, Information retrieval, Large language models, Media, Memory management, Pipelines, Relevance grader, Retrieval augmented generation, Retrieval-augmented generation, Vector database, Vector search, Vectors},
	pages = {198--203},
}

@inproceedings{boronat_mdre-llm_2025,
	title = {{MDRE}-{LLM}: {A} {Tool} for {Analyzing} and {Applying} {LLMs} in {Software} {Reverse} {Engineering}},
	doi = {10.1109/SANER64311.2025.00090},
	abstract = {Understanding and maintaining software systems often requires extracting high-level abstractions, such as domain models, from source code. MDRE-LLM addresses this challenge by integrating Large Language Models (LLMs) with traditional Model-Driven Reverse Engineering (MDRE) techniques, offering an innovative approach to automate and enhance domain model recovery. The tool supports flexible granularity strategies and validates LLM -generated models against deterministic baselines. MDRE-LLM addresses diverse use cases, including analyzing legacy systems with minimal documentation, rapidly compre-hending large-scale codebases, and validating LLM performance in reverse engineering tasks. These capabilities have the potential to improve software analysis and refactoring while advance AI-driven research and education by fostering systematic experimentation and collaboration. The tool and a webcast are available at https://zenodo.org/uploads/14072106.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Boronat, Artur and Mustafa, Jawad},
	month = mar,
	year = {2025},
	note = {ISSN: 2640-7574},
	keywords = {Aging, Analytical models, Collaboration, Documentation, domain model recovery, large language models, Large language models, RAG, Reverse engineering, Software systems, Source coding, Systematics},
	pages = {850--854},
}

@inproceedings{wang_mainstream_2025,
	title = {Mainstream {AI} {Technology} {Analysis} for {Vertical} {Domain} {Digital} {Consultants}},
	doi = {10.1109/CCDC65474.2025.11090774},
	abstract = {In the process of digital transformation, enterprises and institutions require intelligent resources that can be flexibly adjusted in accordance with business development. AI Agents are well-suited to meet the diverse needs of current and future businesses. With the assistance of AI Agents, enterprises can eliminate repetitive task execution, reduce human error, alleviate process bottlenecks, and prevent personnel overload. This paper, based on the author's ongoing research projects and a substantial body of relevant information, analyzes mainstream artificial intelligence technologies required for building digital consultants in the vertical field of the industry. The content encompasses Knowledge Graph (KG), Large Language Model (LLM), Retrieval-Augmented Generation (RAG), classification and characteristics of AI Agents, and selection principles for constructing digital consultants in the industry vertical field. It is anticipated that this paper will contribute to time and cost savings for similar vertical AI applications.},
	booktitle = {2025 37th {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Wang, Jiajun and Wang, Jing},
	month = may,
	year = {2025},
	note = {ISSN: 1948-9447},
	keywords = {AI Agent, Artificial intelligence, Artificial Intelligence, Buildings, Business, Costs, Digital transformation, Industries, Knowledge Graph (KG), Knowledge graphs, Large Language Model (LLM), Large language models, Personnel, Retrieval augmented generation, Retrieval Enhancement Generation (RAG)},
	pages = {2261--2266},
}

@book{bahree_notitle_2024,
	isbn = {978-1-63343-694-7},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10745288},
	abstract = {Generative AI can transform your business by streamlining the process of creating text, images, and code. This book will show you how to get in on the action! Generative AI in Action is the comprehensive and concrete guide to generative AI you’ve been searching for. It introduces both AI’s fundamental principles and its practical applications in an enterprise context—from generating text and images for product catalogs and marketing campaigns, to technical reporting, and even writing software. Inside, author Amit Bahree shares his experience leading Generative AI projects at Microsoft for nearly a decade, starting well before the current GPT revolution. Inside Generative AI in Action you will find: A practical overview of of generative AI applications Architectural patterns, integration guidance, and best practices for generative AI The latest techniques like RAG, prompt engineering, and multi-modality The challenges and risks of generative AI like hallucinations and jailbreaks How to integrate generative AI into your business and IT strategy Generative AI in Action is full of real-world use cases for generative AI, showing you where and how to start integrating this powerful technology into your products and workflows. You’ll benefit from tried-and-tested implementation advice, as well as application architectures to deploy GenAI in production at enterprise scale.},
	publisher = {Manning},
	author = {Bahree, Amit},
	year = {2024},
	note = {Publication Title: Generative AI in Action},
	keywords = {architectural patterns, Bard, ChatGPT, Copilot, enterprise, ethics, hallucinations, integration, jailbreaks, LLMs, model fine tuning, multi-modality, prompt engineering, RAG, safety},
}

@inproceedings{liu_think_2025,
	title = {{THINK}: {Tackling} {API} {Hallucinations} in {LLMs} via {Injecting} {Knowledge}},
	doi = {10.1109/SANER64311.2025.00029},
	abstract = {Large language models (LLMs) have made significant strides in code generation but often struggle with API hallucination issues, especially for the third-party library. Existing approaches attempt to enhance LLMs by incorporating documentation. However, they face three main challenges: the introduction of irrelevant information that distracts the model; reliance solely on documentation that results in discrepancies between API descriptions and practical usage; and the absence of comprehensive error post-processing mechanisms. To address these challenges, we propose THINK11THINK's benchmark and code is available at https://github.com/Leah-Ljx/think., a knowledge injection method that leverages a custom API knowledge database with two phases: pre-execution enhancement and post-execution optimization. The former reduces irrelevant information and integrates multiple knowledge sources, while the latter identifies seven API error types and suggests three heuristic correction strategies. We manually construct a benchmark by collecting and filtering complex API-related tasks from GitHub to evaluate the effectiveness of our method. The experimental results demonstrate that our method can significantly improve the correctness of API usage in the context of LLMs. We reduce the error rate of programs from 61.18\% to 16.64\% for GPT-3.5 and from 41.49\% to 5.58\% for GPT-4o across tasks involving different libraries.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Liu, Jiaxin and Zhang, Yating and Wang, Deze and Li, Yiwei and Dong, Wei},
	month = mar,
	year = {2025},
	note = {ISSN: 2640-7574},
	keywords = {API Hallucination, Benchmark testing, Code Generation, Codes, Documentation, Error analysis, Large language models, Large Language Models, Libraries, Optimization, Prompt Engineering, Retrieval-Augmented Generation, Software, Software development management, Software reliability},
	pages = {229--240},
}

@inproceedings{shafi_personalized_2025,
	title = {Personalized {Mental} {Health} {Assistance} with {Large} {Language} {Models}},
	doi = {10.1109/COMPSAC65507.2025.00109},
	abstract = {Mental health challenges continue to rise globally, yet access to effective and personalized support remains insufficient. While Large Language Models (LLMs) have shown its promise in this area, most existing solutions lack personalization, pose privacy risks, and often generate unreliable or generic responses. In this study, we present a novel approach that enhances LLM-based mental health support through fine-tuning on a combination of public and synthetically generated mental health datasets. We further propose a dynamic prompt strategy that extracts relevant mental health entities from patient conversations, such as symptoms and emotions, and retrieves relevant information from diverse data sources. We leverage function calling with Retrieval-Augmented Generation (RAG) to produce context-aware, personalized responses. Empirical comparisons with existing models demonstrate that our approach achieves higher accuracy and generates responses that are better aligned with individual user needs.},
	booktitle = {2025 {IEEE} 49th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Shafi, Fozle Rabbi and Hossain, M. Anwar and Choudhury, Salimur},
	month = jul,
	year = {2025},
	note = {ISSN: 2836-3795},
	keywords = {Accuracy, artificial intelligence, Computational modeling, Data mining, Emotion recognition, fine-tuning, function calling, large language model, Large language models, Mental health, mental health assistance, Privacy, retrieval augmented generation, Retrieval augmented generation, Soft sensors},
	pages = {815--823},
}

@inproceedings{sonawane_implementation_2025,
	title = {Implementation of an {Interactive} {Query} {System} {Using} {Nomic} {Text} {Embed}, {DeepSeek} {R1} 1.{5B}, and {Cosine} {Similarity} rankers},
	doi = {10.1109/ICOCT64433.2025.11118878},
	abstract = {Our work in this research builds and demonstrates an interactive query system using Retrieval Augmented Generation (RAG) to generate accurate responses. For evaluation, the system uses Nomic Text Embed and ChromaDB, and is evaluated on the rag-mini-wikipedia dataset from Hugging Face. The cosine similarity re-ranking improves retrieval precision. Long context query (up to 128,000 tokens) can be handled by DeepSeek R1 1.5B and coherence and reasoning can be improved. Chain of Thought (CoT) and Chain of Notes prompt engineering techniques improve factual accuracy of this work, reduce hallucinations and enhance multi-hop reasoning. The evaluation follows LangSmith framework in terms of accuracy. Through this study we show that combining advanced embeddings, reranking, and LLMs improves the main performance of the query in AI driven knowledge retrieval. Our approach achieves 73\% accuracy, surpassing both the vanilla BERT model (59.68\% accuracy) and the BERT (SQuAD) model (72.36\% accuracy) reported in related work of march 2024 . These improvements reflect the effectiveness of combining advanced embeddings, re-ranking, and a locally hosted LLM for AI-driven knowledge retrieval. The evaluation methodology follows the LangSmith framework, highlighting how robust retrieval and carefully engineered prompts can boost system performance on complex QA tasks.},
	booktitle = {2025 {International} {Conference} on {Computing} {Technologies} ({ICOCT})},
	author = {Sonawane, Vedant and Sambare, Govinda B. and Ambala, Srinivas and Kadam, Ganesh},
	month = jun,
	year = {2025},
	keywords = {Accuracy, ChromaDB, Cognition, Custom RAG dataset, Deepseek r1 1.5B, Education, Engineering students, Faces, Graphics processing units, LangChain, LangSmith Framework, Large language models, Long Context Large Language Models (LLMs), Prompt engineering, Prompt Engineering, Query Decomposition, Retrieval augmented generation, Retrieval-Augmented Generation, System performance},
	pages = {1--6},
}

@inproceedings{guettala_building_2024,
	title = {Building {Advanced} {RAG} {Q}\&{A} with {Multiple} {Data} {Sources} {Using} {Langchain}: {A} {Multi}-{Search} {Agent} {RAG} {Application} in {Ubiquitous} {Learning}},
	doi = {10.1109/ICCDA64887.2024.10867361},
	abstract = {The integration of Large Language Models (LLMs) in Question-Answering (QA) systems has made significant progress, yet they often fail to generate precise answers for queries beyond their training data and hallucinating. To address this, our study develops an advanced Retrieval-Augmented Generation (RAG) pipeline method using the LangChain framework, featuring a decision-making agent that dynamically selects the most effective tools and data sources for accurate and contextually relevant responses. By incorporating multiple data sources and diverse tools, our system mitigates the limitations of traditional LLMs, enhancing their effectiveness in ubiquitous learning environments. Evaluation through various case studies shows significant improvements in accuracy, relevance, and contextual appropriateness. The multi-search agent RAG system efficiently retrieves and synthesizes information, supporting continuous and context-aware learning and enriching the user experience. This research advances AI-based educational technologies, delivering a powerful solution for information retrieval and synthesis, as well as laying the foundations for intelligent question-and-answer systems of the future.},
	booktitle = {2024 2nd {International} {Conference} on {Computing} and {Data} {Analytics} ({ICCDA})},
	author = {Guettala, Manel and Bourekkache, Samir and Kazar, Okba and Harous, Saad},
	month = nov,
	year = {2024},
	keywords = {Accuracy, context-aware Learning, Deep Learning, human-computer Interaction (HCI), Large language models, Natural language processing, Natural Language Processing (NLP), Pipelines, Retrieval augmented generation, Scalability, Security, Soft sensors, Training data, Transfer Learning, User experience},
	pages = {1--7},
}

@inproceedings{sandakelum_real-time_2025,
	title = {Real-{Time} {Knowledge} {Retrieval} for {Banking} {Chatbots}: {A} {RAG}-{Based} {Approach} to {Employee} {Assistance}},
	doi = {10.1109/SCSE65633.2025.11030999},
	abstract = {In the corporate sector, the experience of an employee is significant to the service they provide. Especially in sectors such as banking, where domain knowledge is crucial in providing efficient service in internal and external operations. However, in recent years within Sri Lanka, experienced professionals have started to migrate and change jobs due to the financial crisis. This affects organizations because they have to constantly work with new recruits. This research aims to provide a solution to this problem by finding a method to develop a chatbot using the RAG method that can provide assistance to new recruits with domain knowledge. The RAG is supposed to store knowledge, such as information about products, policies, and relevant domain knowledge as its context. Provides answers from the domain-specific knowledge rather than the general knowledge of an LLM. Furthermore, this research focuses on optimizing the RAG model in main aspects of the RAG such as query translation, retrieval, in order to provide more accurate and reliable outputs to the user. The results of the research are to build a chatbot that helps fresh recruits find knowledge to answer customer queries during their training period. In addition, the study highlights ways to improve the response of the RAG model to be accurate and relevant for the domain of the banking sector in Sri Lanka.},
	booktitle = {2025 {International} {Research} {Conference} on {Smart} {Computing} and {Systems} {Engineering} ({SCSE})},
	author = {Sandakelum, Dishan and Rajapakse, Chathura and Jayalath, Nirasha},
	month = apr,
	year = {2025},
	note = {ISSN: 2997-7363},
	keywords = {Accuracy, banking, Banking, chatbot, Chatbots, Customer services, knowledge management, Knowledge management, Modeling, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Training, Translation},
	pages = {1--6},
}

@inproceedings{okutan_leveraging_2024,
	title = {Leveraging {RAG}-{LLM} to {Translate} {C}++ to {Rust}},
	doi = {10.1109/ICAA64256.2024.00024},
	abstract = {Despite their similarities, translating C++ code into the Rust language is a challenging task on account of differences in syntax, ecosystem, philosophy, and idioms. Large Language Models (LLMs) using Retrieval Augmented Generation (RAG) are effective at solving challenging programming language tasks, including source code generation and program translation. We leveraged RAG LLMs for translating C++ code into Rust and created Cpp2Rust, an LLM-based C++-to-Rust transpiler grounding the OpenAI ChatGPT4 model with similar C++ and Rust code snippet pairs from LeetCode. Preliminary analysis results show that 84\% of the solutions produced by Cpp2Rust that had no compile errors were accepted as correct solutions on LeetCode.},
	booktitle = {2024 {International} {Conference} on {Assured} {Autonomy} ({ICAA})},
	author = {Okutan, Ahmet and Merten, Samuel and Michael, Christoph C. and Ryjikov, Ben},
	month = oct,
	year = {2024},
	keywords = {C++ languages, C++ to Rust, Codes, Computer languages, Ecosystems, Grounding, Large language models, Large Language Models, Philosophical considerations, Program processors, Source coding, Syntactics, Transpilation},
	pages = {102--105},
}

@inproceedings{cheung_hallucination_2025,
	title = {Hallucination {Detection} with {Small} {Language} {Models}},
	doi = {10.1109/ICDEW67478.2025.00033},
	abstract = {Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating “Yes” tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Cheung, Ming},
	month = may,
	year = {2025},
	note = {ISSN: 2473-3490},
	keywords = {Chatbots, Conferences, Context modeling, Data engineering, Data mining, Databases, Hallucination, Large language models, Large Language Models, Question answering (information retrieval), Reliability, Retrieval augmented generation},
	pages = {230--238},
}

@inproceedings{sharma_integrating_2025,
	title = {Integrating {Blockchain} with {LLM} for {Real}-{Time} {Insight} into {Transactions} in an {E}-{Auction} {System}},
	doi = {10.1109/SKIMA66621.2025.11155428},
	abstract = {Real-time data retrieval and analytics play a crucial role in digital systems, requiring efficient, secure, and automated mechanisms for extracting insights from structured and unstructured data. Traditional methods rely on centralized databases and SQL (Structure Query Language) queries, which may lack security, transparency, adaptability and automation. Blockchain (BC) technology introduces a decentralized and tamper-proof ledger that enhances data integrity, while large language models (LLMs) leverage natural language processing (NLP) and excel in contextual understanding to facilitate more intuitive and flexible querying mechanisms. Implementing these technologies brings promising solutions, but some challenges remain, particularly in retrieving contextual information and data analysis. Hence, this research paper explores the integration of BC with LLM in an e-auction system using three different integration alternatives: the Pandas Query Engine (PQE), Retrieval-Augmented Generation (RAG) and Custom Query Pipeline (CQP). We compare the accuracy, and the time elapsed on querying of these methods in extracting relevant information from online auction event logs. Among three integration alternatives, RAG provides better result in terms of accuracy and PQE provides quicker result in terms of response time. Our results provide guidance on selecting retrieval strategies for AI-driven blockchain analytics, with implications for bid monitoring, regulatory compliance, fraud detection and recommendations to users of e-auction systems about efficient bidding. Further, we have discussed about cost of using these techniques.},
	booktitle = {2025 {International} {Conference} on {Software}, {Knowledge}, {Information} {Management} \& {Applications} ({SKIMA})},
	author = {Sharma, Sujan and Koirala, Ravi and Matalonga, Santiago and Dahal, Keshav},
	month = jun,
	year = {2025},
	keywords = {Accuracy, auction, blockchain, Blockchains, Data mining, LLM, Natural language processing, Pipelines, PQE, RAG, Real-time systems, Retrieval augmented generation, Security, Structured Query Language, Time factors},
	pages = {1--6},
}

@inproceedings{lu_synergizing_2024,
	title = {Synergizing {Internal} and {External} {Knowledge}: {Prompt} {Engineering} for {Efficient} and {Effective} {Large} {Language} {Model} {Reasoning}},
	doi = {10.1109/SMC54092.2024.10831420},
	abstract = {Large language models (LLMs), such as ChatGPT, have demonstrated remarkable capability in question answering but face challenges when it comes to knowledge-based rea-soning, such as limited training data and hallucination. To address these challenges, integrating LLMs with knowledge graphs (KGs) has emerged as a promising solution. However, the cost associated with training and inference of LLMs is high. Our method integrates the Retrieval-Augmented Generation (RAG) paradigm, incorporating relevant information from KGs alongside the question to enhance LLMs' reasoning process without training. Moreover, we propose a novel concept of self-knowledge motivation to reduce the overhead of inference, which prompts LLMs to integrate retrieved information with their internal knowledge for reasoning before seeking additional queries to KGs. Experimental results showcase improvements in answer accuracy and a reduction in LLMs' API calls compared to the latest published state-of-the-art (SOTA) method employing an identical paradigm, underscoring the efficiency and effectiveness of our method.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Lu, Gewei and He, Chaofan and Shen, Liping},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Cognition, Knowledge based systems, Knowledge graphs, Large language models, Prompt engineering, Question answering (information retrieval), Retrieval augmented generation, Training, Training data},
	pages = {1217--1223},
}

@article{guo_enhancing_2025,
	title = {Enhancing {Code} {Transformation} in {Large} {Language} {Models} {Through} {Retrieval}-{Augmented} {Fine}-{Tuning}},
	volume = {71},
	issn = {1558-4127},
	doi = {10.1109/TCE.2025.3565294},
	abstract = {Large language models (LLMs) have made substantial advancements in knowledge reasoning and are increasingly utilized in specialized domains such as code completion, legal analysis, and medical transcription, where accuracy is paramount. In such applications, document-specific precision is more critical than general reasoning capabilities. This paper proposes a novel approach based on Retrieval-Augmented Fine-Tuning (RAFT) to enhance model-generated outputs, particularly in code transformation tasks. RAFT integrates domain-specific knowledge, optimizing in-domain retrieval-augmented generation by training the model to discern the relationship between prompts, retrieved documents, and target outputs. This enables the model to extract relevant information while mitigating the impact of noise. Experimental results demonstrate that the proposed method improves accuracy of 2.4\% and CodeBLEU of 1.3\% for VB-to-C\# code conversion, highlighting its effectiveness in domain-specific applications.},
	number = {1},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Guo, Jing-Ming and Liu, Po-Yang and Zeng, Yi-Chong and Chen, Ting-Ju},
	month = feb,
	year = {2025},
	keywords = {Accuracy, Adaptation models, C\# languages, Codes, Data mining, Deep learning, large language model, Large language models, natural language processing, Retrieval augmented generation, retrieval-augmented fine-tuning, Syntactics, Training, Tuning, vision transformer},
	pages = {2342--2346},
}

@inproceedings{pratap_harnessing_2025,
	title = {Harnessing {Large} {Language} {Models} for {Sustainable} {Materials} and {Manufacturing}: {Frameworks}, {Applications} \& {Innovations}},
	doi = {10.1109/IS3C65361.2025.11130957},
	abstract = {The integration of Large Language Models (LLMs) into materials and manufacturing offers a transformative approach to achieving ISO 9001 -compliant product quality, aligning with the goals of Industry 5.0. This work proposes MatManQ (Large Language Model in Material and Manufacturing for Product Quality and Control), a framework that leverages LLMs for quality control, assurance, and agentic AIdriven decision-making. A case study on alloy behavior using a self-curated dataset demonstrates the framework's capabilities. Five open-source LLMs-Mixtral-8x7B-327, TinyLlama-1.1B, deepset/roberta, Gemini, and FLAN-T5-were fine-tuned using a Retrieval-Augmented Generation (RAG) approach. Mixtral-8x7B-327 achieved the highest F1 score of {\textbackslash}mathbf92.1\%, attributed to its sparse Mixture-of-Experts architecture, enabling precise and efficient reasoning. The study highlights the potential of LLMs to unify material and manufacturing insights for enhanced product quality, while outlining key challenges and future directions for industrial deployment.},
	booktitle = {2025 {Seventh} {International} {Symposium} on {Computer}, {Consumer} and {Control} ({IS3C})},
	author = {Pratap, Ayush and Sharma, Nidhi and Sardana, Neha and Husing, Pao-Ann},
	month = jun,
	year = {2025},
	note = {ISSN: 2770-0496},
	keywords = {Agentic AI, Fifth Industrial Revolution, IC Fabrication, Industry 5.0, Informatics, Large language models, Large Language Models, Materials Informatics, Metals, Product design, Quality assessment, Quality control, Quality Control, Retrieval augmented generation, Smart manufacturing, Technological innovation, Trustworthy AI},
	pages = {1--4},
}

@inproceedings{zeng_not_2024,
	title = {Not {Just} {Imitation}: {Enhancing} {Role}-{Playing} {Through} {Retrieve} {Fine}-{Tuned} {Collaborative} {Large} {Language} {Model}},
	doi = {10.1109/SWC62898.2024.00194},
	abstract = {In this paper, we focus on enhancing the credibility and personalization of large language models in role-playing scenarios, aiming to facilitate natural and believable humancomputer interaction. Current dialogue systems often fall short in accurately portraying character styles, leading to issues of role illusion. To address this, we introduce an innovative method, the Retrieve Fine-Tuned Collaborative Large Language Model (RFTC-LLM), which employs a two-stage strategy combining fine-tuning with retrieval-augmented generation (RAG). By employing instruction-based fine-tuning, the model is directed to learn the character’s expression style and preferences, and the use of a role-specific knowledge graph aids in constraining responses to ensure relevance and avoid misleading answers. This approach effectively resolves the confusion surrounding role-specific knowledge, thereby boosting the credibility and authenticity of the dialogue system. Experimental results demonstrate the superiority of our method across multiple role-playing dialogue datasets, particularly in significantly reducing instances of role illusion.},
	booktitle = {2024 {IEEE} {Smart} {World} {Congress} ({SWC})},
	author = {Zeng, Yating and Guo, Bin and Jing, Yao and Wang, Hao and Ding, Yasan and Liang, Yunji and Yu, Zhiwen},
	month = dec,
	year = {2024},
	note = {ISSN: 2993-396X},
	keywords = {Boosting, Collaboration, Human computer interaction, human-computer interaction, Knowledge graphs, large language models, Large language models, Prevention and mitigation, Retrieval augmented generation, role-playing},
	pages = {1234--1241},
}

@inproceedings{garza_privcomp-kg_2024,
	title = {{PrivComp}-{KG}: {Leveraging} {KG} and {LLM} for {Compliance} {Verification}},
	doi = {10.1109/TPS-ISA62245.2024.00021},
	abstract = {Regulatory documents are complex and lengthy, making full compliance a challenging task for businesses. Similarly, privacy policies provided by vendors frequently fall short of the necessary legal standards due to insufficient detail. To address these issues, we propose a solution that leverages a Large Language Model (LLM) in combination with Semantic Web technology. This approach aims to clarify regulatory requirements and ensure that organizations’ privacy policies align with the relevant legal frameworks, ultimately simplifying the compliance process, reducing privacy risks, and improving efficiency. In this paper, we introduce a novel tool, the Privacy Policy Compliance Verification Knowledge Graph, referred to as PrivComp-KG. PrivComp-KG is designed to efficiently store and retrieve comprehensive information related to privacy policies, regulatory frameworks, and domain-specific legal knowledge. By utilizing LLM and Retrieval Augmented Generation (RAG), we can accurately identify relevant sections in privacy policies and map them to the corresponding regulatory rules. Our LLM-based retrieval system has demonstrated a high level of accuracy, achieving a correctness score of 0.9, outperforming other models in privacy policy analysis. The extracted information from individual privacy policies is then integrated into the PrivComp-KG. By combining this data with contextual domain knowledge and regulatory rules, PrivComp-KG can be queried to assess each vendor’s compliance with applicable regulations. We demonstrate the practical utility of PrivComp-KG by verifying the compliance of privacy policies across various organizations. This approach not only helps policy writers better understand legal requirements but also enables them to identify gaps in existing policies and update them in response to evolving regulations.},
	booktitle = {2024 {IEEE} 6th {International} {Conference} on {Trust}, {Privacy} and {Security} in {Intelligent} {Systems}, and {Applications} ({TPS}-{ISA})},
	author = {Garza, Leon and Elluri, Lavanya and Piplai, Aritran and Kotal, Anantaa and Gupta, Deepti and Joshi, Anupam},
	month = oct,
	year = {2024},
	keywords = {Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Law, Organizations, Policy Compliance, Privacy, Privacy Policy, Regulation, Retrieval augmented generation, Security, Semantic Web, Standards organizations},
	pages = {97--106},
}

@inproceedings{sahin_llm_2024,
	title = {{LLM} and {RAG}-{Based} {Question} {Answering} {Assistant} for {Enterprise} {Knowledge} {Management}},
	doi = {10.1109/UBMK63289.2024.10773564},
	abstract = {Large language models (LLM) have become integral to many natural language processing applications, particularly in the area of automatic question answering. In this study, a question answering system was developed to enable Adesso Turkiye employees to access internal company information quickly and accurately. A Retrieval Augmented Generation (RAG)-based question answering framework was constructed by utilizing multiple large language models and embedding techniques, along with content curated by experts in human resources and information security. The performance of the system was evaluated using ROUGE, BLEU, and accuracy metrics, and the results indicated high levels of success. Future work will focus on enhancing performance through the use of different language models, enriching the system with datasets from various domains, and integrating the developed system into MS Teams to ensure accessibility for employees.},
	booktitle = {2024 9th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Şahin, Gürkan and Varol, Karya and Pak, Burcu Kuleli},
	month = oct,
	year = {2024},
	note = {ISSN: 2521-1641},
	keywords = {Accuracy, Companies, Computer science, generative artificial intelligence, GPT, Information security, Knowledge management, large language models, Large language models, Measurement, natural language processing, question answering, Question answering (information retrieval), retrieval augmented generation (RAG), Usability, Vectors},
	pages = {1--6},
}

@inproceedings{agliata_generative_2024,
	title = {Generative {AI} and {Emotional} {Health}: {Innovations} with {Haystack}},
	doi = {10.1109/ISCC61673.2024.10733569},
	abstract = {Generative artificial intelligence (AI) is poised to revolutionize the healthcare sector by enhancing research methodologies, diagnostic procedures, and treatment protocols. This paper investigates the application of key generative AI technologies, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Pre-trained Generative Transformer models, with a particular focus on their implementation in medical imaging, drug discovery, and electronic health record management. Utilizing the Haystack framework, this study integrates these technologies to optimize data access and retrieval. The research addresses the primary challenges in healthcare AI adoption, such as data quality, model interpretability, ethical considerations, and regulatory compliance. By leveraging the Haystack framework, we identify future research opportunities, emphasizing the integration of multimodal data, the personalization of treatments, and the development of transparent AI systems. The study underscores the critical role of interdisciplinary collaboration between AI researchers and healthcare professionals in maximizing the benefits of these technologies, managing their complexities, and ensuring their successful integration into healthcare systems. Our findings demonstrate the potential of generative AI to significantly improve clinical decision-making and patient care, while also highlighting the importance of ethical guidelines and robust data security measures.},
	booktitle = {2024 {IEEE} {Symposium} on {Computers} and {Communications} ({ISCC})},
	author = {Agliata, Antonio and Pilato, Antonio and Mariacarmen, Sorrentino and Bottiglieri, Salvatore and Nardo, Emanuel Di and Ciaramella, Angelo},
	month = jun,
	year = {2024},
	note = {ISSN: 2642-7389},
	keywords = {Drug discovery, Electronic medical records, Emotional Well-Being, Ethics, Generative adversarial networks, Generative AI, Generative Pre-trained Transformer, Guidelines, Medical services, NLP, Protocols, Retrieval-Augmented Generation, Technological innovation, Transformers, User-Centered AI Design},
	pages = {1--4},
}

@inproceedings{tomkou_bridging_2025,
	title = {Bridging {Industrial} {Expertise} and {XR} with {LLM}-{Powered} {Conversational} {Agents}},
	doi = {10.1109/DCOSS-IoT65416.2025.00158},
	abstract = {This paper introduces a novel integration of Retrieval-Augmented Generation (RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR) technologies to address knowledge transfer challenges in industrial environments. The proposed system embeds domain-specific industrial knowledge into XR environments through a natural language interface, enabling hands-free, context-aware expert guidance for workers. We present the architecture of the proposed system consisting of an LLM Chat Engine with dynamic tool orchestration and an XR application featuring voice-driven interaction. Performance evaluation of various chunking strategies, embedding models, and vector databases reveals that semantic chunking, balanced embedding models, and efficient vector stores deliver optimal performance for industrial knowledge retrieval. The system's potential is demonstrated through early implementation in multiple industrial use cases, including robotic assembly, smart infrastructure maintenance, and aerospace component servicing. Results indicate potential for enhancing training efficiency, remote assistance capabilities, and operational guidance in alignment with Industry 5.0's human-centric and resilient approach to industrial development.},
	booktitle = {2025 21st {International} {Conference} on {Distributed} {Computing} in {Smart} {Systems} and the {Internet} of {Things} ({DCOSS}-{IoT})},
	author = {Tomkou, Despina and Fatouros, George and Andreou, Andreas and Makridis, Georgios and Liarokapis, Fotis and Dardanis, Dimitrios and Kiourtis, Athanasios and Soldatos, John and Kyriazis, Dimosthenis},
	month = jun,
	year = {2025},
	note = {ISSN: 2325-2944},
	keywords = {Conversational AI, eXtended Reality, Extended reality, Knowledge Management, Large language models, Large Language Models, Performance evaluation, Remote Assistance, Retrieval augmented generation, Retrieval-Augmented Generation, Robotic assembly, Semantics, Smart manufacturing, Smart Manufacturing, Smart systems, Training, Vectors},
	pages = {1050--1056},
}

@inproceedings{grislain_rag_2025,
	title = {{RAG} with {Differential} {Privacy}},
	doi = {10.1109/CAI64502.2025.00150},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to provide Large Language Models (LLM) with fresh and relevant context, mitigating the risk of hallucinations and improving the overall quality of responses in environments with large and fast moving knowledge bases. However, the integration of external documents into the generation process raises significant privacy concerns. Indeed, when added to a prompt, it is not possible to guarantee a response will not inadvertently expose confidential data, leading to potential breaches of privacy and ethical dilemmas. This paper explores a practical solution to this problem suitable to general knowledge extraction from personal data. It shows differentially private token generation is a viable approach to private RAG.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Grislain, Nicolas},
	month = may,
	year = {2025},
	keywords = {Artificial Intelligence, Differential privacy, Electronic medical records, Ethics, Intelligent agents, Knowledge based systems, Knowledge retrieval, Large language models, Privacy, Protection, Retrieval augmented generation, Security, Security and Privacy Protection},
	pages = {847--852},
}

@inproceedings{bhuva_interviewedge_2025,
	title = {{InterviewEdge}: {A} {Smart} {Interview} {Assistant}},
	doi = {10.1109/SCEECS64059.2025.10940590},
	abstract = {A structured, objective approach to non-coding technical interviews can enhance fairness and effectiveness in hiring, for which the latest AI technology can be applied instead of human evaluation. Towards this, this paper presents InterviewEdge: A Smart Interview Assistant, an AI-powered system designed to automate the evaluation of theoretical answers in programming languages. The proposed system employs Artificial Intelligence (AI) technologies such as the Retrieval Augmented Generation (RAG) and Large Language Model (LLM) for the retrieval of relevant content for technical assessment of the candidates’ responses. The proposed work is evaluated using criteria such as technical accuracy, response completeness, display of language-specific knowledge, and use of clarity \& structure. InterviewEdge provides for unbiased evaluation by ensuring consistent marking of the responses. Feedback to the candidates may help them understand how to improve specific areas of their answers.},
	booktitle = {2025 {IEEE} {International} {Students}' {Conference} on {Electrical}, {Electronics} and {Computer} {Science} ({SCEECS})},
	author = {Bhuva, Jaykumar R. and Parmar, Tanya V. and Prajapati, Harshadkumar B. and Dabhi, Vipul K.},
	month = jan,
	year = {2025},
	note = {ISSN: 2688-0288},
	keywords = {Accuracy, ChromDB, Computer languages, Error analysis, Human computer interaction, Interview Assessment, Interviews, Langchain, Large Language Model, Large language models, Measurement, Retrieval augmented generation, Retrieval Augmented Generation, System analysis and design, Technical management},
	pages = {1--6},
}

@inproceedings{gogineni_llm-powered_2025,
	title = {{LLM}-{Powered} {Multi}-{Agent} {Systems}: {A} {Technical} {Framework} for {Collaborative} {Intelligence} {Through} {Optimized} {Knowledge} {Retrieval} and {Communication}},
	doi = {10.1109/AIRC64931.2025.11077480},
	abstract = {This paper presents a comprehensive technical framework for constructing effective multi-agent systems powered by large language models (LLMs). We examine how multiple LLM-based agents can collaborate to solve complex problems beyond the capabilities of single agents through specialized knowledge integration and optimized communication protocols. Our novel architecture enables efficient collaboration between heterogeneous LLM agents, each with domain-specific capabilities and knowledge bases. Experimental results demonstrate that our multi-agent LLM system achieves 42\% higher accuracy on complex knowledge tasks, 37\% reduction in hallucinations, and 29\% faster convergence on collaborative problem-solving compared to baseline approaches. By implementing optimized knowledge retrieval mechanisms and structured communication patterns, our system reduces token usage by 45\% while maintaining semantic fidelity across agent interactions. These findings establish key design principles for building more effective and reliable collaborative LLM-based multi-agent systems for enterprise applications},
	booktitle = {2025 6th {International} {Conference} on {Artificial} {Intelligence}, {Robotics} and {Control} ({AIRC})},
	author = {Gogineni, Vineeth},
	month = may,
	year = {2025},
	keywords = {Accuracy, Collaboration, Convergence, Databases, Knowledge engineering, knowledge retrieval, Large language models, multi-agent systems, prompt engineering, Prompt engineering, retrieval-augmented generation, Robots, Semantics, vector databases, Vectors},
	pages = {452--456},
}

@inproceedings{yang_enhancing_2025,
	title = {Enhancing {LLM} {Question} {Answering} with {RAG} through {Dense} {Vector} {Search} and {Re}-{Ranking}},
	doi = {10.1109/AVSS65446.2025.11149916},
	abstract = {Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing Large Language Models (LLMs) by incorporating external knowledge through information retrieval (IR) techniques. However, in question-answering tasks, RAG often retrieves documents that are only semantically similar to the query, which may not provide the most relevant information for generating accurate responses. To address this limitation, we propose an improved retrieval pipeline that combines dense vector search with a re-ranking mechanism to more effectively identify and extract highly relevant knowledge from the retrieved content. We evaluated our approach on two Chinese datasets, TTQA and TMMLU+, using 17 different LLMs. Experimental results show that our method improves performance by up to 21.24\% over baseline approaches, particularly on two finance-related subsets, after incorporating domain-specific financial regulations to enhance the knowledge base used in the TMMLU+ dataset.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Advanced} {Visual} and {Signal}-{Based} {Systems} ({AVSS})},
	author = {Yang, Te-Lun and Liu, Jyi-Shane and Tseng, Yuen-Hsien and Jang, Jyh-Shing and Chang, Ming-Ching and Chen, Wei-Chao},
	month = aug,
	year = {2025},
	note = {ISSN: 2643-6213},
	keywords = {Accuracy, Knowledge based systems, Large language models, Pipelines, Question answering (information retrieval), Regulation, Retrieval augmented generation, Servers, Vectors, Visualization},
	pages = {1--6},
}

@inproceedings{ailapuram_development_2025,
	title = {Development of a {Medical} {Chatbot} {Using} {Generative} {AI} and {Retrieval}-{Augmented} {Techniques}},
	doi = {10.1109/FiCloud66139.2025.00068},
	abstract = {Quick and accurate access to medical information is crucial for patients and healthcare professionals alike. Traditional search engines often fail to concise and context-specific responses to medical queries. This paper introduces a medical chatbot that combines large language models (LLMs) with retrieval-augmented generation (RAG) techniques to generate reliable and contextually relevant answers. The system integrates the Mistral-7B-Instructv0.3 model hosted on Hugging Face and uses Pinecone, a vector database, for efficient semantic search. Built using the Flask web framework, the chatbot retrieves the most relevant medical texts, constructs dynamic prompts, and generates accurate, user-friendly responses. Experiments with common medical questions demonstrate the chatbot’s ability to deliver timely, medically sound answers, highlighting the advantages of combining retrieval mechanisms with generative models in healthcare applications.},
	booktitle = {2025 12th {International} {Conference} on {Future} {Internet} of {Things} and {Cloud} ({FiCloud})},
	author = {Ailapuram, Raveena and Sharif, Mhd Saeed and Elmedany, Wael},
	month = aug,
	year = {2025},
	note = {ISSN: 2996-1017},
	keywords = {Accuracy, Chatbots, Databases, Generative AI, Large language models, Medical Chatbot, Medical services, Mistral-7B-Instruct-v0.3, Natural Language Processing (NLP), Pinecone Vector Database, Reliability, Retrieval augmented generation, RetrievalAugmented Generation, Standards, Vectors},
	pages = {445--450},
}

@inproceedings{quddus_enhanced_2024,
	title = {Enhanced {VLSI} {Assertion} {Generation}: {Conforming} to {High}-{Level} {Specifications} and {Reducing} {LLM} {Hallucinations} with {RAG}},
	doi = {10.30420/566438009},
	abstract = {Assertion-based verification (ABV) is widely used in VLSI design. However, manual assertion writing is timeconsuming and may not adhere to high-level specifications. Generative AI techniques like LLMs automate this but can introduce hallucination. We propose an automatic assertion generation framework using Retrieval-Augmented Generation (RAG) and LLMs. It generates assertions from designer-tailored specifications, ensuring conformance with high-level specifications and reducing hallucinations. We applied this to an AXI4-Lite protocol case study, verifying SystemVerilog Assertions (SVAs) against golden RTL using Bounded Model Checking (BMC). Results showed improved accuracy, conformance, and integration with ABV.},
	booktitle = {{DVCon} {Europe} 2024; {Design} and {Verification} {Conference} and {Exhibition} {Europe}},
	author = {Quddus, Hafiz Abdul and Hossain, Md Sanowar and Cevahir, Ziya and Jesser, Alexander and Amin, Md Nur},
	month = oct,
	year = {2024},
	pages = {57--62},
}

@inproceedings{rasaq_large_2025,
	title = {Large {Language} {Models} {Maintenance} {Approach} for {Commercial}/{Fleets}},
	doi = {10.1109/RAMS48127.2025.10935195},
	abstract = {This paper presents a Large Language Model (LLM) framework to assess and map each failure mechanism and failure mode of individual Airworthiness Directives (ADs) available in the historical dataset. In addition, this framework can provide insights into related maintenance procedures and practices to help maintenance crews react faster and more effectively.},
	booktitle = {2025 {Annual} {Reliability} and {Maintainability} {Symposium} ({RAMS})},
	author = {Rasaq, Lukmon and Yadav, Om Prakash and Siddula, Madhuri and Walthall, Rhonda},
	month = jan,
	year = {2025},
	note = {ISSN: 2577-0993},
	keywords = {Accuracy, Airworthiness Directives, Atmospheric modeling, AviationQA, Boeing 767, Failure analysis, Finetuning, Large Language Model, Large language models, LoRA, Maintenance, Organizations, PEFT, Random access memory, Retrieval augmented generation, Retrieval Augmented Generation, Training, Tuning},
	pages = {1--6},
}

@inproceedings{miyaji_empowering_2025,
	title = {Empowering {Business} {Decisions} and {Knowledge} {Management} {Through} {Advanced} {RAG}-{Driven} {QA} {Systems}},
	doi = {10.1109/CAI64502.2025.00016},
	abstract = {In recent decades, the rapid evolution of Artificial Intelligence (AI) has revolutionized business processes, enabling significant improvements in knowledge management, analytics, and decision-making. This paper explores the application of Advanced Retrieval-Augmented Generation (RAG) techniques in corporate databases to enhance data exploration and decision support in business environments through the development of a Question Answering (QA) system. The QA system is designed to assist with technical problem-solving and business information management in a case study conducted within a consulting firm. To process the diverse resources, we employed OpenAl's Whisper for audio transcription, while we used Python's Un-structured library alongside Pytesseract and OpenCV to perform document parsing and Optical Character Recognition (OCR). We evaluated the QA system powered by OpenAl's GPT-4o-mini using a range of Query Augmentation techniques including Hyde, Query Rewrite, Rephrasal, and Step Back Prompting, as well as Reranking methods such as mT5, BM25, BGE m3, and RankGPT. To evaluate the system, we employed RAGAS framework to create a meaningful test set and to assess the performance, using Answer Relevancy and Faithfulness. The combination of Step Back Prompting and RankGPT achieved an improvement of 19.9\% in Answer Relevancy compared to Naive RAG, marking a significant advancement over traditional Large Language Models (LLMs) without RAG integration. This study highlights the potential of RAG-enhanced QA solutions for advancing knowledge management and decision support in business contexts.},
	booktitle = {2025 {IEEE} {Conference} on {Artificial} {Intelligence} ({CAI})},
	author = {Miyaji, Renato and Moulin, Renato and Monção, Samuel and Machado, Leonardo},
	month = may,
	year = {2025},
	keywords = {Business, Databases, Knowledge management, Knowledge Management, Optical character recognition, Pipelines, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation, System performance, Usability, Vectors},
	pages = {55--60},
}

@inproceedings{selva_kumar_overcoming_2024,
	title = {Overcoming {LLM} {Challenges} using {RAG}-{Driven} {Precision} in {Coffee} {Leaf} {Disease} {Remediation}},
	doi = {10.1109/ICETCS61022.2024.10543859},
	abstract = {This research introduces an innovative AI-driven precision agriculture system, leveraging YOLOv8 for disease identification and Retrieval Augmented Generation (RAG) for context-aware diagnosis. Focused on addressing the challenges of diseases affecting the coffee production sector in Karnataka, The system integrates sophisticated object detection techniques with language models to address the inherent constraints associated with Large Language Models (LLMs). Our methodology not only tackles the issue of hallucinations in LLMs, but also introduces dynamic disease identification and remediation strategies. Real-time monitoring, collaborative dataset expansion, and organizational involvement ensure the system’s adaptability in diverse agricultural settings. The effect of the suggested system extends beyond automation, aiming to secure food supplies, protect livelihoods, and promote eco-friendly farming practices. By facilitating precise disease identification, the system contributes to sustainable and environmentally conscious agriculture, reducing reliance on pesticides. Looking to the future, the project envisions continuous development in RAG-integrated object detection systems, emphasizing scalability, reliability, and usability. This research strives to be a beacon for positive change in agriculture, aligning with global efforts toward sustainable and technologically enhanced food production.},
	booktitle = {2024 {International} {Conference} on {Emerging} {Technologies} in {Computer} {Science} for {Interdisciplinary} {Applications} ({ICETCS})},
	author = {Selva Kumar, S and Khan, Afifah Khan Mohammed Ajmal and Banday, Imadh Ajaz and Gada, Manikantha and Shanbhag, Vibha Venkatesh},
	month = apr,
	year = {2024},
	keywords = {Government, GPT-3.5, LLM, NLP, Object detection, Object Detection, Pesticides, Precision Agriculture, Production, RAG, Real-time systems, Reliability engineering, Scalability, YOLOv8},
	pages = {1--6},
}

@inproceedings{yadav_aeroquery_2024,
	title = {{AeroQuery} {RAG} and {LLM} for {Aerospace} {Query} in {Designs}, {Development}, {Standards}, {Certifications}},
	doi = {10.1109/CONECCT62155.2024.10677028},
	abstract = {In the realm of avionics and aerospace, the demand for swift access to critical data is hindered by vast documentation, causing hallucinations, delays, and inefficiencies.To address this issue, our approach leverages the concepts of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).Our approach aims to overcome the limitations of LLMs by incorporating real-time data retrieval capabilities through RAG, enabling seamless access to current information. This envisioned chatbot utilizes advanced natural language processing and proactive pattern identification to streamline information retrieval and communication across various aerospace domains.By leveraging advancements in text summarization and utilizing models like Google’s PaLM2, Facebook’s LLaMA, or OpenAI’s GPT-4, we aim to enhance the performance of chatbots in information retrieval. This involves generating training examples and improving text summarization to efficiently address general inquiries related to standards and communication protocols within the aerospace sector but not limited to this.For instance, an aerospace engineer can quickly obtain relevant information on industry standards or communication protocols through the chatbot equipped with RAG, effortlessly taps into external sources to provide up-to-date and relevant information, reducing the need for exhaustive explanations and improving efficiency in information retrieval and communication processes.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Electronics}, {Computing} and {Communication} {Technologies} ({CONECCT})},
	author = {Yadav, Surendra},
	month = jul,
	year = {2024},
	note = {ISSN: 2766-2101},
	keywords = {Aerospace, Aerospace electronics, Chatbots, Generative AI, Industries, Industry-Specific AI, Information retrieval, Large language models, Large Language Models, Protocols, Retrieval-Augmented Generation, Text summarization},
	pages = {1--6},
}

@book{bustos_notitle_2024,
	isbn = {978-1-83588-761-5},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10769230},
	abstract = {Unleash the transformative potential of GenAI with this comprehensive guide that serves as an indispensable roadmap for integrating large language models into real-world applications. Gain invaluable insights into identifying compelling use cases, leveraging state-of-the-art models effectively, deploying these models into your applications at scale, and navigating ethical considerations.Key FeaturesGet familiar with the most important tools and concepts used in real scenarios to design GenAI appsInteract with GenAI models to tailor model behavior to minimize hallucinationsGet acquainted with a variety of strategies and an easy to follow 4 step frameworks for integrating GenAI into applicationsBook DescriptionExplore the transformative potential of GenAI in the application development lifecycle. Through concrete examples, you will go through the process of ideation and integration, understanding the tradeoffs and the decision points when integrating GenAI. With recent advances in models like Google Gemini, Anthropic Claude, DALL-E and GPT-4o, this timely resource will help you harness these technologies through proven design patterns. We then delve into the practical applications of GenAI, identifying common use cases and applying design patterns to address real-world challenges. From summarization and metadata extraction to intent classification and question answering, each chapter offers practical examples and blueprints for leveraging GenAI across diverse domains and tasks. You will learn how to fine-tune models for specific applications, progressing from basic prompting to sophisticated strategies such as retrieval augmented generation (RAG) and chain of thought. Additionally, we provide end-to-end guidance on operationalizing models, including data prep, training, deployment, and monitoring. We also focus on responsible and ethical development techniques for transparency, auditing, and governance as crucial design patterns.What you will learnConcepts of GenAI: pre-training, fine-tuning, prompt engineering, and RAGFramework for integrating AI: entry points, prompt pre-processing, inference, post-processing, and presentationPatterns for batch and real-time integrationCode samples for metadata extraction, summarization, intent classification, question-answering with RAG, and moreEthical use: bias mitigation, data privacy, and monitoringDeployment and hosting options for GenAI modelsWho this book is forThis book is not an introduction to AI/ML or Python. It offers practical guides for designing, building, and deploying GenAI applications in production. While all readers are welcome, those who benefit most include: Developer engineers with foundational tech knowledge Software architects seeking best practices and design patterns Professionals using ML for data science, research, etc., who want a deeper understanding of Generative AI Technical product managers with a software development background This concise focus ensures practical, actionable insights for experienced professionals},
	publisher = {Packt Publishing},
	author = {Bustos, Juan Pablo and Soria, Luis Lopez and Arsanjani, Dr. Ali},
	year = {2024},
	note = {Publication Title: Generative AI Application Integration Patterns: Integrate large language models into your applications},
}

@inproceedings{yu_textual_2024,
	title = {Textual {Differential} {Privacy} for {Context}-{Aware} {Reasoning} with {Large} {Language} {Model}},
	doi = {10.1109/COMPSAC61105.2024.00135},
	abstract = {Large language models (LLMs) have demonstrated proficiency in various language tasks but encounter difficulties in specific domain or scenario. These challenges are mitigated through prompt engineering techniques such as retrieval-augmented generation, which improves performance by integrating contextual information. However, concerns regarding the privacy implications of context-aware reasoning architectures persist, particularly regarding the transmission of sensitive data to LLMs service providers, potentially compromising personal privacy. To mitigate these challenges, this paper introduces Tex-tual Differential Privacy, a novel paradigm aimed at safeguarding user privacy in LLMs-based context-aware reasoning. The proposed Differential Embedding Hash algorithm anonymizes sensitive information while maintaining the reasoning capability of LLMs. Additionally, a quantification scheme for privacy loss is proposed to better understand the trade-off between privacy protection and loss. Through rigorous analysis and experimentation, the effectiveness and robustness of the proposed paradigm in mitigating privacy risks associated with context-aware reasoning tasks are demonstrated. This paradigm addresses privacy concerns in context-aware reasoning architectures, enhancing the trust and utility of LLMs in various applications.},
	booktitle = {2024 {IEEE} 48th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Yu, Junwei and Zhou, Jieyu and Ding, Yepeng and Zhang, Lingfeng and Guo, Yuheng and Sato, Hiroyuki},
	month = jul,
	year = {2024},
	note = {ISSN: 2836-3795},
	keywords = {Cognition, Computer architecture, Context-Aware Reasoning, Differential privacy, Differential Privacy, Large Language Model, Large language models, Named -Entity Recognition, Privacy, Retrieval-Augmented Generation, Robustness, Systematics},
	pages = {988--997},
}

@inproceedings{tommasel_semantic_2024,
	title = {Semantic grounding of {LLMs} using knowledge graphs for query reformulation in medical information retrieval},
	doi = {10.1109/BigData62323.2024.10826117},
	abstract = {The widespread adoption of electronic health records has generated a vast amount of patient-related data, mostly presented in the form of unstructured text, which could be used for document retrieval. However, querying these texts in full could present challenges due to their unstructured and lengthy nature, as they may contain noise or irrelevant terms that can interfere with the retrieval process. Recently, large language models (LLMs) have revolutionized natural language processing tasks. However, despite their promising capabilities, their use in the medical domain has raised concerns due to their lack of understanding, hallucinations, and reliance on outdated knowledge. To address these concerns, we evaluate a Retrieval Augmented Generation (RAG) approach that integrates medical knowledge graphs with LLMs to support query refinement in medical document retrieval tasks. Our initial findings from experiments using two benchmark TREC datasets demonstrate that knowledge graphs can effectively ground LLMs in the medical domain.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Tommasel, Antonela and Assent, Ira},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Benchmark testing, Grounding, Information retrieval, knowledge graphs, Knowledge graphs, large language models, Law, medical documents, Natural language processing, Noise, Pipelines, query expansion, Resource management, Retrieval augmented generation, Semantics},
	pages = {4048--4057},
}

@inproceedings{neupane_questions_2024,
	title = {From {Questions} to {Insightful} {Answers}: {Building} an {Informed} {Chatbot} for {University} {Resources}},
	doi = {10.1109/FIE61694.2024.10892994},
	abstract = {This research-to-practice full paper presents Bark-plug v.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings. The objective of Barkplug v.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion. Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks. We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment (RAGAS). Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS). Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and satisfactory user experience, as validated by usability assessments.},
	booktitle = {2024 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Neupane, Subash and Hossain, Elias and Keith, Jason and Tripathi, Himanshu and Ghiasi, Farbod and Golilarz, Noorbakhsh Amiri and Amirlatifi, Amin and Mittal, Sudip and Rahimi, Shahram},
	month = oct,
	year = {2024},
	note = {ISSN: 2377-634X},
	keywords = {Accuracy, Chatbot, Chatbots, information access, Large language models, LLM, Measurement, Pipelines, RAG, Reliability, Retrieval augmented generation, Surveys, University resources, Usability, User experience},
	pages = {1--9},
}

@article{zeeshan_llm-based_2025,
	title = {{LLM}-{Based} {Retrieval}-{Augmented} {Generation}: {A} {Novel} {Framework} for {Resource} {Optimization} in {6G} and {Beyond} {Wireless} {Networks}},
	volume = {63},
	issn = {1558-1896},
	doi = {10.1109/MCOM.001.2400774},
	abstract = {The increasing complexity of wireless networks, driven by diverse use cases and the evolution toward 6G, demands advanced resource optimization techniques for efficient and reliable communication. Traditional methods often struggle with adaptability and scalability in such dynamic environments. This article explores large language models (LLMs) as a potential approach to wireless resource optimization. Trained on massive datasets and capable of human-like reasoning, LLMs offer unique advantages in handling complex, multi-faceted problems. We propose a retrieval-augmented generation (RAG)-based framework that combines LLMs with domain-specific knowledge extracted from wireless standards documents, research articles, network-specific data, and other documents. This integration enhances the accuracy and reliability of LLM-driven decisions, enabling effective solutions to real-world challenges. We demonstrate the effectiveness of our RAG-based approach through a case study on spectrum management, showcasing its potential to outperform traditional methods and adapt to evolving network conditions. Lastly, we conclude with a discussion on the challenges and future directions in this promising area.},
	number = {10},
	journal = {IEEE Communications Magazine},
	author = {Zeeshan, Hafiz Muhammad Ali and Umer, Muhammad and Akbar, Muhammad and Kaushik, Aryan and Jamshed, Muhammad Ali and Jung, Haejoon and Hassan, Syed Ali},
	month = oct,
	year = {2025},
	keywords = {6G mobile communication, Augmented reality, Information retrieval, Large language models, Market research, Resource management, Wireless networks},
	pages = {60--67},
}

@inproceedings{sinha_fact-centric_2024,
	title = {Fact-{Centric} {Knowledge} {Web} for {Information} {Retrieval}},
	doi = {10.1109/WI-IAT62293.2024.00067},
	abstract = {The unreliability of Large Language Models (LLMs) owing to Machine Hallucination necessitates a shift towards Retrieval Augmented Generation (RAG), to ensure the veracity of LLM generated content. Inspired by Knowledge Graph based RAG techniques and their advantages of reduced token cost, improved computational performance and knowledge discovery, we present a “Knowledge Web” Structure and its associated techniques for Information Storage and Retrieval. We also explore the structure's synergy with a Fact Finding LLM Agent, capable of generating answers for multi-hop questions from challenging datasets such as the MuSiQue dataset. While still in the experimental phase, our initial results are promising, indicating the potential efficacy of our approach compared to existing RAG techniques.},
	booktitle = {2024 {IEEE}/{WIC} {International} {Conference} on {Web} {Intelligence} and {Intelligent} {Agent} {Technology} ({WI}-{IAT})},
	author = {Sinha, Reuben and Shiramatsu, Shun},
	month = dec,
	year = {2024},
	keywords = {Costs, Information retrieval, Information Retrieval, Information Storage, Intelligent agents, Knowledge discovery, Knowledge Graph, Knowledge graphs, Large language models, Large Language Models, LLM Agent, Reliability, Research and development, Retrieval augmented generation, Vectors},
	pages = {421--425},
}

@inproceedings{barghi_qrag_2025,
	title = {{QRAG}: {Using} {Learnable} {Graph} {Queries} for {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/CCWC62904.2025.10903812},
	abstract = {Retrieval-augmented generation (RAG) is a critical feature of most LLM pipelines, ensuring the model produces precise, up-to-date, and hallucination-free results. However, traditional RAG, which is based on vector similarity, often fails to capture the whole context needed to respond to a user's prompt. This is especially true in “multi-hop” prompts, where the information needed is in at least one other document that does not match the prompt embedding. Graph-based RAG provides a solution to this problem, but is hindered by the current graph processing ecosystem, and can be complicated to implement. As a response, we introduce QRAG, built on the BitGraph framework, which delivers a scalable, tunable solution for graph-based RAG. QRAG also introduces support for prize-aware graph traversals, which enable faster and more accurate subgraph extraction, possibly eliminating the need for further subgraph refinement.},
	booktitle = {2025 {IEEE} 15th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Barghi, Alexandria},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Benchmark testing, Conferences, Ecosystems, Knowledge graphs, Pipelines, Retrieval augmented generation, Vectors},
	pages = {00561--00568},
}

@inproceedings{ta_fine-tuning_2025,
	title = {Fine-tuning a {Large} {Language} {Model} for the {Indian} {Legal} {System}},
	doi = {10.1109/INFOTEH64129.2025.10959207},
	abstract = {This paper introduces an application based on a large language model customized for the Indian legal system, leveraging the LLama 3.1 8B foundational model. The model is pre-trained on a diverse corpus of legal texts and subsequently fine-tuned with curated Indian legal data to enhance accuracy and contextual relevance in legal responses. Advanced techniques such as Low-Rank Adaptation and Quantized Low-Rank Adaptations are employed to optimize the model’s efficiency while minimizing computational costs during fine-tuning. Pruning, as a compression method is utilized to enhance the model’s performance further and enable its deployment in resource-constrained environments. Additionally, the Retrieval Augmented Generation module is strategically implemented for document-specific queries, ensuring contextually accurate responses when processing legal documents. This research work is backed up by extensive experiments measuring the effectiveness across many precision metrics. The application is also tested against HaluEval, a Hallucination Evaluation Benchmark for factual reliability, and has demonstrated significant improvements in the model’s effectiveness as a resource for the legal domain. This AI-driven tool is the first step in simplifying the legal advisory services and decision-support systems in the Indian judiciary. It also goes a long way in enhancing the legal services experience.},
	booktitle = {2025 24th {International} {Symposium} {INFOTEH}-{JAHORINA} ({INFOTEH})},
	author = {Ta, Rishabh and Salunke, Ramit and R, Rahul and Nv, Ritvik and Upadhyaya, Sujatha R},
	month = mar,
	year = {2025},
	note = {ISSN: 2767-9470},
	keywords = {Accuracy, Adaptation models, Benchmark testing, Computational modeling, Context modeling, Large language models, Law, Measurement, Reliability, Retrieval augmented generation},
	pages = {1--6},
}

@inproceedings{rorseth_rage_2024,
	title = {{RAGE} {Against} the {Machine}: {Retrieval}-{Augmented} {LLM} {Explanations}},
	doi = {10.1109/ICDE60146.2024.00430},
	abstract = {This paper demonstrates RAGE, an interactive tool for explaining Large Language Models (LLMs) augmented with retrieval capabilities; i.e., able to query external sources and pull relevant information into their input context. Our explanations are counterfactual in the sense that they identify parts of the input context that, when removed, change the answer to the question posed to the LLM. RAGE includes pruning methods to navigate the vast space of possible explanations, allowing users to view the provenance of the produced answers.},
	booktitle = {2024 {IEEE} 40th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Rorseth, Joel and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw},
	month = may,
	year = {2024},
	note = {ISSN: 2375-026X},
	keywords = {Data engineering, Data models, Explainable AI, Large language models, Large Language Models, Navigation, Retrieval-Augmented Generation},
	pages = {5469--5472},
}

@inproceedings{mashnoor_timelyhls_2025,
	title = {{TimelyHLS}: {LLM}-{Based} {Timing}-{Aware} and {Architecture}-{Specific} {FPGA} {HLS} {Optimization}},
	doi = {10.1109/COINS65080.2025.11125726},
	abstract = {Achieving timing closure and design-specific optimizations in FPGA-targeted High-Level Synthesis (HLS) remains a significant challenge due to the complex interaction between architectural constraints, resource utilization, and the absence of automated support for platform-specific pragmas. In this work, we propose TimelyHLS, a novel framework integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automatically generate and iteratively refine HLS code optimized for FPGA-specific timing and performance requirements. TimelyHLS is driven by a structured architectural knowledge base containing FPGA-specific features, synthesis directives, and pragma templates. Given a kernel, TimelyHLS generates HLS code annotated with both timing-critical and design-specific pragmas. The synthesized RTL is then evaluated using commercial toolchains, and simulation correctness is verified against reference outputs via custom testbenches. TimelyHLS iteratively incorporates synthesis logs and performance reports into the LLM engine for refinement in the presence of functional discrepancies. Experimental results across 10 FPGA architectures and diverse benchmarks show that TimelyHLS reduces the need for manual tuning by up to 70\%, while achieving up to 4× latency speedup (e.g., 3.85× for Matrix Multiplication, 3.7× for Bitonic Sort) and over 50\% area savings in certain cases (e.g., 57\% FF reduction in Viterbi). TimelyHLS consistently achieves timing closure and functional correctness across platforms, highlighting the effectiveness of LLM-driven, architecture-aware synthesis in automating FPGA design.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Omni}-layer {Intelligent} {Systems} ({COINS})},
	author = {Mashnoor, Nowfel and Akyash, Mohammad and Kamali, Hadi and Azar, Kimia},
	month = aug,
	year = {2025},
	note = {ISSN: 2996-5330},
	keywords = {Codes, Field programmable gate arrays, FPGA, High-Level Synthesis, Large language models, Large Language Models, Manuals, Optimization, Resource management, Retrieval augmented generation, Retrieval-Augmented Generation, Timing, Timing Closure, Tuning, Viterbi algorithm},
	pages = {1--6},
}

@inproceedings{gopi_enhancing_2024,
	title = {Enhancing {Engineering} {Education} {Through} {LLM}-{Driven} {Adaptive} {Quiz} {Generation}: {A} {RAG}-{Based} {Approach}},
	doi = {10.1109/FIE61694.2024.10893146},
	abstract = {This research-to-practice study aims to develop an Artificial Intelligence (AI) MCQ generation system for engineering students, with a focus on adaptive learning, educational technology, and innovative assessment tools, to enhance personalized learning. Engineering education faces significant academic performance challenges, with first-year retention rates in STEM fields ranging between 27\% to 46\%, largely due to poor academic achievements. Multiple Choice Questions (MCQs) identify misconceptions, reinforce knowledge retention, and offer efficient assessment methods for engineering education. This interactive method improves attention and memory retention, reinforces knowledge, and improves comprehension. In this context, the emergence of Large Language Models (LLMs) such as GPT-4 has marked a significant advancement. Our literature review method employed a systematic approach, analyzing peer-reviewed articles, conference papers, and authoritative reports to uncover the trends and challenges in AI-driven quiz generation. The notable gap identified in our literature review is the lack of LLM-based adaptive quiz generation methods specifically for engineering education. Our methodology involved sourcing relevant structured datasets, data pre-processing, embedding generation, vector database storage, hybrid-search retrieval, LLM query results feed, prompt engineering, and context-based response. In this research, we adopted Vectara as a vector database tool for its automatic data ingestion capabilities and seamless integration with generative AI applications. Prompt engineering involves a dual-prompt approach, where the Contextual Question Prompt formulates questions based on user topics and chat history, while the Answer Question Prompt manages MCQ responses with explanations, ensuring relevant and contextually accurate interactions. Evaluation includes topic relevancy, answer relevancy, and a contextual relevancy score. Preliminary results indicate promising results for the generation of accurate and contextually appropriate questions with minimal hallucinations. The quiz generation system was deployed using Streamlit cloud-based architecture to showcase the functionality. Looking forward, we aim to expand the dataset to include more diverse engineering disciplines and to refine the retrieval algorithms to better handle complex diagrams and mathematical expressions commonly found in engineering texts.},
	booktitle = {2024 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Gopi, Sreekanth and Sreekanth, Devananda and Dehbozorgi, Nasrin},
	month = oct,
	year = {2024},
	note = {ISSN: 2377-634X},
	keywords = {Accuracy, AI quiz generation, Databases, engineering education, GPT-4, Large Language Models, Learning (artificial intelligence), LLM evaluation, Market research, Mathematical models, personalized learning, prompt engineering, Prompt engineering, RAG, STEM, Systematic literature review, Systematics, Vec-tara, Vectors},
	pages = {1--8},
}

@inproceedings{martinez-romo_generative_2025,
	title = {Generative {AI} for {Education}: {A} {Retrieval}-{Augmented} {System} for {Effective} {Feedback} in {Self}-{Assessment}},
	doi = {10.1109/EDUCON62633.2025.11016446},
	abstract = {The application of generative AI in education has shown significant potential to enhance learning outcomes by providing personalized, adaptive feedback to students. In this work, we present a novel Retrieval-Augmented Generation (RAG) system designed to improve the explanations and feedback provided to students during self-assessment activities. The system we developed is grounded in the course's reference material, ensuring that the feedback remains accurate, consistent, and contextually relevant to the student's curriculum. The system retrieves information directly from the textbook, reducing ambiguity and interpretation errors, and generates responses tailored to the specific needs of each student. The feedback is not only designed to correct misconceptions but also to reinforce key concepts, making the system a valuable tool for self-guided learning. In this study, we also explore the importance of prompt engineering in creating effective AI-generated feedback. We detail the iterative process used to optimize the prompts and the strategies employed to ensure high-quality, interpretable responses. The findings from this work suggest that generative AI, when integrated with subject-specific textbooks and careful prompt engineering, can significantly enhance the educational experience by providing dynamic, and contextually accurate feedback. This approach opens new possibilities for AI-driven education tools, contributing to more personalized and effective learning experiences.},
	booktitle = {2025 {IEEE} {Global} {Engineering} {Education} {Conference} ({EDUCON})},
	author = {Martinez-Romo, Juan and Araujo, Lourdes and Plaza, Laura and López-Ostenero, Fernando},
	month = apr,
	year = {2025},
	note = {ISSN: 2165-9567},
	keywords = {Accuracy, computer science, Computer science, Engineering education, formative feedback, generative IA, Iterative methods, Prompt engineering, Retrieval augmented generation, Self-assessment tools},
	pages = {1--9},
}

@inproceedings{liu_enhancing_2025,
	title = {Enhancing {Large} {Language} {Models} with {Pseudo}-and {Multisource}-{Knowledge} {Graphs} for {Open}-ended {Question} {Answering}},
	doi = {10.1109/ICDEW67478.2025.00019},
	abstract = {Mitigating the hallucinations of Large Language Models is a crucial task. Although some existing methods employ self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Meanwhile, Knowledge Graph (KG) enhancement approaches fail to address the generalization across different KG sources and the enhancement of openended questions simultaneously. To tackle these limitations, we propose a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\&AKV). Enhancement of open-ended question-answering begins with leveraging the Pseudo-Graph Generation to provide the related knowledge framework. Subsequently, Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For preciseanswered questions, we observe a minimum accuracy improvement of 7.5\%. Moreover, PG\&AKV also exhibits generalizability across different KG sources. Utilizing KG different from the question sources, PG\&AKV can even achieve at least a 3.5\% performance improvement. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of open-ended questions.},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Liu, Jiaxiang and Zhou, Tong and Chen, Yubo and Zhao, Jun and Liu, Kang},
	month = may,
	year = {2025},
	note = {ISSN: 2473-3490},
	keywords = {Accuracy, Conferences, Data engineering, Knowledge graphs, Knowledgeenhanced LLMs, Large language models, Open-ended QA, Question answering (information retrieval), Retrieval augmented generation, Retrieval-augmented generation},
	pages = {97--106},
}

@inproceedings{sharma_mitigating_2025,
	title = {Mitigating {Hallucination} — {ZeroG}: {An} {Advanced} {Knowledge} {Management} {Engine}},
	doi = {10.1109/ACDSA65407.2025.11165971},
	abstract = {The growth of digital documents presents significant challenges in efficient management and knowledge extraction. Traditional methods often struggle with complex documents, leading to issues such as hallucinations and high latency in responses from Large Language Models (LLMs). ZeroG, an innovative approach, significantly mitigates these challenges by leveraging knowledge distillation and prompt tuning to enhance model performance.ZeroG utilizes a smaller model that replicates the behavior of a larger teacher model, ensuring contextually relevant and grounded responses, by employing a black-box distillation approach, it creates a distilled dataset without relying on intermediate features, optimizing computational efficiency. This method significantly enhances accuracy and reduces response times, providing a balanced solution for modern document management.Incorporating advanced techniques for document ingestion and metadata utilization, ZeroG improves the accuracy of question-and-answer systems. The integration of graph databases and robust metadata management further streamlines information retrieval, allowing for precise and context-aware responses. By transforming how organizations interact with complex data, ZeroG enhances productivity and user experience, offering a scalable solution for the growing demands of digital document management.},
	booktitle = {2025 {International} {Conference} on {Artificial} {Intelligence}, {Computer}, {Data} {Sciences} and {Applications} ({ACDSA})},
	author = {Sharma, Anantha and John, Sheeba Elizabeth and Nikroo, Fatemeh Rezapoor and Bhatt, Krupali and Zambre, Mrunal and Wikhe, Aditi},
	month = aug,
	year = {2025},
	keywords = {Accuracy, Computational modeling, Context modeling, Document Management, Information retrieval, Knowledge Distillation, Large language models, Large Language Models (LLMs), Latency Reduction, Maximal Marginal Relevance (MMR), Metadata, Organizations, Productivity, Prompt Tuning, Response Accuracy, Retrieval-Augmented Generation (RAG), Tuning, User experience, Zero-Shot Prompting},
	pages = {1--6},
}

@inproceedings{beining_generating_2024,
	title = {Generating {Commit} {Messages} for {Configuration} {Files} in {5G} {Network} {Deployment} {Using} {LLMs}},
	doi = {10.23919/CNSM62983.2024.10814636},
	abstract = {Network automation is crucial for improving network performance. Commit messages describes the different actions of the modification of network configuration files and deployments. This paper presents experiments and studies on automated commit message generation in the deployment of 5G networks. We extracted data from repositories of various projects engineered in Orange’s 5G network. We then developed five prompts for experiments to identify the most suitable methods for this task. To select large language models, we used an in-house GPT-4 interface provided by Orange, and locally deployed popular large models such as Llama3, Mistral. We used both automated and human evaluation methods, selecting BLEU, ROUGE, and METEOR as our metrics for automated assessment. Our experiments shows that commit messages for configuration files generated by Large Language Models (LLMs) have better scores when using one-shot and Retrieval-Augmented Generation (RAG) technologies, for messages generated both by humans and bots.},
	booktitle = {2024 20th {International} {Conference} on {Network} and {Service} {Management} ({CNSM})},
	author = {Beining, Yang and Alassane, Samba and Guillaume, Fraysse and Sihem, Cherrared},
	month = oct,
	year = {2024},
	note = {ISSN: 2165-963X},
	keywords = {5G mobile communication, Automation, Commit Message Generation, Data mining, Large Language Model, Large language models, Measurement, Meteors, network automation, Retrieval augmented generation},
	pages = {1--7},
}

@inproceedings{liu_graphcoder_2024,
	title = {{GraphCoder}: {Enhancing} {Repository}-{Level} {Code} {Completion} via {Coarse}-to-fine {Retrieval} {Based} on {Code} {Context} {Graph}},
	abstract = {The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs’ general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.CCS Concepts• Software and its engineering → Search-based software engineering; • Information systems → Language models; Query representation; • Mathematics of computing → Graph algorithms.},
	booktitle = {2024 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Liu, Wei and Yu, Ailun and Zan, Daoguang and Shen, Bo and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Wang, Qianxiang},
	month = oct,
	year = {2024},
	note = {ISSN: 2643-1572},
	keywords = {Accuracy, Code completion, Code graphs, Codes, Computational modeling, Filtering, Information systems, Large language model, Mathematical models, Process control, Retrieval augmented generation, Software, Software algorithms, Software engineering},
	pages = {570--581},
}

@inproceedings{yao_assertgpt_2025,
	title = {{AssertGPT}: {LLMs}-empowered {Assertion}-based {Verification} of {Programmable} {Networks}},
	doi = {10.23919/APNOMS67058.2025.11181349},
	abstract = {Assertion-based verification is essential for ensuring that the operational behavior of programmable networks aligns with their intended design specifications. However, reliance on manual assertion generation prolongs verification cycles and introduces deployment barriers due to its labor intensity, limiting scalability in practical implementations. While Large Language Models (LLMs) have demonstrated transformative potential in automating engineering tasks with their excellent natural language understanding, their application to assertion-based network verification remains underexplored. In this study, we propose leveraging LLMs for assertion-based verification of programmable networks. We introduce AssertGPT, an LLMdriven automated assertion generation framework designed to streamline the verification workflow. Firstly, we construct a highquality dataset in the field of assertion-based network verification, and further develop a domain-specific LLM tailored for automated assertion generation in network verification scenarios by synergizing custom instruction refinement with the Retrieval Augmented Generation (RAG) technique. To mitigate hallucinations, we develop a multi-stage assertion checker with a dynamic feedback loop, enabling iterative refinement of AssertGPT’s performance through continuous assertion regeneration guided by syntactic and semantic verification outcomes. An evaluation of diverse network verification requirements and multiple assertion languages (e.g., those in AssertP4, DBVal) underscores AssertGPT’s efficacy in automated assertion generation for assertionbased network verification.},
	booktitle = {2025 25th {Asia}-{Pacific} {Network} {Operations} and {Management} {Symposium} ({APNOMS})},
	author = {Yao, Ying and Tian, Le and Pan, Fan and Hu, Yuxiang and Yang, Xingyuan and Zhan, Qi and Yue, Qiqiang},
	month = sep,
	year = {2025},
	note = {ISSN: 2576-8565},
	keywords = {Assertion Generation, Assertion-based, Feedback loop, Iterative methods, Large Language Model, Large language models, Limiting, Manuals, Natural language processing, Network Verification, Programmable Network, Retrieval augmented generation, Scalability, Semantics, Syntactics},
	pages = {1--6},
}

@inproceedings{abeywardana_enhancing_2025,
	title = {Enhancing {Automated} {Grading} with {Capabilities} of {LLMs}: {Using} {Prompt} {Engineering} and {RAG} {Techniques}},
	doi = {10.1109/ICARC64760.2025.10962827},
	abstract = {This research explores the potential of Large Language Models (LLMs) to automate the grading process in education by harnessing LLMs’ sophisticated understanding of language and instructions following nature. We explore the effectiveness of providing subject knowledge and utilizing prompt engineering techniques to grade the students’ written answers for different question types and various theoretical subjects. A grading rubric was employed to ensure consistency and fairness in the assessment process. The study results highlighted the importance of providing external knowledge within the prompt to enhance the automated student answer grading utilizing LLMs. Including grading rubrics, model answers, and course content significantly enhanced the accuracy of scores assigned by the LLM, reducing deviations from human evaluator scores. Providing course content or model answers also helped define the expected answer scope and guide the LLM in determining other possible correct answers. Using prompt engineering techniques within the prompt failed to outperform the basic prompt, suggesting the need for further exploration and refinement in prompt design strategies.},
	booktitle = {2025 5th {International} {Conference} on {Advanced} {Research} in {Computing} ({ICARC})},
	author = {Abeywardana, Thisuri and Nandadewa, Nethmini and Wickramasinghe, Vimansa and Rohanadheera, Sudam and Nadungodage, Thilini and Hewagamage, Priyantha},
	month = feb,
	year = {2025},
	keywords = {Accuracy, AES, Analytical models, Cognition, Computational modeling, Context modeling, Education, Grading, Knowledge engineering, Large language models, LLM, Prompt engineering, RAG},
	pages = {1--6},
}

@inproceedings{sunde_evaluation_2025,
	title = {On the evaluation of {StartupGPT}: {A} {Retrieval}-{Augmented} {AI} {Chatbot} for {Delivering} {Research}-{Driven} {Guidance} to {Startups}},
	doi = {10.1109/IWSiB66663.2025.00009},
	abstract = {[Introduction] The study addresses the challenge of transferring research knowledge to the industry, with a particular focus on small businesses and startups, which often lack access to empirical insights. Large Language Models (LLMs), particularly those using Retrieval-Augmented Generation (RAG), offer potential for embedding knowledge from startup research into an interactive chatbot to support startup mentorship. However, empirical work exploring this application is limited. [Objective] The primary objective of this research is to design and evaluate a version of "StartupGPT," an AI-driven chatbot that uses LLMs and RAG to provide advice for software startups by leveraging a knowledge base rooted in software startup research. [Methodology] The study follows the Design Science Research Methodology (DSRM) and spans three iterative cycles, with this paper focusing on Cycle 3. The prototype was tested with 11 startup founders, who provided both qualitative and quantitative feedback on the chatbot’s usefulness and satisfaction. [Results] The findings from user tests indicate that StartupGPT was generally perceived as relevant, reliable, and helpful. However, limitations were noted in its responses, which users found overly theoretical, lacking in concrete examples, and insufficiently personalized for specific startup contexts. [Conclusion] Future LLM-based interaction designed for startups should focus on improving interactivity, incorporating more context-aware and specific advice, and leveraging advanced AI techniques, such as fine-tuning, to better align the chatbot’s responses with the unique needs of individual startups.},
	booktitle = {2025 {IEEE}/{ACM} {International} {Workshop} on {Software}-{Intensive} {Business} ({IWSiB})},
	author = {Sunde, Helene Fønstelien and Lovise Ahlgren, Thea and Jaccheri, Letizia and Nguyen-Duc, Anh},
	month = may,
	year = {2025},
	keywords = {Business, Chatbots, Industries, Iterative methods, Knowledge based systems, large language models, Large language models, Online Mentor, Prototypes, Reliability theory, Retrieval augmented generation, Retrieval-Augmented Generation, Software, Startup, Startup Mentor},
	pages = {25--32},
}

@inproceedings{sokolov_generative_2024,
	title = {Generative {Reader} {Optimization} in the {RAG}-{System}},
	doi = {10.1109/NeuroNT62606.2024.10585446},
	abstract = {This paper proposes the simple methodology for optimizing a generative reader subsystem as part of a RAG QA-system. Described methodology can be used as a preliminary optimization that could help building the question-answering system of the reasonable quality in a short time period. The main principle of this methodology is the usage of the OpenAI 's GPT-4 generative model as a gold reference generative reader. Our methodology describes the procedure for generation of the synthetic dataset and defines main optimization metrics. Usage of synthetic datasets makes it possible to accurately control that all changes in the QA-system make its answers closer in average to the gold reference reader. Based on the developed methodology we performed the set of basic optimization experiments to quickly find the better configuration of the reader subsystem. These experiments made it possible to significantly improve the quality of the answers of the reader subsystem. As a result, we achieved relative improvement of the semantic similarity between the answers of our reader and the reference one based on GPT-4 for almost 50\% by BLEURT scale and 26\% by the SAS scale. Our methodology was verified on the QA-dataset in Russian language but it's also applicable to any other language without significant modifications.},
	booktitle = {2024 {V} {International} {Conference} on {Neural} {Networks} and {Neurotechnologies} ({NeuroNT})},
	author = {Sokolov, Andrey P. and Zamelin, Pavel and Kamelina, Yulia and Plastova, Polina},
	month = jun,
	year = {2024},
	keywords = {Buildings, Gold, LLM, Measurement, Prompt engineering, QA-systems, Question answering (information retrieval), RAG, retriever augmented generation, semantic similarity, Semantics, Synthetic aperture sonar},
	pages = {135--138},
}

@inproceedings{serhan_vulnerabilities_2025,
	title = {Vulnerabilities of a {Medical} and {Scientific} based {RAG} {System}},
	doi = {10.1109/ACTEA66485.2025.11189974},
	abstract = {When dealing with content like Medical or scientific, things become more complex. Nowadays, Software engineers are increasingly adding semantic search capabilities to applications using RAG. A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) to extract the right answer using an LLM. RAG systems aim to reduce the problem of LLM hallucination and provide appropriate links to sources or references for the generated responses and remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherited from the quality of retrieved information and from LLMs limited input context window to name a few. In this paper, we present an experience report on the limitations of RAG systems from multiple case studies from separate domains: medical and scientific. We share the lessons learned and show how to prevent those limitations to consider when designing a RAG system. We show that an enhancement of up to 19\% could be added on RAG systems when implementing those suggestions.},
	booktitle = {2025 {Sixth} {International} {Conference} on {Advances} in {Computational} {Tools} for {Engineering} {Applications} ({ACTEA})},
	author = {Serhan, Jean H. and Nehme, Bechara F.},
	month = sep,
	year = {2025},
	note = {ISSN: 2993-3765},
	keywords = {fine-tuning, Large language models, LLM, medical systems, RAG, scientific systems, Semantic search, Software, vulnerabilities},
	pages = {1--6},
}

@inproceedings{sheng_talk2traffic_2025,
	title = {{Talk2Traffic}: {Interactive} and {Editable} {Traffic} {Scenario} {Generation} for {Autonomous} {Driving} with {Multimodal} {Large} {Language} {Model}},
	doi = {10.1109/CVPRW67362.2025.00364},
	abstract = {Deploying autonomous vehicles (AVs) requires testing in diverse and challenging scenarios to ensure safety and reliability, yet collecting real-world data remains prohibitively expensive. While simulation-based approaches offer costeffective alternatives, most existing methods lack sufficient support for intuitive, interactive editing of generated scenarios. This paper presents Talk2Traffic, a novel framework that leverages multimodal large language models (MLLMs) to enable interactive and editable traffic scenario generation. Talk2Traffic allows human users to generate various traffic scenarios through multimodal inputs (text, speech, and sketches). Our approach first employs an MLLM-based interpreter to extract structured representations from these inputs. These representations are then translated into executable Scenic code using a retrieval-augmented generation mechanism to reduce hallucinations and ensure syntactic correctness. Furthermore, a human feedback guidance module enables iterative refinement and editing of scenarios through natural language instructions. Experiments demonstrate that Talk2Traffic outperforms state-of-the-art methods in generating challenging scenarios. Qualitative evaluations further illustrate the framework can handle diverse input modalities and support scenario editing.},
	booktitle = {2025 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Sheng, Zihao and Huang, Zilin and Qu, Yansong and Leng, Yue and Chen, Sikai},
	month = jun,
	year = {2025},
	note = {ISSN: 2160-7516},
	keywords = {autonomous driving, Autonomous vehicles, Codes, Large language models, multimodal large language models, Retrieval augmented generation, Safety, Scenario generation, Semantics, Syntactics, Testing, traffic scenario generation, Translation},
	pages = {3788--3797},
}

@inproceedings{murata_effectiveness_2025,
	title = {On the {Effectiveness} of the {ChatGPT} {Model} with {Fine}-tuning in {Question}-{Answer} {Systems} on {Slack}},
	doi = {10.1109/ICBIR65229.2025.11163304},
	abstract = {QABot is an app on Slack designed for universities. When students ask questions on the app, they receive responses from teachers and TAs, but the system is designed so that students do not know who has responded. In other words, all messages, such as questions and answers, between students and teachers are forwarded to the local server where the app is running. By using the app, students no longer directly ask specific teachers or TAs via DM, and the workload can be distributed. However, there are cases where similar questions arise from various students. In this paper, we have given QABot a mechanism for creating answer drafts using ChatGPT. Three models of ChatGPT are examined: 4o mini, 4o mini using fine-tuning, and 4o mini using Simple RAG, which can refer to past questions and answers. These models are evaluated by using BertScore. 4o mini using fine-tuning demonstrated good results with statistical significance.},
	booktitle = {2025 10th {International} {Conference} on {Business} and {Industrial} {Research} ({ICBIR})},
	author = {Murata, Hiroaki and Handa, Hisashi},
	month = may,
	year = {2025},
	keywords = {Business, Chatbots, ChatGPT, fine-tuning, question-answer system, RAG, Servers, Slack},
	pages = {472--475},
}

@inproceedings{tamanna_navigating_2024,
	title = {Navigating the {Roadblocks}: {Lessons} {Learned} from a {Gardening} {Society} {Conversational} {RAG} {Bot}},
	doi = {10.1109/AIC61668.2024.10731053},
	abstract = {Large Language Model (LLM) Retrieval-Augmented Generation (RAG) chatbots hold immense potential for enhancing user engagement and information access. However, bringing such a system to life on a real-world platform presents unique challenges. This article explores the design of the chatbot, and the hurdles encountered in productionizing an LLM-RAG chatbot for a gardening society website. We delve into specific roadblocks faced during development, including•Domain-Specific Knowledge Acquisition: Training the LLM on a comprehensive gardening knowledge base proved crucial to ensure accurate and relevant responses.•Balancing Open-Endedness with Focus: While LLMs excel at open-ended conversation, enabling the chatbot to answer gardening queries effectively required focused training strategies.•Integration with Website Infrastructure: Seamless integration of the chatbot into the society's website involved addressing technical considerations and ensuring a user-friendly experience.We present the solutions implemented to overcome these challenges, offering valuable insights for those considering deploying similar LLM-RAG chatbots for niche online communities. The article concludes by discussing the lessons learned and the potential impact of such chatbots on the future of online user interactions.},
	booktitle = {2024 {IEEE} 3rd {World} {Conference} on {Applied} {Intelligence} and {Computing} ({AIC})},
	author = {{Tamanna} and Rana, Subarna and Choudhary, Priyanka},
	month = jul,
	year = {2024},
	keywords = {Accuracy, chatbot, Chatbots, gardening, Generative AI, Knowledge acquisition, Knowledge based systems, Large language models, Navigation, Oral communication, prompts, RAG, Training},
	pages = {618--626},
}

@article{krishnamurthy_yours_2024,
	title = {Yours {Truly}: {A} {Credibility} {Framework} for {Effortless} {LLM}-{Powered} {Fact} {Checking}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3520187},
	abstract = {In an era where social media portrays subjective realities, discerning truth from propaganda has become increasingly challenging. The proposed system addresses this issue with an end-to-end credibility framework to make fact-checking effortless and intuitive. Recognizing the subjective nature of claims, the system provides a robust method to assess the veracity of social media claims. Twitter, a key platform for public opinion exchange, influences cultural beliefs, political affiliations, and crisis responses. This work offers a pragmatic solution to navigate manipulative claims, reducing the cognitive effort to distinguish fact from fabrication and breaking misinformation chains sooner. Yours Truly uses FactStore, an extensive real-time database of fact-checked claims from Indian and International fact-checking initiatives. This database powers the search function, enabling time-sensitive searches with increased coverage to retrieve relevant context. The system breaks down compound sentences into atomic claims, verifying each with iterative context retrieval. Each claim is further fact-checked with multiple articles using text and semantic search. These matched articles are reranked for relevance using a technique called query-based committee selector. The top-ranked results provide context to an instruction fine-tuned Large Language Model, which infers the truth value of input claims. This approach tackles claims’ ambiguity and complexity and returns an interpretable credibility report explaining the inferred truth value. Yours Truly achieves an impressive F1 Score of 94\% The framework is easily extensible to verify contents from other social media platforms, as it only relies on text without metadata and effectively handles long-form texts by atomizing compound statements. Yours Truly outperforms contemporary fact-checking systems on multiple misinformation baselines. It generalizes well across various text forms and information domains, demonstrating a high level of automation.},
	journal = {IEEE Access},
	author = {Krishnamurthy, Vallidevi and Balaji, Varshini},
	year = {2024},
	keywords = {COVID-19, Databases, Fake news, Fake news detection, few-shot learning, large language models, Pandemics, Real-time systems, retrieval augmented generation, Social networking (online), Surveys, Transformers, Vaccines, vector search, Voting},
	pages = {195152--195173},
}

@inproceedings{yang_chain--summary_2025,
	title = {Chain-of-{Summary}: {An} {Efficient} {Multi}-{Clustering} {Framework} for {Hierarchical} {Abstraction}},
	doi = {10.1109/COMPSAC65507.2025.00090},
	abstract = {Large language models (LLMs) perform exceptionally well in text generation tasks, but they often face issues of outdated or inaccurate content (i.e., "hallucination") when handling dynamic real-world data. Retrieval-augmented generation (RAG) alleviates this limitation by incorporating external knowledge; however, significant challenges remain when processing long or complex documents, particularly in capturing document-level semantic context.To address this, we propose a novel hierarchical summarization framework—Chain of Summarization (CoS). This framework combines two complementary strategies that significantly enhance model adaptability and computational efficiency: (1) a multi-clustering summarization strategy (e.g., Gaussian Mixture Models (GMM) and hierarchical clustering); and (2) a sequential summarization strategy combined with GMM. Experimental results show that the CoS framework, through its multi-clustering strategy, significantly outperforms single clustering methods, achieving a 10\% performance improvement over baseline methods on the QASPER dataset. Additionally, the combination of sequential summarization and GMM reduced processing time by 50\%, while maintaining high performance on the NarrativeQA dataset, with its ROUGE L score improving by approximately 15.7\% compared to NaiveRAG. These results demonstrate the significant application potential of the CoS framework in RAG tasks, providing an efficient, scalable, and robust solution for handling long-document processing challenges.},
	booktitle = {2025 {IEEE} 49th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Yang, Chongchong and Zhu, Wenhao and Yang, Zhiqiang and Liu, Chaoqian and Hong, Jianfeng},
	month = jul,
	year = {2025},
	note = {ISSN: 2836-3795},
	keywords = {Adaptation models, Clustering algorithms, Computational efficiency, Computational modeling, Gaussian mixture model, Hierarchical Summarization, Large language models, Multi-Clustering, Retrieval augmented generation, Retrieval-Augmented Generation, Robustness, Semantics, Tuning},
	pages = {673--682},
}

@inproceedings{yadav_enhancing_2024,
	title = {Enhancing {Response} {Generation} {Systems}: {Knowledge} {Graph} \& {Generative} {AI} {Synergy} for {Business} {Communication} and {Strategic} {Decision} {Making}},
	doi = {10.1109/ICECET61485.2024.10698632},
	abstract = {In the contemporary business environment, it is crucial to have an efficient and precise response generation system to build client trust, optimize operations, and provide customized solutions. Current response management systems often face challenges such as insufficient depth, scalability issues, and inconsistencies. There is an urgent requirement for a comprehensive system that can retrieve information for generating high-quality insights and convert it to a reply. To address these needs, an approach has been explored that integrates a domain-oriented Knowledge Graph (KG), vector embeddings, Large Language Model (LLM) and utilize the pathway parsing technique that allows for in-depth multi-hop analysis within the KG, resulting in detailed and contextually rich data retrieval. This combination enhances the performance and precision of handling inquiries, streamlines entity extraction and step identification, leverages KG for Standard Operating Procedure (SOP) guidance, and offers superior recommendations or strategies for informed decision making. The concept will be illustrated through a business study focusing on the collections department use case, which involves customer correspondence. This approach ensures a more efficient, and accurate responses, leading to reduced human intervention and latency, along with that customer satisfaction is improved and business processes are streamlined. By adopting this method, businesses can enhance their communication, make data-driven decisions, and ultimately achieve better results in the competitive market. The efficacy is evident in the practical instances, owing to its profound grasp of context.},
	booktitle = {2024 {International} {Conference} on {Electrical}, {Computer} and {Energy} {Technologies} ({ICECET}},
	author = {Yadav, Divyanshi and Para, Hitesh and Sandhu, Komal and Selvakumar, Prakash},
	month = jul,
	year = {2024},
	keywords = {Business, Decision making, Decision Making, Email Handling, Faces, Focusing, Generative AI, Information Retrieval, Knowledge graphs, Knowledge Graphs, Large Language Model, Large language models, Natural Language Processing, Natural Language Understanding, Pathways parsing, Response Generation, Retrieval Augmented Generation, Scalability, Standards, Vectors, Word Embeddings},
	pages = {1--6},
}

@inproceedings{bernardi_report_2024,
	title = {Report {Generation} from {X}-{Ray} imaging by {Retrieval}-{Augmented} {Generation} and improved {Image}-{Text} {Matching}},
	doi = {10.1109/IJCNN60899.2024.10650332},
	abstract = {Creating radiology reports is a vital but time-intensive task that involves analyzing images, consulting documents, and evaluating data. This process, heavily reliant on human effort, is prone to errors that can vary with the radiologists experience. Consequently, automating the generation of radiology reports is a key research goal due to its potential impact on medical procedures and patient care.This work proposes a multimodal approach specifically designed for generating radiological reports from chest X-rays (CXRs). Our method integrates a LLaMa large language model with Retrieval Augmented Generation (RAG), enhanced by a modified ALBEF embedding model that exploits efficient organ semantic segmentation and triple contrastive loss (called EALBEF). The combination of these two components allows radiological report generation that surpasses current state-of-the-art methods in terms of quality and accuracy. Our approach demonstrates a significant enhancement in the radiologist-specific metrics (e.g., RadCliQ), as well as across various generic lexical-based metrics (e.g., GLEU). Quantitative analyses of the models outputs reveal a notable increase in fluency and accuracy, with a marked reduction in issues such as hallucinations and source-reference divergences in the generated reports.},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Bernardi, Mario Luca and Cimitile, Marta},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {Accuracy, Deep Learning, Image-Text Match, Large language models, Large Language Models (LLMs), Measurement, Radiology, Refining, Report Generation, Semantic segmentation, Statistical analysis, X-Ray imaging},
	pages = {1--8},
}

@inproceedings{bustamante_raccoon_2025,
	title = {{RACCOON}: {Grounding} {Embodied} {Question}-{Answering} with {State} {Summaries} from {Existing} {Robot} {Modules}},
	doi = {10.1109/ICRA55743.2025.11127843},
	abstract = {Explainability is vital for establishing user trust, also in robotics. Recently, foundation models (e.g. vision-language models, VLMs) fostered a wave of embodied agents that answer arbitrary queries about their environment and their interactions with it. However, naively prompting VLMs to answer queries based on camera images does not take into account existing robot architectures which represent the robot's tasks, skills, and beliefs about the state of the world. To overcome this limitation, we propose RACCOON, a framework that combines foundation models' responses with a robot's internal knowledge. Inspired by Retrieval-Augmented Generation (RAG), RACCOON selects relevant context, retrieves information from the robot's state, and utilizes it to refine prompts for an LLM to answer questions accurately. This bridges the gap between the model's adaptability and the robot's domain expertise.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Bustamante, Samuel and Knauer, Markus and Thun, Jeremias and Schneyer, Stefan and Albu-Schäffer, Alin and Weber, Bernhard and Stulp, Freek},
	month = may,
	year = {2025},
	keywords = {Adaptation models, Cameras, Foundation models, Grounding, Retrieval augmented generation, Robot vision systems, Robots},
	pages = {4322--4329},
}

@inproceedings{liang_research_2025,
	title = {Research on {Large} {Model}-{Based} {Information} {Processing} {Technology} and {Its} {Application} in {Local} {Life} in {Macau}},
	doi = {10.1109/AINIT65432.2025.11035618},
	abstract = {This study focuses on advancing the informatization of Macau's tourism sector through Large Language Model (LLM) technology, aiming to deliver precise and efficient travel services to both locals and visitors. A localized information repository has been established, consolidating public service data such as transportation, accommodation, dining, and sanitation facilities. By applying Natural Language Processing (NLP) techniques, particularly Text-to-SQL, the research facilitates the automatic translation of natural language queries into SQL queries, streamlining interactions with databases. Moreover, intelligent agent technology has been utilized to manage complex queries and enhance tourism planning and recommendations. The study employs a Retrieval-Augmented Generation (RAG) model to mitigate information bias and address the “hallucination” issue often encountered in large models, thereby boosting the precision of information retrieval.},
	booktitle = {2025 {IEEE} 6th {International} {Seminar} on {Artificial} {Intelligence}, {Networking} and {Information} {Technology} ({AINIT})},
	author = {Liang, ZhanFan and Liao, Haofan and Huang, Hai and Zhong, Xier},
	month = apr,
	year = {2025},
	keywords = {Accuracy, Knowledge based systems, LLM, Macau Tourism, Natural language processing, Planning, RAG, Structured Query Language, Technological innovation, Translation, Transportation, User experience, Weather forecasting},
	pages = {01--05},
}

@article{wang_hierarchical_2025,
	title = {Hierarchical {Index} {Retrieval}-{Driven} {Wireless} {Network} {Intent} {Translation} {With} {LLM}},
	volume = {24},
	issn = {1558-0660},
	doi = {10.1109/TMC.2025.3564937},
	abstract = {Intent-Based Networking (IBN) represents an emerging network management concept that is designed to fulfill user service requirements through automation. At its core, IBN is capable of translating user intent into network policies, thereby enabling automated configuration and management. However, the application of IBN has been limited by challenges associated with automation and intelligence. The recent widespread adoption of Large Language Model (LLM) has partially mitigated these issues. Nonetheless, hardware heterogeneity and high dynamic networks remain significant challenges for IBN: (i) Devices from different vendors are challenging to manage uniformly; (ii) Aligning service demands with rapidly changing network status is difficult. To address these challenges, we propose LIT, a framework of LLM-empowered Intent Translation with manual guidance. LIT incorporates Retrieval-Augmented Generation (RAG) to reference hardware manuals and enhance the generation results of LLMs. To reduce noise from retrieval results, we optimized the general RAG process. Additionally, LIT introduces MoE (Mixture of Experts) to adjust parameter values according to network status by synthesizing results from multiple expert models. Experiments demonstrate that LIT alleviates the challenges faced by IBN, achieving a 57.5\% improvement in F1 score compared to the baseline.},
	number = {10},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Wang, Jingyu and Guo, Lingqi and Wu, Jianyu and Yan, Caijun and Sun, Haifeng and Zhang, Lei and Zhuang, Zirui and Qi, Qi and Liao, Jianxin},
	month = oct,
	year = {2025},
	keywords = {Adaptation models, Computational modeling, Hardware, Indexes, intent translation, Intent-based networking, large language model, Manuals, Natural language processing, Optimization, retrieval-augmented generation, Training, Translation, Wireless networks},
	pages = {9837--9851},
}

@inproceedings{bin_zaman_chowdhury_durghotona_2024,
	title = {Durghotona {GPT}: {A} {Web} {Scraping} and {Large} {Language} {Model} {Based} {Framework} to {Generate} {Road} {Accident} {Dataset} {Automatically} in {Bangladesh}},
	doi = {10.1109/ICCIT64611.2024.11021969},
	abstract = {Road accidents pose significant concerns globally. It leads to large financial losses, injuries, disabilities and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named ‘Durghotona GPT’ that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers— Prothom Alo, Dhaka Tribune and The Daily Star leveraging web scraping techniques. The collected news was then processed using the newest available LLMs. These LLMs are: GPT-4, GPT-3.5 and Llama-3. The framework efficiently extracts relevant information, categorizes reports and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors and communication gaps. The authors’ evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 96\% accuracy in the authors’ evaluation. So, it can be considered as a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning and public health. The authors also developed an interface for ‘Durghotona GPT’ for easy use as a part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.},
	booktitle = {2024 27th {International} {Conference} on {Computer} and {Information} {Technology} ({ICCIT})},
	author = {Bin Zaman Chowdhury, Md Thamed and Islam, Md. Ridwanul and Hossain, Moazzem},
	month = dec,
	year = {2024},
	note = {ISSN: 2474-9656},
	keywords = {Accuracy, artificial intelligence, automation, data analytics, data mining, Data mining, large language models, Large language models, machine learning, Machine learning, newspaper analysis, Prompt engineering, Public healthcare, Retrieval augmented generation, road accident, Road accidents, Safety, Urban planning, web scraping},
	pages = {50--55},
}

@inproceedings{jing_when_2025,
	title = {When {Large} {Language} {Models} {Meet} {Vector} {Databases}: {A} {Survey}},
	doi = {10.1109/AIxMM62960.2025.00008},
	abstract = {This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration’s impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.},
	booktitle = {2025 {Conference} on {Artificial} {Intelligence} x {Multimedia} ({AIxMM})},
	author = {Jing, Zhi and Su, Yongye and Han, Yikun},
	month = feb,
	year = {2025},
	keywords = {Costs, Data handling, Data mining, Large language models, Large Language Models, Multimedia databases, Prototypes, Retrieval augmented generation, Retrieval-Augmented Generation, Surveys, Systematic literature review, Vector Databases, Vectors},
	pages = {7--13},
}

@inproceedings{farooqui_large_2025,
	title = {Large {Language} {Models} for {Mental} {Health} {Counselling}},
	doi = {10.1109/CONIT65521.2025.11167888},
	abstract = {Mental health challenges are becoming increasingly prevalent worldwide, there is a growing demand for scalable and accessible support systems. This paper explores the application of Large Language Models (LLMs) in providing personalized counselling for mental health. This study evaluates the performance of three pre-trained models—T5-Small, T5-Base and BART-large on a dataset of mental health counselling queries. Metrics such as BLEU, ROUGE, and perplexity were used to assess the quality of model-generated responses before and after fine-tuning. Further, we evaluated the finetuned models using BERTScore. Results reveal that fine-tuning significantly improves model coherence and relevance, with noticeable reductions in perplexity. The findings highlight the potential of LLMs in assisting mental health professionals by providing empathetic and contextually relevant responses.},
	booktitle = {2025 5th {International} {Conference} on {Intelligent} {Technologies} ({CONIT})},
	author = {Farooqui, Maaz Ahmad and Nehra, Vibha and Sinha, Ayushi},
	month = jun,
	year = {2025},
	keywords = {BART, BERTScore, BLEU, Coherence, Context modeling, Employee welfare, Explainable AI, Fine-Tuning, Large language models, Large Language Models, Mental health, Mental Health Counselling, Perplexity, Prevention and mitigation, Retrieval augmented generation, ROUGE, Semantics, Sensitivity, T5-Base, T5-Small},
	pages = {1--8},
}

@inproceedings{zhou_research_2024,
	title = {Research on {AI}-assisted design method of substation based on {LLM}+{RAG}},
	volume = {2024},
	doi = {10.1049/icp.2024.4307},
	abstract = {In the digital age, in order to solve the problems of illusion, timeliness, data security, etc. of large language model technology (LLM), retrieval-augmented generation (RAG) has emerged as a feasible solution. Therefore, the architecture concept of LLM+RAG was proposed, in order to assist the design work as an efficient AI tool. Therefore, this paper adopts a layered design, and realizes the end-to-end process from professional knowledge acquisition to intelligent suggestion generation through six core functional layers: knowledge base construction layer, retrieval enhancement layer, context construction layer, LLM interaction layer, response optimization layer and interaction integration layer. In the design process, various language processing, retrieval, context model generation and other technologies are combined to complete a feasible RAG+LLM AI- assisted design architecture. Finally, through the real-time updated professional knowledge base, multi-way recall mechanism and chain reasoning method, the tool can provide reliable design suggestions for design work and improve the efficiency and quality of substation design. This study provides a design idea of a reliable RAG+LLM architecture, and finally completes the construction of an AI-assisted design tool using this architecture.},
	booktitle = {6th {International} {Conference} on {Artificial} {Intelligence} and {Advanced} {Manufacturing} ({AIAM} 2024)},
	author = {Zhou, Liang and Lv, Zhengyu and Zhang, Xinhui and Hu, Junpeng and Pan, Longfei and Pang, Xing and Shi, Jianyong},
	month = oct,
	year = {2024},
	pages = {728--734},
}

@article{sun_taming_2025,
	title = {Taming {Unleashed} {Large} {Language} {Models} {With} {Blockchain} for {Massive} {Personalized} {Reliable} {Healthcare}},
	volume = {29},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2025.3528526},
	abstract = {The digital health field's pursuit of massive, personalized healthcare continuously faces constraints from doctors' resources and capacity limitations. Recently, the emergence of large language models (LLMs), with their remarkable comprehension and processing abilities, has revolutionized digital health and enhanced massive, personalized healthcare. Although these LLMs have achieved significant advancements, they have also introduced inevitable hallucinations, which impact patient safety when used in massive applications. To address these challenges, this study proposes a digital hospital for a massive, personalized, reliable healthcare service named the Chat Chain-Brain-based Doctor (CHATCBD). In addition, this study transforms the LLM-based diagnostic process into a digital hospital architecture, designs a controllable AI agents framework, and develops a self-audit mechanism to enhance their reliability. The proposed CHATCBD uses blockchain technology to decentralize external regulation of the LLMs' personalized diagnoses. It introduces a blockchain-based personalized routing management mechanism to improve patient-centered decision-making and designs a blockchain-based audit framework based on a proposed mathematical model that ensures both the professionalism and honesty of audits, serving as a safety net for addressing LLM hallucinations. The results of extensive experiments conducted on 13 datasets from multiple perspectives demonstrate that the proposed CHATCBD system can significantly enhance the capabilities of LLMs in personalized healthcare.},
	number = {6},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Sun, Lianshan and Liu, Diandong and Wang, Maoxue and Han, Yongyi and Zhang, Yanqing and Zhou, Biwei and Ren, Yi and zhu, Peng},
	month = jun,
	year = {2025},
	keywords = {Agent, blockchain, Blockchains, Cognition, Collaboration, digital health, Electronic healthcare, History, Hospitals, large language model, Medical diagnostic imaging, Medical services, reliability, Reliability, Retrieval augmented generation},
	pages = {4498--4511},
}

@inproceedings{pandini_exploratory_2025,
	title = {An {Exploratory} {Study} on {Architectural} {Smell} {Refactoring} {Using} {Large} {Languages} {Models}},
	doi = {10.1109/ICSA-C65153.2025.00070},
	abstract = {Architectural smells are abundant in codebases and regularly hinder the development of stable and maintainable code. Understanding and removing these elements can consume a huge amount of developers' time, who often need to prioritize implementing new features. This causes a substantial increase in Technical Debt, compromising the scalability and maintainability of the codebases, at time bringing the development to a standstill. Meanwhile, the use of Large Language Models for small error correction is constantly growing, bringing the attention of an ever-wider audience to these technologies. This study explores a first approach to use Large Language Models to suggest refactoring for architectural smells, with a focus on Cyclic Dependencies smells. We study the use of detailed prompt and Retrieval-Augmented Generation (RAG) to enhance LLMs, and we study local vs cloud LLMs. The results are promising, also validated with a series of interviews with students and developers, and highlight how additional and precise context is key to enhance the use of LLMs to propose refactoring suggestions. A multi-agent approach seems to be more suited when increasing the complexity of the smells.},
	booktitle = {2025 {IEEE} 22nd {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Pandini, Gabriele and Martini, Antonio and Videsjorden, Adela Nedisan and Fontana, Francesca Arcelli},
	month = mar,
	year = {2025},
	note = {ISSN: 2768-4288},
	keywords = {Architectural Smell, Codes, Complexity theory, Error correction, Interviews, Large language models, LLM, RAG, Refactoring, Retrieval augmented generation, Scalability, Software architecture},
	pages = {462--471},
}

@book{palmer_notitle_2024,
	isbn = {978-1-83620-724-5},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10769331},
	abstract = {Master retrieval-augmented generation architecture and fine-tune your AI stack, along with discovering real-world use cases and best practices to create powerful AI appsKey FeaturesGet to grips with the fundamentals of LLMs, vector databases, and Python frameworksImplement effective retrieval-augmented generation strategies with MongoDB AtlasOptimize AI models for performance and accuracy with model compression and deployment optimizationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe era of generative AI is upon us, and this book serves as a roadmap to harness its full potential. With its help, you’ll learn the core components of the AI stack: large language models (LLMs), vector databases, and Python frameworks, and see how these technologies work together to create intelligent applications. The chapters will help you discover best practices for data preparation, model selection, and fine-tuning, and teach you advanced techniques such as retrieval-augmented generation (RAG) to overcome common challenges, such as hallucinations and data leakage. You’ll get a solid understanding of vector databases, implement effective vector search strategies, refine models for accuracy, and optimize performance to achieve impactful results. You’ll also identify and address AI failures to ensure your applications deliver reliable and valuable results. By evaluating and improving the output of LLMs, you’ll be able to enhance their performance and relevance. By the end of this book, you’ll be well-equipped to build sophisticated AI applications that deliver real-world value.What you will learnUnderstand the architecture and components of the generative AI stackExplore the role of vector databases in enhancing AI applicationsMaster Python frameworks for AI developmentImplement Vector Search in AI applicationsFind out how to effectively evaluate LLM outputOvercome common failures and challenges in AI developmentWho this book is forThis book is for software engineers and developers looking to build intelligent applications using generative AI. While the book is suitable for beginners, a basic understanding of Python programming is required to make the most of it.},
	publisher = {Packt Publishing},
	author = {Palmer, Rachelle and Perlmutter, Ben and Gangadhar, Ashwin and Larew, Nicholas and Narváez, Sigfrido and Rueckstiess, Thomas and Weller, Henry and Alake, Richmond and Ranjan, Shubham},
	year = {2024},
	note = {Publication Title: Building AI Intensive Python Applications: Create intelligent apps with LLMs and vector databases},
}

@inproceedings{russo_scaling_2024,
	title = {Scaling {LLM}-{Based} {Knowledge} {Graph} {Generation}: {A} {Case} {Study} of {Italian} {Geopolitical} {News}},
	doi = {10.1109/BigData62323.2024.10825937},
	abstract = {Geopolitical news provides vast amounts of information essential for understanding international relations and political events. However, organizing this information into a coherent, structured format poses challenges due to the complexity and dynamic nature of the domain. This paper introduces a scalable system leveraging Large Language Models to build continuously updated Knowledge Graphs from Italian geopolitical news. The system features a modular architecture, including a Collector Node for scalable article extraction, a Redis-based reliable queue to manage large-scale data ingestion, and a Named Entity Recognition/Relation Extraction Engine to standardize entity-relation triples. The framework addresses key challenges, such as continuous updating and hallucination mitigation, ensuring the reliability of the graph. Our evaluations demonstrate significant improvements in scalability, uniformity of extracted triples, and graph accuracy, making this architecture particularly suitable for real-time geopolitical analysis.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Russo, Diego and Orlando, Gian Marco and Romano, Antonio and Riccio, Giuseppe and Gatta, Valerio La and Postiglione, Marco and Moscato, Vincenzo},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Data mining, Engines, Feature extraction, International relations, Knowledge Graph (KG), Knowledge graphs, Large language models, Large Language Models (LLMs), Named Entity Recognition (NER), Prevention and mitigation, Real-time systems, Relation Extraction (RE), Reliability, Retrieval-Augmented Generation (RAG), Scalability},
	pages = {3494--3497},
}

@inproceedings{tihanyi_cybermetric_2024,
	title = {{CyberMetric}: {A} {Benchmark} {Dataset} based on {Retrieval}-{Augmented} {Generation} for {Evaluating} {LLMs} in {Cybersecurity} {Knowledge}},
	doi = {10.1109/CSR61664.2024.10679494},
	abstract = {Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different cybersecurity fields, including topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. The research community needs a diverse, accurate, and up-to-date dataset to test the general knowledge of LLMs in cybersecurity. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q\&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions, respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b. The CyberMetric dataset is publicly available for the research community and can be downloaded from the projects' website: https://github.com/CyberMetric.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Cyber} {Security} and {Resilience} ({CSR})},
	author = {Tihanyi, Norbert and Ferrag, Mohamed Amine and Jain, Ridhi and Bisztray, Tamas and Debbah, Merouane},
	month = sep,
	year = {2024},
	keywords = {Accuracy, Benchmark testing, Computer security, NIST Standards, Problem-solving, Reverse engineering, Risk management},
	pages = {296--302},
}

@inproceedings{du_sc-telco_2024,
	title = {{SC}-{Telco} {RAG}: {Enhancing} {RAG} through {Structured} {Contexts} in {Telecommunications}},
	doi = {10.1109/GCWkshp64532.2024.11100862},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional language understanding and generation capabilities in the field of knowledge question-answering, with applications spanning across various industries. However, when it comes to the telecommunications domain, the complexity of the standards and specifications poses a significant challenge for LLMs. The sheer volume of standards, coupled with the frequent use of specialized terminology and concepts, necessitates a higher level of comprehension and accuracy in responses from the LLMs. This paper introduces the SC-Telco Retrieval-Augmented Generation (RAG) framework, a specialized approach designed to navigate these challenges effectively. By integrating structured contexts into the retrieval process and employing a two-stage fine-tuning technique, this framework significantly enhances LLMs’ question-answering accuracy within the telecommunications domain. The innovative methodology has been validated through notable success in "specializing Large Language Models for Telecom Networks by ITU AI/ML in 5G Challenge", achieving an accuracy of 80.75\% and demonstrating the framework’s practical impact and domain-specific effectiveness.},
	booktitle = {2024 {IEEE} {Globecom} {Workshops} ({GC} {Wkshps})},
	author = {Du, Junjie and Ding, Jianbing and Li, Jingyi and Wang, Xidong and Ye, Xiaozhou and Ouyang, Ye},
	month = dec,
	year = {2024},
	note = {ISSN: 2166-0077},
	keywords = {5G mobile communication, Accuracy, Adaptation models, Context modeling, Data models, Domain-specific question-answering, Fine-tuning, Industries, ITU, Large language models, Large Language Models, Retrievalaugmented generation, Standards, Telecommunications},
	pages = {1--6},
}

@inproceedings{jabarulla_medblock-bot_2025,
	title = {{MedBlock}-{Bot}: {A} {Blockchain}-{Enabled} {RAG} {System} for {Providing} {Feedback} to {Large} {Language} {Models} {Accessing} {Pediatric} {Clinical} {Guidelines}},
	doi = {10.1109/CBMS65348.2025.00172},
	abstract = {Accessing reliable clinical knowledge quickly is an everyday challenge for clinicians. Large Language Models (LLMs) can assist healthcare professionals by providing this knowledge, but their responses often deviate from expert consensus or are not up to date necessitating reliable validation and possible correction. To address this, we introduce MedBlock-Bot, an interactive Streamlit-based system integrating a blockchain-enabled Retrieval-Augmented Generation (RAG) framework for expert-driven assessment and immutable feedback storage within a permissioned consortium network. Unlike traditional feedback mechanisms that may be altered or lost, MedBlock-Bot employs smart contracts to securely store and verify any feedback, ensuring transparency and auditability. We evaluated the system using three opensource LLMs-BioMistral, HippoMistral, and LLaMa 3.1-on clinical guideline interpretation for neonates with hypoplastic left heart syndrome. Human experts assessed model responses based on accuracy and relevance, revealing variations in adherence to the guideline knowledge. Additionally, deploying the blockchain component in a local permissioned environment (Ganache) ensured efficient transaction processing and tamperproof feedback retrieval without gas cost concerns. Our results demonstrate the integration of blockchain for LLM feedback review enhancing trust, accountability, and structured knowledge retention. Clinicians can access past expert assessments for validation, while developers can leverage this feedback for potential model refinement. Taking the long-term impact into account this approach targets towards a reliable and dynamic representation of clinical knowledge and consensus. Open-Source Code: https://github.com/yaseen28/MedBlock-Bot},
	booktitle = {2025 {IEEE} 38th {International} {Symposium} on {Computer}-{Based} {Medical} {Systems} ({CBMS})},
	author = {Jabarulla, Mohamed Yaseen and Oeltze-Jafra, Steffen and Beerbaum, Philipp and Uden, Theodor},
	month = jun,
	year = {2025},
	note = {ISSN: 2372-9198},
	keywords = {Accuracy, Blockchain, Blockchains, Clinical care, Computational modeling, Guidelines, Large language models, Medical language models, Pediatric cardiology, RAG, Reliability, Retrieval augmented generation, Reviews, Scalability, Smart contracts},
	pages = {845--850},
}

@inproceedings{houssel_towards_2024,
	title = {Towards {Explainable} {Network} {Intrusion} {Detection} using {Large} {Language} {Models}},
	doi = {10.1109/BDCAT63179.2024.00021},
	abstract = {Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents. However, their applicability to threat detection problems remains unclear. This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explain-ability. Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS. Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments. Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge. Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS. Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.},
	booktitle = {2024 {IEEE}/{ACM} {International} {Conference} on {Big} {Data} {Computing}, {Applications} and {Technologies} ({BDCAT})},
	author = {Houssel, Paul R. B. and Singh, Priyanka and Layeghy, Siamak and Portmann, Marius},
	month = dec,
	year = {2024},
	keywords = {Computational modeling, Computer architecture, Decision making, Explainability, Large Language Model, Large language models, Natural language processing, Network intrusion detection, Network Intrusion Detection System, Retrieval augmented generation, Threat assessment, Time complexity, Transformers},
	pages = {67--72},
}

@inproceedings{wang_research_2025,
	title = {Research on {Intelligent} {Recommendation} {System} of {Audit} {Regulations} {Based} on {Large} {Language} {Model} {Driven} by {Multi}-{Task} {Fine}-{Tuning}},
	doi = {10.1109/ICAACE65325.2025.11020300},
	abstract = {The accuracy and efficiency of audit judgment heavily depend on the intelligence level of regulatory recommendation systems. Traditional methods face severe challenges due to issues such as dynamic updates of regulations and complex cross-domain associations. This paper proposes an intelligent regulatory recommendation framework that integrates multitask fine-tuning with retrieval-enhanced generation (RAG). By constructing a three-level task instruction set covering concept understanding, problem classification, and regulation matching (27,600 data points), and combining lightweight fine-tuning techniques (LoRA) to optimize the domain adaptability of the Qwen-7B model, experiments show that the fine-tuned model significantly outperforms baseline models in tasks such as audit issue summarization (ROUGE-L increased by 201\%) and regulation classification (F1 reached 0.9977). After integrating RAG technology, the recommendation accuracy improves to 92.6\%, and hallucination problems decrease by 64\%. This study provides theoretical paradigms and practical tools for designing intelligent audit systems, promoting the transition of audit processes from “human-led” to “human-machine collaboration.”.},
	booktitle = {2025 8th {International} {Conference} on {Advanced} {Algorithms} and {Control} {Engineering} ({ICAACE})},
	author = {Wang, Yang},
	month = mar,
	year = {2025},
	keywords = {Accuracy, Adaptation models, Collaboration, Data models, Human-machine systems, intelligent auditing, large language models, Large language models, multitask fine-tuning, Recommender systems, Regulation, regulatory recommendations, retrieval-enhanced generation, Terminology, Training},
	pages = {2586--2590},
}

@inproceedings{zhu_enhancing_2024,
	title = {Enhancing {Supply} {Chain} {Efficiency} {Through} {Retrieve}-{Augmented} {Generation} {Approach} in {Large} {Language} {Models}},
	doi = {10.1109/BigDataService62917.2024.00025},
	abstract = {This paper delves into the fascinating integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) for optimizing supply chain management operations. RAG combines the robust retrieval capabilities of information retrieval systems with the generative prowess of neural language models to create a powerful tool that bolsters data protection while expanding the knowledge base to capture supply chain intricacies. This innovative methodology revolves around a dual-component system that employs a retrieval module to pinpoint relevant information from a knowledge base, while a generation module crafts contextualized responses using large language models. Through iterative retrieval strategies and tailored chunk optimization techniques, RAG enables contextualized analysis, predictive insights, and data-driven decision-making that streamlines processes from demand forecasting to inventory optimization. An experimental setup mimicking enterprise data classification assesses RAG's efficacy, employing recursive retrieval, multi-hop querying, and integration of generative and retrieval processes. Results showcase RAG's potential to revolutionize supply chain logistics, enhancing operational agility, minimizing disruptions, and fortifying data security. The impacts span improved forecasting accuracy, inventory level optimization, supplier risk assessment, and comprehensive supply chain reporting. However, RAG necessitates stringent ethical considerations and robust countermeasures against exploitation. Future work centers on system scalability, advanced evaluation metrics, and interdisciplinary collaboration between machine learning, retrieval systems, and supply chain domains. Overall, this paper presents a groundbreaking approach to optimizing supply chain management operations that could significantly impact the industry's future.},
	booktitle = {2024 {IEEE} 10th {International} {Conference} on {Big} {Data} {Computing} {Service} and {Machine} {Learning} {Applications} ({BigDataService})},
	author = {Zhu, Beilei and Vuppalapati, Chandrasekar},
	month = jul,
	year = {2024},
	note = {ISSN: 2690-828X},
	keywords = {Decision making, Deep Learning, Ethics, Knowledge based systems, Large language models, LLM, Machine learning, Optimization, RAG, Scalability, Supply chain management, Supply Chain Operations, Supply chains, Technological innovation, Unstructured Big Data},
	pages = {117--121},
}

@article{setyawan_soekamto_queries_2025,
	title = {From {Queries} to {Courses}: {SKYRAG}’s {Revolution} in {Learning} {Path} {Generation} via {Keyword}-{Based} {Document} {Retrieval}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3535618},
	abstract = {Large Language Models (LLMs) hold immense potential for transforming education by automating the generation of personalized learning paths. However, traditional LLMs often suffer from hallucinations and content irrelevance. To address these challenges, we propose SKYRAG, a Separated Keyword Retrieval Augmentation Generation system that enhances the learning path generation process by integrating advanced retrieval mechanisms with LLMs. SKYRAG retrieves relevant course materials from Massive Open Online Course (MOOC) platforms, aligning them with individual learner profiles to provide personalized and coherent learning paths. Compared with Naïve RAG, SKYRAG demonstrates superior performance in terms of accuracy, relevance, and user satisfaction, as confirmed by human evaluations across four domains. By improving retrieval precision and addressing the limitations of traditional methods, SKYRAG represents a significant advancement in educational technology. This study contributes to the growing body of research on AI-driven learning systems and highlights SKYRAG’s potential for widespread adoption in dynamic educational environments.},
	journal = {IEEE Access},
	author = {Setyawan Soekamto, Yosua and Christopher Limanjaya, Leonard and Kaleb Purwanto, Yoshua and Kang, Dae-Ki},
	year = {2025},
	keywords = {Accuracy, Computational modeling, Context modeling, Data models, educational technology, human-centric design, large language models, Large language models, Mathematical models, personalized learning path, Retrieval augmented generation, Semantics, Training data, Transformers},
	pages = {21434--21455},
}

@article{toprani_llm_2025,
	title = {{LLM} {Agentic} {Workflow} for {Automated} {Vulnerability} {Detection} and {Remediation} in {Infrastructure}-as-{Code}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3560911},
	abstract = {This paper presents a multi-agent, AI-driven strategy employing Large Language Models (LLMs), retrieval-augmented generation, and a continuously updated knowledge base for the detection and remediation of security vulnerabilities win cloud frameworks. By examining Infrastructure as Code (IaC) templates alongside pertinent best-practice snippets, the system discerns context-specific misconfigurations commonly overlooked by static tools, achieving a detection rate of 85\% with some occurrences of false positives. Automated remediation guidance, anchored in current security standards, provides actionable solutions that seamlessly integrate into standard continuous integration/continuous development (CI/CD) workflows. Experimental results indicate the solution’s efficacy and scalability, heralding a proactive, context-aware approach to IaC security.},
	journal = {IEEE Access},
	author = {Toprani, Dheer and Madisetti, Vijay K.},
	year = {2025},
	keywords = {Best practices, CI/CD, Cognition, Infrastructure-as-code, large language models, Large language models, LLM workflows, Organizations, Retrieval augmented generation, Runtime, Scalability, Security, security automation, Static analysis, Vectors, vulnerability detection},
	pages = {69175--69181},
}

@inproceedings{feng_intelligent_2025,
	title = {Intelligent {Automation} of {Network} {Security} {Operations} via {Intention}-{Driven} {Agents} and {Large} {Language} {Models}},
	doi = {10.1109/CISAT66811.2025.11181948},
	abstract = {As modern networks grow in scale and complexity, traditional manual security operations struggle to maintain efficiency, consistency, and adaptability. This paper proposes a novel framework that leverages large language models (LLMs), intention understanding, and autonomous agents to achieve end-to-end automation of network-security operations. By combining the Model-Context Protocol (MCP) with retrieval-augmented generation (RAG), the system interprets human intents expressed in natural language and dynamically maps them to executable security workflows. Key operational functions including configuration inspection, vulnerability hardening, topology mapping, fault diagnosis, and compliance patrol are abstracted into intention-driven tasks executed by agents. A prototype deployed in an enterprise-scale environment shows mean task-completion-time reductions of 82.37\% and audit-accuracy gains of 6–15\% over expert baselines. These results mark a step toward fully autonomous, intention-aware network-security management.},
	booktitle = {2025 8th {International} {Conference} on {Computer} {Information} {Science} and {Application} {Technology} ({CISAT})},
	author = {Feng, Guocong and Pan, Yuan and Zhang, Chunmei and Huang, Kaitian},
	month = jul,
	year = {2025},
	keywords = {Inspection, Intelligent automation, Intention-driven agents, large language models, Large language models, Model-Context Protocol, Natural languages, Network security, Network topology, Protocols, Prototypes, Retrieval augmented generation, retrieval-augmented generation, security-operations automation, Topology},
	pages = {220--224},
}

@article{guo_intent-based_2025,
	title = {Intent-{Based} {Autonomous} {Network} {Framework} {Guided} by {Large} {Language} {Model}},
	volume = {22},
	issn = {1558-3783},
	doi = {10.1109/TASE.2025.3610906},
	abstract = {With the rapid development of next-generation networks, the highly heterogeneous and dynamic nature of networks poses significant challenges for automated network management. Autonomous Network (AN), as a new network paradigm, aims to provide customers with a zero-wait, zero-touch, and zero-fault experience. AN facilitates network management through intent-driven interactions and provides on-demand resource orchestration and service scheduling. However, accurately translating user intents into commands and allocating resources on demand for services remain significant challenges for AN. Therefore, this paper proposes IAN, an intent-based AN framework guided by the Large Language Model (LLM). In the intent translation phase, IAN introduces RAG to enhance command generation quality by retrieving from manuals. In the resource allocation phase, the method utilizes LLM to analyze service characteristics, thereby guiding the training and inference of the resource allocation model to effectively distribute resources uniformly across emerging services. Experimental results demonstrate that IAN improves performance by 52.66\% in intent translation tasks and increases overall gain by 33.57\% in resource allocation tasks compared to other models. Note to Practitioners—In this study, the goal is to design an intent-driven autonomous framework. This framework translates user-input natural language intents into network configurations and continuously updates these configurations based on computations of downstream network states to achieve on-demand resource allocation. To accomplish this, we designed two main modules: the Intent Translator and the Resource Allocator. In the Intent Translator, we propose an LLM-empowered intent translation approach. The introduction of RAG enhances the performance of LLMs in intent translation tasks, effectively mitigating hallucinations and generating higher-quality, deployable commands. In the Resource Allocator, we present an LLM-guided resource allocation method. Leveraging their strong contextual understanding, LLMs can better analyze the attributes of emerging services, thereby guiding the training and inference of resource allocation models. Through this approach, the proposed framework achieves high-quality intent translation and efficient resource allocation, enabling intent-driven network autonomy.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Guo, Lingqi and Zhang, Lei and Wang, Jingyu and Wu, Jianyu and Yan, Yuhang and Sun, Haifeng and He, Bo and Qi, Qi and Liao, Jianxin},
	year = {2025},
	keywords = {Analytical models, Autonomous network, Autonomous networks, Computer architecture, Feedback loop, intent translation, large language model, Large language models, Manuals, resource allocation, Resource management, Training, Translation, Videoconferences},
	pages = {22185--22197},
}

@inproceedings{wang_multi-agent_2025,
	title = {Multi-{Agent} {RAG}-{Based} {Framework} for {Personalized} {Interaction} with {Large} {Language} {Models}},
	doi = {10.1109/ISAEECE66033.2025.11159949},
	abstract = {With the breakthrough advancements of large language models (LLMs) in the field of natural language processing, their applications in tasks such as text generation and understanding have become increasingly widespread. However, in practical industrial management scenarios-particularly within traditional sectors such as the electric power industry-tasks such as information integration and data summarization still largely depend on manual processing or conventional human-computer interaction methods. These approaches commonly suffer from low summarization efficiency, heavy repetitive workload, high communication costs, and inaccurate information transmission. This paper proposes a personalized interaction method for large language models based on a multi-agent RAG framework. By incorporating hierarchical text agents, the framework enables more efficient and accurate integration of industry-specific information and the generation of personalized work summaries. It employs a hybrid mechanism of fixed and dynamic agents to generate text content tailored to the needs of multi-level managers in response to user feedback. An industry-specific dataset, ElePWL (Electric Power Work Logs), was constructed, and leading Chinese LLMs-ChatGLM3, DeepSeek-V3, and Tongyi Qianwen-Turbo-were deployed for ablation experiments. Control and experimental groups were established to evaluate the semantic similarity between generated summaries and reference texts. The results demonstrate that the proposed framework consistently improves the semantic alignment of personalized reports with standard references.},
	booktitle = {2025 10th {International} {Symposium} on {Advances} in {Electrical}, {Electronics} and {Computer} {Engineering} ({ISAEECE})},
	author = {Wang, Qian and Duan, Wuxuan and Wang, Haiyan and Luan, Zhirong and Wang, Yu and Wang, Yulu},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Chinese LLMs, Costs, Human computer interaction, Hybrid power systems, Information processing, Large language models, Manuals, multi-agent, Natural language processing, personalized interaction method, RAG framework, Semantics, Standards},
	pages = {675--680},
}

@inproceedings{le_framework_2024,
	title = {A {Framework} for {Vietnamese} {Question}-{Answering} in {Law} {Domain}},
	doi = {10.1109/DSC63484.2024.00108},
	abstract = {The popularity of building question-answering systems using Large Language Models (LLMs) has surged. Many projects leverage the Retrieval Augmented Generated (RAG) technique, involving two basic steps: retrieval and reading. In this research, we introduce an enhanced approach, termed CRRR (Classifier - Rewrite - Retriever - Reader), tailored for the Vietnamese legal domain. Our framework begins with a classifier to discern whether a given question pertains to law. Rather than solely focusing on refining LLM or embedding models for better responses, we prioritize enhancing the process of rewriting input questions. These rewritten queries are then utilized by a search engine to gather external information, aiding the reader in generating answers. The rewriter component is trainable using reinforcement learning, incorporating feedback from both human and AI sources.},
	booktitle = {2024 {IEEE} 9th {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	author = {Le, Thang V. Q. and Vu, Dinh-Hong and Pham, Van-Huy and Le, Anh-Cuong and Nguyen, Nguyen P.},
	month = aug,
	year = {2024},
	keywords = {Cyberspace, Data models, Data science, Focusing, Large language models, Large Language Models, Law, question-answering, RAG, Refining, Reinforcement learning, Retrieval augmented generation, Search engines},
	pages = {726--731},
}

@inproceedings{tamanna_chatgpt_2025,
	title = {Chatgpt {Inaccuracy} {Mitigation} {During} {Technical} {Report} {Understanding}: {Are} we {There} {Yet}?},
	doi = {10.1109/ICSE55347.2025.00145},
	abstract = {Hallucinations, the tendency to produce irrelevant/incorrect responses, are prevalent concerns in generative AIbased tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q\&A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4 \% correct when producing answers to the questions, due to two reasons 1) limitations to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME (ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3\% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME.},
	booktitle = {2025 {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Tamanna, Salma Begum and Uddin, Gias and Wang, Song and Xia, Lan and Zhang, Longyu},
	month = apr,
	year = {2025},
	note = {ISSN: 1558-1225},
	keywords = {Benchmark testing, Chatbots, ChatGPT, Codes, Computer bugs, Hallucination, Prevention and mitigation, Software, Software engineering, Software Issue Reports, Surveys, Technological innovation, Terminology},
	pages = {2290--2302},
}

@article{salek_large_2025,
	title = {A {Large} {Language} {Model}-{Supported} {Threat} {Modeling} {Framework} for {Transportation} {Cyber}-{Physical} {Systems}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3603580},
	abstract = {Increased reliance on automation and connectivity exposes transportation cyber-physical systems (CPS) to many cyber vulnerabilities. Existing threat modeling frameworks are often narrow in scope, labor-intensive, and require substantial cybersecurity expertise. To this end, we introduce the Transportation Cybersecurity and Resiliency Threat Modeling Framework (TraCR-TMF), a large language model (LLM)-based threat modeling framework for transportation CPS that requires limited cybersecurity expert intervention. TraCR-TMF identifies threats, potential attack techniques (i.e., methods to exploit vulnerabilities), and relevant countermeasures (e.g., attack detection and mitigation strategies) for transportation CPS. Three LLM-based approaches support these identifications: (i) a retrieval-augmented generation approach requiring no cybersecurity expert intervention, (ii) an in-context learning approach with low intervention from cybersecurity experts, and (iii) a supervised fine-tuning approach with moderate cybersecurity expert intervention. TraCR-TMF offers LLM-based attack path identification for critical assets based on vulnerabilities across different transportation CPS entities. Additionally, it incorporates the Common Vulnerability Scoring System (CVSS) scores of previously exploited vulnerabilities to prioritize threat mitigations. The framework was evaluated through two use cases. First, the framework identified relevant attack techniques for various transportation CPS applications, about 73\% of which were validated by cybersecurity experts as correct. Second, the framework was used to identify attack paths for a target asset in a real-world cyberattack incident. TraCR-TMF successfully predicted exploitations, like lateral movement of adversaries, data exfiltration, and data encryption for ransomware, as reported in the incident. These findings demonstrate TraCR-TMF’s efficacy in transportation CPS threat modeling while reducing the need for extensive involvement of cybersecurity experts. To facilitate real-world adoption, all our codes are shared via an open-source repository.},
	journal = {IEEE Access},
	author = {Salek, M Sabbir and Chowdhury, Mashrur and Munir, Muhaimin Bin and Cai, Yuchen and Hasan, Mohammad Imtiaz and Tine, Jean-Michel and Khan, Latifur and Rahman, Mizanur},
	year = {2025},
	keywords = {Analytical models, Computer security, Cyberattack, cybersecurity, Data models, intelligent transportation systems, large language model, Large language models, Mathematical models, Prevention and mitigation, Resilience, Threat modeling, Transportation, transportation cyber-physical systems},
	pages = {163046--163070},
}

@inproceedings{li_mccoder_2025,
	title = {{MCCoder}: {Streamlining} {Motion} {Control} with {LLM}-{Assisted} {Code} {Generation} and {Rigorous} {Verification}},
	doi = {10.1109/CASE58245.2025.11163835},
	abstract = {Large Language Models (LLMs) have demonstrated significant potential in code generation. However, in the factory automation sector—particularly motion control—manual programming, alongside inefficient and unsafe debugging practices, remains prevalent. This stems from the complex interplay of mechanical and electrical systems and stringent safety requirements. Moreover, most current AI-assisted motion control programming efforts focus on PLCs, with little attention given to high-level languages and function libraries. To address these challenges, we introduce MCCoder, an LLM-powered system tailored for generating motion control code, integrated with a soft-motion controller. MCCoder improves code generation through a structured workflow that combines multitask decomposition, hybrid retrieval-augmented generation (RAG), and iterative self-correction, utilizing a well-established motion library. Additionally, it integrates a 3D simulator for intuitive motion validation and logs of full motion trajectories for data verification, significantly enhancing accuracy and safety. In the absence of benchmark datasets and metrics tailored for evaluating motion control code generation, we propose MCEVAL, a dataset spanning motion tasks of varying complexity. Experiments show that MCCoder outperforms baseline models using Advanced RAG, achieving an overall performance gain of 33.09\% and a 131.77\% improvement on complex tasks in the MCEVAL dataset. MCCoder is publicly available at https://github.com/MCCodeAI/MCCoder.},
	booktitle = {2025 {IEEE} 21st {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Li, Yin and Wang, Liangwei and Piao, Shiyuan and Yang, Boo-Ho and Li, Ziyue and Zeng, Wei and Tsung, Fugee},
	month = aug,
	year = {2025},
	note = {ISSN: 2161-8089},
	keywords = {Codes, Complexity theory, Libraries, Motion control, Programming, Prompt engineering, Retrieval augmented generation, Safety, Three-dimensional displays, Trajectory},
	pages = {1597--1603},
}

@inproceedings{sandaruwan_integrating_2025,
	title = {Integrating {Large} {Language} {Models} for {Automated} {Vulnerability} {Scanning} and {Reporting} in {Network} {Hosts}},
	doi = {10.1109/SCSE65633.2025.11031059},
	abstract = {This research explores integrating Large Language Models (LLMs) like GPT-4 and Claude 3.5 into cybersecurity vulnerability scanning to enhance automation and effectiveness. Current tools’ reliance on manual updates and human expertise is highlighted. A literature review identified effective modular architectures and Retrieval-Augmented Generation (RAG) systems for grounding LLMs with cybersecurity knowledge.A Proof of Concept (PoC) tool, developed in Python and tested on the Metasploitable system, evaluated three LLM implementations: GPT-4 Omni, GPT-4 Omni with RAG, and Claude 3.5 Sonnet. The results showed GPT-4 Omni outperformed Claude 3.5, with RAG significantly improving performance. The tool achieved 80\% accuracy in identifying and resolving vulnerabilities.The study underscores the potential of LLMs to revolutionize vulnerability scanning, making advanced cybersecurity more accessible and effective. Future work should address limitations, enable interactive sessions, create new exploits, and tackle more complex challenges.},
	booktitle = {2025 {International} {Research} {Conference} on {Smart} {Computing} and {Systems} {Engineering} ({SCSE})},
	author = {Sandaruwan, M. Tharuka and Wijayanayake, Janaka and Senanayake, Janaka},
	month = apr,
	year = {2025},
	note = {ISSN: 2997-7363},
	keywords = {Accuracy, Automation, Computer architecture, Computer security, cybersecurity, exploiting, Grounding, Large language models, llm, Manuals, Retrieval augmented generation, scanning, Systematic literature review, Systems engineering and theory, vulnerability},
	pages = {1--7},
}

@inproceedings{hang_trumorgpt_2024,
	title = {{TrumorGPT}: {Query} {Optimization} and {Semantic} {Reasoning} over {Networks} for {Automated} {Fact}-{Checking}},
	doi = {10.1109/CISS59072.2024.10480162},
	abstract = {In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for automated fact-checking. TrumorGPT aims to distinguish "trumors", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework merges machine learning with natural language processing techniques, leveraging a large language model (LLM) with few-shot learning for knowledge graph construction and semantic reasoning. TrumorGPT addresses the "hallucination" issue common in LLMs and the limitations of static training data by incorporating retrieval-augmented generation. This approach involves accessing and utilizing information from regularly updated knowledge graphs that consist of the latest news and information, ensuring that fact-checking of TrumorGPT is based on the most recent data. Accessing updated knowledge graphs greatly enhances the proficiency of TrumorGPT in delivering accurate and reliable information promptly. Evaluating with extensive datasets, TrumorGPT demonstrates superior performance in automated fact-checking. Its ability to effectively conduct automated fact-checking across various platforms marks a critical step forward in the fight against misinformation, enhancing trust and accuracy in the digital information age.},
	booktitle = {2024 58th {Annual} {Conference} on {Information} {Sciences} and {Systems} ({CISS})},
	author = {Hang, Ching Nam and Yu, Pei-Duo and Tan, Chee Wei},
	month = mar,
	year = {2024},
	note = {ISSN: 2837-178X},
	keywords = {Cognition, Fact-checking, knowledge graph, Knowledge graphs, large language models, Query processing, retrieval-augmented generation, semantic reasoning, Semantics, Social networking (online), Training data, Voting},
	pages = {1--6},
}

@inproceedings{ginting_saycomply_2025,
	title = {{SayComply}: {Grounding} {Field} {Robotic} {Tasks} in {Operational} {Compliance} {Through} {Retrieval}-{Based} {Language} {Models}},
	doi = {10.1109/ICRA55743.2025.11128684},
	abstract = {This paper addresses the problem of task planning for robots that must comply with operational manuals in real-world settings. Task planning under these constraints is essential for enabling autonomous robot operation in domains that require adherence to domain-specific knowledge. Current methods for generating robot goals and plans rely on common sense knowledge encoded in large language models. However, these models lack grounding of robot plans to domain-specific knowledge and are not easily transferable between multiple sites or customers with different compliance needs. In this work, we present SayComply, which enables grounding robotic task planning with operational compliance using retrievalbased language models. We design a hierarchical database of operational, environment, and robot embodiment manuals and procedures to enable efficient retrieval of the relevant context under the limited context length of the LLMs. We then design a task planner using a tree-based retrieval augmented generation (RAG) technique to generate robot tasks that follow user instructions while simultaneously complying with the domain knowledge in the database. We demonstrate the benefits of our approach through simulations and hardware experiments in real-world scenarios that require precise context retrieval across various types of context, outperforming the standard RAG method. Our approach bridges the gap in deploying robots that consistently adhere to operational protocols, offering a scalable and edge-deployable solution for ensuring compliance across varied and complex real-world environments. Project website: saycomply.github.io.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Ginting, Muhammad Fadhil and Kim, Dong-Ki and Kim, Sung-Kyun and Krishna, Bandi Jai and Kochenderfer, Mykel J. and Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar},
	month = may,
	year = {2025},
	keywords = {Databases, Grounding, Hardware, Manuals, Planning, Protocols, Retrieval augmented generation, Robot sensing systems, Robots, Service robots},
	pages = {13730--13736},
}

@article{mozo_adapting_2025,
	title = {Adapting {LLMs} for {Satellite} {Communications}: {Methodology}, {Challenges}, and {Impact}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3605022},
	abstract = {The application of large language models (LLMs) to specialized fields, such as Satellite Communications (SatCom), presents unique challenges due to the extensive and cutting-edge knowledge required. SatCom encompasses a wide range of technical details, protocols, and operational guidelines that must be addressed to produce effective and accurate models for practical use. This paper presents a fine-tuning approach for adapting 7-billion-parameter instructed LLMs (Llama-3v and Mistral) to SatCom, using a proprietary corpus sourced from the European Space Agency (ESA) consisting of domain-specific PDF documents. The confidential nature of this corpus imposes constraints on both model training and evaluation, demanding a sensible text extraction pipeline capable of handling complex structures, such as tables, to preserve critical information. Our fine-tuning methodology employs a carefully configured process, followed by an automatic evaluation framework using a curated Q\&A set tailored to SatCom. Models were created in both non-quantified and 8-bit quantized formats, ensuring feasibility for desktop-level inference. The fine-tuned models demonstrated a 6,6\% improvement over the baseline LLM, as well as significant gains when compared to retrieval-augmented generation (RAG) methods. These results indicate a promising advancement in the development of LLMs for domain-specific applications within the SatCom field.},
	journal = {IEEE Access},
	author = {Mozo, Alejandro and Gálvez, Sergio and Christou, Ioannis T. and Vogiatzis, Dimitrios and Navarro, Tomás and Valverde, Francisco L.},
	year = {2025},
	keywords = {Accuracy, Adaptation models, Artificial intelligence, Biological system modeling, Computational modeling, Data models, European Space Agency, evaluation models, fine tuning LLMs, large language models, Portable document format, preprocessing for LLMs, satellite communications, Satellite communications, Satellites, Training},
	pages = {155675--155696},
}

@inproceedings{boppana_open-source_2024,
	title = {An {Open}-{Source} {RAG} {Architecture} for {LLMs}},
	doi = {10.1109/TENCON61640.2024.10903064},
	abstract = {Accurate product classification in e-Commerce and supply chain management is essential to smooth operations and enhance the customer experience. While Large Language Models (LLMs) perform exceptionally in natural language processing, they encounter issues like model hallucination and dependence on outdated information. Furthermore, LLMs often rely on outdated data. This paper introduces an open source cloud-based RAG model, using Amazon Web Services (AWS) and vector databases to address these issues. The RAG architecture combines retrieval-based and generation-based methods, allowing them to supplement responses with up-to-date information from external sources, thus reducing the risk of model hallucination. The project employs a Vector DB deployed in EC2 to improve contextual understanding and retrieval capabilities of these large language models. Through comprehensive experimentation and AWS deployment, the RAG system improved contextual comprehension and increased the accuracy of the generated output. Semantic similarity search results significantly improve retrieval performance.},
	booktitle = {{TENCON} 2024 - 2024 {IEEE} {Region} 10 {Conference} ({TENCON})},
	author = {Boppana, Lakshmi and Bhadoria, Manav and Kodali, Ravi Kishore},
	month = dec,
	year = {2024},
	note = {ISSN: 2159-3450},
	keywords = {Accuracy, Data models, IEEE Regions, Knowledge based systems, Large language models, machine learning, natural language processing, Natural language processing, recovered generation, Semantics, Supply chain management, Vectors, Web services},
	pages = {43--46},
}

@inproceedings{painter_enhancing_2023,
	title = {Enhancing {Drug} {Safety} {Documentation} {Search} {Capabilities} with {Large} {Language} {Models}: {A} {User}-{Centric} {Approach}},
	doi = {10.1109/CSCI62032.2023.00015},
	abstract = {Integrating Large Language Models (LLMs) to enhance complex business document retrieval represents an emerging field known as retrieval-augmented generation (RAG). In highly regulated domains like drug safety (pharmacovigilance), its application has remained largely unexplored. This technology brings numerous advantages, including expedited staff on-boarding, enhanced comprehension of contextual queries, and swift information retrieval through natural language inquiries, surpassing conventional keyword searches. This study delves into various operational tasks, such as locating regulatory process guidance, navigating intricate scenarios for advice, and ensuring the LLM's competence in recognizing uncertainties to prevent misinformation. LLMs empower users to engage with documentation using natural language, markedly improving search efficiency. The case study underscores LLM's effectiveness in delivering prompt guidance within pharmacovigilance and adverse event processing and reporting, offering a user-centric solution that streamlines the search for intricate business documentation.},
	booktitle = {2023 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Painter, Jeffery L. and Mahaux, Olivia and Vanini, Marco and Kara, Vijay and Roshan, Christie and Karwowski, Marcin and Chalamalasetti, Venkateswara Rao and Bate, Andrew},
	month = dec,
	year = {2023},
	note = {ISSN: 2769-5654},
	keywords = {Data privacy, Documentation, drug safety, Drugs, Information retrieval, large language models, Large language models, LLM, pharmacovigilance, retrieval-augmented generation, Uncertainty, User experience},
	pages = {49--56},
}

@inproceedings{moutaoukkil_queries_2025,
	title = {From {Queries} to {Understanding}: {Designing} the {Next}-{Generation} {Search} {Engine}},
	doi = {10.1109/IRASET64571.2025.11008326},
	abstract = {Imagine typing a question into a search engine and getting not just a list of blue links, but a thoughtful, accurate answer that understands exactly what you need. From queries to understanding, the way we access and interpret information has been transformed. This paper explores the evolution of intelligent information retrieval systems (IRS), particularly in the context of large language models (LLMs) and their implications for future IR research and applications. We aim to address critical questions regarding the strengths and weaknesses of LLMs in enhancing IR systems. We discuss traditional IR methods, their limitations, and the rise of LLMs, emphasizing the need for a balanced approach that combines retrieval and generation techniques to improve user experience and satisfaction. The paper proposes a framework for future research that leverages LLMs to create intelligent, transparent, and responsible information retrieval system (IRS).},
	booktitle = {2025 5th {International} {Conference} on {Innovative} {Research} in {Applied} {Science}, {Engineering} and {Technology} ({IRASET})},
	author = {Moutaoukkil, Assmaa and Mezouary, Ali E L},
	month = may,
	year = {2025},
	keywords = {Accuracy, Buildings, ethical considerations, Ethics, information retrieval, large language models, Large language models, Next generation networking, Retrieval augmented generation, Retrieval-Augmented Generation, search engine, Search engines, Shape, Technological innovation, User experience},
	pages = {1--7},
}

@inproceedings{sahin_large_2024,
	title = {Large {Language} {Model} {Powered} {In}-{House} {Question} {Answering} {Assistant}},
	doi = {10.1109/ASYU62119.2024.10757102},
	abstract = {Large language models are widely used in many natural language processing applications today. Automatic question answering is one of the areas where language models are frequently used. In this study, a question answering system was developed that allows Adesso Türkiye employees to access internal company information quickly and accurately. A Retrieval Augmented Generation (RAG)-based question answering structure was created by giving content prepared by experts in the field of human resources and information security as input to the large language model. As a result of the experiments, high accuracy results were obtained on datasets. In future studies, it is aimed to increase performance by using different language models and to make the developed system available to employees by integrating it into MS Teams.},
	booktitle = {2024 {Innovations} in {Intelligent} {Systems} and {Applications} {Conference} ({ASYU})},
	author = {Şahin, Gürkan and Varol, Karya and Pak, Burcu Kuleli},
	month = oct,
	year = {2024},
	note = {ISSN: 2770-7946},
	keywords = {Accuracy, Companies, generative artificial intelligence, gpt, Information security, Intelligent systems, large language models, Large language models, Libraries, natural language processing, question answering, Question answering (information retrieval), rag, Technological innovation, User interfaces, Vectors},
	pages = {1--6},
}

@inproceedings{samarajeewa_causal_2024,
	title = {Causal {Reasoning} in {Large} {Language} {Models} using {Causal} {Graph} {Retrieval} {Augmented} {Generation}},
	doi = {10.1109/HSI61632.2024.10613566},
	abstract = {Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.},
	booktitle = {2024 16th {International} {Conference} on {Human} {System} {Interaction} ({HSI})},
	author = {Samarajeewa, Chamod and De Silva, Daswin and Osipov, Evgeny and Alahakoon, Damminda and Manic, Milos},
	month = jul,
	year = {2024},
	note = {ISSN: 2158-2254},
	keywords = {Benchmark testing, Cognition, Large language models, Limiting, Measurement, Semantics, Vectors},
	pages = {1--6},
}

@inproceedings{taylor_self-directed_2024,
	title = {Self-{Directed} {Learning} for {Community} {Health} {Workers} in {Malawi} {Through} {Generative} {AI}},
	doi = {10.1109/ICHI61247.2024.00092},
	abstract = {In many lower and middle-income countries, a lack of resources affects the availability and quality of education and training. In the healthcare domain, access to knowledge can make the difference between life and death. Timely access to technical and clinical guidelines to support decisions is crucial. Healthcare workers need access to up-to-date guidelines on case definitions for surveillance, treatment protocols, and relevant clinical and medical knowledge. However, guidelines documents tend to be bulky and complex and may change over time in response to health priorities, research, or public health emergencies. Generative AI has proven to be a disruptive technology in health care, but its limitations and applicability are subject to experimentation. We present evidence that Large Language Models (LLMs) can be leveraged to facilitate needs-driven and self-directed learning regarding guidelines for healthcare professionals in Malawi. We developed an application called IntelSurv that uses GPT -4 to achieve a ‘chat’ -like functionality where users ask questions about priority diseases, seek clarification on the use of case identification forms, and have access to technical guidelines published by the Ministry of Health. IntelSurv is both a web app and a mobile app and can run either online or offline. Healthcare professionals engaged in disease surveillance and community health in two major cities in Malawi tested the tool and gave positive feedback on its impact. We report on the development of the tool, and its use of GPT -4. We discuss choices of features and functionalities in response to testing and feedback from users.},
	booktitle = {2024 {IEEE} 12th {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Taylor, Amelia and Magwira, Macphail and Chamangwana, Chimwemwe and Chapuma, Evelyn and Liwewe, Thokozani and Kankhwali, Chisomo},
	month = jun,
	year = {2024},
	note = {ISSN: 2575-2634},
	keywords = {answers dataset, community health workers, feedback loop, Generative AI, GPT-4, LLMs, Medical services, prompting, Public healthcare, questions, RAG, self-directed learning, Surveillance, Time factors, Training, Urban areas},
	pages = {574--579},
}

@inproceedings{ji_rela_2024,
	title = {{RELA}: {An} {Online}-{Offline} {Framework} for {Rules} {Enhanced} {Logs} {Analysis} {Based} on {Large} {Language} {Models}},
	doi = {10.1109/ICARCE63054.2024.00087},
	abstract = {Recently, Large Language Models (LLMs) have played a pivotal role in the field of Artificial Intelligence Operations (AIOps), particularly in the analysis of log data. However, the use of open-source LLMs for automated log analysis often falls short of expectations in offline environments in practice. To address these challenges, this paper introduces an innovative Online-Offline framework named RELA to enhance the performance of LLMs in log analysis. In the online phase of our framework, GPT-4 meticulously analyzes logs to determine the presence of anomalies and deduces rules based on these determinations, which are then compiled into a comprehensive rules base. In the offline phase, open-source LLMs leverage this rules base to significantly enhance the effectiveness of log analysis by retrieving the most applicable rules. To validate the effectiveness of our proposed framework, we meticulously annotated a dataset specifically designed for a distinct offline scenario, focusing on two primary tasks: the detection of log anomalies and the explanation of these anomalies. Our experimental results demonstrate that the implementation of the rules base leads to a 3\% to 25.3\% increase in the accuracy of anomaly detection. Additionally, assessments conducted by domain experts validate the improvements, highlighting a substantial enhancement in the credibility of the explanations provided by LLMs within this framework. These findings not only underscore the practicality but also the efficacy of our approach, establishing a solid foundation for further research and application in AIOps.},
	booktitle = {2024 3rd {International} {Conference} on {Automation}, {Robotics} and {Computer} {Engineering} ({ICARCE})},
	author = {Ji, Xin and Chen, Ruibo and Chen, Yikai and Xiang, Nan and Zhang, Kui},
	month = dec,
	year = {2024},
	keywords = {Accuracy, AIOps, Anomaly detection, Automation, Focusing, Large language models, LLMs, Log analysis, Online-Offline, Retrieval augmented generation, Robots, Solids},
	pages = {430--434},
}

@inproceedings{gummadi_enhancing_2024,
	title = {Enhancing {Communication} and {Data} {Transmission} {Security} in {RAG} {Using} {Large} {Language} {Models}},
	doi = {10.1109/ICSES63445.2024.10763024},
	abstract = {Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources, enabling more useful information and generating accurate responses. This paper explores RAG's architecture and applications, combining generator and retriever models to access and utilize vast external data repositories. While RAG holds significant promise for various Natural Language Processing (NLP) processes like dialogue generation, summarization, and question answering, it also presents unique security challenges that must be addressed to ensure system integrity and reliability. RAG systems face several security threats, including data poisoning, model manipulation, privacy leakage, biased information retrieval, and harmful outputs generation. Generally, in the traditional RAG application, security threat is one of the major concerns. To tighten the security system and enhance the efficiency of the model on processing more complex data this paper outlines key strategies for securing RAG-based applications to mitigate these risks paper outlines key strategies for securing RAG-based applications to mitigate these risks. Ensuring data security through filtering, sanitization, and provenance tracking can prevent data poisoning and enhance the quality of external knowledge sources. Strengthening model security via adversarial training, input validation, and anomaly detection improves resilience against manipulative attacks. Implementing output monitoring and filtering techniques, such as factual verification, language moderation, and bias detection, ensures the accuracy and safety of generated responses. Additionally, robust infrastructure and access control measures, including secure data storage, secure APIs, and regulated model access, protect against unauthorized access and manipulation. Moreover, this study analyzes various use cases for LLMs enhanced by RAG, including personalized recommendations, customer support automation, content creation, and advanced search functionalities. The role of vector databases in optimizing RAG-driven generative AI is also discussed, highlighting their ability to efficiently manage and retrieve large-scale data for improved response generation. By adhering to these security measures and leveraging best practices from leading industry sources such as Databricks, AWS, and Milvus, developers can ensure the robustness and trustworthiness of RAG-based systems across diverse applications.},
	booktitle = {2024 4th {International} {Conference} on {Sustainable} {Expert} {Systems} ({ICSES})},
	author = {Gummadi, Venkata and Udayaraju, Pamula and Sarabu, Venkata Rahul and Ravulu, Chaitanya and Seelam, Dhanunjay Reddy and Venkataramana, S.},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Data models, Data Privacy, Filtering, Large language models, LLM, Query Analysis, RAG, Resilience, Robustness, Safety, Security, Security Enhancement, Training, Vectors},
	pages = {612--617},
}

@inproceedings{manias_semantic_2024,
	title = {Semantic {Routing} for {Enhanced} {Performance} of {LLM}-{Assisted} {Intent}-{Based} {5G} {Core} {Network} {Management} and {Orchestration}},
	doi = {10.1109/GLOBECOM52923.2024.10901065},
	abstract = {Large language models (LLMs) are rapidly emerging in Artificial Intelligence (AI) applications, especially in the fields of natural language processing and generative AI. Not limited to text generation applications, these models inherently possess the opportunity to leverage prompt engineering, where the inputs of such models can be appropriately structured to articulate a model’s purpose explicitly. A prominent example of this is intent-based networking, an emerging approach for automating and maintaining network operations and management. This paper presents semantic routing to achieve enhanced performance in LLM-assisted intent-based management and orchestration of 5G core networks. This work establishes an end-to-end intent extraction framework and presents a diverse dataset of sample user intents accompanied by a thorough analysis of the effects of encoders and quantization on overall system performance. The results show that using a semantic router improves the accuracy and efficiency of the LLM deployment compared to stand-alone LLMs with prompting architectures.},
	booktitle = {{GLOBECOM} 2024 - 2024 {IEEE} {Global} {Communications} {Conference}},
	author = {Manias, Dimitrios Michael and Chouman, Ali and Shami, Abdallah},
	month = dec,
	year = {2024},
	note = {ISSN: 2576-6813},
	keywords = {5G Core Networks, 5G mobile communication, End-to-End Network Management, Intent-Based Networking, Large language models, Large Language Models, Linguistics, Next-Generation Networks, Quantization (signal), Reliability, Retrieval augmented generation, Routing, Semantic Routing, Semantics, System performance, Translation},
	pages = {2924--2929},
}

@inproceedings{xu_grasp_2024,
	title = {{GRASP}: {Municipal} {Budget} {AI} {Chatbots} for {Enhancing} {Civic} {Engagement}},
	doi = {10.1109/BigData62323.2024.10825975},
	abstract = {There are a growing number of AI applications, but none tailored specifically to help residents answer their questions about municipal budget, a topic most are interested in but few have a solid comprehension of. In this research paper, we propose GRASP, a custom AI chatbot framework which stands for Generation with Retrieval and Action System for Prompts. GRASP provides more truthful and grounded responses to user budget queries than traditional information retrieval systems like general Large Language Models (LLMs) or web searches. These improvements come from the novel combination of a Retrieval-Augmented Generation (RAG) framework ("Generation with Retrieval") and an agentic workflow ("Action System"), as well as prompt engineering techniques, the incorporation of municipal budget domain knowledge, and collaboration with local town officials to ensure response truthfulness. During testing, we found that our GRASP chatbot provided precise and accurate responses for local municipal budget queries 78\% percent of the time, while GPT-4o and Gemini were only accurate 60\% and 35\% of the time, respectively. GRASP chatbots greatly reduce the time and effort needed for the general public to get an intuitive and correct understanding of their town’s budget, thus fostering greater communal discourse, improving government transparency, and allowing citizens to make more informed decisions.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Xu, Jerry and Wang, Justin and Leung, Joley and Gu, Jasmine},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Accuracy, Chatbots, Knowledge engineering, LLMs, Local government, Municipal Documents, Prompt engineering, Prompt Engineering, RAG, ReAct Agent, Reliability, Solids, Testing, Urban areas, Web search},
	pages = {7438--7442},
}

@inproceedings{rangana_educational_2025,
	title = {Educational {Material} {Development} with {Active} {Learning} for {Secondary} {School} {Subjects}},
	doi = {10.1109/IALP68296.2024.11156564},
	abstract = {This work addresses the critical challenge of transforming static educational content into interactive learning experiences in secondary education, with a specific focus on multilingual environments in Sri Lanka. We propose LK-Active- Learner - an AI-driven platform that leverages knowledge graphs and Retrieval-Augmented Generation (GraphRAG) to create contextually relevant, personalized assessment materials in both Sinhala and English. By combining document AI for text extraction, knowledge graph construction, and question generation techniques, our system aims to enhance student engagement and comprehension while supporting educators through automated examination material development. This paper presents the theoretical framework, system architecture, and evaluation methodology for the proposed platform. This research contributes to educaional technology advancement in low-resource languages and offers a scalable framework for supporting active learning strategies in diverse linguistic contexts.},
	booktitle = {2025 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	author = {Rangana, K Sachith and Dias, Gihan},
	month = aug,
	year = {2025},
	note = {ISSN: 2159-1970},
	keywords = {Active learning, Active Learning in Education, Digital transformation, Educational technology, Generative AI, GraphRAG, Grounding, Knowledge graphs, Knowledge Graphs, Low-Resource Languages, Multilingual, Question generation, Retrieval augmented generation, Systems architecture},
	pages = {54--59},
}

@incollection{bergeret_retrievalx2010augmented_2025,
	title = {Retrieval\&\#x2010;{Augmented} {Generation}},
	isbn = {978-1-394-28130-5},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10982315},
	abstract = {{\textless}p{\textgreater}This chapter discusses the challenges and considerations in deploying retrieval\&\#x2010;augmented generation (RAG) systems, computational efficiency, and ethical concerns. It aims to provide a comprehensive understanding of RAG's potential to revolutionize artificial intelligence (AI) applications and interactions, offering valuable insights for AI practitioners, business leaders, and technology enthusiasts alike. RAG represents a sophisticated approach to enhancing language model capabilities by leveraging external knowledge sources. At its core, RAG's architecture is designed to bridge the gap between vast external knowledge repositories and the generative capabilities of language models, creating a system that combines the best of both worlds. The RAG architecture consists of three primary components that work in harmony to process user queries and generate accurate, contextually relevant responses: the Retrieval Module; the Augmentation Module; and the Generation Module.{\textless}/p{\textgreater}},
	booktitle = {{GenAI} on {AWS}: {A} {Practical} {Approach} to {Building} {Generative} {AI} {Applications} on {AWS}},
	publisher = {Wiley},
	author = {Bergeret, Olivier and Abbasi, Asif and Farvault, Joel},
	year = {2025},
	keywords = {Accuracy, Adaptation models, Artificial intelligence, Data models, Information retrieval, Knowledge based systems, Large language models, Prompt engineering, Retrieval augmented generation, Soft sensors},
	pages = {263--294},
}

@article{neumann_llm-driven_2025,
	title = {An {LLM}-{Driven} {Chatbot} in {Higher} {Education} for {Databases} and {Information} {Systems}},
	volume = {68},
	issn = {1557-9638},
	doi = {10.1109/TE.2024.3467912},
	abstract = {Contribution: This research explores the benefits and challenges of developing, deploying, and evaluating a large language model (LLM) chatbot, MoodleBot, in computer science classroom settings. It highlights the potential of integrating LLMs into LMSs like Moodle to support self-regulated learning (SRL) and help-seeking behavior. Background: Computer science educators face immense challenges incorporating novel tools into LMSs to create a supportive and engaging learning environment. MoodleBot addresses this challenge by offering an interactive platform for both students and teachers. Research Questions: Despite issues like bias, hallucinations, and teachers’ and educators’ resistance to embracing new (AI) technologies, this research investigates two questions: (RQ1) To what extent do students accept MoodleBot as a valuable tool for learning support? (RQ2) How accurately does MoodleBot churn out responses, and how congruent are these with the established course content? Methodology: This study reviews pedagogical literature on AI-driven chatbots and adopts the retrieval-augmented generation (RAG) approach for MoodleBot’s design and data processing. The technology acceptance model (TAM) evaluates user acceptance through constructs like perceived usefulness (PU) and Ease of Use. Forty-six students participated, with 30 completing the TAM questionnaire. Findings: LLM-based chatbots like MoodleBot can significantly improve the teaching and learning process. This study revealed a high accuracy rate (88\%) in providing course-related assistance. Positive responses from students attest to the efficacy and applicability of AI-driven educational tools. These findings indicate that educational chatbots are suitable for integration into courses to improve personalized learning and reduce teacher administrative burden, although improvements in automated fact-checking are needed.},
	number = {1},
	journal = {IEEE Transactions on Education},
	author = {Neumann, Alexander Tobias and Yin, Yue and Sowe, Sulayman and Decker, Stefan and Jarke, Matthias},
	month = feb,
	year = {2025},
	keywords = {Accuracy, Adaptation models, Chatbots, Computer science, Databases, Education, higher education, Information systems, Information technology, large language model (LLM), Mentoring, moodle, moodlebot, Vectors},
	pages = {103--116},
}

@inproceedings{yilmaz_grounded_2025,
	title = {Grounded {Answer} {Generation} over {Multimodal} {Financial} {Records} via {Semantic} {Indexing}},
	doi = {10.1109/UBMK67458.2025.11206956},
	abstract = {The automated interpretation of unstructured financial records, including receipts and invoices, has become increasingly critical for intelligent document understanding. A Retrieval-Augmented Generation (RAG) framework is presented to address question answering over multilingual financial documents characterized by noisy OCR output, variable layouts, and visually embedded information. In the proposed approach, textual content is encoded using multilingual sentence encoders, while visual information is processed through multimodal language models; both representations are stored in a unified semantic index. At inference time, retrieval scores from each modality are fused to guide evidence selection, which is then provided to an instruction-tuned generator. The factuality of generated answers is subsequently validated using a lightweight verifier Large Language Model(LLM) Judge that classifies answers as grounded, partially grounded, or hallucinated. The system is trained and evaluated on a real-world dataset of 2,536 financial documents in Turkish and English, achieving 86.7\% grounded answer accuracy and reducing hallucination rates by more than 50\% compared to text-only retrieval. The contributions are: (i) an end-to-end multimodal RAG architecture for financial question answering,(ii)a curated multilingual benchmark dataset of real financial documents, and (iii) an efficient groundedness verification method based on LLM judgment, establishing a reproducible baseline for multimodal document understanding.},
	booktitle = {2025 10th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Yılmaz, Rabia Eda and Taysi, Mehmet Anıl and Özmen, Ayşe İrem and İnce, Gökhan},
	month = sep,
	year = {2025},
	note = {ISSN: 2521-1641},
	keywords = {AI-Assisted Document Management, Benchmark testing, Computer architecture, Embedding, Large Language Models, Multilingual, Multimodal Retrieval, Optical character recognition, Pipelines, Question answering (information retrieval), Reliability engineering, Retrieval augmented generation, Semantic search, Semantic Search, Visualization},
	pages = {160--165},
}

@inproceedings{sun_trustnavgpt_2024,
	title = {{TrustNavGPT}: {Modeling} {Uncertainty} to {Improve} {Trustworthiness} of {Audio}-{Guided} {LLM}-{Based} {Robot} {Navigation}},
	doi = {10.1109/IROS58592.2024.10801932},
	abstract = {Large language models (LLMs) exhibit a wide range of promising capabilities – from step-by-step planning to commonsense reasoning –that provide utility for robot navigation. However, as humans communicate with robots in the real world, ambiguity and uncertainty may be embedded inside spoken instructions. While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present TrustNavGPT, an LLM-based audio-guided navigation agent that uses affective cues in spoken communication—elements such as tone and inflection that convey meaning beyond words—allowing it to assess the trustworthiness of human commands and make effective, safe decisions. Experiments across a variety of simulation and real-world setups show a 70.46\% success rate in catching command uncertainty and an 80\% success rate in finding the target, 48.30\%, and 55\% outperform existing LLM-based navigation methods, respectively. Additionally, TrustNavGPT shows remarkable resilience against adversarial attacks, highlighted by a 22\%+ less decrease ratio than the existing LLM navigation method in success rate. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation. For more information, visit the TrustNav project page.},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Sun, Xingpeng and Zhang, Yiran and Tang, Xindi and Bedi, Amrit Singh and Bera, Aniket},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Computational efficiency, Large language models, Navigation, Planning, Resilience, Retrieval augmented generation, Robots, Semantics, Social robots, Uncertainty},
	pages = {8794--8801},
}

@inproceedings{chaudhary_developing_2024,
	title = {Developing a {Llama}-{Based} {Chatbot} for {CI}/{CD} {Question} {Answering}: {A} {Case} {Study} at {Ericsson}},
	doi = {10.1109/ICSME58944.2024.00075},
	abstract = {This paper presents our experience developing a Llama-based chatbot for question answering about continuous integration and continuous delivery (CI/CD) at Ericsson, a multinational telecommunications company. Our chatbot is designed to handle the specificities of CI/CD documents at Ericsson, employing a retrieval-augmented generation (RAG) model to enhance accuracy and relevance. Our empirical evaluation of the chatbot on industrial CI/CD-related questions indicates that an ensemble retriever, combining BM25 and embedding retrievers, yields the best performance. When evaluated against a ground truth of 72 CI/CD questions and answers at Ericsson, our most accurate chatbot configuration provides fully correct answers for 61.11\% of the questions, partially correct answers for 26.39\%, and incorrect answers for 12.50\%. Through an error analysis of the partially correct and incorrect answers, we discuss the underlying causes of inaccuracies and provide insights for further refinement. We also reflect on lessons learned and suggest future directions for further improving our chatbot's accuracy.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Chaudhary, Daksh and Vadlamani, Sri Lakshmi and Thomas, Dimple and Nejati, Shiva and Sabetzadeh, Mehrdad},
	month = oct,
	year = {2024},
	note = {ISSN: 2576-3148},
	keywords = {Accuracy, Chatbot-Enabled Software Engineering, Chatbots, Continuous integration, Continuous Integration and Continuous Delivery (CI/CD), Feedback loop, Large Language Models (LLMs), Production, Prototypes, Question answering (information retrieval), Retrieval-Augmented Generation (RAG), Software maintenance, Telecommunications, Usability},
	pages = {707--718},
}

@inproceedings{saad_closed_2024,
	title = {Closed {Domain} {Question}-{Answering} {Techniques} in an {Institutional} {Chatbot}},
	doi = {10.1109/ICECCME62383.2024.10796881},
	abstract = {This paper introduces BrockportGPT, a specialized chatbot designed for SUNY Brockport that addresses the unique challenges of institutional question answering that general purpose Large Language Models (LLMs) face. Our approach leverages a combination of vanilla sequence-to-sequence modeling, fine tuning LLaMA-2, and Retrieval Augmented Generation (RAG). The methodology involves extensive data collection through web scraping, synthetic question generation, and a comprehensive look at information retrieval strategies, including the implementation of a question-topic classification system. Comparative analysis of the three approaches shows that finetuning and RAG have competitive performance, with RAG providing the most accurate and contextually relevant results, and finetuning having the superior dialog coherence. Through these results, BrockportGPT offers a model for developing an institutional chatbot, highlighting the potential for AI-driven education tools to improve information accessibility and dissemination processes.},
	booktitle = {2024 4th {International} {Conference} on {Electrical}, {Computer}, {Communications} and {Mechatronics} {Engineering} ({ICECCME})},
	author = {Saad, Matthew and Qawaqneh, Zakariya},
	month = nov,
	year = {2024},
	keywords = {Accuracy, Chatbots, Closed-domain question answering, Computational modeling, Context modeling, Data models, institutional chatbot, Large Language Models (LLMs), Question answering (information retrieval), Retrieval augmented generation, Solid modeling, Training, Tuning},
	pages = {1--8},
}

@inproceedings{s_rag-based_2024,
	title = {A {RAG}-based {Medical} {Assistant} {Especially} for {Infectious} {Diseases}},
	doi = {10.1109/ICICT60155.2024.10544639},
	abstract = {Infectious diseases like COVID-19 have gained international attention recently. Furthermore, there are significantly fewer doctors per capita in densely populated nations like India, which hurts those in need. Under such circumstances, natural language processing techniques might make it feasible to create an intelligent and engaging chatbot system. The primary objective of the effort is to develop an interactive solution that is entirely open source and can be easily installed on a local computer using the most recent data. Even though there are numerous chatbots on the market, proposed solutions highlight the need to provide individualized and sympathetic responses. Getting Back While the data is stored in the graph database as nodes and relationships, and the knowledge graph is constructed on top of it, augmented generation is utilized to extract the pertinent content from the data. To improve the generator’s context, pertinent sections are collected during the question-answering process. This reduces hallucinations and increases the correctness of abstractions by providing external knowledge streams. Furthermore, the research study employs a text-to-speech model that was replicated from a physician’s voice recording to narrate the produced responses, thereby augmenting user confidence and interaction. Academic institutions and healthcare organizations can benefit from this work by better understanding the value and effectiveness of applying NLP techniques to infectious disease research.},
	booktitle = {2024 {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {S, Stewart Kirubakaran and G, Jasper Wilsie Kathrine and E, Grace Mary Kanaga and J, Mahimai Raja and Singh A, Ruban Gino and E, Yuvaraajan},
	month = apr,
	year = {2024},
	note = {ISSN: 2767-7788},
	keywords = {Artificial intelligence, chatbot, Chatbots, COVID-19, Databases, Infectious diseases, knowledge graph, large language model, natural language processing, Recording, Reliability, retrieval augmented generation},
	pages = {1128--1133},
}

@article{abdulnazar_large_2024,
	title = {Large {Language} {Models} for {Clinical} {Text} {Cleansing} {Enhance} {Medical} {Concept} {Normalization}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3472500},
	abstract = {Most clinical information is only available as free text. Large language models (LLMs) are increasingly applied to clinical data to streamline communication, enhance the accuracy of clinical documentation, and ultimately improve healthcare delivery. This study focuses on a corpus of anonymized clinical narratives in German. On the one hand it evaluates the use of ChatGPT for text cleansing, i.e., the automatic rephrasing of raw text into a more readable and standardized form, and on the other hand for retrieval-augmented generation (RAG). In both tasks, the final goal was medical concept normalization (MCN), i.e., the annotation of text segments with codes from a controlled vocabulary using natural language processing. We found that ChatGPT (GPT-4) significantly improves precision and recall compared to simple dictionary matching. For all scenarios, the importance of the underlying terminological basis was also demonstrated. Maximum F1 scores of 0.607, 0.735 and 0.754 (i.e, for top 1, 5 and 10 matches) were achieved through a pipeline including document cleansing, bi-encoder-based term matching based on a large domain dictionary linked to SNOMED CT, and finally re-ranking using RAG.},
	journal = {IEEE Access},
	author = {Abdulnazar, Akhila and Roller, Roland and Schulz, Stefan and Kreuzthaler, Markus},
	year = {2024},
	keywords = {Accuracy, Biological system modeling, Chatbots, ChatGPT, Clinical diagnosis, Codes, Data integrity, Large language models, medical concept normalization, Medical services, Natural language processing, retrieval augmented generation, Text analysis, text cleansing, Training, Unified modeling language},
	pages = {147981--147990},
}

@inproceedings{omeed_integrating_2024,
	title = {Integrating {Computer} {Vision} and language model for interactive {AI} - {Robot}},
	doi = {10.1109/SSD61670.2024.10548649},
	abstract = {This research presents an innovative approach to integrating robotic systems with artificial intelligence, addressing long-standing challenges in their functionality. Utilizing YOLOv9 object detection model, Large Language Model LLM, Retrieval Augmented Generation RAG, alongside a 6 degree of freedom robotic arm 6DOF, the design achieves seamless interaction and multifaceted functionality. Key design choices, including hardware selection (Raspberry Pi 58GB) and utilization of various techniques, optimize performance. Experimental results demonstrate a confidence level of 1.0 at a confidence threshold of 0.970 for all classes combined. In the F1-Confidence curve, the all-classes curve achieves an F1 score of 0.84 at a confidence threshold of 0.524, showcasing overall strong performance and a balanced trade-off between precision and recall. The synthesis of diverse methodological approaches underscores the developmental trajectory of this sophisticated robotic assistant, showcasing its transformative potential in promoting effective operations beyond conventional paradigms.},
	booktitle = {2024 21st {International} {Multi}-{Conference} on {Systems}, {Signals} \& {Devices} ({SSD})},
	author = {Omeed, Holan K. and Alani, Ahmed O. and Rasul, Ibrahim H. and Ashir, Abubakar M. and Mohammed, Sava Ahmed},
	month = apr,
	year = {2024},
	note = {ISSN: 2474-0446},
	keywords = {AI-Enhanced Robotics, Artificial Intelligence, Computational modeling, Computer vision, Hardware, Large Language Model Applications, Manipulators, Object detection, Robotic Personal Assistant, Robotics, System integration, Systems architecture},
	pages = {124--131},
}

@inproceedings{ou_cygpt_2024,
	title = {{CyGPT}: {Knowledge} {Graph}-{Based} {Enhancement} {Techniques} for {Large} {Language} {Models} in {Cybersecurity}},
	doi = {10.1109/DSC63484.2024.00036},
	abstract = {Large Language Models (LLMs) excel in numerous Natural Language Processing (NLP) tasks but encounter significant challenges in practical applications, including hallucinations, outdated information, and a lack of domain-specific external knowledge. This study proposes a collaborative, training-free reasoning approach, leveraging close cooperation between Knowledge Graphs (KG) and LLMs for cybersecurity applications. Our approach employs the ‘Joint Reasoning Chain,’ which dynamically integrates information from network security-specific knowledge graphs, serving as an external knowledge base to enhance the domain-specific external knowledge of LLMs. This cooperative method not only improves reliable knowledge-based reasoning but also enhances the traceability of decision-making processes. Furthermore, we introduce a novel GPT-based technique to evaluate answer quality and have performed systematic experiments on a purpose-built test set. The results confirm that our method significantly boosts GPT’s performance in network security knowledge, demonstrating the potential of knowledge graphs to augment LLMs’ reasoning abilities and their applicability in specialized fields.},
	booktitle = {2024 {IEEE} 9th {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	author = {Ou, Lu and Ni, Xiaoya and Wu, Wei and Tian, Zhihong},
	month = aug,
	year = {2024},
	keywords = {Cognition, Computer security, Cybersecurity, Data models, Knowledge based systems, Knowledge engineering, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Natural language processing, Network security, Retrieval-Augmented Generation, Systematics},
	pages = {216--223},
}

@inproceedings{tundik_building_2024,
	title = {Building {Domain} {Specific} {Chatbot}: {A} {Telco} {Case} {Study}},
	doi = {10.1109/CINTI63048.2024.10830834},
	abstract = {The generative AI and LLMs are getting commonly used solution to ease the daily work. There are special domains where the out of box solution cannot provide sufficient outcomes as the domains have their own specialties, jargons or rules. In this paper we investigate telco related specialties and how they can be handled to tailor a general purpose chatbot.},
	booktitle = {2024 {IEEE} 24th {International} {Symposium} on {Computational} {Intelligence} and {Informatics} ({CINTI})},
	author = {Tündik, Máté Ákos and Kovács, Ferenc and Blaskó, Márk},
	month = nov,
	year = {2024},
	note = {ISSN: 2471-9269},
	keywords = {6G, Buildings, Chatbots, Computational intelligence, Computational modeling, domain specific chatbot, generative AI, Generative AI, Informatics, large language models, multimodal chatbot, retrieval augmented generation, telecommunication},
	pages = {135--140},
}

@inproceedings{mahendru_venn_2024,
	title = {Venn {Diagram} {Prompting}: {Accelerating} {Comprehension} with {Scaffolding} {Effect}},
	doi = {10.1109/WSAI62426.2024.10828919},
	abstract = {We introduce Venn Diagram (VD) Prompting, an innovative prompting technique which allows Large Language Models (LLMs) to combine and synthesize information across complex, diverse and long-context documents in knowledge-intensive question-answering tasks. Generating answers from multiple documents involves numerous steps to extract relevant and unique information and amalgamate it into a cohesive response. To improve the quality of the final answer, multiple LLM calls or pretrained models are used to perform different tasks such as summarization, reorganization and customization. The approach covered in the paper focuses on replacing the multi-step strategy via a single LLM call using VD prompting. Our proposed technique also aims to eliminate the inherent position bias in the LLMs, enhancing consistency in answers by removing sensitivity to the sequence of input information. It overcomes the challenge of inconsistency traditionally associated with varying input sequences. We also explore the practical applications of the VD prompt based on our examination of the prompt's outcomes. In the experiments performed on four public benchmark question-answering datasets, VD prompting continually matches or surpasses the performance of a meticulously crafted instruction prompt which adheres to optimal guidelines and practices.},
	booktitle = {2024 6th {World} {Symposium} on {Artificial} {Intelligence} ({WSAI})},
	author = {Mahendru, Sakshi and Pandit, Tejul},
	month = jun,
	year = {2024},
	keywords = {Benchmark testing, Context modeling, Data mining, Guidelines, Large language model, Large language models, Linguistics, prompt engineering, Question answering (information retrieval), retrieval-augmented generation, Sensitivity, Set theory, Standards},
	pages = {39--48},
}

@inproceedings{lan_lamlad_2025,
	title = {{LAMLAD}: {LLM}-{Based} {Adversarial} {Attack} {Against} {Machine} {Learning} for {Android} {Malware} {Detection}},
	doi = {10.1109/CNS66487.2025.11195008},
	abstract = {The increasing sophistication and volume of Android malware have driven the adoption of Machine Learning (ML) models for scalable and accurate threat detection. However, these models remain vulnerable to adversarial attacks that subtly manipulate input features to evade classification. In this paper, we introduce LAMLAD, a novel adversarial attack framework that exploits the generative and reasoning capabilities of Large Language Models (LLMs) to bypass ML-based malware detectors. LAMLAD employs a two-agent architecture composed of an LLM manipulator, which crafts realistic feature-level perturbations without altering core malicious behaviors, and an LLM analyzer, which guides the modification process to ensure successful evasion. To improve efficiency and context awareness, LAMLAD integrates Retrieval-Augmented Generation (RAG) into the LLM workflow. By targeting Drebin features, LAMLAD enables stealthy and high-confidence attacks against widely used Android malware classifiers. We evaluate LAMLAD against three representative ML-based Android malware detectors and compare it with two state-of-the-art adversarial attack techniques. Results demonstrate that LAMLAD achieves an attack success rate of 97\% with an average of 3 attempts per adversarial example, highlighting its potency, efficiency, and adaptability in real-world adversarial scenarios.},
	booktitle = {2025 {IEEE} {Conference} on {Communications} and {Network} {Security} ({CNS})},
	author = {Lan, Tianwei and Nait-Abdesselam, Farid},
	month = sep,
	year = {2025},
	note = {ISSN: 2994-5895},
	keywords = {Cognition, Context awareness, Detectors, Machine learning, Malware, Manipulators, Perturbation methods, Retrieval augmented generation, Solid modeling, Threat assessment},
	pages = {1--9},
}

@inproceedings{leung_rag_2024,
	title = {{RAG} for {Question}-{Answering} for {Vocal} {Training} {Based} on {Domain} {Knowledge} {Base}},
	doi = {10.1109/BESC64747.2024.10780718},
	abstract = {Although Large language models (LLMs) are well-known due to their superior capacity for text generation and logical inference, they are found to be inaccurate in domain-specific question-answering tasks. The powerful generator still tends to generate content even when the LLM does not have sufficient knowledge at all, which is known as the hallucination problem. We find there is a research void in applying LLMs in the vocal training industry, which requires intensive expert knowledge in any chatbot or intelligent tutor services. This paper details employing Retrieval-Augmented Generation (RAG) technology to develop a domain-specific language model, addressing inherent challenges such as hallucination, where large models generate plausible but inaccurate content, and lack of domain specificity. By segmenting the knowledge base and establishing semantic similarities between user queries and knowledge data, the project lays a solid foundation for integrating RAG, significantly improving response accuracy and contextual relevance. The report highlights the successful implementation of RAG, enhancing system intelligence and personalization for user-specific needs, discusses challenges and solutions during the implementation process, and outlines future directions to expand RAG capabilities and improve user experiences.},
	booktitle = {2024 11th {International} {Conference} on {Behavioural} and {Social} {Computing} ({BESC})},
	author = {Leung, Chun-hung Jonas and Yi, Yicheng and Kuai, Le and Li, Zongxi and Yeung, Siu-kei Au and Lee, Kwok-wah John and Ho, Ka-him Kelvin and Hung, Kevin},
	month = aug,
	year = {2024},
	note = {ISSN: 2689-8284},
	keywords = {Accuracy, Adaptation models, Computational modeling, Context modeling, Knowledge based systems, Semantics, Social computing, Solids, Testing, Training},
	pages = {1--6},
}

@inproceedings{cheng_intelligent_2025,
	title = {Intelligent {Retrieval} and {Knowledge} {Question}-{Answering} {System} for {Semi}-{Structured} {Operational} {Manuals}},
	doi = {10.1109/CASE58245.2025.11164142},
	abstract = {In the field of industrial production and manufacturing, various management systems such as Manufacturing Execution Systems (MES), Enterprise Resource Planning (ERP), and Advanced Planning Systems (APS) play a crucial role in production monitoring, performance evaluation, and decision support. These systems enable managers to obtain real-time production data, analyze process anomalies, and formulate appropriate adjustment strategies to ensure production efficiency and quality consistency. However, when users are unfamiliar with system operations, they often need to search through extensive Standard Operating Procedure (SOP) manuals to find relevant operational guidance, which is both time-consuming and detrimental to operational efficiency. In recent years, the integration of Generative Artificial Intelligence (Generative AI) with Retrieval-Augmented Generation (RAG) technology has enabled systems to automatically retrieve and synthesize relevant knowledge content in response to user queries, providing real-time assistance. However, when handling unstructured or semi-structured data, traditional text segmentation methods may disrupt the original hierarchical structure and contextual relationships of documents, leading to retrieval results that fail to accurately match user queries. Moreover, if text segments lack complete source attribution, RAG models may struggle to clearly indicate the origin of the information in their responses, thereby affecting the credibility of the generated content. To address these challenges, this study proposes a method for extracting semi-structured data to enhance the quality of document parsing. By implementing this approach, the final output of the RAG system can generate high-quality responses, precisely annotate interface locations and procedural steps, and assist field personnel in utilizing management systems more efficiently.},
	booktitle = {2025 {IEEE} 21st {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Cheng, You-Wei and Hsu, Chia-Yu and Lan, Yu-Ying},
	month = aug,
	year = {2025},
	note = {ISSN: 2161-8089},
	keywords = {Accuracy, Data mining, Generative AI, Manuals, Manufacturing, Production, Real-time systems, Retrieval augmented generation, Standards, Systems operation},
	pages = {1675--1679},
}

@inproceedings{chen_autokg_2023,
	title = {{AutoKG}: {Efficient} {Automated} {Knowledge} {Graph} {Generation} for {Language} {Models}},
	doi = {10.1109/BigData59044.2023.10386454},
	abstract = {Traditional methods of linking large language models (LLMs) to knowledge bases via the semantic similarity search often fall short of capturing complex relational dynamics. To address these limitations, we introduce AutoKG, a lightweight and efficient approach for automated knowledge graph (KG) construction. For a given knowledge base consisting of text blocks, AutoKG first extracts keywords using a LLM and then evaluates the relationship weight between each pair of keywords using graph Laplace learning. We employ a hybrid search scheme combining vector similarity and graph-based associations to enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a more comprehensive and interconnected knowledge retrieval mechanism compared to the semantic similarity search, thereby enhancing the capabilities of LLMs in generating more insightful and relevant outputs.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Chen, Bohan and Bertozzi, Andrea L.},
	month = dec,
	year = {2023},
	keywords = {Big Data, Computer architecture, Data visualization, Graph Learning, Knowledge based systems, Knowledge Graph, Knowledge graphs, Language model, Retrieval-augmented Generation, Semantics, Training},
	pages = {3117--3126},
}

@article{mudassar_yamin_applications_2024,
	title = {Applications of {LLMs} for {Generating} {Cyber} {Security} {Exercise} {Scenarios}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3468914},
	abstract = {This study proposes a novel approach leveraging Large Language Models (LLMs) to generate dynamic and complex adaptable cybersecurity exercise scenarios. Motivated by Turing’s seminal exploration into machine cognition, which questions the ability of machines to mimic human thought and intelligence. By exploiting the generative potential of LLMs, our methodology simulates a wide range of cyber threats, both known and novel, thereby enhancing cybersecurity training and awareness. This approach transforms the potential for ‘hallucination’ inherent in LLMs into a potential advantage, enabling the creation of complex exercise scenarios that push the boundaries of traditional cybersecurity training. The innovation lies in the sophisticated application of AI, aiming to advance the preparedness of security professionals against diverse cyber threats. The scenarios generated through this method were subject to meticulous testing and a rigorous evaluation process involving (Generated Pre-Trained Transformer) GPT models and expert review to ensure their realism and applicability. In this paper, we introduce ‘CyExec,’ a novel approach leveraging GPT to dynamically generate cybersecurity training scenarios. Furthermore, the prompts provided to the LLMs were meticulously designed to adopt a Retrieval-Augmented Generation (RAG) approach, enriching the complexity and relevance of the scenarios. This incorporation of RAG, alongside the inspiration drawn from Turing’s exploration of machine intelligence, showcases an advanced application of AI in cybersecurity training, reflecting a deep understanding of how machines can augment our capabilities to anticipate and mitigate cyber threats.},
	journal = {IEEE Access},
	author = {Mudassar Yamin, Muhammad and Hashmi, Ehtesham and Ullah, Mohib and Katt, Basel},
	year = {2024},
	keywords = {bounded rationality, Computer crime, Computer security, Cyber security exercise scenarios, Data models, Ethics, generative configurations, Halluciation in LLMs, large language models, Manuals, Organizations, Security, Testing, Training, Transformers},
	pages = {143806--143822},
}

@inproceedings{egersdoerfer_ioagent_2025,
	title = {{IOAgent}: {Democratizing} {Trustworthy} {HPC} {I}/{O} {Performance} {Diagnosis} {Capability} via {LLMs}},
	doi = {10.1109/IPDPS64566.2025.00036},
	abstract = {As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for dataintensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. The recent rapid progress in large language models (LLMs) opens the door to creating an automated tool that democratizes trustworthy I/O performance diagnosis capabilities to domain scientists. However, LLMs face significant challenges in this task, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions. In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates various new designs, including a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to continue asking questions about the diagnosis. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Based on this test suite, extensive evaluations were conducted, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.},
	booktitle = {2025 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Egersdoerfer, Chris and Sareen, Arnav and Bez, Jean Luca and Byna, Suren and Xu, Dongkuan DK and Dai, Dong},
	month = jun,
	year = {2025},
	note = {ISSN: 1530-2075},
	keywords = {Accuracy, Complexity theory, Corporate acquisitions, Distributed processing, Faces, hpc, i/o, large language model, Large language models, Navigation, parallel file system, Systematics},
	pages = {322--334},
}

@inproceedings{sarcevic_enhancing_2024,
	title = {Enhancing {Programming} {Education} with {Open}-{Source} {Generative} {AI} {Chatbots}},
	doi = {10.1109/MIPRO60963.2024.10569736},
	abstract = {This paper describes the development of an Open-Source Generative AI Chatbot, utilizing free Large Language Models (LLM) to enrich the student learning experience for a university course in “Introduction to Programming”. The article aims to provide a step-by-step guide for selecting, fine-tuning, and evaluating available models. As a first step in choosing the appropriate LLM, which provides the most accurate responses while not requiring excessive computing power, the article will cover a discussion of the advantages and disadvantages of local vs. cloud-available models. After selecting a few promising models, the next stage includes fine-tuning LLMs to answer domain-specific questions using a dataset containing essential rules, guidelines, and explanatory content regarding the subject. The crucial aspect of selecting a model was evaluating answers, and in this context, both human and automatic evaluation techniques will be presented. Finally, it is possible to enhance the model performance and accuracy by incorporating Retrieval-Augmented Generation (RAG) techniques and exploring the influence of various factors, such as different vector databases, model temperatures, maximum token lengths, prompt templates, embeddings, repetition penalties, and chunking sizes. Our results show that chatbots have significant potential to improve academic support and learning efficiency, as well as personalized education in general.},
	booktitle = {2024 47th {MIPRO} {ICT} and {Electronics} {Convention} ({MIPRO})},
	author = {Šarčević, Antonia and Tomičić, Ivan and Merlin, Andrija and Horvat, Marko},
	month = may,
	year = {2024},
	note = {ISSN: 2623-8764},
	keywords = {Accuracy, chatbots, Chatbots, Computational modeling, digital learning, education, Education, Electric potential, Generative AI, generative models, large language models, natural language processing, Temperature},
	pages = {2051--2056},
}

@inproceedings{huang_driverp_2024,
	title = {{DriveRP}: {RAG} and {Prompt} {Engineering} {Embodied} {Parallel} {Driving} in {Cyber}-{Physical}-{Social} {Spaces}},
	doi = {10.1109/DTPI61353.2024.10778684},
	abstract = {In recent years, numerous technological advancements in Artificial Generative Intelligences (AGIs) have demonstrated significant potential to transform the intelligence acquisition mechanisms in connected autonomous vehicles (CAVs). Integrating technologies like ChatGPT into CAVs can enhance human-machine interactions. However, the emergence of such new traffic entities may introduce unforeseen hallucinations and complex risks that surpass our current understanding. To address these challenges, Retrieval-Augmented Generation (RAG) and prompt engineering technologies are being explored to enhance the reliability and safety of autonomous driving systems. RAG retrieves relevant contextual information, such as driving experiences and real-time road network status, from external databases to ensure that foundation models have access to accurate and timely data for informed decision-making. Prompt engineering optimizes the performance of large language models in autonomous driving systems by designing and refining prompts that guide the models’ responses, thereby improving their relevance and accuracy in various driving scenarios. Together, these technologies enhance the robustness and trustworthiness of autonomous driving systems. This paper proposes DriveRP, a framework that integrates RAG and prompt engineering within the Descriptive-Predictive-Prescriptive Intelligence framework of Parallel Driving theory. DriveRP aims to enhance the safety and interpretability of autonomous vehicle trajectory planning, decision-making, and motion control, ultimately achieving the "6S" goals. Grounded in Digital Twins and Metaverse-embodied parallel driving theory, DriveRP provides the infrastructure and foundational intelligence for parallel driving with Multi-modal Large Lange Models(MLLMs). Additionally, the paper discusses future trends and potential research directions, focusing on the "6S" goals of parallel driving: Smart, Safe, Secure, Sensitive, Sustainable, and Serviceable.},
	booktitle = {2024 {IEEE} 4th {International} {Conference} on {Digital} {Twins} and {Parallel} {Intelligence} ({DTPI})},
	author = {Huang, Jun and Ma, Hao and Zhang, Tengchao and Lin, Fei and Ma, Siji and Wang, Xiao and Wang, Fei-Yue},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Autonomous vehicles, Autonomous Vehicles, Decision making, Digital Twin, Digital twins, Intelligent Transportation Systems, Large Language Models, Metaverse, Motion control, Parallel Driving, Prompt engineering, Retrieval-Augmented Generation, Safety, Trajectory planning, Transforms, Transportation},
	pages = {547--553},
}

@article{lin_pe-gpt_2025,
	title = {{PE}-{GPT}: {A} {New} {Paradigm} for {Power} {Electronics} {Design}},
	volume = {72},
	issn = {1557-9948},
	doi = {10.1109/TIE.2024.3454408},
	abstract = {Large language models (LLMs) have shown exciting potential in powering the growth of many industries, yet their adoption in the power electronics (PE) sector is hindered by a lack of specialized PE technical expertise and challenges in processing PE-specific data. This study presents a pioneering approach to establish a multimodal LLM tailored for PE design applications, named PE-GPT. The methodology involves enhancing PE-GPT with retrieval augmented generation from a PE knowledge base, and proposes a hybrid framework that integrates an LLM agent with metaheuristic algorithms, Model Zoo, and Simulation Repository. This enhances its multimodal processing capabilities and enables integration into the existing design workflow. The PE-GPT methodology is demonstrated with two case studies: modulation design of the dual-active bridge (DAB) converter and circuit parameter design of the buck converter. PE-GPT demonstrates a 22.2\% increase in correctness compared to human experts. Against other leading LLMs, PE-GPT shows a 35.6\% improvement in correctness and a 15.4\% enhancement in consistency, reducing hallucination. Hardware experiments validate PE-GPT’s multimodal capabilities in optimizing a five-degree-of-freedom modulation strategy for the DAB converter. The generalization of PE-GPT to other PE design applications and associated AI ethical considerations are also discussed. This research concludes by outlining inspiring future research directions, encouraging researchers to expand the boundaries of the PE industry and advance toward a more intelligent era.},
	number = {4},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Lin, Fanfan and Li, Xinze and Lei, Weihao and Rodriguez-Andina, Juan J. and Guerrero, Josep M. and Wen, Changyun and Zhang, Xin and Ma, Hao},
	month = apr,
	year = {2025},
	keywords = {Adaptation models, Analytical models, Data models, Integrated circuit modeling, Knowledge based systems, Large language model (LLM), Metaheuristics, Modulation, multimodal AI, Physics, physics-informed AI, power converter design, Power electronics, power electronics (PE) design, Vectors},
	pages = {3778--3791},
}

@inproceedings{weerathunge_optimizing_2025,
	title = {Optimizing {Response} {Consistency} of {Large} {Language} {Models} in {Medical} {Education} through {Prompt} {Engineering}},
	doi = {10.1109/ICARC64760.2025.10963313},
	abstract = {This research focuses on optimizing the response consistency of Large Language Models (LLMs) in medical education through advanced prompt engineering techniques. LLMs often give different answers to the same question, making self-consistency a critical parameter for assessing their performance. Addressing this inconsistency is essential in high-stakes fields like healthcare, where reliable and accurate information is important. The study employed custom prompt engineering strategies, including zero-shot, few-shot, and Chain-of-Thought (CoT) prompting, to improve LLM output consistency and accuracy. We implemented a retrieval-augmented generation (RAG) framework to use external knowledge from trusted medical resources, keeping the responses accurate and contextually appropriate. Responses were scored on several dimensions: content relevance, completeness, and clinical correctness, and assessed for consistency by asking repeated queries. The results showed significant enhancement in the consistency and accuracy of the responses, proving the effectiveness of the presented method. This work outlines suggestions for the use of LLMs in a way that can be incorporated into medical education while considering the limitations. It underscores the need for further exploration of prompt engineering to improve LLM performance and establishes these tools as reliable resources for training healthcare professionals.},
	booktitle = {2025 5th {International} {Conference} on {Advanced} {Research} in {Computing} ({ICARC})},
	author = {Weerathunge, Tharindu and Jayalal, Shantha and Wijayasiriwardhane, Keerthi},
	month = feb,
	year = {2025},
	keywords = {Accuracy, Large language models, Large Language Models, Medical Education, Medical services, Prompt engineering, Prompt Engineering, Reliability engineering, Response Accuracy, Response Consistency, Retrieval augmented generation, Training},
	pages = {1--6},
}

@article{panagoulias_lyricel_2025,
	title = {{LYRICEL}: {Knowledge} {Graphs} {Combined} {With} {Large} {Language} {Models} and {Machine} {Learning} for {Cross}-{Cultural} {Analysis} of {Lyrics}—{The} {Case} of {Greek} {Songs}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3597213},
	abstract = {This paper presents LYRICEL, a framework integrating Knowledge Graph (KG) representation learning, Large Language Models (LLMs), and machine learning for reliable, explainable, and validatable cross-cultural lyric analysis. The core component, Sequential Language Model Integration (SLMI), enhances the interpretability and reliability of transformer-based LLMs by addressing explainability and validation challenges through Retrieval-Augmented Generation (RAG), hybrid search, and rule-based evaluation. An important feature of LYRICEL is its use of KG visualizations, which serve as dynamic links to improve interpretability and validatability by structuring data relationships and sources. These visualizations are central to advancements in four areas: KG representation learning, knowledge acquisition, temporal KGs, and knowledge-aware applications. Tested on Greek folk music with models like GPT-4o and BERT, LYRICEL’s trustworthiness is assessed using the VIRTSI model, which quantifies cognitive trust in human-computer interactions. The framework shows strong potential for cross-cultural applications, particularly in languages such as Modern Greek which encompasses a rich cultural heritage spanning centuries of history and traditions resulting in a complex study. The outcomes of GPT-enabled LYRICEL are compared to ChatGPT alone and show a significant improvement in the reliability and efficiency of interactions that can reach a global audience, enhancing the accessibility and understanding of diverse cultural heritages.},
	journal = {IEEE Access},
	author = {Panagoulias, Dimitrios P. and Tsichrintzi, Evangelia-Aikaterini and Sotiropoulos, Dionisios N. and Chrysafiadi, Konstantina and Sakkopoulos, Evangelos and Tsihrintzis, George A. and Virvou, Maria},
	year = {2025},
	keywords = {Accuracy, AI-based poem analysis, artificial intelligence, Artificial intelligence, Chatbots, ChatGPT, Cultural differences, cultural heritage, e-learning, Education, educational software, generative AI, Integrated circuit modeling, intelligent tutoring systems, knowledge graphs, Knowledge graphs, LLMs, machine learning, natural language processing, Reliability, Semantics, Transformers},
	pages = {141985--142006},
}

@inproceedings{haryanto_contextualized_2024,
	title = {Contextualized {AI} for {Cyber} {Defense}: {An} {Automated} {Survey} {Using} {LLMs}},
	doi = {10.1109/SIN63213.2024.10871242},
	abstract = {This paper surveys the potential of contextualized AI in enhancing cyber defense capabilities, revealing significant research growth from 2015 to 2024. We identify a focus on robustness, reliability, and integration methods, while noting gaps in organizational trust and governance frameworks. Our study employs two LLM-assisted literature survey methodologies: (A) ChatGPT 4 for exploration, and (B) Gemma 2:9b for filtering with Claude 3.5 Sonnet for full-text analysis. We discuss the effectiveness and challenges of using LLMs in academic research, providing insights for future researchers.},
	booktitle = {2024 17th {International} {Conference} on {Security} of {Information} and {Networks} ({SIN})},
	author = {Haryanto, Christoforus Yoga and Elvira, Anne Maria and Nguyen, Trung Duc and Vu, Minh Hieu and Hartanto, Yoshiano and Lomempow, Emily and Arakala, Arathi},
	month = dec,
	year = {2024},
	keywords = {artificial intelligence, Artificial intelligence, Chatbots, Computer crime, cyber defense strategy, cyber security, Filtering, meta-analysis, retrieval augmented generation, Robustness, Security, Surveys},
	pages = {1--8},
}

@inproceedings{yahia_empathetic_2025,
	title = {Empathetic {AI} for {Mental} {Health}: {A} {Comparative} {Study} of {Retrieval}-{Augmented} {Language} {Models} in {CBT} {Applications}},
	doi = {10.1109/IMSA65733.2025.11166690},
	abstract = {The global mental health crisis has underscored the urgent need for scalable, accessible, and cost-effective support systems. This study presents MindHaven, an AI-driven chatbot designed to deliver personalized mental health support based on Cognitive Behavioral Therapy (CBT) principles. Leveraging state-of-the-art Large Language Models (LLMs), including Llama-3.2-3B-Instruct, BlenderBot, and Retrieval-Augmented Generation (RAG)-enhanced architectures, we systematically evaluate chatbot efficacy in generating empathetic, contextually relevant, and clinically informed responses. A rigorous comparative analysis was conducted across twelve transformer-based models, fine-tuned on 99,086 structured therapeutic dialogues. Model performance was assessed using quantitative NLP metrics (ROUGE, BLEU, Semantic Similarity) and qualitative human evaluations. Results indicate that Llama-3.2-3B-Instruct achieved the highest semantic similarity score of 0.8123, while DistilGPT-2 with RAG demonstrated a 15.4\% improvement in response factuality over non-RAG models. Additionally, Llama-3.2-3B-bnb-4bit obtained the lowest training loss (0.6051), significantly outperforming larger models such as MedAlpaca-7B. These findings suggest that parameter-efficient architectures can rival larger models in mental health applications. Despite promising advancements, challenges remain in long-term context retention, ethical safeguards, and crisis intervention capabilities. Future work will focus on multi-lingual generalization, dynamic memory augmentation, and psychologist-in-the-loop validation to ensure clinical safety and efficacy. By bridging cutting-edge AI with evidence-based mental health interventions, this research lays a foundation for the responsible deployment of scalable, AI-assisted therapeutic solutions, with potential applications in digital psychiatry, mental wellness apps, and hybrid human-AI therapy frameworks.},
	booktitle = {2025 {Intelligent} {Methods}, {Systems}, and {Applications}​ ({IMSA})},
	author = {Yahia, Ahmed and Bakry, Mohamed and Radwan, Nada and Yossef, Nourhan and Amin, Ahmed and Ahmed, Ibrahim and Eldin Saad, Noha Gamal},
	month = jul,
	year = {2025},
	keywords = {Artificial Intelligence, Chatbot, Chatbots, Cognitive Behavioral Therapy, Large language models, Large Language Models, Medical treatment, Mental health, Mental Health, Psychiatry, Retrieval augmented generation, Safety, Semantics, Training, Transformers},
	pages = {93--98},
}

@inproceedings{su_parameter_2024,
	title = {Parameter recommendation system enhanced by {RAG} based on process knowledge},
	doi = {10.1109/EIT63098.2024.10762220},
	abstract = {This article proposes an innovative method that utilizes the Neo4j graph database as a vector database to segment text, images, and table contents in process files and convert them into vector form for storage. On this basis, a parameter recommendation system was designed and implemented. The system efficiently retrieves parameter reference questions raised by users through vector similarity matching technology, and passes the matched relevant results to advanced machine learning models for further analysis. Ultimately, the system is able to accurately answer users' questions and recommend corresponding parameter ranges, significantly improving the efficiency and accuracy of enterprises in obtaining specific parameter requirements.},
	booktitle = {2024 3rd {International} {Conference} on {Electronics} and {Information} {Technology} ({EIT})},
	author = {Su, Aihua and Sun, Jiawei and Dong, Kejing and Ling, Weiqing},
	month = sep,
	year = {2024},
	keywords = {Accuracy, Analytical models, Data models, Databases, Electrical impedance tomography, Image segmentation, Information technology, langchain, large language model, Machine learning, Neo4j data base, recommended process parameters system, Recommender systems, Vectors},
	pages = {467--471},
}

@book{antic_notitle_2024,
	isbn = {978-1-80324-144-9},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10769274},
	abstract = {Updated to include three new chapters on transformers, natural language understanding (NLU) with explainable AI, and dabbling with popular LLMs from Hugging Face and OpenAIKey FeaturesLeverage ready-to-use recipes with the latest LLMs, including Mistral, Llama, and OpenAI modelsUse LLM-powered agents for custom tasks and real-world interactionsGain practical, in-depth knowledge of transformers and their role in implementing various NLP tasks with open-source and advanced LLMsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionHarness the power of Natural Language Processing to overcome real-world text analysis challenges with this recipe-based roadmap written by two seasoned NLP experts with vast experience transforming various industries with their NLP prowess. You’ll be able to make the most of the latest NLP advancements, including large language models (LLMs), and leverage their capabilities through Hugging Face transformers. Through a series of hands-on recipes, you’ll master essential techniques such as extracting entities and visualizing text data. The authors will expertly guide you through building pipelines for sentiment analysis, topic modeling, and question-answering using popular libraries like spaCy, Gensim, and NLTK. You’ll also learn to implement RAG pipelines to draw out precise answers from a text corpus using LLMs. This second edition expands your skillset with new chapters on cutting-edge LLMs like GPT-4, Natural Language Understanding (NLU), and Explainable AI (XAI)—fostering trust and in your NLP models. By the end of this book, you'll be equipped with the skills to apply advanced text processing techniques, use pre-trained transformer models, build custom NLP pipelines to extract valuable insights from text data to drive informed decision-making.What you will learnUnderstand fundamental NLP concepts along with their applications using examples in PythonClassify text quickly and accurately with rule-based and supervised methodsTrain NER models and perform sentiment analysis to identify entities and emotions in textExplore topic modeling and text visualization to reveal themes and relationships within textLeverage Hugging Face and OpenAI LLMs to perform advanced NLP tasksUse question-answering techniques to handle both open and closed domainsApply XAI techniques to better understand your model predictionsWho this book is forThis updated edition of the Python Natural Language Processing Cookbook is for data scientists, machine learning engineers, and developers with a background in Python. Whether you’re looking to learn NLP techniques, extract valuable insights from textual data, or create foundational applications, this book will equip you with basic to intermediate skills. No prior NLP knowledge is necessary to get started. All you need is familiarity with basic programming principles. For seasoned developers, the updated sections offer the latest on transformers, explainable AI, and Generative AI with LLMs.},
	publisher = {Packt Publishing},
	author = {Antić, Zhenya and Chakravarty, Saurabh},
	year = {2024},
	note = {Publication Title: Python Natural Language Processing Cookbook: Over 60 recipes for building powerful NLP solutions using Python and LLM libraries},
}

@inproceedings{p_ai-based_2025,
	title = {{AI}-{Based} {Versatile} {Troubleshooting} {Guide} for {System} of {Systems} in {Aircraft}},
	doi = {10.1109/SPACE65882.2025.11170869},
	abstract = {The current day automotive systems are complex in nature and comprise systems of systems. The automotive domain spans ground vehicles to aircraft and spacecraft. Due to the system-of-systems nature, the maintenance of such systems is very challenging and time-consuming. The maintenance logs/records for handling the troubleshooting of such systems are available for several automotive systems. To handle the challenges of troubleshooting such complex systems and leveraging the huge troubleshooting records available in the design and maintenance houses, it is proposed to develop an AI-based application which act as an aid for troubleshooting such complex systems. The program incorporates cutting-edge AI technologies and must be user-friendly for the maintenance team.This paper focuses on aircraft systems, presenting two distinct approaches to troubleshooting. The first approach employs a stored data concept where a web-based application, supported by a comprehensive database, allows users to select the system, subsystem, and problem description, and retrieve the corresponding Fault Isolation manual. The second approach uses generative AI approaches, training several LLMs, including BERT along with GPT-2, as well as RAG methodology using Mistral-Nemo-Instruct-2407, on the dataset.},
	booktitle = {2025 {IEEE} {Space}, {Aerospace} and {Defence} {Conference} ({SPACE})},
	author = {P, Jayaprakash K and K, Somasekhar and P, Santhya and Menon, Rajalakshmi and Amrutesh, Aniverthy},
	month = jul,
	year = {2025},
	keywords = {Accuracy, AI-based troubleshooting, Aircraft, Aircraft maintenance, Complex systems, Fault Isolation manuals, Generative AI, Generative AI techniques, GPT-2, Large Language Models (LLMs), Maintenance, Manuals, Real-time systems, System of systems, Training, Vehicle dynamics},
	pages = {1--6},
}

@inproceedings{purohit_graphaide_2024,
	title = {{GraphAide}: {Advanced} {Graph}-{Assisted} {Query} and {Reasoning} {System}},
	doi = {10.1109/BigData62323.2024.10825705},
	abstract = {Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
	month = dec,
	year = {2024},
	note = {ISSN: 2573-2978},
	keywords = {Accuracy, Cognition, Knowledge graphs, Large language models, Pattern matching, Retrieval augmented generation, Scalability, Semantic Web, Semantics, Usability},
	pages = {3485--3493},
}

@inproceedings{liu_siqa_2025,
	title = {{SiQA}: {A} {Large} {Multi}-{Modal} {Question} {Answering} {Model} for {Structured} {Images} {Based} on {RAG}},
	doi = {10.1109/ICASSP49660.2025.10888359},
	abstract = {Existing Large Multimodal Models (LMMs) demonstrate excellent performance in handling visual tasks in everyday scenarios. However, they still face challenges in understanding structured images, such as flowcharts and organizational charts, which are characterized by text-rich and complex hierarchical components. In this paper, we propose SiQA, a knowledge construction and Retrieval-Augmented Generation(RAG)-based multimodal Question-Answering model designed for Structured Images. SiQA operates in three stages: Knowledge Graph (KG) generation, retrieval-augmented, and answer generation. First, a KG representing the semantics of the structured images is generated through component analysis. We then performed similarity retrieval between the KG and queries, using a node-first algorithm to construct the most relevant subgraph. Finally, after performing an encoding alignment on the multimodal information, it is fed into the LLM to generate the answer. Additionally, we introduce a new dataset, OCQA1, which includes 5,112 questions derived from 1,000 Organizational Charts. We evaluated SiQA’s structured image detection and question-answering capabilities on the FD-DETR (a flowchart dataset) and SCQA, and verified its effectiveness and strong generalization ability through comparisons with existing state-of-the-art (SOTA) methods.},
	booktitle = {{ICASSP} 2025 - 2025 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Liu, Jiawang and Tao, Ye and Wang, Fei and Li, Hui and Qin, Xiugong},
	month = apr,
	year = {2025},
	note = {ISSN: 2379-190X},
	keywords = {Encoding, Faces, Flowcharts, Knowledge graphs, multimodal, QA, Question answering (information retrieval), RAG, Semantics, Signal processing, Signal processing algorithms, Speech processing, structured images, Visualization},
	pages = {1--5},
}

@inproceedings{fortuna_natural_2024,
	title = {Natural {Language} {Interaction} with a {Household} {Electricity} {Knowledge}-based {Digital} {Twin}},
	doi = {10.1109/SmartGridComm60555.2024.10738062},
	abstract = {Domain specific digital twins, representing a digital replica of various segments of the smart grid, are foreseen as able to model, simulate, and control the respective segments. At the same time, knowledge-based digital twins, coupled with AI, may also empower humans to understand aspects of the system through natural language interaction in view of planning and policy making. This paper is the first to assess and report on the potential of Retrieval Augmented Generation (RAG) question answers related to household electrical energy measurement aspects leveraging a knowledge-based energy digital twin. Relying on the recently published electricity consumption knowledge graph that actually represents a knowledge-based digital twin, we study the capabilities of ChatGPT, Gemini and Llama in answering electricity related questions. Furthermore, we compare the answers with the ones generated through a RAG techniques that leverages an existing electricity knowledge-based digital twin. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Communications}, {Control}, and {Computing} {Technologies} for {Smart} {Grids} ({SmartGridComm})},
	author = {Fortuna, Carolina and Hanžel, Vid and Bertalanič, Blaz},
	month = sep,
	year = {2024},
	note = {ISSN: 2474-2902},
	keywords = {Chatbots, Data analysis, Digital twins, Electricity, house-holds, Knowledge based systems, knowledge graph, knowledge-based digital twin, large language models, Natural languages, Planning, Question answering (information retrieval), retrieval augmented generation, Robustness, Smart grids},
	pages = {8--14},
}

@inproceedings{chi_rtlexplain_2025,
	title = {{RTLExplain}: {A} {Structured} {Approach} to {RTL} {Code} {Summarization} and {Question} {Answering} for {Medium}-to-{Large} {Designs} {Using} {LLMs}},
	doi = {10.1109/MLCAD65511.2025.11189167},
	abstract = {Large Language Models (LLMs) show promise in assisting with Register Transfer Level (RTL) design tasks, including code summarization, documentation, and question answering. However, directly applying LLMs to entire RTL codebases often leads to low accuracy in these tasks. This is primarily because LLMs are less exposed to RTL code during pretraining, limiting their ability to understand RTL-specific semantics and structural dependencies. To overcome this challenge, we propose RTLExplain, which builds project-specific knowledge bases to enhance LLM performance on RTL design tasks. Our method is entirely offline and requires no additional training or fine-tuning. Experiments on code summarization using the generated knowledge bases demonstrate consistent improvements across various medium-to-large RTL projects, even when variable names are obfuscated. Furthermore, we use these knowledge bases to support Retrieval-Augmented Generation (RAG) for question answering tasks. Results show that our enhanced knowledge bases, when combined with RAG, improve question-answering accuracy by 37\% compared to naïve prompting and 27\% compared to conventional RAG.},
	booktitle = {2025 {ACM}/{IEEE} 7th {Symposium} on {Machine} {Learning} for {CAD} ({MLCAD})},
	author = {Chi, Ting-Hsun and Mackin, Charles and Shi, Luyao and Vijayaraghavan, Prashanth and Tsai, Hsinyu and Degan, Ehsan},
	month = sep,
	year = {2025},
	keywords = {Accuracy, Codes, data dependency, documentation, Documentation, Hardware, hardware assistant, Knowledge based systems, Large language models, Large Language Models, Question answering (information retrieval), RAG, Register transfer level, Semantics, summarization, Training, Verilog},
	pages = {1--7},
}

@article{tang_automatic_2025,
	title = {Automatic {Retrieval}-{Augmented} {Generation} of {6G} {Network} {Specifications} for {Use} {Cases}},
	volume = {63},
	issn = {1558-1896},
	doi = {10.1109/MCOM.002.2400280},
	abstract = {6G open radio access networks (O-RAN) promises to open data interfaces to enable plug-and-play service apps, many of which are consumer and business-facing. Opening up 6G access lowers the barrier to innovation but raises the challenge of required communication specifications that are not fully known to all service designers. As such, business innovators must either be familiar with 6G standards, or consult with experts. Enabling consistent, unbiased, rapid, and low-cost requirement assessment and specification generation is crucial to the O-RAN innovation ecosystem. Here, we discuss our initiative to bridge service specification gaps between network service providers and business innovators leveraging large language models (LLMs). We first review the state-of-the-art and motivation in 6G plug-and-play services, capabilities, potential use cases, and LLMs. We identify an ample innovation space for hybrid use cases that may require diverse and variational wireless functionalities across its operating time. We show that the network specification can be automated, and present the first automatic retrieval-augmented network service specification framework for 6G use cases. To enable public acceptance and feedback, a website interface is published for the research and industrial communities to experiment with the framework. We hope this review highlights the need for emerging foundation models for this area and motivates researcher engagement and contribution to the community through our framework.},
	number = {4},
	journal = {IEEE Communications Magazine},
	author = {Tang, Yun and Guo, Weisi},
	month = apr,
	year = {2025},
	keywords = {6G mobile communication, Databases, Knowledge engineering, Low latency communication, Open RAN, Real-time systems, Reliability, Sensors, Technological innovation, User experience},
	pages = {95--102},
}

@inproceedings{bei_manufacturing_2024,
	title = {Manufacturing {Domain} {QA} with {Integrated} {Term} {Enhanced} {RAG}},
	doi = {10.1109/IJCNN60899.2024.10649905},
	abstract = {Large Language Models (LLMs) have demonstrated powerful capabilities, yet LLMs face issues like hallucination in certain domain-specific areas. Consequently, an increasing number of domain-specific models are emerging. The current paradigm for domain-specific models involves training with domain data, followed by the employment of Retrieval-Augmented Generation (RAG) to mitigate hallucination issues. However, in precision-critical domains such as manufacturing, if the knowledge documents are of low quality or contain noise, the context retrieved through simple semantic matching by RAG may not necessarily benefit model output. Additionally, there can be issues like getting "lost in the middle" due to irrelevant or excessive context. To overcome this, we introduce the Integrated Term Enhancement Methodology (ITEM). Inspired by Chinese educational methods focused on key term elucidation, ITEM extracts and explains critical terms precisely from knowledge documents to form a comprehensive Term Dictionary for retrieving terms and explanations to enhance query capabilities. This methodology refines query responses by providing more accurate and contextually relevant information. To assess ITEM's effectiveness, we utilize the Chinese Mould Manufacturing Dataset (CMMD) and Contextualized Adaptive Response Assessment (CARA) metric method. Our experiment demonstrates that ITEM significantly outperforms existing retrieval enhancement Dense Retrievers by over 17.0\% in accuracy while requiring only 80\% of their token length. Moreover, the accuracy of our method exceeded that of GPT-4 by 5.0\%. This advancement represents a significant leap in context-specific retrieval in LLMs, especially beneficial for specialized domains. The results underscore ITEM's potential as a transformative method in the field, offering new perspectives on integrating domain-specific knowledge into LLMs.},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Bei, Yijun and Fang, Zhibin and Mao, Shenyu and Yu, Shuyi and Jiang, Yan and Tong, Yining and Cai, Weimin},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {Accuracy, Large language models, Measurement, Neural networks, Noise, Semantics, Training},
	pages = {1--8},
}

@book{meyer_notitle_2024,
	isbn = {978-1-83546-959-0},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10769215},
	abstract = {Unlock the power of GenAI by effortlessly linking your C\# and Python apps with cutting-edge models, orchestrating diverse AI services with finesse, and crafting bespoke applications through immersive, real-world examplesKey FeaturesLink your C\# and Python applications with the latest AI models from OpenAICombine and orchestrate different AI services such as text and image generatorsCreate your own AI apps with real-world use case examples that show you how to use basic generative AI, create images, process documents, use a vector databasePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the fast-paced world of AI, developers are constantly seeking efficient ways to integrate AI capabilities into their apps. Microsoft Semantic Kernel simplifies this process by using the GenAI features from Microsoft and OpenAI. Written by Lucas A. Meyer, a Principal Research Scientist in Microsoft’s AI for Good Lab, this book helps you get hands on with Semantic Kernel. It begins by introducing you to different generative AI services such as GPT-3.5 and GPT-4, demonstrating their integration with Semantic Kernel. You’ll then learn to craft prompt templates for reuse across various AI services and variables. Next, you’ll learn how to add functionality to Semantic Kernel by creating your own plugins. The second part of the book shows you how to combine multiple plugins to execute complex actions, and how to let Semantic Kernel use its own AI to solve complex problems by calling plugins, including the ones made by you. The book concludes by teaching you how to use vector databases to expand the memory of your AI services and how to help AI remember the context of earlier requests. You’ll also be guided through several real-world examples of applications, such as RAG and custom GPT agents. By the end of this book, you'll have gained the knowledge you need to start using Semantic Kernel to add AI capabilities to your applications.What you will learnWrite reusable AI prompts and connect to different AI providersCreate new plugins that extend the capabilities of AI servicesUnderstand how to combine multiple plugins to execute complex actionsOrchestrate multiple AI services to accomplish a taskLeverage the powerful planner to automatically create appropriate AI callsUse vector databases as additional memory for your AI tasksDeploy your application to ChatGPT, making it available to hundreds of millions of usersWho this book is forThis book is for beginner-level to experienced .NET or Python software developers who want to quickly incorporate the latest AI technologies into their applications, without having to learn the details of every new AI service. Product managers with some development experience will find this book helpful while creating proof-of-concept applications. This book requires working knowledge of programming basics.},
	publisher = {Packt Publishing},
	author = {Meyer, Lucas A.},
	year = {2024},
	note = {Publication Title: Building AI Applications with Microsoft Semantic Kernel: Easily integrate generative AI capabilities and copilot experiences into your applications},
}

@article{xu_large_2024,
	title = {Large {Multi}-{Modal} {Models} ({LMMs}) as {Universal} {Foundation} {Models} for {AI}-{Native} {Wireless} {Systems}},
	volume = {38},
	issn = {1558-156X},
	doi = {10.1109/MNET.2024.3427313},
	abstract = {Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.},
	number = {5},
	journal = {IEEE Network},
	author = {Xu, Shengzhe and Kurisummoottil Thomas, Christo and Hashash, Omar and Muralidhar, Nikhil and Saad, Walid and Ramakrishnan, Naren},
	month = sep,
	year = {2024},
	keywords = {6G mobile communication, AI-native, Alignment, Artificial intelligence, Cognition, Grounding, Instructibility, Large language models, Large multi-modal models, Mathematical models, Natural language processing, Next generation networking, Sensors, Symbols, Universal foundation model, Wireless communication, Wireless sensor networks},
	pages = {10--20},
}

@inproceedings{zhu_enhancing_2024-1,
	title = {Enhancing {Large} {Language} {Models} with {Knowledge} {Graphs} for {Robust} {Question} {Answering}},
	doi = {10.1109/ICPADS63350.2024.00042},
	abstract = {In recent years, large language models (LLMs) have shown rapid development, becoming one of the most popular topics in the field of artificial intelligence. LLMs have demonstrated powerful generalization and learning capabilities, and their performance on various language tasks has been remarkable. Despite their successes, LLMs face significant challenges, particularly in domain-specific tasks that require structured knowledge, often leading to issues such as hallucinations. To mitigate these challenges, we propose a novel system, SynaptiQA, which integrates LLMs with Knowledge Graphs (KGs) to answer more questions about knowledge. Our approach leverages the generative capabilities of LLMs to create and optimize KG queries, thereby improving the accuracy and contextual relevance of responses. Experimental results in an industrial data set demonstrate that SynaptiQA outperforms baseline models and naive retrieval-augmented generation (RAG) systems, demonstrating improved accuracy and reduced hallucinations. This integration of KGs with LLMs paves the way for more reliable and interpretable domain-specific question answering systems.},
	booktitle = {2024 {IEEE} 30th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Zhu, Zhui and Qi, Guangpeng and Shang, Guangyong and He, Qingfeng and Zhang, Weichen and Li, Ningbo and Chen, Yunzhi and Hu, Lijun and Zhang, Wenqiang and Dang, Fan},
	month = oct,
	year = {2024},
	note = {ISSN: 2690-5965},
	keywords = {Accuracy, Artificial Intelligence, Cognition, Data models, Distributed databases, Faces, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Question answering (information retrieval), Reliability, Vectors},
	pages = {262--269},
}

@inproceedings{tsai_ai-enhanced_2024,
	title = {{AI}-{Enhanced} {Virtual} {Nursing} {Systems}: {Revolutionizing} {Patient} {Education} in {Modern} {Healthcare}},
	doi = {10.1109/ICOT64290.2024.10936938},
	abstract = {Traditional patient education relies on static brochures, lacking interactivity and increasing healthcare providers' workload. This study introduces a virtual nurse dialogue system using ASR and a fine-tuned LLama3 8B LLM trained on Traditional Chinese medical texts. A RAG framework with a structured knowledge base enhances accuracy and minimizes misinformation. The system features an RWD-based interface with 3D animations and synchronized voice outputs for an immersive experience. Experimental results show 95\% accuracy and responses under 5 seconds, with 90\% user satisfaction, demonstrating its potential to enhance patient education and reduce healthcare burdens.},
	booktitle = {2024 {International} {Conference} on {Orange} {Technology} ({ICOT})},
	author = {Tsai, Hsin-Chun and Chen, Meng-Wei and Wang, Jhing-Fa},
	month = dec,
	year = {2024},
	keywords = {Accuracy, dialogue system, Education, health education, Knowledge based systems, large language model, Large language models, Medical services, Solid modeling, Synchronization, Three-dimensional displays, Time factors, User interfaces, virtual nurse},
	pages = {1--5},
}

@inproceedings{zhang_llm-assisted_2025,
	title = {{LLM}-assisted {Performance} {Estimation} of {Embedded} {Software} on {RISC}-{V} {Processors}},
	doi = {10.1109/DDECS63720.2025.11006767},
	abstract = {In this paper, we present a methodology that combines a Large Language Model (LLM) with a traditional Machine Learning (ML) approach to estimate the performance of embedded software on RISC-V processors across different microarchitectures. In particular, we employ a Retrieval-Augmented Generation (RAG)-based LLM to extract performance-related information from processor specifications and source code. Additionally, we leverage the predictive capabilities of ML models to create Predictive Models (PMs) for RISC-V processors. To demonstrate the effectiveness of our hybrid approach, we present results on the performance estimation of open-source benchmarks using the generated PMs, with open-source RISC-V-based Register Transfer Level (RTL) implementations as reference models. Our results demonstrate that our proposed LLM-assisted methodology provides highly accurate predictions, with Mean Absolute Percentage Errors (MAPEs) of only 2.50\% for SweRV core and 11.90\% for RSD core, respectively. In comparison with the state-of-the-art methodology, our approach achieves significant improvements, reducing the MAPE by 61.54\% for SweRV and 36.02\% for RSD.},
	booktitle = {2025 {IEEE} 28th {International} {Symposium} on {Design} and {Diagnostics} of {Electronic} {Circuits} and {Systems} ({DDECS})},
	author = {Zhang, Weiyan and Hassan, Muhammad and Drechsler, Rolf},
	month = may,
	year = {2025},
	note = {ISSN: 2473-2117},
	keywords = {Accuracy, Benchmark testing, Embedded software, embedded system, Embedded systems, large language model, Large language models, machine learning, Machine learning, performance estimation, Predictive models, Program processors, Register transfer level, RISC-V, Source coding},
	pages = {7--12},
}

@inproceedings{patel_agentic_2025,
	title = {Agentic retrieval-augmented generation for financial {QA}: a {SingleStore} optimization approach},
	volume = {2025},
	doi = {10.1049/icp.2025.1296},
	abstract = {As the adoption of large language models (LLMs) accelerates, many small and medium enterprises lack the resources needed to train and maintain domain-specialized systems. Retrieval augmented generation (RAG) architectures provide a cost-effective alternative by leveraging external knowledge bases for enhanced performance. However, in high-stakes domains such as finance, ensuring accuracy and reliability remains a significant challenge. This paper introduces an Agentic RAG framework designed to improve financial question answering by integrating an adaptive, multi-step retrieval process within a Single Vector Store (SingleStore) setting. Utilizing a benchmark of financial questions (FinanceBench), our experiments show a jump in accuracy from the previously reported 50\% to around 70\% in single-store retrieval. The iterative retrieval and reasoning approach mitigates many common issues such as incomplete context and hallucinations while preserving the efficiency benefits of SingleStore. The results underscore the potential for more robust, cost-efficient financial chatbots tailored to the specialized needs of financial institutions.},
	booktitle = {Parul {University} {International} {Conference} on {Engineering} and {Technology} 2025 ({PiCET} 2025)},
	author = {Patel, Yug and Badre, Snehlata},
	month = may,
	year = {2025},
	pages = {208--213},
}

@inproceedings{guo_root_2025,
	title = {Root {Cause} {Analysis} of {Power} {Grid} {5G} {Network} {Faults} {Based} on {Large} {Language} {Model}},
	doi = {10.1109/CSCWD64889.2025.11033346},
	abstract = {The growing complexity and diversity of 5G network architecture (e.g., power grid 5G network) have made security risk assessment and root cause analysis increasingly challenging. Recent advances in large language models (LLMs) have the potential to transform this landscape. However, existing LLMs-based solutions primarily focus on understanding the language of 5G telecommunications, while overlooking potential security vulnerabilities in the data flows. To facilitate LLMs' in-depth application, this paper presents RCA-LLM, a novel fault root cause analysis framework for 5G networks developed from tailored LLMs-based solutions. In explicit terms, RCA-LLM is trained by inputting processed and organized fault information for fine-tuning, and combined with retrieval-augmented generation (RAG) technology to significantly improve the accuracy of 5G fault analysis. Our experimental results indicate that RCA-LLM performs well in fault analysis, effectively supporting users in diagnosing and resolving fault issues. Model evaluation results further demonstrate that the model significantly improves fault analysis accuracy and has high practical value. In addition, RCA-LLM provides important reference value for efficient operation and maintenance management of 5G and future power grid networks, while also offering new ideas for advancing intelligent fault analysis.},
	booktitle = {2025 28th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design} ({CSCWD})},
	author = {Guo, Zhaorui and Zou, Jing and Xin, Peizhe and Zhao, Xiongfei and Hu, Tian and Zhuang, Shangyuan and Sun, Jiyan and Liu, Yinlong and Ma, Wei},
	month = may,
	year = {2025},
	note = {ISSN: 2768-1904},
	keywords = {5G mobile communication, 5G network, Accuracy, Analytical models, Large language models, Power grids, Risk management, Root cause analysis, Signalling messages, Telecommunications, Training, Transforms},
	pages = {624--629},
}

@inproceedings{singla_hicon_2024,
	title = {{HICON} {AI}: {Higher} {Education} {Counseling} {Bot}},
	doi = {10.1109/ICPCSN62568.2024.00131},
	abstract = {As more and more students are seeking counseling services to help them pursue higher education opportunities abroad, it has become apparent that there is a lack of automation and a monopolization of counseling agencies, which presents a challenge for these students. To address this issue, this study has developed HICON AI: Higher Education Counselor. HI-CON made from Higher - Counseling is an innovative application that offers personalized college selection and preparation recommendations to students. By asking a set of defined questions, the bot gets to know the user and provides tailored guidance based on the information provided, utilizing refined Machine Learning models and Retrieval Augmented Generation. This study has specifically used Llama 2, LLM by meta for the considered use case, because of its high performance and financial viability. The developed new product ensures the highest level of accuracy and reliability.},
	booktitle = {2024 4th {International} {Conference} on {Pervasive} {Computing} and {Social} {Networking} ({ICPCSN})},
	author = {Singla, Arjun Dev and Tripathi, Shashank and Victoria, A Helen},
	month = may,
	year = {2024},
	keywords = {Accuracy, Automation, Categorizer, Education, Employee welfare, HICON AI, LLMs, Machine learning, Machine Learning, Natural Language Processing, Pervasive computing, Resume Screener, Retrieval Augmented Generation, Social networking (online), text to Speech},
	pages = {779--784},
}

@inproceedings{patel_large_2025,
	title = {Large {Language} {Models}: {Evolution}, {Architecture}, {Applications}, and {Future} {Horizons}},
	doi = {10.1109/ICSCSA66339.2025.11170884},
	abstract = {Large Language Models (LLMs) have revolutionized artificial intelligence and natural language processing since the advent of transformer architectures in last decade. Trained on vast amounts of data with parameter counts ranging from billions to trillions, models such as GPT-4, Claude, Grok, and Llama demonstrate remarkable proficiency in language comprehension, generation, and manipulation, including access to additional domains such as conversational systems, code generation, scientific writing, and creative content creation. This survey summarizes the evolution of LLMs, from early statistical methods and neural networks to modern transformer-based frameworks, emphasizing milestones like GPT-3’s few-shot learning and GPT-4o’s multimodal advancements. We present a comparative analysis of prominent LLMs, evaluating their architectures, parameter scales, and performance on benchmarks such as MMLU and MATH, with particular attention to Grok’s real-time knowledge integration. The study highlights LLMs’ transformative applications in academia, healthcare, education, and creative industries while addressing critical challenges, including biases, misinformation, computational demands, and interpretability limitations. We explore mitigation strategies such as retrieval-augmented generation and parameter-efficient fine-tuning. Further- more, we outline future directions, including multimodal LLMs, extended context processing, and ethical AI development, providing a comprehensive guide for researchers and practitioners shaping the responsible advancement of LLMs.},
	booktitle = {2025 5th {International} {Conference} on {Soft} {Computing} for {Security} {Applications} ({ICSCSA})},
	author = {Patel, Sanskruti and Dholakiya, Radhika Ashish},
	month = aug,
	year = {2025},
	keywords = {Adaptation models, AI Applications, Computational modeling, Computer architecture, Ethical AI, Few shot learning, Industries, Large language models, Large Language Models, Medical services, Natural language processing, Natural Language Processing, Retrieval augmented generation, Transformers},
	pages = {1922--1929},
}

@inproceedings{dong_survey_2024,
	title = {A {Survey} of {LLM}-based {Agents}: {Theories}, {Technologies}, {Applications} and {Suggestions}},
	doi = {10.1109/AIoTC63215.2024.10748304},
	abstract = {AI Agent has presented potential towards Artificial General Intelligence (AGI), which is expected to autonomously perceive the environments, make decisions and take actions. However, most of existing AI agents tend to train in confined environments with limited knowledge, yielding sub-optimal performance. Benefiting from the remarkable progress of large language models (LLMs), diverse LLM-based agents emerge. These agents employ LLM as the central brain to perceive, plan, and memorize, etc, which exhibit human-level intelligence across multifarious applications and obtain satisfactory performance. In this paper, we propose a survey of LLM-based agents from the perspective of theories, technologies, applications and suggestions, respectively. Specifically, we first deliver a recapitulative review of the theory foundation, which includes Large Language Models, Chain of Thought and AI Alignment, Retrieval-Augmented Generation, Embodied AI, etc; With this, we then present the key technologies, comprising four critical components: Perception, Planning, Memory and Action; Subsequently, we briefly explore some domain-related and evaluation applications; Finally, we provide pertinent suggestions based on the observations of significant challenges for LLM-based agents.},
	booktitle = {2024 3rd {International} {Conference} on {Artificial} {Intelligence}, {Internet} of {Things} and {Cloud} {Computing} {Technology} ({AIoTC})},
	author = {Dong, Xiaofei and Zhang, Xueqiang and Bu, Weixin and Zhang, Dan and Cao, Feng},
	month = sep,
	year = {2024},
	keywords = {Applications, Artificial general intelligence, Benchmark testing, Cloud computing, Computers, Internet of Things, Key Technologies, Large language models, LLM-based Agent, Planning, Reviews, Solids, Suggestions, Surveys, Theory Foundation},
	pages = {407--413},
}

@inproceedings{garima_harnessing_2024,
	title = {Harnessing the {Power} of {Language} {Models} for {Intelligent} {Digital} {Health} {Services}},
	doi = {10.23919/ITUK62727.2024.10772761},
	abstract = {This research proposes a novel framework that integrates state-of-the-art large language models (LLMs) with curated medical knowledge bases to enable personalized, reliable, and user-centric digital health services. The architecture combines advanced generative models, retrieval-augmented generation, and domain adaptation strategies to ensure the safety and ethical alignment of AI-driven health recommendations. Empirical evaluations, including automated benchmarks and user studies, demonstrate the framework’s ability to provide accurate, relevant, and personalized health information that resonates with patients and providers. The results highlight the potential of this approach to bridge the gap between general-purpose LLMs and domain-specific healthcare applications. However, the work also underscores the challenges in responsibly developing and deploying generative AI for healthcare, such as safety, robustness, fairness, privacy, and interpretability. The research advocates for multidisciplinary collaboration to address these challenges and realize the potential of AI in enhancing health and well-being worldwide. By prioritizing patient agency, clinical validity, and ethical practices, this work contributes to the growing body of knowledge at the intersection of AI and healthcare, laying the foundation for future research and innovation in personalized, equitable, and trustworthy AI health services.},
	booktitle = {2024 {ITU} {Kaleidoscope}: {Innovation} and {Digital} {Transformation} for a {Sustainable} {World} ({ITU} {K})},
	author = {Garima, Sogani and Swapnil, Morande and Shashank, Shah},
	month = oct,
	year = {2024},
	keywords = {Electronic healthcare, ethical AI, Ethics, generative AI, Generative AI, ITU, Knowledge based systems, knowledge retrieval, language models, Large language models, personalized healthcare, Privacy, Robustness, Safety, Technological innovation},
	pages = {1--8},
}

@inproceedings{rajbhar_aiced-prep_2025,
	title = {{AIced}-{Prep} -{AI} {Based} {Mock} {Interview} {Evaluator}},
	doi = {10.1109/ICDICI66477.2025.11135157},
	abstract = {AIced Prep is an AI-driven system designed to provide individuals with realistic mock interviews and personalized feedback, particularly targeting freshers, job seekers, students, and recruitment agencies. The system aims to build confidence, improve performance, and enhance candidates' chances of success in interviews. To achieve this, the project integrates a Question Generation System using RetrievalAugmented Generation(RAG) and Large Language Models (LLMs) for generating relevant, unique, and non-repetitive questions. The project fo- cuses on two primary use cases: interview question generation and resume classification and mock interview. By leveraging the Mistral-7B LLM for contextually accurate question generation and FAISS for efficient vector-based retrieval, the system dynami- cally processes documents, including PDFs, to generate questions. The web-based interface, built using FastAPI and Streamlit, ensures users have an interactive and accessible experience. This hybrid approach addresses common challenges in question generation, such as repetitive questions and irrelevant content, by combining information retrieval techniques with generative AI methods. The ultimate goal of AIced Prep is to make quality interview preparation accessible to everyone, providing scalable, efficient, and personalized interview preparation solution.},
	booktitle = {2025 6th {International} {Conference} on {Data} {Intelligence} and {Cognitive} {Informatics} ({ICDICI})},
	author = {Rajbhar, Snehal and Shelke, Siddhi and Singh, Ayush and Singh, Yash and Shinde, Pravin},
	month = jul,
	year = {2025},
	keywords = {Cognitive informatics, Generative AI, Generative Artificial Intelligence, Hybrid power systems, Information retrieval, Interviews, Large language models, LLM, Machine learning, Machine Learning, Mock Interview, Natural Language Processing, Question generation, Recruitment, Resumes, Text Generation},
	pages = {1840--1843},
}

@article{ge_innovative_2025,
	title = {An {Innovative} {Solution} to {Design} {Problems}: {Applying} the {Chain}-of-{Thought} {Technique} to {Integrate} {LLM}-{Based} {Agents} {With} {Concept} {Generation} {Methods}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3494054},
	abstract = {To enhance the application capabilities of large language models (LLMs) in conceptual design, this study explores how to achieve deep integration between LLM-based agents and concept generation methods using the chain-of-thought (CoT) technique and evaluates its feasibility. Using GPT-4 as a case study, we designed two agents: IntelliStorm (based on the unstructured brainstorming method) and EvoluTRIZ (based on the structured TRIZ method). Thirty participants were recruited, and through two experimental phases spaced one month apart, a comparative analysis of the effects of collaboration groups (human-agent vs. human-human) and concept generation methods (brainstorming vs. TRIZ) on participants’ physiological activation and creative thinking performance were conducted. The results show that the involvement of LLM-based agents can effectively reduce participants’ electrodermal activity(EDA) response levels, indicating a reduction in cognitive load. Moreover, participants maintained their distinct physiological patterns and performance advantages across different concept generation methods. For example, IntelliStorm, like brainstorming, evokes stronger responses to information stimuli, demonstrating superior thinking fluency; EvoluTRIZ, like the TRIZ, exhibits a higher frequency of information responses, showcasing enhanced thinking elaboration. However, originality tends to favor human-human collaboration. The findings confirm that integrating LLMs with traditional concept generation methods is an effective strategy made possible by combining CoT and retrieval-augmented generation (RAG) technologies. In the future, LLM-based agents are expected to achieve broader application in the design field by incorporating additional concept generation methods.},
	journal = {IEEE Access},
	author = {Ge, Shijun and Sun, Yuanbo and Cui, Yin and Wei, Dapeng},
	year = {2025},
	keywords = {Brain modeling, chain-of-thought fine-tuning, Cognitive load, Collaboration, Computational modeling, concept generation method, Creativity, Design methodology, EDA, Heuristic algorithms, human-agent collaboration, human-human collaboration, Knowledge based systems, Large language models, LLM-based agent, Particle swarm optimization, Training},
	pages = {10499--10512},
}

@inproceedings{anh-khoa_gvec_2024,
	title = {{GVEC}: {A} {Generative} {Vietnamese} {Chatbot} for {Economy}, {Using} {Vietnamese} {Economy} {Information} {Database} from {VnEconomy} {Community}},
	doi = {10.1109/MAPR63514.2024.10660907},
	abstract = {Currently, the problem of automatic question answering (Q/A) with applications in chatbot services has become essential, addressing various issues in daily life. There are many approaches to solving this problem, but each presents its own challenges. The generative AI approach has recently gained widespread attention due to its ability to provide smooth, human-like responses. However, it has the drawback of producing coherent-sounding but inaccurate or fabricated content, known as "hallucinations". In this study, we aim to find a solution for the question-answering task in the context of the Vietnamese economy. We propose the Generative Vietnamese Economy Chatbot (GVEC), based on the Vietnam Economy Information Database (VEID) retrieved from VnEconomy systems. Our proposition is to apply Retrieval-Augmented Generation (RAG) to various LLM systems and test them on our specially designed benchmark, called the Vietnamese Numeric Economy Information Question/Answer Datasets (VNEIQAD), generated by our Question/Answers Generating with Numerical Information (QAGwNI) algorithm. This will help better evaluate the solution in the economic context.},
	booktitle = {2024 {International} {Conference} on {Multimedia} {Analysis} and {Pattern} {Recognition} ({MAPR})},
	author = {Anh-Khoa, Ngo-Ho and Anh-Khoi, Ngo-Ho and Khuong-Duy, Vo},
	month = aug,
	year = {2024},
	note = {ISSN: 2770-6850},
	keywords = {automatic question answering, Benchmark testing, chatbot, Chatbots, Economics, generative AI, Generative AI, Multimedia databases, Question answering (information retrieval), Vietnamese language, Web services},
	pages = {1--6},
}

@inproceedings{anaissi_fine-tuning_2024,
	title = {Fine-{Tuning} {LLMs} for {Reliable} {Medical} {Question}-{Answering} {Services}},
	doi = {10.1109/ICDMW65004.2024.00026},
	abstract = {We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author = {Anaissi, Ali and Braytee, Ali and Akram, Junaid},
	month = dec,
	year = {2024},
	note = {ISSN: 2375-9259},
	keywords = {Accuracy, Information services, Large language models, Large Language Models, LLaMA-2, Matrix decomposition, medical question-answering, Medical services, Mistral, Noise, Overfitting, Reliability engineering, Retrieval augmented generation, retrieval on demand, Stability analysis},
	pages = {146--153},
}

@incollection{ots_securing_2025,
	title = {Securing the {Entire} {Application}},
	isbn = {978-1-394-29111-3},
	url = {https://ieeexplore.proxyucr.elogim.com/document/10955782},
	abstract = {Summary {\textless}p{\textgreater}In this chapter, we are going to expand our view from securing the Azure OpenAI service itself and look at securing the surrounding components of a typical large language model (LLM) application hosted in Azure. The application tier consists of the LLM service. In this tier, the LLM is orchestrated and exposed to the presentation tier. To enable audit log collection, we need to configure log export on the Front Door profile, similar to that of the Azure OpenAI configuration. Storage Account supports two access modes: centrally managed identity using Entra ID, and local authentication using shared access keys. Compared to Azure OpenAI, Storage Account supports disabling the local authentication in a more mature way. Local authentication is disabled in the portal UI under Settings \&\#x2010; Allow Storage Account Key access. This has to be changed from Enabled to Disabled to block local authentication.{\textless}/p{\textgreater}},
	booktitle = {Securing {Microsoft} {Azure} {OpenAI}},
	publisher = {Wiley},
	author = {Ots, Karl},
	year = {2025},
	keywords = {Databases, Grounding, Organizations, Retrieval augmented generation, Security, Threat modeling, Training, Training data, Tuning, Virtual machines},
	pages = {125--296},
}

@inproceedings{mohan_enhancing_2025,
	title = {Enhancing {Agricultural} {Advisory} {Services} with {Multilingual} {LLaMA} and {RAG}},
	doi = {10.1109/INCIP64058.2025.11019716},
	abstract = {Agriculture, being a vital component of global food security, necessitates the development of innovative solutions to address the knowledge gap experienced by numerous farmers, particularly in developing countries where access to expert advice is scarce. In these areas, agricultural producers often depend on helplines for essential guidance, but the exorbitant costs and limited accessibility of these services create substantial obstacles. By automating responses to agricultural inquiries, the burden on traditional helpline systems can be alleviated, enabling farmers to access prompt and precise information. The combination of artificial intelligence and agriculture offers a chance to tackle these challenges, with advanced language models, especially transformers, demonstrating significant potential in comprehending intricate agricultural queries and delivering appropriate responses. This paper investigates how large language models (llms) can simplify the process of finding answers for farmers by utilizing their advanced language processing abilities. By analyzing a vast collection of over four million farmer inquiries from tamil nadu, india, encompassing diverse agricultural scenarios, this study showcases the efficacy of llms in bridging information gaps and equipping farmers with immediate access to crucial knowledge.},
	booktitle = {2025 {International} {Conference} on {Next} {Generation} {Communication} \& {Information} {Processing} ({INCIP})},
	author = {Mohan, G. Bharathi and Adhitya, C.M. Jayanth and Mithilesh, A.},
	month = jan,
	year = {2025},
	keywords = {Agricultural Information Systems, Agriculture, Artificial Intelligence (AI), Biological system modeling, Farmer Inquiries, Food security, Information processing, Large language models, Large Language Models (LLMs), Metadata, Multilingual, Natural language processing, Natural Language Processing (NLP), Precision agriculture, Precision Agriculture, Solids, Transformers},
	pages = {550--556},
}

@inproceedings{alharthi_llm-powered_2025,
	title = {{LLM}-{Powered} {Automated} {Cloud} {Forensics}: {From} {Log} {Analysis} to {Investigation}},
	doi = {10.1109/CLOUD67622.2025.00012},
	abstract = {Cloud forensics is a crucial yet challenging field, as traditional forensic techniques struggle to handle the large-scale, dynamic nature of cloud environments. Manual forensic analysis is time-consuming, error-prone, and often fails to detect evolving cyber threats. This paper presents a novel tool leveraging Large Language Models (LLMs) to fully automate cloud forensic investigations. Our approach utilizes few-shot learning to classify log data, extract forensic intelligence, and reconstruct attack timelines. We evaluate LLM-based automation against traditional machine learning models, including Random Forest, XGBoost, and Gradient Boosting, using cloud forensic log datasets. Experimental results demonstrate that LLMs improve forensic accuracy, precision, and recall while reducing the need for extensive feature engineering. However, challenges such as hallucination risks, adversarial manipulation, and forensic explainability must be addressed to ensure the reliability of AI-driven investigations. To mitigate these risks, we explore Retrieval-Augmented Generation (RAG) for context-aware forensic intelligence and propose hybrid AI models integrating rule-based forensic validation. Our findings highlight the potential of LLM-driven forensic automation to enhance cloud security operations while outlining key areas for future research, including adversarial robustness, forensic transparency, and multi-cloud scalability.},
	booktitle = {2025 {IEEE} 18th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Alharthi, Dalal and Yasaei, Rozhin},
	month = jul,
	year = {2025},
	note = {ISSN: 2159-6190},
	keywords = {Accuracy, Adaptation models, Adaptive Prompt Engineering, Automation, Biological system modeling, Cloud computing security, Cloud Forensics, Cloud Security, Forensic Intelligence, Forensics, Large language models, Large Language Models (LLMs), Log Prioritization, Manuals, Robustness, Threat assessment, Threat Detection},
	pages = {12--22},
}

@inproceedings{shrestha_fairrag_2024,
	title = {{FairRAG}: {Fair} {Human} {Generation} via {Fair} {Retrieval} {Augmentation}},
	doi = {10.1109/CVPR52733.2024.01140},
	abstract = {Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outper-forms existing methods in terms of demographic diversity, image-text alignment and image fidelity while incurring minimal computational overhead during inference.},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Shrestha, Robik and Zou, Yang and Chen, Qiuyu and Li, Zhiheng and Xie, Yusheng and Deng, Siqi},
	month = jun,
	year = {2024},
	note = {ISSN: 2575-7075},
	keywords = {bias, Computational modeling, Computer vision, diffusion, fairness, generative-ai, Image databases, Image synthesis, RAG, stable-diffusion, Text to image, Training data, Visualization},
	pages = {11996--12005},
}

@inproceedings{abdullah_rails_2025,
	title = {{RAILS}: {Retrieval}-{Augmented} {Intelligence} for {Learning} {Software} {Development}},
	doi = {10.1109/HPEC67600.2025.11196549},
	abstract = {Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to assist software development, yet they often produce incomplete code or incorrect imports, especially when lacking access to external or project-specific documentation. We introduce RAILS (Retrieval-Augmented Intelligence for Learning Software Development), a framework that augments LLM prompts with semantically retrieved context from curated Java resources using FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop guided by compiler feedback to refine suggestions. We evaluated RAILS on 78 real-world Java import error cases spanning standard libraries, GUI APIs, external tools, and custom utilities. Despite using the same LLM, RAILS outperforms baseline prompting by preserving intent, avoiding hallucinations, and surfacing correct imports even when libraries are unavailable locally. Future work will integrate symbolic filtering via PostgreSQL and extend support to other languages and IDEs.},
	booktitle = {2025 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Abdullah, Wali Mohammad and Islam, Md. Morshedul and Parmar, Devraj and Patel, Happy Hasmukhbhai and Prabhakaran, Sindhuja and Saha, Baidya},
	month = sep,
	year = {2025},
	note = {ISSN: 2643-1971},
	keywords = {Code Repair Automation, Codes, Compilation Feedback, Graphical user interfaces, Java, Java Import Resolution, Large language models, Large Language Models, Libraries, Maintenance engineering, Rails, Retrieval-Augmented Generation, Software development management, Software reliability, Standards},
	pages = {1--6},
}

@article{mundlamuri_evolution_2025,
	title = {The {Evolution} of {AI}: {From} {Classical} {Machine} {Learning} to {Modern} {Large} {Language} {Models}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3621344},
	abstract = {This paper provides a comprehensive review of the evolution of artificial intelligence from early symbolic, rule-based systems to modern large language models (LLMs) and retrieval-augmented generation (RAG) architectures. Early AI was dominated by symbolic reasoning and expert systems using hand-crafted rules, before a paradigm shift toward data-driven learning occurred with the advent of machine learning, notably the backpropagation algorithm that enabled training of multi-layer neural networks to learn complex tasks. This breakthrough ushered in the deep learning era: convolutional neural networks (CNNs) achieved landmark results in computer vision, recurrent neural networks (RNNs such as LSTM) excelled at sequential data processing, and deep reinforcement learning (RL) systems mastered challenging decision-making tasks. In natural language processing (NLP), distributed word embedding models like Word2Vec and GloVe replaced sparse representations, capturing semantic relationships in vector space. To handle rare and out-of-vocabulary words, subword tokenization techniques such as Byte Pair Encoding (BPE) and WordPiece were introduced. These advances culminated in the Transformer architecture, based on self-attention mechanisms, which now underpins most state-of-the-art LLMs. Transformer-based LLMs, pre-trained on large text corpora, demonstrate advanced capabilities in language generation and understanding. Beyond model architectures, the review highlights how LLMs are augmented with external knowledge via RAG. Retrieval-augmented generation combines an LLM with a vector database of text embeddings, allowing the model to retrieve relevant external information to ground its outputs. This approach significantly improves factual accuracy and relevance, mitigating issues like hallucination, though ensuring complete truthfulness remains challenging. Additional obstacles include the limited context windows of LLMs (on the order of a few thousand tokens) and the computational complexity and latency of inference. The review also examines societal implications of advanced AI, such as bias amplification from training data and other ethical considerations in deployment. Finally, it outlines future directions, including extended context lengths, improved interpretability and alignment, greater efficiency, and neuro-symbolic hybrid approaches, aiming to develop more robust and trustworthy AI systems.},
	journal = {IEEE Access},
	author = {Mundlamuri, Rahul and Gunnam, Ganesh Reddy and Mysari, Nikhil Kumar and Pujuri, Jayakanth},
	year = {2025},
	keywords = {Artificial intelligence, deep learning, Deep learning, Encoding, Expert systems, Hidden Markov models, large language models, Large language models, machine learning, Machine learning, neural networks, Neural networks, reinforcement learning, Training, transformers, Transformers},
	pages = {178302--178341},
}

@inproceedings{lee_can_2025,
	title = {Can an {LLM} {Find} {Its} {Way} {Around} a {Spreadsheet}?},
	doi = {10.1109/ICSE55347.2025.00101},
	abstract = {Spreadsheets are routinely used in business and scientific contexts, and one of the most vexing challenges is performing data cleaning prior to analysis and evaluation. The ad-hoc and arbitrary nature of data cleaning problems, such as typos, inconsistent formatting, missing values, and a lack of standardization, often creates the need for highly specialized pipelines. We ask whether an LLM can find its way around a spreadsheet and how to support end-users in taking their free-form data processing requests to fruition. Just like RAG retrieves context to answer users' queries, we demonstrate how we can retrieve elements from a code library to compose data preprocessing pipelines. Through comprehensive experiments, we demonstrate the quality of our system and how it is able to continuously augment its vocabulary by saving new codes and pipelines back to the code library for future retrieval.},
	booktitle = {2025 {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Lee, Cho-Ting and Neeser, Andrew and Xu, Shengzhe and Katyan, Jay and Cross, Patrick and Pathakota, Sharanya and Norman, Marigold and Simeone, John and Chandrasekaran, Jaganmohan and Ramakrishnan, Naren},
	month = apr,
	year = {2025},
	note = {ISSN: 1558-1225},
	keywords = {Business, Cleaning, code generation, Codes, data cleaning, Data preprocessing, end-user programming, Libraries, LLMs, Pipelines, Programming, Software engineering, Standardization, Vocabulary},
	pages = {294--306},
}

@inproceedings{sahin_adaptation_2025,
	title = {Adaptation and {Use} of {Interpretive} {AI} in {Data}-{Driven} {Industry} 4.0: {A} {Case} {Study} on {Industrial} {Machines}},
	doi = {10.1109/UBMK67458.2025.11207004},
	abstract = {In recent years, several researchers have been attracted to analyzing the performance of mechanical and electronic machines using large language models (LLMs). LLMs can take the user query in formal or informal language, convert it to structured language, and make inferences on the knowledge base that contains information about machine performance. In other words, LLMs can be trained using this knowledge base as a training set and answer user queries as a chatbot that understands and generates natural language. This chatbot can facilitate and speed up the monitoring task for industrial machines. The current work analyzes the vibration data of the Radiant Tube Furnace (RTF) Combustion Air Fan used in the Borçelik Group’s galvanizing lines and makes it easily understandable with the help of a chatbot. Our data set for fine-tuning a pre-trained LLM consists of approximately 870,000 entries categorized with performance levels to analyze machine performance and detect anomalies. The chatbot, supported by the LLMs, aims to provide users with easy access to operational details, such as the last time the machine crashed or the performance level of the machine in the last week, last month, etc. The designed and implemented chatbot was tested on manually generated queries, which resulted in 75\% precision. The proposed model is now available on the Hugging Face platform.},
	booktitle = {2025 10th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Şahin, Emir Esad and Kurt, Beril and Dehkharghani, Rahim and Öper, Merve and Kaçar, Saygın},
	month = sep,
	year = {2025},
	note = {ISSN: 2521-1641},
	keywords = {Adaptation models, ai using by human, Atmospheric modeling, Chatbots, Fourth Industrial Revolution, Furnaces, Galvanizing, Industry 4.0, Interpretive Artificial Intelligence, Knowledge based systems, LLM, Natural languages, RAG, Transformers, Vibrations},
	pages = {137--142},
}

@inproceedings{sahin_adaptation_2025-1,
	title = {Adaptation and {Use} of {Interpretive} {AI} in {Data}-{Driven} {Industry} 4.0: {A} {Case} {Study} on {Industrial} {Machines}},
	doi = {10.1109/UBMK67458.2025.11206830},
	abstract = {In recent years, several researchers have been attracted to analyzing the performance of mechanical and electronic machines using large language models (LLMs). LLMs can take the user query in formal or informal language, convert it to structured language, and make inferences on the knowledge base that contains information about machine performance. In other words, LLMs can be trained using this knowledge base as a training set and answer user queries as a chatbot that understands and generates natural language. This chatbot can facilitate and speed up the monitoring task for industrial machines. The current work analyzes the vibration data of the Radiant Tube Furnace (RTF) Combustion Air Fan used in the Borçelik Group’s galvanizing lines and makes it easily understandable with the help of a chatbot. Our data set for fine-tuning a pre-trained LLM consists of approximately 870,000 entries categorized with performance levels to analyze machine performance and detect anomalies. The chatbot, supported by the LLMs, aims to provide users with easy access to operational details, such as the last time the machine crashed or the performance level of the machine in the last week, last month, etc. The designed and implemented chatbot was tested on manually generated queries, which resulted in 75 \% precision. The proposed model is now available on the Hugging Face platform.},
	booktitle = {2025 10th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Şahin, Emir Esad and Kurt, Beril and Dehkharghani, Rahim and Öper, Merve and Kaçar, Saygın},
	month = sep,
	year = {2025},
	note = {ISSN: 2521-1641},
	keywords = {Adaptation models, ai using by human, Atmospheric modeling, Chatbots, Fourth Industrial Revolution, Furnaces, Galvanizing, Industry 4.0, Interpretive Artificial Intelligence, Knowledge based systems, LLM, Natural languages, RAG, Transformers, Vibrations},
	pages = {137--142},
}

@inproceedings{xu_automated_2024,
	title = {Automated {C}/{C}++ {Program} {Repair} for {High}-{Level} {Synthesis} via {Large} {Language} {Models}},
	doi = {10.1109/MLCAD62225.2024.10740262},
	abstract = {In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular {\textbackslash}mathrmC / {\textbackslash}mathrmC++ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.},
	booktitle = {2024 {ACM}/{IEEE} 6th {Symposium} on {Machine} {Learning} for {CAD} ({MLCAD})},
	author = {Xu, Kangwei and Zhang, Grace Li and Yin, Xunzhao and Zhuo, Cheng and Schlichtmann, Ulf and Li, Bing},
	month = sep,
	year = {2024},
	keywords = {Codes, Costs, Iterative methods, Large language models, Machine learning, Maintenance engineering, Manuals, Optimization, Software, Training},
	pages = {1--9},
}

@inproceedings{dong_maxmind_2024,
	title = {{MaxMind}: {A} {Memory} {Loop} {Network} to {Enhance} {Software} {Productivity} {Based} on {LLMs}},
	doi = {10.1109/SWC62898.2024.00239},
	abstract = {Large language models can help facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity. Current research often overlooks the significance of converting real-time task experiences into system memory and fails to recognize the pivotal role of differentiating the knowledge value for future reference. This paper provides a novel system model, MaxMind, to address these issues through two novel designs: (1) evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing, and (2) enhancing a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation. To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system that aligns with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in the task success rate, with an improvement rate of approximately 3\%-6\% per round in this implementation example. Note that as the memories continue to accumulate, this cumulative improvement may become substantial. These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.},
	booktitle = {2024 {IEEE} {Smart} {World} {Congress} ({SWC})},
	author = {Dong, Yuchen and Fang, Xiaoxiang and Hu, Yuchen and Jiang, Renshuang and Jiang, Zhe},
	month = dec,
	year = {2024},
	note = {ISSN: 2993-396X},
	keywords = {Knowledge engineering, Large language models, Large Language Models, MaxMind, Memory Networks, Philosophical considerations, Productivity, RAG, Real-time systems, Recycling, Software, Software Productivity, Tool Generation},
	pages = {1562--1567},
}

@inproceedings{liu_chatchisel_2024,
	title = {{ChatChisel}: {Enabling} {Agile} {Hardware} {Design} with {Large} {Language} {Models}},
	doi = {10.1109/ISEDA62518.2024.10618053},
	abstract = {With the increasing complexity of integrated circuits, agile hardware design methodologies are crucial. Modern HDLs like Chisel enhance design quality, but manual implementations remain error-prone and time-consuming. Large language models (LLMs) offer potential for design automation through natural language but face challenges in generating large circuits using Verilog. We evaluate LLM capabilities for Chisel and Verilog generation, demonstrating superior Chisel generation ability. We introduce ChatChisel, the first language-based agile hardware design workflow that generates Chisel from language specifications. ChatChisel utilizes four LLM-based modules for decomposing, generating, error-correcting, and composing hardware designs. Techniques like LLM collaboration and RAG enhance ChatChisel's performance. Using GPT-3.5-turbo, we implemented an RV32I RISC-V CPU with 5-stage pipeline and dynamic branch prediction. We also validate our approach with extensive evaluations. Our experimental results reveal that ChatChisel can outperform LLM-based hardware design with Verilog by an average of 31.86\%, implying a significant design capability enhancement and design process acceleration with ChatChisel.},
	booktitle = {2024 2nd {International} {Symposium} of {Electronics} {Design} {Automation} ({ISEDA})},
	author = {Liu, Tianyang and Tian, Qi and Ye, Jianmin and Fu, LikTung and Su, Shengchu and Li, Junyan and Wan, Gwok-Waa and Zhang, Layton and Wong, Sam-Zaak and Wang, Xi and Yang, Jun},
	month = may,
	year = {2024},
	keywords = {Agile Hardware Design, Chisel, Design automation, Hardware, Integrated circuits, Large language models, LLM, Manuals, Natural languages, Pipelines, RISC-V},
	pages = {710--716},
}

@inproceedings{iyenghar_development_2025,
	title = {On the {Development} and {Application} of a {Structured} {Dataset} for {Data}-{Driven} {Risk} {Assessment} in {Industrial} {Functional} {Safety}},
	doi = {10.1109/WFCS63373.2025.11077569},
	abstract = {This paper introduces and makes available the first structured dataset for data-driven risk assessment in industrial machinery functional safety. The dataset comprises 7800 hazard scenarios generated using a rigorous, ISO 12100 and ISO 138491 grounded methodology, including plausibility assessment. The dataset spans ten hazard types, incorporating contextual parameters and corresponding required performance level (PLr) for each hazard scenario. The utility of the dataset and its application as a benchmark is demonstrated by comparing LLM-only and RAG-enhanced hazard classification as one example application. Experiments show that that RAG significantly improves LLM accuracy in classifying hazard scenarios and predicting PLr by providing access to critical contextual information (e.g. guidance from ISO 13849-1 for determination of PLr), resulting in a 36.4\% average accuracy increase (72.8\% accuracy) compared to the LLM-only baseline (36.4\% accuracy). Furthermore, RAG reduced misclassification rates across all hazard types, with the highest misclassification dropping from over 70\% in the LLM-only case to a maximum of 29\% with RAG indicating significantly enhanced classification confidence confirmed by statistical tests. These findings demonstrate the dataset’s ability to address the gap in structured data for evaluating LLMs in functional safety tasks, potentially leading to more reliable and efficient automated risk assessments. This work provides a valuable, open-source resource (GitHub repositry) to facilitate collaborative research and adoption of AI in routine functional safety workflows.},
	booktitle = {2025 {IEEE} 21st {International} {Conference} on {Factory} {Communication} {Systems} ({WFCS})},
	author = {Iyenghar, Padma},
	month = jun,
	year = {2025},
	note = {ISSN: 2835-8414},
	keywords = {Accuracy, Artificial intelligence, Benchmark testing, dataset, hazards, Hazards, ISO Standards, LLM, Machinery, machinery safety, performance level, RAG, Reliability, Risk management, safety function, Scenario generation, Software development management},
	pages = {1--8},
}

@inproceedings{rajesh_learn_2024,
	title = {Learn {Like} {Feynman}: {Developing} and {Testing} an {AI}-{Driven} {Feynman} {Bot}},
	doi = {10.1109/TALE62452.2024.10834370},
	abstract = {The Feynman learning technique is an active learning strategy that helps learners simplify complex information through student-led teaching and discussion. In this paper, we present the development and usability testing of the Feynman Bot, which uses the Feynman technique to assist self-regulated learners who lack peer or instructor support. The Bot embodies the Feynman learning technique by encouraging learners to discuss their lecture material in a question-answer-driven discussion format. The Feynman Bot was developed using a large language model with Langchain in a Retrieval-Augmented-Generation framework to leverage the reasoning capability required to generate effective discussion-oriented questions. To test the Feynman bot, a controlled experiment was conducted over three days with fourteen participants. Formative and summative assessments were conducted, followed by a self-efficacy survey. We found that participants who used the Feynman Bot experienced higher learning gains than the Passive Learners' group. Moreover, Feynman Bot Learners' had a higher level of comfort with the subject after using the bot. We also found typing to be the preferred input modality method over speech, when interacting with the bot. The high learning gains and improved confidence with study material brought about by the Feynman Bot makes it a promising tool for self-regulated learners.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Teaching}, {Assessment} and {Learning} for {Engineering} ({TALE})},
	author = {Rajesh, Akshaya and Khan, Sumbul},
	month = dec,
	year = {2024},
	keywords = {Active learning, Active Learning, AI Learning bots, Chatbots, Cognition, Context modeling, Education, Feynman Technique, Large Language Model, Large language models, Prompt engineering, Self-regulated learning, Surveys, Testing, Usability},
	pages = {1--8},
}

@inproceedings{zhou_construction_2024,
	title = {Construction of a {Multimodal} {Knowledge} {Graph} for {Power} {Grid} {Construction} {Safety} {Based} on {Large} {Language} {Models}},
	doi = {10.1109/NPSPE62515.2024.00013},
	abstract = {In response to the difficulties of safety management and the complexity of information at power grid construction sites, a multimodal knowledge graph construction method based on large language models is proposed. Data from the construction site is collected and filtered, and an ontology for safety management at the construction site is constructed. The ontology is then used as retrieval augmented generation(RAG) for assistance, enabling multimodal large model image extraction, resulting in structured data in the power grid safety field. Finally, the extracted results are displayed using a graph database, completing the construction of the multimodal knowledge graph. The constructed knowledge graph includes multimodal data from the construction site, allowing for quick querying of on-site safety incidents, providing safety managers with a valuable tool for site management.},
	booktitle = {2024 {International} {Conference} on {New} {Power} {System} and {Power} {Electronics} ({NPSPE})},
	author = {Zhou, Xiaofa and Shi, Jianyong and Dong, Lei and Zhang, You and Pan, Jin and Huang, Hao},
	month = aug,
	year = {2024},
	keywords = {Accuracy, Data mining, Data models, Information filters, Knowledge extraction, Knowledge graph, Knowledge graphs, Large language model, Large language models, Ontologies, Ontology, Power electronics, Power grids, Safety management},
	pages = {21--28},
}

@inproceedings{safaei_kglm-qa_2024,
	title = {{KGLM}-{QA}: {A} {Novel} {Approach} for {Knowledge} {Graph}-{Enhanced} {Large} {Language} {Models} for {Question} {Answering}},
	doi = {10.1109/IKT65497.2024.10892783},
	abstract = {Large language models excel in various natural language processing tasks but often struggle with knowledge-intensive queries, particularly those involve rare entities or require precise factual information. This paper presents a novel framework that enhances capabilities of an LLM-based question answering system by incorporating structured knowledge from knowledge graphs. Our approach employs entity extraction, semantic similarity scoring, and adaptive graph exploration to efficiently navigate and extract relevant information from knowledge graphs. The core of the presented solution is a knowledge graph-enhanced language model process that iteratively refines subgraph exploration and answer generation, complemented by a fallback mechanism for robustness across diverse question types. Experiments on location-based questions from the Entity Questions dataset demonstrate significant improvements in the quality of responses. Using the Gemini 1.5 Flash model, our system achieved an accuracy increase from 36\% to 71\% for partially correct answers and from 22\% to 69\% for exactly correct answers, as evaluated by human assessors. This approach offers a promising direction for developing more reliable and accurate question answering systems, particularly for queries involving long-tail entities or specific factual knowledge.},
	booktitle = {2024 15th {International} {Conference} on {Information} and {Knowledge} {Technology} ({IKT})},
	author = {Safaei, Alireza Akhavan and Saboori, Pegah and Ramezani, Reza and Nematbakhsh, Mohammadali},
	month = dec,
	year = {2024},
	note = {ISSN: 2476-2180},
	keywords = {Accuracy, Adaptation models, Data mining, Heavily-tailed distribution, Knowledge graph, Knowledge graphs, Large language models, Large Language Models, Navigation, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Robustness, Semantics},
	pages = {234--240},
}

@inproceedings{fichtenkamm_towards_2024,
	title = {Towards an {FA} {ChatBot} with {Retrieval}-{Augmented} {Language} {Modeling}},
	doi = {10.1109/IPFA61654.2024.10691083},
	abstract = {Failure Analysis (FA) data storages, like databases or file shares, host a lot of textual data comprising important information about products, best practices, or past cases. FA engineers require this information at their fingertips to accomplish their tasks efficiently. However, common keyword search interfaces provided by most information systems and databases are insufficient as they force an engineer to formulate a correct query, read returned documents, and manually extract required knowledge from them. In this paper, we suggest a cost-effective approach that augments retrieval systems with the capabilities of modern Large Language Models (LLMs) to provide straight-to-point answers to engineers’ questions. Preliminary evaluation shows that the suggested system can generate high-quality responses to a set of simple benchmark queries but also lacks complex reasoning capabilities for answering complex queries.},
	booktitle = {2024 {IEEE} {International} {Symposium} on the {Physical} and {Failure} {Analysis} of {Integrated} {Circuits} ({IPFA})},
	author = {Fichtenkamm, Maik and Kofler, Markus and Schekotihin, Konstantin and Burmer, Christian},
	month = jul,
	year = {2024},
	note = {ISSN: 1946-1550},
	keywords = {AI, Analytical models, Databases, Failure analysis, generative models, Knowledge engineering, Large language models, Measurement, Pipelines, retrieval-augmented generation},
	pages = {1--8},
}

@inproceedings{zhang_system_2024,
	title = {The {System} {Design} {Methodology} of an {Interactive} {Big} {Data} {Platform} {Based} on {Universal} {LLMs}},
	doi = {10.1109/ICISE-IE64355.2024.11025394},
	abstract = {This work elaborates on the design and implementation of an interactive big data platform system based on a universal LLM, including the main technical implementation plans and module designs of the system. The system employs a stepwise Text-To-SQL approach, utilizing few-shot techniques, RAG, and multi-source heterogeneous data access technologies, which can significantly enhance the direct executability of SQL. Based on this research, we have upgraded the original China Mobile Smart University Big Data Platform, empowered by AI to provide interactive data querying capabilities. This addresses the high professional barriers in data querying and analysis, and the weak front-end user perception of traditional big data platforms. It enables more people to harness the power of data.},
	booktitle = {2024 5th {International} {Conference} on {Information} {Science} and {Education} ({ICISE}-{IE})},
	author = {Zhang, Zhixian and Tang, Lina and Bai, Yin and Yu, Xiaoting and Yang, Kairui and Jiang, Peng},
	month = dec,
	year = {2024},
	keywords = {Big Data, Bigdata, Data models, database, Databases, Education, Industries, Information science, Large language models, Large Language Models, SQL, Structured Query Language, System analysis and design, Text to SQL, Usability},
	pages = {40--44},
}

@inproceedings{pang_remed_2024,
	title = {{REMED}: {Retrieval}-{Augmented} {Medical} {Document} {Query} {Responding} with {Embedding} {Fine}-{Tuning}},
	doi = {10.1109/IJCNN60899.2024.10651011},
	abstract = {While advanced Large Language Models (LLMs) exhibit considerable promise, their tendency to generate unreliable information poses significant challenges, particularly in high-risk domains like healthcare. However, the advent of Retrieval-Augmented Generation (RAG) offers a novel solution tailored for the medical realm. This study further enhances retrieval accuracy by introducing REMED, a specialized medical document retrieval framework designed to address the hallucination problem prevalent in LLMs. The REMED framework integrates dataset construction, an efficient embedding fine-tuning EM-FT model, retrieval-augmented generation, and human evaluation of LLM responses. The EM-FT model can end-to-end fine-tune the medical sentence representations in large pre-trained models through an efficient embedding fine-tuning method, thereby enhancing the performance of medical retrieval. We adopt contrastive learning as the loss function to optimize the performance of the EM-FT model, enabling it to accurately capture the similarity between query and relevant documents. This approach not only improves the retrieval accuracy of positively related contents but also effectively reduces the matching with negatively related contents. Compared to direct dense vector retrieval, fine-tuning query and content vectors first and then performing dense retrieval tasks significantly improved the performance. Through validation on two datasets, we demonstrate that our EM-FT method improves recall and precision on MMD by 3.2\%-6.0\% and on MPD by 14.4\%-42.6\% compared to using the embedding model directly for retrieval. Furthermore, through human evaluation on the PULSE-7Bv5 model, we further confirm the effectiveness of our retrieval results in improving the quality of generated text.},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Pang, Tianqi and Tan, Kehui and Yao, Yujun and Liu, Xiangyang and Meng, Fanlong and Fan, Chenyou and Zhang, Xiaofan},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {Accuracy, Contrastive Learning, Data privacy, Large language models, Large Language Models, Medical Dataset, Medical Document Retrieval, Medical services, Neural networks, Refining, Vectors},
	pages = {1--8},
}

@inproceedings{wei_unleashing_2025,
	title = {Unleashing the {Power} of {LLM} to {Infer} {State} {Machine} {From} the {Protocol} {Implementation}},
	doi = {10.1109/IWQoS65803.2025.11143461},
	abstract = {State machines are essential for enhancing protocol analysis to identify vulnerabilities. However, inferring state machines from network protocol implementations is challenging due to complex code syntax and semantics. Traditional dynamic analysis methods often miss critical state transitions due to limited coverage, while static analysis faces path explosion issues. To overcome these challenges, we introduce a novel state machine inference approach utilizing Large Language Models (LLMs), named ProtocolGPT. This method employs retrieval augmented generation technology to enhance a pre-trained model with specific knowledge from protocol implementations. Through effective prompt engineering, we accurately identify and infer state machines. To the best of our knowledge, our approach represents the first state machine inference that leverages the source code of protocol implementations. Our evaluation of six protocol implementations shows that our method achieves a precision of over 90 \%, outperforming the baselines by more than 30 \%. Furthermore, integrating our approach with protocol fuzzing improves coverage by more than 20 \% and uncovers two 0-day vulnerabilities compared to baseline methods.},
	booktitle = {2025 {IEEE}/{ACM} 33rd {International} {Symposium} on {Quality} of {Service} ({IWQoS})},
	author = {Wei, Haiyang and Chen, Ligeng and Du, Zhengjie and Wu, Yuhan and Huang, Haohui and Liu, Yue and Cheng, Guang and Xu, Fengyuan and Wang, Linzhang and Mao, Bing},
	month = jul,
	year = {2025},
	note = {ISSN: 2766-8568},
	keywords = {Codes, Large language models, Large Language Models, Protocol Reverse Engineering, Protocols, Reverse engineering, Security, Semantics, Software, Software Security, Source coding, State Machine, Static analysis, Syntactics},
	pages = {1--10},
}

@inproceedings{hao_rap_2025,
	title = {{RAP}: {Retrieval}-{Augmented} {Personalization} for {Multimodal} {Large} {Language} {Models}},
	doi = {10.1109/CVPR52734.2025.01355},
	abstract = {The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human’s daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs’ personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user’s name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts’ information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/.},
	booktitle = {2025 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hao, Haoran and Han, Jiaming and Li, Changsheng and Li, Yu-Feng and Yue, Xiangyu},
	month = jun,
	year = {2025},
	note = {ISSN: 2575-7075},
	keywords = {Databases, Image recognition, Large language models, multimodal large language models, Oral communication, Pattern recognition, personalization, Pipelines, Question answering (information retrieval), Real-time systems, retrieval-augmented generation, Training, Visualization},
	pages = {14538--14548},
}

@inproceedings{alenjareghi_llm-driven_2025,
	title = {{LLM}-{Driven} {FMEA} for {Safe} {Human}-{Robot} {Collaboration} in {Disassembly}},
	doi = {10.1109/ICHMS65439.2025.11154327},
	abstract = {Disassembly operations often present unstructured and unpredictable scenarios, such as handling hazardous materials, addressing ergonomic strain, and managing dynamic robot interactions that pose safety risks. To tackle these challenges, we propose an innovative use of large language models (LLMs) to enhance failure mode and effect analysis (FMEA) in the context of human-robot collaboration (HRC) for disassembly tasks. We developed an LLM system leveraging retrieval-augmented generation (RAG) for real-time risk analysis and recommendation generation. RAG retrieves domain-specific information from the FMEA knowledge database, enabling accurate risk analysis, contextual understanding, and relevant recommendations based on user input and operational data. Evaluation of the model demonstrated precision, achieving a BLEU score of 86.5, a ROUGE-L score of 89.2, and a cosine similarity score of 0.92 compared to ISO reference documents. These results underscore the system's capability to produce domain-specific recommendations closely aligned with safety technical specifications. The system accelerates the FMEA process by identifying failure modes, prioritizing risks, and proposing mitigation strategies in real time, enhancing adaptability to collaborative disassembly complexities. This model proactively addresses hazards like toxic material exposure or robotic arm glitches, transforming risk management with precision and efficiency. A customized prototype demonstrates its effectiveness, featuring real-time risk analysis, domain-specific knowledge retrieval, and context-aware recommendations. Its user-friendly design and adaptability to dynamic scenarios, such as hazardous material handling, showcase its potential to redefine risk assessment in collaborative settings.},
	booktitle = {2025 {IEEE} 5th {International} {Conference} on {Human}-{Machine} {Systems} ({ICHMS})},
	author = {Alenjareghi, Morteza Jalali and Keivanpour, Samira and Chinniah, Yuvin Adnarain and Jocelyn, Sabrina},
	month = may,
	year = {2025},
	keywords = {Accuracy, Collaboration, Disassembly, FMEA, Hazardous materials, Human-Robot Collaboration, Human-robot interaction, Knowledge graphs, Large language models, Large Language Models, Prevention and mitigation, Real-time systems, Safety, Synthetic data, Usability},
	pages = {295--301},
}

@inproceedings{shiri_decompose_2024,
	title = {Decompose, {Enrich}, and {Extract}! {Schema}-aware {Event} {Extraction} using {LLMs}.},
	doi = {10.23919/FUSION59988.2024.10706385},
	abstract = {Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method’s superior performance compared to baseline approaches.},
	booktitle = {2024 27th {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {Shiri, Fatemeh and Moghimifar, Farhad and Haffari, Reza and Li, Yuan-Fang and Nguyen, Van and Yoo, John},
	month = jul,
	year = {2024},
	keywords = {Accuracy, Benchmark testing, Cognition, Data mining, Decision making, event argument extraction, Event detection, event extraction, information extraction, Knowledge graphs, Large language models, Natural languages, Reliability},
	pages = {1--8},
}

@inproceedings{bakharia_shaping_2024,
	title = {Shaping {Programming} and {Data} {Science} {Education}: {Insights} from {GenAI} {Technical} {Book} {Trends}},
	doi = {10.1109/ICALT61570.2024.00040},
	abstract = {As GenAI technologies, particularly Large Language Models (LLMs), continue to revolutionize programming and data science, it is increasingly vital for educators to adapt computer science curricula. This paper presents a review of recent technical books on AI-Assisted programming and utilizes the findings to guide curriculum changes in higher education. Our analysis underscores the necessity for novel teaching strategies, emphasizing skills like problem decomposition, top-down design, and advanced debugging. Furthermore, it emphasizes the crucial expansion of curricula to encompass courses on developing applications based on LLMs, utilizing libraries such as LangChain and incorporating Retrieval Augmented Generation functionality. Our analysis reveals a significant gap in technical literature regarding the ethical and societal impacts of GenAI, highlighting the urgent need for programming curricula to evolve and equip students with the skills required to ethically develop AI-enhanced software products. This paper advocates for curriculum development that not only aligns with the latest industry trends but also contributes to research on AI-assisted coding and its future impact.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Advanced} {Learning} {Technologies} ({ICALT})},
	author = {Bakharia, Aneesha and Abdi, Solmaz},
	month = jul,
	year = {2024},
	note = {ISSN: 2161-377X},
	keywords = {AI-assisted programming, Data science, Data science education, Debugging, Education, Encoding, Ethics, Market research, Programming curriculum development, Programming education, Reviews},
	pages = {116--120},
}

@inproceedings{paket_it_2024,
	title = {{IT} {Service} {Desk} {Ticket} {Classification} via {Large} {Language} {Models}},
	doi = {10.1109/UBMK63289.2024.10773473},
	abstract = {Service desk systems are utilized in companies to enable employees to forward their issues or requests to IT support operators. These tickets are assigned by support operators to the appropriate categories based on their personal experiences and the manually prepared keyword-matching catalogs. The dependency on individual experiences and the difficulty of maintaining and updating manually prepared catalogs increase the misclassification rate. This study aims to address a challenging multi-class classification problem in the Turkish language, which includes 172 classes, by utilizing large language models (LLMs). For this purpose, next-generation LLMs such as Titan, Llama, and Mistral are compared with BERT-based models using zero-shot, fine-tuning, and RAG methods within this st udy. This research shows that BERT-based classification models outperform the LLMs, which are not specifically trained for classification tasks with several classes.},
	booktitle = {2024 9th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Paket, Ezgi and Şenerkek, Göksu and Akyol, Fatma Betül and Salman, Furkan},
	month = oct,
	year = {2024},
	note = {ISSN: 2521-1641},
	keywords = {Companies, Computational modeling, Computer science, Large language models, large language models (LLMs), multi-class classification, Next generation networking, service desk, ticket classification},
	pages = {135--140},
}

@inproceedings{wang_construction_2025,
	title = {Construction and {Application} of a {Multimodal} {Knowledge} {Graph} for {Social} {Media} {Opinion} {Analysis} {Based} on {Large} {Language} {Models}},
	doi = {10.1109/CAIBDA65784.2025.11182762},
	abstract = {The rapid expansion of multimodal content on social media brings both prospects and challenges to public opinion analysis. While such data provides deep insights into collective sentiment and societal dynamics, it also heightens the risks of misinformation spread and cognitive overload for analytical models. To tackle these issues, this paper presents a new approach to constructing a multimodal knowledge graph enabled by large language models, aiming to boost reasoning, traceability, and verifiability in social media opinion analysis. The proposed architecture incorporates LLM-enhanced multimodal semantic alignment, event-centric temporal relation extraction, and cross-modal confidence calibration to construct dynamic knowledge graphs for evolving public discourse. Experiments on a curated dataset of real-world events, which includes adversarial samples with inserted logical inconsistencies, demonstrate that our method outperforms traditional RAG approaches. Specifically, it achieves a 23.9 \% improvement in F1 score for information tracing and an 8.0 \% increase in accuracy for misinformation detection. These results validate the framework's capacity to reduce information overload and identify latent logical inconsistencies. However, challenges remain in real-time adaptation and resolving deep semantic contradictions, indicating future paths in combining dynamic network structures and domain-specific knowledge for early misinformation containment.},
	booktitle = {2025 5th {International} {Conference} on {Artificial} {Intelligence}, {Big} {Data} and {Algorithms} ({CAIBDA})},
	author = {Wang, Junyao and Duan, Tongle and Yao, Sen and Wang, Yasong},
	month = jun,
	year = {2025},
	keywords = {Accuracy, Cognition, Cross-modal Reasoning, Event Tracing, Fake news, Knowledge graphs, Large language models, LLMs, Misinformation Detection, Multimodal Knowledge Graph, RAG, Real-time systems, Semantic Explainability, Semantics, Social Media Opinion Analysis, Social networking (online), Topology, Trajectory},
	pages = {399--405},
}

@inproceedings{wijesinghe_artificial_2024,
	title = {Artificial {Intelligence} {Model} to {Suggest} {Sewing} {Operations} {Through} {Sketch} {Analysis}},
	doi = {10.1109/ICARC61713.2024.10499761},
	abstract = {With experienced apparel technicians migrating abroad, inexperienced newcomers struggle analyzing sketches to determine optimal construction methods. Manual interpretation takes extensive time and multiplies sampling expenses from missteps. This research demonstrates large language models (LLMs) like GPT-4 can accurately suggest sewing sequences from drawings to slash costs. We developed an AI ensemble leveraging computer vision and LLMs to annotate sketches with likely production directives at over 70\% precision. Compared to novice technicians' 60\% accuracy, this automates breakdown drafting for rapid iterations. By training on past image-operation pairings, the model reliably recommends stitch types, tools and steps for new sketches. Confidence metrics identify when human oversight is beneficial. This intelligence augmentation assists upskilling novices while increasing development efficiency. The methodology decreased sampling costs by 30\% and duration by 25\% over manual approaches in trials. By expediting iterations with automated validations, it enables agile adjustments closer to market needs. The system can expand across garment categories given sufficient data. This pioneering research tackles industry migration effects through AI, breaking dependence on disappearing expertise. With proven cost and time reductions, adoption can accelerate sector-wide production efficiency.},
	booktitle = {2024 4th {International} {Conference} on {Advanced} {Research} in {Computing} ({ICARC})},
	author = {Wijesinghe, Abhiru and Rajapakse, Chathura and Asanka, Dinesh},
	month = feb,
	year = {2024},
	keywords = {Analytical models, Apparel Industry, Computational modeling, Costs, Electric breakdown, Industries, Large Language Models, Measurement, Multi-modality, Retrieval Augmented Generation, Sewing Operation, Training},
	pages = {97--102},
}

@inproceedings{barros_anchor_2024,
	title = {Anchor {Your} {Embeddings} {Through} the {Storm}: {Mitigating} {Instance}-to-{Document} {Semantic} {Gap}},
	doi = {10.1109/IJCNN60899.2024.10650518},
	abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing with their remarkable ability to generate coherent responses. Despite their impressive capabilities, LLMs grapple with the challenges of learning from private data while ensuring the relevance and timeliness of the information they provide. To overcome this challenge, retrieval-based strategies for generation have become essential, though they require clean data and a complex indexing pipeline. In this paper, we introduce Anchor Embeddings, a novel embedding enhancing technique designed to mitigate the instance-to-document semantic gap that often hinders the retrieval process. Our method proposes the usage of an anchor embedding to serve as a semantic beacon, adding more context to smaller text segments extracted from the same source. This extra embedding acts as a holistic representation regarding the original document that is merged into the instance embeddings. It can be derived based on a diversity of strategies that involve semantic representation and localization cues. Our empirical analysis on the MS MARCO dataset reveals that Anchor Embeddings can significantly outperform traditional retrieval methods, boasting up to an 14\% performance improvement. The elegance of our approach lies in its simplicity and robustness, providing more specific context while maintaining the same time complexity for retrieval of the baseline approach.},
	booktitle = {2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Barros, Ramiro N. and Arguello, Kristen K. and Wehrmann, Jônatas},
	month = jun,
	year = {2024},
	note = {ISSN: 2161-4407},
	keywords = {anchor embeddings, large language models, Large language models, Neural networks, Pipelines, RAG, retrieval, Robustness, Scalability, Semantics, Storms},
	pages = {1--8},
}

@inproceedings{s_emotionsyncai-driven_2025,
	title = {{EmotionSync}:{AI}-{Driven} {Emotion}-{Aware} {Recommendation} {System} for {Smart} {Task} {Prioritization}, {Well}-{Being} {Productivity}},
	doi = {10.1109/ICAECA63854.2025.11012546},
	abstract = {What if AI has the ability to sense your emotions in addition to hearing what you say? Emotions are the language of the body, thoughts are the language of the mind, and words are the link between them; they convey sentiment, meaning, and purpose. AI, however, has trouble understanding this synergy; this paper presents Emotion Syn, an AI-powered Emotion-Aware Recommendation System that turns speech into tailored insights. It uses OpenAI Whisper to transcribe speech inputs, Named Entity Recognition (NER) to extract context, and a refined EmoRoBERTa model (93\% accuracy) to identify emotions. A RAG based LLM enhances personalization by retrieving relevant knowledge from books, past interactions, and external sources. Transformer models (T5) refine suggestions and Multi-Armed Bandit (MAB) algorithms produce adaptive task and well-being recommendations, dynamically prioritizing schedules through interaction with Google Calendar. The system is a fluid, emotion-aware AI companion that comprehends not just what people say but also how they feel thanks to an ongoing feedback loop that improves ideas. Emotion Syn converts emotional intelligence into action, bridging the gap between wellbeing and productivity.},
	booktitle = {2025 3rd {International} {Conference} on {Advancements} in {Electrical}, {Electronics}, {Communication}, {Computing} and {Automation} ({ICAECA})},
	author = {s, Atchaya and L, Sharmila and B, Pavithra and S, Nisha A},
	month = apr,
	year = {2025},
	keywords = {Adaptation models, Artificial intelligence, Context Aware AI, Context modeling, Emotion Recognition, MAB, Mental Well-Being, Productivity, RAG, Recommender systems, Schedules, Speech recognition, Telecommunication computing, Transformer Models, Transformers, Videos},
	pages = {1--6},
}

@article{shao_wirelessllm_2024,
	title = {{WirelessLLM}: {Empowering} {Large} {Language} {Models} {Towards} {Wireless} {Intelligence}},
	volume = {9},
	issn = {2509-3312},
	doi = {10.23919/JCIN.2024.10582827},
	abstract = {The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in large language models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.},
	number = {2},
	journal = {Journal of Communications and Information Networks},
	author = {Shao, Jiawei and Tong, Jingwen and Wu, Qiong and Guo, Wei and Li, Zijian and Lin, Zehong and Zhang, Jun},
	month = jun,
	year = {2024},
	keywords = {Adaptation models, Communication system security, Data models, large language models, multi-modal models, power allocation, protocol understanding, Sensors, spectrum sensing, Task analysis, Wireless communication, wireless communications, Wireless sensor networks},
	pages = {99--112},
}

@inproceedings{isbarov_enhanced_2024,
	title = {Enhanced document retrieval with topic embeddings},
	doi = {10.1109/AICT61888.2024.10740455},
	abstract = {Document retrieval systems have experienced a revitalized interest with the advent of retrieval-augmented generation (RAG). RAG architecture offers a lower hallucination rate than LLM-only applications. However, the accuracy of the retrieval mechanism is known to be a bottleneck in the efficiency of these applications. A particular case of subpar retrieval performance is observed in situations where multiple documents from several different but related topics are in the corpus. We have devised a new vectorization method that takes into account the topic information of the document. The paper introduces this new method for text vectorization and evaluates it in the context of RAG. Furthermore, we discuss the challenge of evaluating RAG systems, which pertains to the case at hand.},
	booktitle = {2024 {IEEE} 18th {International} {Conference} on {Application} of {Information} and {Communication} {Technologies} ({AICT})},
	author = {Isbarov, Jafar and Huseynova, Kavsar},
	month = sep,
	year = {2024},
	note = {ISSN: 2472-8586},
	keywords = {Accuracy, Databases, document retrieval, Information and communication technology, Information retrieval, text embeddings},
	pages = {1--5},
}

@inproceedings{kurniawan_ai_2024,
	title = {{AI} {Legal} {Companion}: {Enhancing} {Access} to {Justice} and {Legal} {Literacy} for the {Public}},
	doi = {10.1109/ICISS62896.2024.10751371},
	abstract = {The purpose of this paper is to discuss the importance of integrating artificial intelligence (AI) into the legal sector to address the challenge of legal complexity and initiating the development of customized AI Legal Companion as the solution to improve access to justice, enhance legal literacy among the public and introduce new innovations to the legal field. Qualitative research is used during the research and an interview is conducted with the legal expert to get deeper information from the legal perspective in utilizing AI in legal sectors. To improve the AI knowledge base, the utilization of advanced AI techniques, Retrieval Augmented Generation (RAG) is used to provide the AI technologies, particularly ChatGPT models to provide quick, more accurate, and easily understandable legal information to the users and help people with lack of legal information with legal assistance and education. The paper focuses on the potential of AI being implemented in the legal domain and emphasizes the importance of collaboration between legal experts and technologies. The result of this paper shows that AI can provide easy access to justice and legal literacy for the public, but the legal sector is enormous therefore, human legal assistance is still important to address the misinformation provided by the AI.},
	booktitle = {2024 {International} {Conference} on {ICT} for {Smart} {Society} ({ICISS})},
	author = {Kurniawan, Donny and Hiererra, Siti Elda},
	month = sep,
	year = {2024},
	keywords = {Accuracy, artificial intelligence, Artificial intelligence, Chatbots, Complexity theory, Interviews, Law, legal assistance, legal literacy, Privacy, Reviews, Technological innovation, User experience},
	pages = {1--6},
}

@article{liu_toward_2025,
	title = {Toward {Autonomous} {Network} {Management}: {An} {Intelligent} and {Secure} {Agentic} {Framework} for {Network} {Configuration}},
	volume = {39},
	issn = {1558-156X},
	doi = {10.1109/MNET.2025.3583298},
	abstract = {The increasing scale and complexity of network infrastructures demand efficient and secure automated configuration solutions. Existing approaches often face challenges such as incompatibility with devices from different manufacturers, limited flexibility in network resource allocation, high demands for personnel expertise, and insufficient security in configuration content. To address these issues, we propose an innovative agent-based framework that leverages large language models (LLMs) to achieve secure and intelligent network configuration. The framework integrates a multi-agent system, which includes: an intent recognition and rewriting agent that uses the Chain of Thought (CoT) method to accurately interpret user intent; a self-reflection retrieval agent that combines hybrid retrieval methods with self-reflection mechanisms to optimize knowledge retrieval; a security agent specialized in network security verification; and a content integration agent that generates final configuration files. Experimental results on a custom dataset demonstrate transformative improvements in accuracy, efficiency, and security. Our framework substantially outperforms both original large language models and strong Retrieval-Augmented Generation (RAG) baselines, with metric scores such as CodeBLEU and BLEU increasing by over 44\% compared to the RAG baseline.},
	number = {5},
	journal = {IEEE Network},
	author = {Liu, Teng and Huang, Xiaohong and Xie, Kun},
	month = sep,
	year = {2025},
	keywords = {Accuracy, Agent, Automation, Codes, Collaboration, Intent recognition, Knowledge based systems, Large language models, Large Language Models, NETCONF, Network Management, Network topology, Security, Semantic communication},
	pages = {21--29},
}

@book{de_notitle_2025,
	isbn = {978-1-83620-032-1},
	url = {https://ieeexplore.proxyucr.elogim.com/document/11107332},
	abstract = {A practical guide to building, deploying, automating, monitoring, and scaling ML and LLM solutions in productionKey FeaturesBuild reproducible ML pipelines with Azure ML CLI and GitHub ActionsAutomate ML workflows end to end, including deployment and monitoringApply LLMOps principles to deploy and manage generative AI responsibly across cloudsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEffective machine learning (ML) now demands not just building models but deploying and managing them at scale. Written by a seasoned senior software engineer with high-level expertise in both MLOps and LLMOps, Hands-On MLOps on Azure equips ML practitioners, DevOps engineers, and cloud professionals with the skills to automate, monitor, and scale ML systems across environments. The book begins with MLOps fundamentals and their roots in DevOps, exploring training workflows, model versioning, and reproducibility using pipelines. You'll implement CI/CD with GitHub Actions and the Azure ML CLI, automate deployments, and manage governance and alerting for enterprise use. The author draws on their production ML experience to provide you with actionable guidance and real-world examples. A dedicated section on LLMOps covers operationalizing large language models (LLMs) such as GPT-4 using RAG patterns, evaluation techniques, and responsible AI practices. You'll also work with case studies across Azure, AWS, and GCP that offer practical context for multi-cloud operations. Whether you're building pipelines, packaging models, or deploying LLMs, this guide delivers end-to-end strategy to build robust, scalable systems. By the end of this book, you'll be ready to design, deploy, and maintain enterprise-grade ML solutions with confidence. What you will learnUnderstand the DevOps to MLOps transitionBuild reproducible, reusable pipelines using the Azure ML CLISet up CI/CD for training and deployment workflowsMonitor ML applications and detect model/data driftCapture and secure governance and lineage dataOperationalize LLMs using RAG and prompt flowsApply MLOps across Azure, AWS, and GCP use casesWho this book is forThis book is for DevOps and Cloud engineers and SREs interested in or responsible for managing the lifecycle of machine learning models. Professionals who are already familiar with their ML workloads and want to improve their practices, or those who are new to MLOps and want to learn how to effectively manage machine learning models in this environment, will find this book beneficial. The book is also useful for technical decision-makers and project managers looking to understand the process and benefits of MLOps.},
	publisher = {Packt Publishing},
	author = {De, Banibrata},
	year = {2025},
	note = {Publication Title: Hands-On MLOps on Azure: Automate, secure, and scale ML workflows with the Azure ML CLI, GitHub, and LLMOps},
}

@inproceedings{ghassel_are_2024,
	title = {Are {Large} {Language} {Models} {General}-{Purpose} {Solvers} for {Dialogue} {Breakdown} {Detection}? {An} {Empirical} {Investigation}},
	doi = {10.1109/CCECE59415.2024.10667232},
	abstract = {This study addresses the challenge of dialogue breakdown—characterized as incoherence, irrelevance, or any disruption that significantly hampers the flow of the conversation. The impact of dialogue breakdowns has become critical with the adoption of large language models in various industries for companies with dialogue-based systems, such as Salesforce, Amazon, and Microsoft. Leveraging the Dialogue Breakdown Detection Challenge Dataset, we investigate the performance of generalist large language models, including ChatGPT, GPT-4, and Mistral-Medium, in identifying instances of dialogue breakdown without domain-specific fine-tuning. Through a series of experiments employing zero-shot and few-shot prompting techniques combined with chain-of-thought reasoning, this research sets a new benchmark in the field. Our findings reveal that GPT-4 outperforms both specialized models previously considered state-of-the-art and other generalist models in detecting dialogue breakdowns by over a 3\% margin, achieving an accuracy of 82.0\%. To our knowledge, this study is the first to demonstrate the enhanced capability of generalist large language models in this domain. Our experiments found that when detecting dialogue breakdowns, larger models like GPT-4 are less sensitive to how they are prompted. In contrast, smaller models like ChatGPT and Mistral-Medium can improve their performance by using prompting techniques that combine few-shot learning with the chain-of-thought method. This work proposes future research directions, including enhanced error analysis and developing an Ensemble RAG approach for improved generalization in dialogue breakdown detection.},
	booktitle = {2024 {IEEE} {Canadian} {Conference} on {Electrical} and {Computer} {Engineering} ({CCECE})},
	author = {Ghassel, Abdellah and Zhu, Xiaodan and Thomas, Stephen W.},
	month = aug,
	year = {2024},
	note = {ISSN: 2576-7046},
	keywords = {Companies, Computational modeling, conversational artificial intelligence, dialogue breakdown, Electric breakdown, Error analysis, Industries, large language models, Large language models, Oral communication},
	pages = {674--679},
}
