
@misc{vrettos_accurate_2025,
	title = {Accurate and {Energy} {Efficient}: {Local} {Retrieval}-{Augmented} {Generation} {Models} {Outperform} {Commercial} {Large} {Language} {Models} in {Medical} {Tasks}},
	shorttitle = {Accurate and {Energy} {Efficient}},
	url = {http://arxiv.org/abs/2506.20009},
	doi = {10.48550/arXiv.2506.20009},
	abstract = {Background The increasing adoption of Artificial Intelligence (AI) in healthcare has sparked growing concerns about its environmental and ethical implications. Commercial Large Language Models (LLMs), such as ChatGPT and DeepSeek, require substantial resources, while the utilization of these systems for medical purposes raises critical issues regarding patient privacy and safety. Methods We developed a customizable Retrieval-Augmented Generation (RAG) framework for medical tasks, which monitors its energy usage and CO2 emissions. This system was then used to create RAGs based on various open-source LLMs. The tested models included both general purpose models like llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs o4-mini model. A dataset of medical questions was used for the evaluation. Results Custom RAG models outperformed commercial models in accuracy and energy consumption. The RAG model built on llama3.1:8B achieved the highest accuracy (58.5\%) and was significantly better than other models, including o4-mini and DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption and CO2 footprint among all models, with a Performance per kWh of 0.52 and a total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x times more accuracy points per kWh and 172\% less electricity usage while maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs can be leveraged to develop RAGs that outperform commercial, online LLMs in medical tasks, while having a smaller environmental impact. Our modular framework promotes sustainable AI development, reducing electricity usage and aligning with the UNs Sustainable Development Goals.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Vrettos, Konstantinos and Klontzas, Michail E.},
	month = jun,
	year = {2025},
	note = {arXiv:2506.20009 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 18 pages, 3 Figures},
	annote = {Comment: 18 pages, 3 Figures},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XX8TX7YK\\Vrettos and Klontzas - 2025 - Accurate and Energy Efficient Local Retrieval-Augmented Generation Models Outperform Commercial Lar.pdf:application/pdf},
}

@misc{zhang_culturesynth_2025,
	title = {{CultureSynth}: {A} {Hierarchical} {Taxonomy}-{Guided} and {Retrieval}-{Augmented} {Framework} for {Cultural} {Question}-{Answer} {Synthesis}},
	shorttitle = {{CultureSynth}},
	url = {http://arxiv.org/abs/2509.10886},
	doi = {10.48550/arXiv.2509.10886},
	abstract = {Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation{\textbackslash}footnote\{Benchmark is available at https://github.com/Eyr3/CultureSynth.\}.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Zhang, Xinyu and Zhang, Pei and Luo, Shuang and Tang, Jialong and Wan, Yu and Yang, Baosong and Huang, Fei},
	month = sep,
	year = {2025},
	note = {arXiv:2509.10886 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted as a Findings paper at EMNLP 2025},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZMQE2W6Y\\Zhang et al. - 2025 - CultureSynth A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question.pdf:application/pdf},
}

@misc{cherukuri_large_2025,
	title = {Large {Language} {Models} for {Oral} {History} {Understanding} with {Text} {Classification} and {Sentiment} {Analysis}},
	url = {http://arxiv.org/abs/2508.06729},
	doi = {10.48550/arXiv.2508.06729},
	abstract = {Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71\%), followed by Llama (84.99\%) and Qwen (83.72\%). For sentiment analysis, Llama slightly outperformed Qwen (82.66\%) and ChatGPT (82.29\%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Cherukuri, Komala Subramanyam and Moses, Pranav Abishai and Sakata, Aisa and Chen, Jiangping and Chen, Haihua},
	month = aug,
	year = {2025},
	note = {arXiv:2508.06729 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XVLMBNEY\\Cherukuri et al. - 2025 - Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis.pdf:application/pdf},
}

@misc{boraud_reservoirchat_2025,
	title = {{ReservoirChat}: {Interactive} {Documentation} {Enhanced} with {LLM} and {Knowledge} {Graph} for {ReservoirPy}},
	shorttitle = {{ReservoirChat}},
	url = {http://arxiv.org/abs/2507.05279},
	doi = {10.48550/arXiv.2507.05279},
	abstract = {We introduce a tool designed to improve the capabilities of Large Language Models (LLMs) in assisting with code development using the ReservoirPy library, as well as in answering complex questions in the field of Reservoir Computing. By incorporating external knowledge through Retrieval-Augmented Generation (RAG) and knowledge graphs, our approach aims to reduce hallucinations and increase the factual accuracy of generated responses. The system provides an interactive experience similar to ChatGPT, tailored specifically for ReservoirPy, enabling users to write, debug, and understand Python code while accessing reliable domain-specific insights. In our evaluation, while proprietary models such as ChatGPT-4o and NotebookLM performed slightly better on general knowledge questions, our model outperformed them on coding tasks and showed a significant improvement over its base model, Codestral-22B.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Boraud, Virgile and Bendi-Ouis, Yannis and Bernard, Paul and Hinaut, Xavier},
	month = jul,
	year = {2025},
	note = {arXiv:2507.05279 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7UMP9EM2\\Boraud et al. - 2025 - ReservoirChat Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy.pdf:application/pdf},
}

@misc{gondhalekar_multifinrag_2025,
	title = {{MultiFinRAG}: {An} {Optimized} {Multimodal} {Retrieval}-{Augmented} {Generation} ({RAG}) {Framework} for {Financial} {Question} {Answering}},
	shorttitle = {{MultiFinRAG}},
	url = {http://arxiv.org/abs/2506.20821},
	doi = {10.48550/arXiv.2506.20821},
	abstract = {Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span hundreds of pages and combine diverse modalities, including dense narrative text, structured tables, and complex figures. Answering questions over such content often requires joint reasoning across modalities, which strains traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines due to token limitations, layout loss, and fragmented cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation framework purpose-built for financial QA. MultiFinRAG first performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM, which produces both structured JSON outputs and concise textual summaries. These outputs, along with narrative text, are embedded and indexed with modality-aware similarity thresholds for precise retrieval. A tiered fallback strategy then dynamically escalates from text-only to text+table+image contexts when necessary, enabling cross-modal reasoning while reducing irrelevant context. Despite running on commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Gondhalekar, Chinmay and Patel, Urjitkumar and Yeh, Fang-Chun},
	month = jun,
	year = {2025},
	note = {arXiv:2506.20821 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computational Engineering, Finance, and Science},
	annote = {Comment: Preprint Copy},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7NVT3VX8\\Gondhalekar et al. - 2025 - MultiFinRAG An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Qu.pdf:application/pdf},
}

@misc{masa_improving_2025,
	title = {Improving {LLMs} with a knowledge from databases},
	url = {http://arxiv.org/abs/2506.05560},
	doi = {10.48550/arXiv.2506.05560},
	abstract = {Large language models (LLMs) are achieving significant progress almost every moment now. Many advanced techniques have been introduced and widely accepted, like retrieval-augmentation generation (RAG), agents, and tools. Tools can query the database to answer questions from structured data files or perform groupings or other statistics. This unlocks huge opportunities, such as it can answer any question, but also poses threats, such as safety, because there is no control over the commands that are created. We would like to discuss whether we can create a new method that improves answers based on dataset/database via some interpretable ML methods, namely enhanced association rules. The advantage would be if the method can be also used in some safe technique like RAG. Association rules have a sound history. Since the introduction of CN2 and aproiri, many enhancements have been made. In parallel, enhanced association rules have been introduced and evolved over the last 40 years. The general problem is typically that there are too many rules. There are some techniques for handling it, but when LLM emerged, it turned out to be the best use case for the RAG technique for LLMs. We proposed a method that generates a ruleset based on defined knowledge patterns, then converts rules into text form via a rule-to-text converter, and includes the result as an RAG into LLM. We compared this method with ChatGPT (even with using agents) and we have discovered a significant improvement in answering questions based on the dataset. We have also tried several strategies how much rules to generate. We found this improvement interesting. Moreover, it can also be improved in many ways as future work, like incorporating other patterns, the use of rule mining as an agent, and many others.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Máša, Petr},
	month = jun,
	year = {2025},
	note = {arXiv:2506.05560 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7KUS8IHG\\Máša - 2025 - Improving LLMs with a knowledge from databases.pdf:application/pdf},
}

@misc{junior_br-taxqa-r_2025,
	title = {{BR}-{TaxQA}-{R}: {A} {Dataset} for {Question} {Answering} with {References} for {Brazilian} {Personal} {Income} {Tax} {Law}, including case law},
	shorttitle = {{BR}-{TaxQA}-{R}},
	url = {http://arxiv.org/abs/2505.15916},
	doi = {10.48550/arXiv.2505.15916},
	abstract = {This paper presents BR-TaxQA-R, a novel dataset designed to support question answering with references in the context of Brazilian personal income tax law. The dataset contains 715 questions from the 2024 official Q{\textbackslash}\&A document published by Brazil's Internal Revenue Service, enriched with statutory norms and administrative rulings from the Conselho Administrativo de Recursos Fiscais (CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings for searching and GPT-4o-mini for answer generation. We compare different text segmentation strategies and benchmark our system against commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics. Results show that our custom RAG pipeline outperforms commercial systems in Response Relevancy, indicating stronger alignment with user queries, while commercial models achieve higher scores in Factual Correctness and fluency. These findings highlight a trade-off between legally grounded generation and linguistic fluency. Crucially, we argue that human expert evaluation remains essential to ensure the legal validity of AI-generated answers in high-stakes domains such as taxation. BR-TaxQA-R is publicly available at https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Júnior, Juvenal Domingos and Faria, Augusto and Oliveira, E. Seiti de and Brito, Erick de and Teotonio, Matheus and Assumpção, Andre and Carmo, Diedre and Lotufo, Roberto and Pereira, Jayr},
	month = may,
	year = {2025},
	note = {arXiv:2505.15916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\5K5YQE28\\Júnior et al. - 2025 - BR-TaxQA-R A Dataset for Question Answering with References for Brazilian Personal Income Tax Law,.pdf:application/pdf},
}

@misc{melton_evaluating_2025,
	title = {Evaluating {Retrieval} {Augmented} {Generative} {Models} for {Document} {Queries} in {Transportation} {Safety}},
	url = {http://arxiv.org/abs/2504.07022},
	doi = {10.48550/arXiv.2504.07022},
	abstract = {Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Melton, Chad and Sorokine, Alex and Peterson, Steve},
	month = apr,
	year = {2025},
	note = {arXiv:2504.07022 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 14 pages, 3 Figures, 3 tables},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\L2IQY7LP\\Melton et al. - 2025 - Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety.pdf:application/pdf},
}

@misc{bilal_onrl-rag_2025,
	title = {{OnRL}-{RAG}: {Real}-{Time} {Personalized} {Mental} {Health} {Dialogue} {System}},
	shorttitle = {{OnRL}-{RAG}},
	url = {http://arxiv.org/abs/2504.02894},
	doi = {10.48550/arXiv.2504.02894},
	abstract = {Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Bilal, Ahsan and Lin, Beiyu},
	month = apr,
	year = {2025},
	note = {arXiv:2504.02894 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: It needs more revisions. I am currently working on it with my co-author},
}

@misc{wang_scholarcopilot_2025,
	title = {{ScholarCopilot}: {Training} {Large} {Language} {Models} for {Academic} {Writing} with {Accurate} {Citations}},
	shorttitle = {{ScholarCopilot}},
	url = {http://arxiv.org/abs/2504.00824},
	doi = {10.48550/arXiv.2504.00824},
	abstract = {Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their ability to support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], which is then used to query a citation database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to improve efficiency. Our model is built upon Qwen-2.5-7B and trained on 500K papers from arXiv. It achieves a top-1 retrieval accuracy of 40.1\% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0\%) and BM25 (9.8\%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality -- measured across relevance, coherence, academic rigor, completeness, and innovation -- significantly surpassing all existing models, including much larger ones like the Retrieval-Augmented Qwen2.5-72B-Instruct. Human studies further demonstrate that ScholarCopilot, despite being a 7B model, significantly outperforms ChatGPT, achieving 100\% preference in citation quality and over 70\% in overall usefulness.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Wang, Yubo and Ma, Xueguang and Nie, Ping and Zeng, Huaye and Lyu, Zhiheng and Zhang, Yuxuan and Schneider, Benjamin and Lu, Yi and Yue, Xiang and Chen, Wenhu},
	month = apr,
	year = {2025},
	note = {arXiv:2504.00824 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\LEKEPJSQ\\Wang et al. - 2025 - ScholarCopilot Training Large Language Models for Academic Writing with Accurate Citations.pdf:application/pdf},
}

@misc{tsuchida_goodevil_2025,
	title = {Good/{Evil} {Reputation} {Judgment} of {Celebrities} by {LLMs} via {Retrieval} {Augmented} {Generation}},
	url = {http://arxiv.org/abs/2503.14382},
	doi = {10.48550/arXiv.2503.14382},
	abstract = {The purpose of this paper is to examine whether large language models (LLMs) can understand what is good and evil with respect to judging good/evil reputation of celebrities. Specifically, we first apply a large language model (namely, ChatGPT) to the task of collecting sentences that mention the target celebrity from articles about celebrities on Web pages. Next, the collected sentences are categorized based on their contents by ChatGPT, where ChatGPT assigns a category name to each of those categories. Those assigned category names are referred to as "aspects" of each celebrity. Then, by applying the framework of retrieval augmented generation (RAG), we show that the large language model is quite effective in the task of judging good/evil reputation of aspects and descriptions of each celebrity. Finally, also in terms of proving the advantages of the proposed method over existing services incorporating RAG functions, we show that the proposed method of judging good/evil of aspects/descriptions of each celebrity significantly outperform an existing service incorporating RAG functions.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Tsuchida, Rikuto and Yokoyama, Hibiki and Utsuro, Takehito},
	month = jul,
	year = {2025},
	note = {arXiv:2503.14382 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\R86KE37E\\Tsuchida et al. - 2025 - GoodEvil Reputation Judgment of Celebrities by LLMs via Retrieval Augmented Generation.pdf:application/pdf},
}

@misc{samuel_agrollm_2025,
	title = {{AgroLLM}: {Connecting} {Farmers} and {Agricultural} {Practices} through {Large} {Language} {Models} for {Enhanced} {Knowledge} {Transfer} and {Practical} {Application}},
	shorttitle = {{AgroLLM}},
	url = {http://arxiv.org/abs/2503.04788},
	doi = {10.48550/arXiv.2503.04788},
	abstract = {AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing and education in agriculture using Large Language Models (LLMs) and a Retrieval-Augmented Generation (RAG) framework. By using a comprehensive open-source agricultural database, AgroLLM provides accurate, contextually relevant responses while reducing incorrect information retrieval. The system utilizes the FAISS vector database for efficient similarity searches, ensuring rapid access to agricultural knowledge. A comparative study of three advanced models: Gemini 1.5 Flash, ChatGPT-4o Mini, and Mistral-7B-Instruct-v0.2 was conducted to evaluate performance across four key agricultural domains: Agriculture and Life Sciences, Agricultural Management, Agriculture and Forestry, and Agriculture Business. Key evaluation metrics included embedding quality, search efficiency, and response relevance. Results indicated that ChatGPT-4o Mini with RAG achieved the highest accuracy at 93\%. Continuous feedback mechanisms enhance response quality, making AgroLLM a benchmark AI-driven educational tool for farmers, researchers, and professionals, promoting informed decision-making and improved agricultural practices.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Samuel, Dinesh Jackson and Skarga-Bandurova, Inna and Sikolia, David and Awais, Muhammad},
	month = feb,
	year = {2025},
	note = {arXiv:2503.04788 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\DN4XHPQZ\\Samuel et al. - 2025 - AgroLLM Connecting Farmers and Agricultural Practices through Large Language Models for Enhanced Kn.pdf:application/pdf},
}

@misc{li_tutorllm_2025,
	title = {{TutorLLM}: {Customizing} {Learning} {Recommendations} with {Knowledge} {Tracing} and {Retrieval}-{Augmented} {Generation}},
	shorttitle = {{TutorLLM}},
	url = {http://arxiv.org/abs/2502.15709},
	doi = {10.48550/arXiv.2502.15709},
	abstract = {The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10\% improvement in user satisfaction and a 5{\textbackslash}\% increase in quiz scores compared to using general LLMs alone.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Li, Zhaoxing and Yazdanpanah, Vahid and Wang, Jindi and Gu, Wen and Shi, Lei and Cristea, Alexandra I. and Kiden, Sarah and Stein, Sebastian},
	month = apr,
	year = {2025},
	note = {arXiv:2502.15709 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\8ENV99M9\\Li et al. - 2025 - TutorLLM Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Genera.pdf:application/pdf},
}

@misc{shang_biomedical_2025,
	title = {Biomedical {Relation} {Extraction} via {Adaptive} {Document}-{Relation} {Cross}-{Mapping} and {Concept} {Unique} {Identifier}},
	url = {http://arxiv.org/abs/2501.05155},
	doi = {10.48550/arXiv.2501.05155},
	abstract = {Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensive texts, serving as a crucial subfield of biomedical text mining. Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences. Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness. Besides, the scarcity of annotated data further hampers model training. Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE. Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data. Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities. Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts. Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Shang, Yufei and Guo, Yanrong and Hao, Shijie and Hong, Richang},
	month = jan,
	year = {2025},
	note = {arXiv:2501.05155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 13 pages, 6 figures},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\C6D4F8AC\\Shang et al. - 2025 - Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Ident.pdf:application/pdf},
}

@misc{mortaheb_rag-check_2025,
	title = {{RAG}-{Check}: {Evaluating} {Multimodal} {Retrieval} {Augmented} {Generation} {Performance}},
	shorttitle = {{RAG}-{Check}},
	url = {http://arxiv.org/abs/2501.03995},
	doi = {10.48550/arXiv.2501.03995},
	abstract = {Retrieval-augmented generation (RAG) improves large language models (LLMs) by using external knowledge to guide response generation, reducing hallucinations. However, RAG, particularly multi-modal RAG, can introduce new hallucination sources: (i) the retrieval process may select irrelevant pieces (e.g., documents, images) as raw context from the database, and (ii) retrieved images are processed into text-based context via vision-language models (VLMs) or directly used by multi-modal language models (MLLMs) like GPT-4o, which may hallucinate. To address this, we propose a novel framework to evaluate the reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response. We train RS and CS models using a ChatGPT-derived database and human evaluator samples. Results show that both models achieve {\textasciitilde}88\% accuracy on test data. Additionally, we construct a 5000-sample human-annotated database evaluating the relevancy of retrieved pieces and the correctness of response statements. Our RS model aligns with human preferences 20\% more often than CLIP in retrieval, and our CS model matches human preferences {\textasciitilde}91\% of the time. Finally, we assess various RAG systems' selection and generation performances using RS and CS.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Mortaheb, Matin and Khojastepour, Mohammad A. Amir and Chakradhar, Srimat T. and Ulukus, Sennur},
	month = jan,
	year = {2025},
	note = {arXiv:2501.03995 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\B6LTL8XC\\Mortaheb et al. - 2025 - RAG-Check Evaluating Multimodal Retrieval Augmented Generation Performance.pdf:application/pdf},
}

@misc{jia_enhancing_2025,
	title = {Enhancing {LLMs} for {Power} {System} {Simulations}: {A} {Feedback}-driven {Multi}-agent {Framework}},
	shorttitle = {Enhancing {LLMs} for {Power} {System} {Simulations}},
	url = {http://arxiv.org/abs/2411.16707},
	doi = {10.48550/arXiv.2411.16707},
	abstract = {The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13\% and 96.85\%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30\% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Jia, Mengshuo and Cui, Zeyu and Hug, Gabriela},
	month = may,
	year = {2025},
	note = {arXiv:2411.16707 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: 16 pages},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\5KY4QGPR\\Jia et al. - 2025 - Enhancing LLMs for Power System Simulations A Feedback-driven Multi-agent Framework.pdf:application/pdf},
}

@misc{munir_towards_2024,
	title = {Towards {Evaluating} {Large} {Language} {Models} for {Graph} {Query} {Generation}},
	url = {http://arxiv.org/abs/2411.08449},
	doi = {10.48550/arXiv.2411.08449},
	abstract = {Large Language Models (LLMs) are revolutionizing the landscape of Generative Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging rapidly. However, when applied to database technologies, specifically query generation for graph databases and Knowledge Graphs (KGs), LLMs still face significant challenges. While research on LLM-driven query generation for Structured Query Language (SQL) exists, similar systems for graph databases remain underdeveloped. This paper presents a comparative study addressing the challenge of generating Cypher queries a powerful language for interacting with graph databases using open-access LLMs. We rigorously evaluate several LLM agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT) reasoning. Our empirical analysis of query generation accuracy reveals that Claude Sonnet 3.5 outperforms its counterparts in this specific domain. Further, we highlight promising future research directions to address the identified limitations and advance LLM-driven query generation for graph databases.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Munir, Siraj and Aldini, Alessandro},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08449 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Emerging Technologies},
	annote = {Comment: Paper accepted and will be presented at CSCI2024 in December 2024, Later will be published at Springer LNCS},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\JCGHXKF9\\Munir and Aldini - 2024 - Towards Evaluating Large Language Models for Graph Query Generation.pdf:application/pdf},
}

@inproceedings{zhu_trustful_2024,
	title = {Trustful {LLMs}: {Customizing} and {Grounding} {Text} {Generation} with {Knowledge} {Bases} and {Dual} {Decoders}},
	shorttitle = {Trustful {LLMs}},
	url = {http://arxiv.org/abs/2411.07870},
	doi = {10.18653/v1/2024.customnlp4u-1.13},
	abstract = {Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm that leverages knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process.},
	urldate = {2025-10-26},
	booktitle = {Proceedings of the 1st {Workshop} on {Customizable} {NLP}: {Progress} and {Challenges} in {Customizing} {NLP} for a {Domain}, {Application}, {Group}, or {Individual} ({CustomNLP4U})},
	author = {Zhu, Xiaofeng and Mandivarapu, Jaya Krishna},
	year = {2024},
	note = {arXiv:2411.07870 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {156--166},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\VEZBNP65\\Zhu and Mandivarapu - 2024 - Trustful LLMs Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders.pdf:application/pdf},
}

@misc{wang_when_2025,
	title = {When {Machine} {Unlearning} {Meets} {Retrieval}-{Augmented} {Generation} ({RAG}): {Keep} {Secret} or {Forget} {Knowledge}?},
	shorttitle = {When {Machine} {Unlearning} {Meets} {Retrieval}-{Augmented} {Generation} ({RAG})},
	url = {http://arxiv.org/abs/2410.15267},
	doi = {10.48550/arXiv.2410.15267},
	abstract = {The deployment of large language models (LLMs) like ChatGPT and Gemini has shown their powerful natural language generation capabilities. However, these models can inadvertently learn and retain sensitive information and harmful content during training, raising significant ethical and legal concerns. To address these issues, machine unlearning has been introduced as a potential solution. While existing unlearning methods take into account the specific characteristics of LLMs, they often suffer from high computational demands, limited applicability, or the risk of catastrophic forgetting. To address these limitations, we propose a lightweight behavioral unlearning framework based on Retrieval-Augmented Generation (RAG) technology. By modifying the external knowledge base of RAG, we simulate the effects of forgetting without directly interacting with the unlearned LLM. We approach the construction of unlearned knowledge as a constrained optimization problem, deriving two key components that underpin the effectiveness of RAG-based unlearning. This RAG-based approach is particularly effective for closed-source LLMs, where existing unlearning methods often fail. We evaluate our framework through extensive experiments on both open-source and closed-source models, including ChatGPT, Gemini, Llama-2-7b-chat, and PaLM 2. The results demonstrate that our approach meets five key unlearning criteria: effectiveness, universality, harmlessness, simplicity, and robustness. Meanwhile, this approach can extend to multimodal large language models and LLM-based agents.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Wang, Shang and Zhu, Tianqing and Ye, Dayong and Zhou, Wanlei},
	month = oct,
	year = {2025},
	note = {arXiv:2410.15267 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	annote = {Comment: 16 pages, 9 figures, 13 tables. To appear in IEEE Transactions on Dependable and Secure Computing (TDSC), 2025},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\HP6VUXMW\\Wang et al. - 2025 - When Machine Unlearning Meets Retrieval-Augmented Generation (RAG) Keep Secret or Forget Knowledge.pdf:application/pdf},
}

@misc{peng_athena_2024,
	title = {Athena: {Retrieval}-augmented {Legal} {Judgment} {Prediction} with {Large} {Language} {Models}},
	shorttitle = {Athena},
	url = {http://arxiv.org/abs/2410.11195},
	doi = {10.48550/arXiv.2410.11195},
	abstract = {Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have prevailed in countless domains, including legal scenarios. With LLMs' rapid technological progress, the development of prompt engineering (PE) as an interface between the LLMs and real-world applications has drawn the attention of all developers. Various PE methods have been proposed to overcome real-world challenges, such as few-shot prompting, chain-of-thought, and retrieval-augmented generation (RAG). However, RAG for legal judgment prediction (LJP) is still underexplored. To address this, we propose "Athena", a novel framework cultivating RAG as a core preprocess component to enhance LLMs' performance on specialized tasks. Athena constructs a knowledge base for accusations, attached with a semantic retrieval mechanism through vectorization. Our experiments show that Athena's overall performance has improved significantly, achieving state-of-the-art results on the CAIL2018 dataset. Our ablation study on the in-context window size parameter further reproduces LLMs' "lost-in-the-middle" phenomenon with a relative positional variation. And with moderate hyper-parameter-tuning, we can achieve at most 95\% of accuracy accordingly. We also study the impact of query rewriting and data distribution, providing possible directions for future research based on former analyses.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Peng, Xiao and Chen, Liang},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11195 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 13 pages, 6 figures},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XF3BYVF3\\Peng and Chen - 2024 - Athena Retrieval-augmented Legal Judgment Prediction with Large Language Models.pdf:application/pdf},
}

@misc{islam_open-rag_2024,
	title = {Open-{RAG}: {Enhanced} {Retrieval}-{Augmented} {Reasoning} with {Open}-{Source} {Large} {Language} {Models}},
	shorttitle = {Open-{RAG}},
	url = {http://arxiv.org/abs/2410.01782},
	doi = {10.48550/arXiv.2410.01782},
	abstract = {Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Islam, Shayekh Bin and Rahman, Md Asib and Hossain, K. S. M. Tozammel and Hoque, Enamul and Joty, Shafiq and Parvez, Md Rizwan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01782 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to EMNLP 2024 Findings. Website: https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\3NHY8DRA\\Islam et al. - 2024 - Open-RAG Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models.pdf:application/pdf},
}

@misc{anwar_evaluating_2024,
	title = {Evaluating {ChatGPT} on {Nuclear} {Domain}-{Specific} {Data}},
	url = {http://arxiv.org/abs/2409.00090},
	doi = {10.48550/arXiv.2409.00090},
	abstract = {This paper examines the application of ChatGPT, a large language model (LLM), for question-and-answer (Q\&A) tasks in the highly specialized field of nuclear data. The primary focus is on evaluating ChatGPT's performance on a curated test dataset, comparing the outcomes of a standalone LLM with those generated through a Retrieval Augmented Generation (RAG) approach. LLMs, despite their recent advancements, are prone to generating incorrect or 'hallucinated' information, which is a significant limitation in applications requiring high accuracy and reliability. This study explores the potential of utilizing RAG in LLMs, a method that integrates external knowledge bases and sophisticated retrieval techniques to enhance the accuracy and relevance of generated outputs. In this context, the paper evaluates ChatGPT's ability to answer domain-specific questions, employing two methodologies: A) direct response from the LLM, and B) response from the LLM within a RAG framework. The effectiveness of these methods is assessed through a dual mechanism of human and LLM evaluation, scoring the responses for correctness and other metrics. The findings underscore the improvement in performance when incorporating a RAG pipeline in an LLM, particularly in generating more accurate and contextually appropriate responses for nuclear domain-specific queries. Additionally, the paper highlights alternative approaches to further refine and improve the quality of answers in such specialized domains.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Anwar, Muhammad and Costa, Mischa de and Hammad, Issam and Lau, Daniel},
	month = aug,
	year = {2024},
	note = {arXiv:2409.00090 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PGRQDWY4\\Anwar et al. - 2024 - Evaluating ChatGPT on Nuclear Domain-Specific Data.pdf:application/pdf},
}

@misc{sannidhi_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} {Meets} {Data}-{Driven} {Tabula} {Rasa} {Approach} for {Temporal} {Knowledge} {Graph} {Forecasting}},
	url = {http://arxiv.org/abs/2408.13273},
	doi = {10.48550/arXiv.2408.13273},
	abstract = {Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google Gemini face challenges such as inaccurate factual recall, hallucinations, biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting. To address these issues, we introduce sLA-tKGF (small-scale language assistant for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG) aided, custom-trained small-scale language models through a tabula rasa approach from scratch for effective tKG forecasting. Our framework constructs knowledge-infused prompts with relevant historical data from tKGs, web search results, and PLLMs-generated textual descriptions to understand historical entity relationships prior to the target time. It leverages these external knowledge-infused prompts for deeper understanding and reasoning of context-specific semantic and temporal information to zero-shot prompt small-scale language models for more accurate predictions of future events within tKGs. It reduces hallucinations and mitigates distributional shift challenges through comprehending changing trends over time. As a result, it enables more accurate and contextually grounded forecasts of future events while minimizing computational demands. Rigorous empirical studies demonstrate our framework robustness, scalability, and state-of-the-art (SOTA) performance on benchmark datasets with interpretable and trustworthy tKG forecasting.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Sannidhi, Geethan and Sakhinana, Sagar Srinivas and Runkana, Venkataramana},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13273 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Paper was accepted at ACM KDD -2024 -- Undergraduate Consortium. Please find the link: https://kdd2024.kdd.org/undergraduate-consortium/},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PDJXXRCN\\Sannidhi et al. - 2024 - Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph F.pdf:application/pdf},
}

@misc{hopp_free_2024,
	title = {Free to play: {UN} {Trade} and {Development}'s experience with developing its own open-source {Retrieval} {Augmented} {Generation} {Large} {Language} {Model} application},
	shorttitle = {Free to play},
	url = {http://arxiv.org/abs/2407.16896},
	doi = {10.48550/arXiv.2407.16896},
	abstract = {Generative artificial intelligence (AI), and in particular Large Language Models (LLMs), have exploded in popularity and attention since the release to the public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in November of 2022. Due to the power of these general purpose models and their ability to communicate in natural language, they can be useful in a range of domains, including the work of official statistics and international organizations. However, with such a novel and seemingly complex technology, it can feel as if generative AI is something that happens to an organization, something that can be talked about but not understood, that can be commented on but not contributed to. Additionally, the costs of adoption and operation of proprietary solutions can be both uncertain and high, a barrier for often cost-constrained international organizations. In the face of these challenges, United Nations Trade and Development (UNCTAD), through its Global Crisis Response Group (GCRG), has explored and developed its own open-source Retrieval Augmented Generation (RAG) LLM application. RAG makes LLMs aware of and more useful for the organization's domain and work. Developing in-house solutions comes with pros and cons, with pros including cost, flexibility, and fostering institutional knowledge. Cons include time and skill investments and gaps and application polish and power. The three libraries developed to produce the app, nlp\_pipeline for document processing and statistical analysis, local\_rag\_llm for running a local RAG LLM, and streamlit\_rag for the user interface, are publicly available on PyPI and GitHub with Dockerfiles. A fourth library, local\_llm\_finetune, is also available for fine-tuning existing LLMs which can then be used in the application.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Hopp, Daniel},
	month = jun,
	year = {2024},
	note = {arXiv:2407.16896 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\J4L6JZUE\\Hopp - 2024 - Free to play UN Trade and Development's experience with developing its own open-source Retrieval Au.pdf:application/pdf},
}

@misc{zhao_empirical_2024,
	title = {An {Empirical} {Study} of {Retrieval} {Augmented} {Generation} with {Chain}-of-{Thought}},
	url = {http://arxiv.org/abs/2407.15569},
	doi = {10.48550/arXiv.2407.15569},
	abstract = {Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become essential tools in daily life. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Augmented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Zhao, Yuetong and Cao, Hongyu and Zhao, Xianyu and Ou, Zhijian},
	month = aug,
	year = {2024},
	note = {arXiv:2407.15569 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted by ISCSLP 2024},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\MN77XEW3\\Zhao et al. - 2024 - An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought.pdf:application/pdf},
}

@misc{ateia_can_2024,
	title = {Can {Open}-{Source} {LLMs} {Compete} with {Commercial} {Models}? {Exploring} the {Few}-{Shot} {Performance} of {Current} {GPT} {Models} in {Biomedical} {Tasks}},
	shorttitle = {Can {Open}-{Source} {LLMs} {Compete} with {Commercial} {Models}?},
	url = {http://arxiv.org/abs/2407.13511},
	doi = {10.48550/arXiv.2407.13511},
	abstract = {Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP) benchmarks across different domains. New competing Open-Source alternatives like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while often offering higher throughput and being less costly to use. Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties. We participated in the 12th BioASQ challenge, which is a retrieval augmented generation (RAG) setting, and explored the performance of current GPT models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning (zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional relevant knowledge from Wikipedia added to the context-window of the LLM might improve their performance. Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains. Our results indicate that the performance gap between commercial and open-source models in RAG setups exists mainly in the zero-shot setting and can be closed by simply collecting few-shot examples for domain-specific use cases. The code needed to rerun these experiments is available through GitHub.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Ateia, Samy and Kruschwitz, Udo},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13511 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Version as accepted at the BioASQ Lab at CLEF 2024},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\NRDHSHH9\\Ateia and Kruschwitz - 2024 - Can Open-Source LLMs Compete with Commercial Models Exploring the Few-Shot Performance of Current G.pdf:application/pdf},
}

@article{tozuka_application_2024,
	title = {Application of {NotebookLM}, a {Large} {Language} {Model} with {Retrieval}-{Augmented} {Generation}, for {Lung} {Cancer} {Staging}},
	issn = {1867-1071, 1867-108X},
	url = {http://arxiv.org/abs/2410.10869},
	doi = {10.1007/s11604-024-01705-1},
	abstract = {Purpose: In radiology, large language models (LLMs), including ChatGPT, have recently gained attention, and their utility is being rapidly evaluated. However, concerns have emerged regarding their reliability in clinical applications due to limitations such as hallucinations and insufficient referencing. To address these issues, we focus on the latest technology, retrieval-augmented generation (RAG), which enables LLMs to reference reliable external knowledge (REK). Specifically, this study examines the utility and reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for staging lung cancer. Materials and methods: We summarized the current lung cancer staging guideline in Japan and provided this as REK to NotebookLM. We then tasked NotebookLM with staging 100 fictional lung cancer cases based on CT findings and evaluated its accuracy. For comparison, we performed the same task using a gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK. Results: NotebookLM achieved 86\% diagnostic accuracy in the lung cancer staging experiment, outperforming GPT-4o, which recorded 39\% accuracy with the REK and 25\% without it. Moreover, NotebookLM demonstrated 95\% accuracy in searching reference locations within the REK. Conclusion: NotebookLM successfully performed lung cancer staging by utilizing the REK, demonstrating superior performance compared to GPT-4o. Additionally, it provided highly accurate reference locations within the REK, allowing radiologists to efficiently evaluate the reliability of NotebookLM's responses and detect possible hallucinations. Overall, this study highlights the potential of NotebookLM, a RAG-LLM, in image diagnosis.},
	urldate = {2025-10-26},
	journal = {Japanese Journal of Radiology},
	author = {Tozuka, Ryota and Johno, Hisashi and Amakawa, Akitomo and Sato, Junichi and Muto, Mizuki and Seki, Shoichiro and Komaba, Atsushi and Onishi, Hiroshi},
	month = nov,
	year = {2024},
	note = {arXiv:2410.10869 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 9 pages, 5 figures, 1 table, 3 ancillary files},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\ADRYA4ZD\\Tozuka et al. - 2024 - Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Canc.pdf:application/pdf},
}

@misc{roffo_exploring_2024,
	title = {Exploring {Advanced} {Large} {Language} {Models} with {LLMsuite}},
	url = {http://arxiv.org/abs/2407.12036},
	doi = {10.13140/RG.2.2.11774.80963},
	abstract = {This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategies, including instruction fine-tuning, parameter-efficient methods like LoRA, and Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced Self-Training (ReST). Additionally, it provides a comprehensive survey of transformer architectures and training techniques for LLMs. The source code can be accessed by contacting the author via email for a request.},
	urldate = {2025-10-26},
	author = {Roffo, Giorgio},
	month = nov,
	year = {2024},
	note = {arXiv:2407.12036 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Keywords: Language Model Benchmarking, Pre-Trained LLM Comparison, LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite, Comprehensive LLM Evaluation Toolkit},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6P95PGA5\\Roffo - 2024 - Exploring Advanced Large Language Models with LLMsuite.pdf:application/pdf},
}

@misc{feldman_ragged_2024,
	title = {{RAGged} {Edges}: {The} {Double}-{Edged} {Sword} of {Retrieval}-{Augmented} {Chatbots}},
	shorttitle = {{RAGged} {Edges}},
	url = {http://arxiv.org/abs/2403.01193},
	doi = {10.48550/arXiv.2403.01193},
	abstract = {Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss implications for the development of more trustworthy LLMs.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Feldman, Philip and Foulds, James R. and Pan, Shimei},
	month = jun,
	year = {2024},
	note = {arXiv:2403.01193 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 7 Pages, 1 Figure, 1 Table},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\YI2WT287\\Feldman et al. - 2024 - RAGged Edges The Double-Edged Sword of Retrieval-Augmented Chatbots.pdf:application/pdf},
}

@misc{xie_finben_2024,
	title = {{FinBen}: {A} {Holistic} {Financial} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {{FinBen}},
	url = {http://arxiv.org/abs/2402.12659},
	doi = {10.48550/arXiv.2402.12659},
	abstract = {LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive evaluation benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 36 datasets spanning 24 financial tasks, covering seven critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, and decision-making. FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source evaluation datasets for text summarization, question answering, and stock trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation in financial LLMs. All datasets, results, and codes are released for the research community: https://github.com/The-FinAI/PIXIU.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and Xu, Yijing and Kang, Haoqiang and Kuang, Ziyan and Yuan, Chenhan and Yang, Kailai and Luo, Zheheng and Zhang, Tianlin and Liu, Zhiwei and Xiong, Guojun and Deng, Zhiyang and Jiang, Yuechen and Yao, Zhiyuan and Li, Haohang and Yu, Yangyang and Hu, Gang and Huang, Jiajia and Liu, Xiao-Yang and Lopez-Lira, Alejandro and Wang, Benyou and Lai, Yanzhao and Wang, Hao and Peng, Min and Ananiadou, Sophia and Huang, Jimin},
	month = jun,
	year = {2024},
	note = {arXiv:2402.12659 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computational Engineering, Finance, and Science},
	annote = {Comment: 26 pages, 11 figures},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\APVTPMIG\\Xie et al. - 2024 - FinBen A Holistic Financial Benchmark for Large Language Models.pdf:application/pdf},
}

@misc{alan_rag-based_2025,
	title = {A {RAG}-based {Question} {Answering} {System} {Proposal} for {Understanding} {Islam}: {MufassirQAS} {LLM}},
	shorttitle = {A {RAG}-based {Question} {Answering} {System} {Proposal} for {Understanding} {Islam}},
	url = {http://arxiv.org/abs/2401.15378},
	doi = {10.48550/arXiv.2401.15378},
	abstract = {Challenges exist in learning and understanding religions, such as the complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect for enlightenment on religion as a question-answering chatbot. However, LLMs also tend to generate false information, known as hallucination. Also, the chatbots' responses can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It must avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called "MufassirQAS". We created a database consisting of several open-access books that include Turkish context. These books contain Turkish translations and interpretations of Islam. This database is utilized to answer religion-related questions and ensure our answers are trustworthy. The relevant part of the dataset, which LLM also uses, is presented along with the answer. We have put careful effort into creating system prompts that give instructions to prevent harmful, offensive, or disrespectful responses to respect people's values and provide reliable results. The system answers and shares additional information, such as the page number from the respective book and the articles referenced for obtaining the information. MufassirQAS and ChatGPT are also tested with sensitive questions. We got better performance with our system. Study and enhancements are still in progress. Results and future works are given.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Alan, Ahmet Yusuf and Karaarslan, Enis and Aydin, Ömer},
	month = mar,
	year = {2025},
	note = {arXiv:2401.15378 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\WYSYDS3K\\Alan et al. - 2025 - A RAG-based Question Answering System Proposal for Understanding Islam MufassirQAS LLM.pdf:application/pdf},
}

@misc{kang_prompt-rag_2024,
	title = {Prompt-{RAG}: {Pioneering} {Vector} {Embedding}-{Free} {Retrieval}-{Augmented} {Generation} in {Niche} {Domains}, {Exemplified} by {Korean} {Medicine}},
	shorttitle = {Prompt-{RAG}},
	url = {http://arxiv.org/abs/2401.11246},
	doi = {10.48550/arXiv.2401.11246},
	abstract = {We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel approach to enhance the performance of generative large language models (LLMs) in niche domains. Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain. To explore and exemplify this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine (CM) documents, finding that KM document embeddings correlated more with token overlaps and less with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from conventional RAG models, operates without the need for embedding vectors. Its performance was assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed existing models, including ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and informativeness. Despite challenges like content structuring and response latency, the advancements in LLMs are expected to encourage the use of Prompt-RAG, making it a promising tool for other domains in need of RAG methods.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Kang, Bongsu and Kim, Jundong and Yun, Tae-Rim and Kim, Chang-Eop},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11246 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: 26 pages, 4 figures, 5 tables},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7AKNUT36\\Kang et al. - 2024 - Prompt-RAG Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exempl.pdf:application/pdf},
}

@misc{kulkarni_reinforcement_2024,
	title = {Reinforcement {Learning} for {Optimizing} {RAG} for {Domain} {Chatbots}},
	url = {http://arxiv.org/abs/2401.06800},
	doi = {10.48550/arXiv.2401.06800},
	abstract = {With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Kulkarni, Mandar and Tangarajan, Praveen and Kim, Kyung and Trivedi, Anusua},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06800 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\BRL6S2VJ\\Kulkarni et al. - 2024 - Reinforcement Learning for Optimizing RAG for Domain Chatbots.pdf:application/pdf},
}

@misc{allu_beyond_2024,
	title = {Beyond {Extraction}: {Contextualising} {Tabular} {Data} for {Efficient} {Summarisation} by {Language} {Models}},
	shorttitle = {Beyond {Extraction}},
	url = {http://arxiv.org/abs/2401.02333},
	doi = {10.48550/arXiv.2401.02333},
	abstract = {The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our approach aims to significantly improve the precision of complex table queries, offering a promising solution to a longstanding challenge in information retrieval.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Allu, Uday and Ahmed, Biddwan and Tripathi, Vishesh},
	month = feb,
	year = {2024},
	note = {arXiv:2401.02333 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Submitted to IEEE},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\EAVL39X4\\Allu et al. - 2024 - Beyond Extraction Contextualising Tabular Data for Efficient Summarisation by Language Models.pdf:application/pdf},
}

@misc{zhang_iag_2023,
	title = {{IAG}: {Induction}-{Augmented} {Generation} {Framework} for {Answering} {Reasoning} {Questions}},
	shorttitle = {{IAG}},
	url = {http://arxiv.org/abs/2311.18397},
	doi = {10.48550/arXiv.2311.18397},
	abstract = {Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Zhang, Zhebin and Zhang, Xinyu and Ren, Yuanhang and Shi, Saijiang and Han, Meng and Wu, Yongkang and Lai, Ruofei and Cao, Zhao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.18397 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\F42SRXGR\\Zhang et al. - 2023 - IAG Induction-Augmented Generation Framework for Answering Reasoning Questions.pdf:application/pdf},
}

@misc{asai_self-rag_2023,
	title = {Self-{RAG}: {Learning} to {Retrieve}, {Generate}, and {Critique} through {Self}-{Reflection}},
	shorttitle = {Self-{RAG}},
	url = {http://arxiv.org/abs/2310.11511},
	doi = {10.48550/arXiv.2310.11511},
	abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11511 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 30 pages, 2 figures, 12 tables},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5NLVQ3H\\Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Critique through Self-Reflection.pdf:application/pdf},
}

@misc{low_answering_2024,
	title = {Answering real-world clinical questions using large language model based systems},
	url = {http://arxiv.org/abs/2407.00541},
	doi = {10.48550/arXiv.2407.00541},
	abstract = {Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2\% - 10\%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24\% (OpenEvidence) to 58\% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65\% vs. 0-9\%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Low, Yen Sia and Jackson, Michael L. and Hyde, Rebecca J. and Brown, Robert E. and Sanghavi, Neil M. and Baldwin, Julian D. and Pike, C. William and Muralidharan, Jananee and Hui, Gavin and Alexander, Natasha and Hassan, Hadeel and Nene, Rahul V. and Pike, Morgan and Pokrzywa, Courtney J. and Vedak, Shivam and Yan, Adam Paul and Yao, Dong-han and Zipursky, Amy R. and Dinh, Christina and Ballentine, Philip and Derieg, Dan C. and Polony, Vladimir and Chawdry, Rehan N. and Davies, Jordan and Hyde, Brigham B. and Shah, Nigam H. and Gombar, Saurabh},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00541 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: 28 pages (2 figures, 3 tables) inclusive of 8 pages of supplemental materials (4 supplemental figures and 4 supplemental tables)},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5S5S66W\\Low et al. - 2024 - Answering real-world clinical questions using large language model based systems.pdf:application/pdf},
}

@misc{jauhiainen_evaluating_2024,
	title = {Evaluating {Students}' {Open}-ended {Written} {Responses} with {LLMs}: {Using} the {RAG} {Framework} for {GPT}-3.5, {GPT}-4, {Claude}-3, and {Mistral}-{Large}},
	shorttitle = {Evaluating {Students}' {Open}-ended {Written} {Responses} with {LLMs}},
	url = {http://arxiv.org/abs/2405.05444},
	doi = {10.48550/arXiv.2405.05444},
	abstract = {Evaluating open-ended written examination responses from students is an essential yet time-intensive task for educators, requiring a high degree of effort, consistency, and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators' time. In our study, we explore the effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in assessing university students' open-ended answers to questions made about reference material they have studied. Each model was instructed to evaluate 54 answers repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers. As of spring 2024, our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs. There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open-ended written responses. Further comparative research is essential to determine the accuracy and cost-effectiveness of using LLMs for educational assessments.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Jauhiainen, Jussi S. and Guerra, Agustín Garagorry},
	month = may,
	year = {2024},
	note = {arXiv:2405.05444 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 18 pages, 6 tables, 1 figure},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GLHCI9AQ\\Jauhiainen and Guerra - 2024 - Evaluating Students' Open-ended Written Responses with LLMs Using the RAG Framework for GPT-3.5, GP.pdf:application/pdf},
}

@misc{kunstmann_eventchat_2024,
	title = {{EventChat}: {Implementation} and user-centric evaluation of a large language model-driven conversational recommender system for exploring leisure events in an {SME} context},
	shorttitle = {{EventChat}},
	url = {http://arxiv.org/abs/2407.04472},
	doi = {10.48550/arXiv.2407.04472},
	abstract = {Large language models (LLMs) present an enormous evolution in the strategic potential of conversational recommender systems (CRS). Yet to date, research has predominantly focused upon technical frameworks to implement LLM-driven CRS, rather than end-user evaluations or strategic implications for firms, particularly from the perspective of a small to medium enterprises (SME) that makeup the bedrock of the global economy. In the current paper, we detail the design of an LLM-driven CRS in an SME setting, and its subsequent performance in the field using both objective system metrics and subjective user evaluations. While doing so, we additionally outline a short-form revised ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly evolving field. Our results reveal good system performance from a user experience perspective (85.5\% recommendation accuracy) but underscore latency, cost, and quality issues challenging business viability. Notably, with a median cost of \$0.04 per interaction and a latency of 5.7s, cost-effectiveness and response time emerge as crucial areas for achieving a more user-friendly and economically viable LLM-driven CRS for SME settings. One major driver of these costs is the use of an advanced LLM as a ranker within the retrieval-augmented generation (RAG) technique. Our results additionally indicate that relying solely on approaches such as Prompt-based learning with ChatGPT as the underlying LLM makes it challenging to achieve satisfying quality in a production environment. Strategic considerations for SMEs deploying an LLM-driven CRS are outlined, particularly considering trade-offs in the current technical landscape.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Kunstmann, Hannes and Ollier, Joseph and Persson, Joel and Wangenheim, Florian von},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04472 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: 27 pages, 3 tables, 5 figures, pre-print manuscript, updated version of manuscript due to typo (previous version, Figure 5 was incorrectly named Figure 6)},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\A4IPQYDB\\Kunstmann et al. - 2024 - EventChat Implementation and user-centric evaluation of a large language model-driven conversationa.pdf:application/pdf},
}

@misc{zeng_federated_2024,
	title = {Federated {Recommendation} via {Hybrid} {Retrieval} {Augmented} {Generation}},
	url = {http://arxiv.org/abs/2403.04256},
	doi = {10.48550/arXiv.2403.04256},
	abstract = {Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Zeng, Huimin and Yue, Zhenrui and Jiang, Qian and Wang, Dong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04256 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6LQZKNCJ\\Zeng et al. - 2024 - Federated Recommendation via Hybrid Retrieval Augmented Generation.pdf:application/pdf},
}

@article{lakatos_investigating_2025,
	title = {Investigating the performance of {Retrieval}-{Augmented} {Generation} and fine-tuning for the development of {AI}-driven knowledge-based systems},
	volume = {7},
	issn = {2504-4990},
	url = {http://arxiv.org/abs/2403.09727},
	doi = {10.3390/make7010015},
	abstract = {The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16\% in terms of the ROGUE score, 15\% in the case of the BLEU score, and 53\% based on the cosine similarity. This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8\% better METEOR score of FN models indicates greater creativity compared to RAG.},
	number = {1},
	urldate = {2025-10-26},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Lakatos, Robert and Pollner, Peter and Hajdu, Andras and Joo, Tamas},
	month = feb,
	year = {2025},
	note = {arXiv:2403.09727 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {15},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\P8GXIXWR\\Lakatos et al. - 2025 - Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development.pdf:application/pdf},
}

@misc{lu_large_2024,
	title = {Large {Language} {Models} {Struggle} in {Token}-{Level} {Clinical} {Named} {Entity} {Recognition}},
	url = {http://arxiv.org/abs/2407.00731},
	doi = {10.48550/arXiv.2407.00731},
	abstract = {Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER. Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Lu, Qiuhao and Li, Rui and Wen, Andrew and Wang, Jinlian and Wang, Liwei and Liu, Hongfang},
	month = aug,
	year = {2024},
	note = {arXiv:2407.00731 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: AMIA 2024 Annual Symposium Proceedings},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6W5HR7GU\\Lu et al. - 2024 - Large Language Models Struggle in Token-Level Clinical Named Entity Recognition.pdf:application/pdf},
}

@misc{guan_mfort-qa_2024,
	title = {{MFORT}-{QA}: {Multi}-hop {Few}-shot {Open} {Rich} {Table} {Question} {Answering}},
	shorttitle = {{MFORT}-{QA}},
	url = {http://arxiv.org/abs/2403.19116},
	doi = {10.48550/arXiv.2403.19116},
	abstract = {In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.},
	urldate = {2025-10-26},
	publisher = {arXiv},
	author = {Guan, Che and Huang, Mengyu and Zhang, Peng},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19116 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 8 pages},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\RYANUDW6\\Guan et al. - 2024 - MFORT-QA Multi-hop Few-shot Open Rich Table Question Answering.pdf:application/pdf},
}

@inproceedings{fortuna_natural_2024,
	title = {Natural {Language} {Interaction} with a {Household} {Electricity} {Knowledge}-based {Digital} {Twin}},
	url = {http://arxiv.org/abs/2406.06566},
	doi = {10.1109/SmartGridComm60555.2024.10738062},
	abstract = {Domain specific digital twins, representing a digital replica of various segments of the smart grid, are foreseen as able to model, simulate, and control the respective segments. At the same time, knowledge-based digital twins, coupled with AI, may also empower humans to understand aspects of the system through natural language interaction in view of planning and policy making. This paper is the first to assess and report on the potential of Retrieval Augmented Generation (RAG) question answers related to household electrical energy measurement aspects leveraging a knowledge-based energy digital twin. Relying on the recently published electricity consumption knowledge graph that actually represents a knowledge-based digital twin, we study the capabilities of ChatGPT, Gemini and Llama in answering electricity related questions. Furthermore, we compare the answers with the ones generated through a RAG techniques that leverages an existing electricity knowledge-based digital twin. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.},
	urldate = {2025-10-26},
	booktitle = {2024 {IEEE} {International} {Conference} on {Communications}, {Control}, and {Computing} {Technologies} for {Smart} {Grids} ({SmartGridComm})},
	author = {Fortuna, Carolina and Hanžel, Vid and Bertalanič, Blaž},
	month = sep,
	year = {2024},
	note = {arXiv:2406.06566 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {8--14},
	annote = {Comment: Accepted at IEEE SmartGridComm'24},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GDYAVYLM\\Fortuna et al. - 2024 - Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin.pdf:application/pdf},
}
