@inproceedings{tamascelli_academic_2025,
  title = {Academic {Advising},
  author = {Tamascelli, Michael and Bunch, Olivia and Fowler, Blake and Taeb, Maryam and Cohen, Achraf},
  year = {2025},
  doi = {10.1145/3696673.3723065},
  url = {https://doi.org/10.1145/3696673.3723065},
  pages = {195 -- 202},
  note = {Type: Conference paper},
  keywords = {academic advising, AI agent, chatbot, higher education, large language models, retrieval-augmented generation, source: ACM},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{quinn_accelerating_2025,
  title = {Accelerating {Retrieval},
  author = {Quinn, Derrick and Nouri, Mohammad and Patel, Neel and Salihu, John and Salemi, Alireza and Lee, Sukhan and Zamani, Hamed and Alian, Mohammad},
  year = {2025},
  doi = {10.1145/3669940.3707264},
  url = {https://doi.org/10.1145/3669940.3707264},
  booktitle = {International {Conference},
  volume = {1},
  pages = {15 -- 32},
  note = {Type: Conference paper},
  keywords = {database acceleration, dense retrieval, retrieval-augmented generation (rag), source: ACM},
  annote = {Cited by: 6},
}

@inproceedings{zhang_survey_2025,
  title = {A {Survey},
  author = {Zhang, Xueqiang and Dong, Xiaofei and Wang, Yiru and Zhang, Dan and Cao, Feng},
  year = {2025},
  doi = {10.1145/3707292.3707383},
  url = {https://doi.org/10.1145/3707292.3707383},
  pages = {318 -- 323},
  note = {Type: Conference paper},
  keywords = {development analysis, key technology, Large models, theory foundation, source: ACM},
  annote = {Cited by: 0},
}

@inproceedings{sun_adaptive_2024,
  title = {Adaptive {In},
  author = {Sun, Zhu and Feng, Kaidong and Yang, Jie and Qu, Xinghua and Fang, Hui and Ong, Yew Soon and Liu, Wenyuan},
  year = {2024},
  doi = {10.1145/3626772.3657808},
  url = {https://doi.org/10.1145/3626772.3657808},
  pages = {966 -- 976},
  note = {Type: Conference paper},
  keywords = {bundle generation, in-context learning, large language models, recommendation, user intent inference, source: ACM},
  annote = {Cited by: 3},
}

@article{shi_amsnet-kg_2025,
  title = {{AMSnet},
  author = {Shi, Yichen and Tao, Zhuofu and Gao, YuHao and Zhou, Tianjia and Chang, Cheng and Wang, Yaxin and Chen, Bingyu and Zhang, Genhao and Liu, Alvin and Yu, Zhiping and Lin, Ting-Jung and He, Lei},
  year = {2025},
  doi = {10.1145/3736166},
  url = {https://doi.org/10.1145/3736166},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AMSnet, EDA, knowledge graph, LLM, RAG, topology design, source: ACM},
  abstract = {High-performance analog and mixed-signal (AMS) circuits are mainly full-custom designed, which is time-consuming and labor-intensive. A significant portion of the effort is experience-driven, which makes the automation of AMS circuit design a formidable challenge. Large language models (LLMs) have emerged as powerful tools for electronic design automation (EDA) applications, fostering advancements in the automatic design process for large-scale AMS circuits. However, the absence of high-quality datasets has led to issues such as model hallucination, which undermines the robustness of automatically generated circuit designs. To address this issue, this article introduces AMSnet-KG, a dataset encompassing various AMS circuit schematics and netlists. We construct a knowledge graph with annotations on detailed functional and performance characteristics. Facilitated by AMSnet-KG, we propose an automated AMS circuit generation framework that utilizes the comprehensive knowledge embedded in LLMs. The flow first formulate a design strategy (e.g., circuit architecture using a number of circuit components) based on required specifications. Next, matched subcircuits are retrieved and assembled into a complete topology, and transistor sizing is obtained through Bayesian optimization. Simulation results of the netlist are automatically fed back to the LLM for further topology refinement, ensuring the circuit design specifications are met. We perform case studies of operational amplifier and comparator design to verify the automatic design flow from specifications to netlists with minimal human effort. The dataset used in this article is available at .},
  annote = {Query date: 2025-10-25 20:50:36},
  issn = {1084-4309},
  month = {oct},
}

@article{zhang_survey_2025-1,
  title = {A {Survey},
  author = {Zhang, Zeyu and Dai, Quanyu and Bo, Xiaohe and Ma, Chen and Li, Rui and Chen, Xu and Zhu, Jieming and Dong, Zhenhua and Wen, Ji-Rong},
  year = {2025},
  doi = {10.1145/3748302},
  url = {https://doi.org/10.1145/3748302},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Agent, Information Processing, Information System, Large Language Model, Memory Mechanism, source: ACM},
  abstract = {Large language model (LLM)-based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this article, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss “what is” and “why do we need” the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at .},
  issn = {1046-8188},
  month = {sep},
}

@article{deng_ai_2025,
  title = {{AI},
  author = {Deng, Zehang and Guo, Yongjian and Han, Changzhou and Ma, Wanlun and Xiong, Junwu and Wen, Sheng and Xiang, Yang},
  year = {2025},
  doi = {10.1145/3716628},
  url = {https://doi.org/10.1145/3716628},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI agent, security, trustworthiness, source: ACM},
  abstract = {An Artificial Intelligence (AI) agent is a software entity that autonomously performs tasks or makes decisions based on pre-defined objectives and data inputs. AI agents, capable of perceiving user inputs, reasoning and planning tasks, and executing actions, have seen remarkable advancements in algorithm development and task performance. However, the security challenges they pose remain under-explored and unresolved. This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities. By systematically reviewing these threats, this article highlights both the progress made and the existing limitations in safeguarding AI agents. The insights provided aim to inspire further research into addressing the security threats associated with AI agents, thereby fostering the development of more robust and secure AI agent applications.},
  issn = {0360-0300},
  month = {feb},
}

@article{lu_adda_2025,
  title = {Adda: {Towards},
  author = {Lu, Kuan and Yang, Zhihui and Wu, Sai and Xia, Ruichen and Zhang, Dongxiang and Chen, Gang},
  year = {2025},
  doi = {10.1145/3725262},
  url = {https://doi.org/10.1145/3725262},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {auto feature engineering, in-database machine learning, LLM agent, source: ACM},
  abstract = {Integrating machine learning (ML) analytics into existing database management systems (DBMSs) not only eliminates the need for costly data transfers to external ML platforms but also ensures compliance with regulatory standards. While some DBMSs have integrated functionalities for training and applying ML models for analytics, these tasks still present challenges, particularly due to limited support for automatic feature engineering (AutoFE), which is crucial for optimizing ML model performance. In this paper, we introduce Adda, an agent-driven in-database feature generation tool designed to automatically create high-quality features for ML analytics directly within the database. Adda interprets ML analytics tasks described in natural language and generates code for feature construction by leveraging the power of large language models (LLMs) integrated with specialized agents. This code is then translated into SQL statements using a predefined set of operators and compiled just-in-time (JIT) into user-defined functions (UDFs). The result is a seamless, fully in-database solution for feature generation, specifically tailored for ML analytics tasks. Extensive experiments across 14 public datasets, with five ML tasks per dataset, show that Adda improves the AUC by up to 33.2\% and reduces end-to-end latency by up to 100x compared to Madlib.},
  month = {jun},
}

@article{qiang_agent-om_2024,
  title = {Agent-{OM},
  author = {Qiang, Zhangcheng and Wang, Weiqing and Taylor, Kerry},
  year = {2024},
  doi = {10.14778/3712221.3712222},
  url = {https://doi.org/10.14778/3712221.3712222},
  journal = {Proc. VLDB Endow.},
  volume = {18},
  number = {3},
  pages = {516--529},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.},
  annote = {Query date: 2025-10-25 20:50:36},
  issn = {2150-8097},
  month = {nov},
}

@article{huang_survey_2025,
  title = {A {Survey},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  year = {2025},
  doi = {10.1145/3703155},
  url = {https://doi.org/10.1145/3703155},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Factuality, Faithfulness, Hallucination, Large Language Models, source: ACM},
  abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
  annote = {Query date: 2025-10-25 20:50:36},
  issn = {1046-8188},
  month = {jan},
}

@article{mo_survey_2025,
  title = {A {Survey},
  author = {Mo, Fengran and Mao, Kelong and Zhao, Ziliang and Qian, Hongjin and Chen, Haonan and Cheng, Yiruo and Li, Xiaoxi and Zhu, Yutao and Dou, Zhicheng and Nie, Jian-Yun},
  year = {2025},
  doi = {10.1145/3759453},
  url = {https://doi.org/10.1145/3759453},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Benchmark and Evaluation, Conversational Retrieval and Generation, Conversational Search, Domain-specific and User-centric Application, Query Reformulation, Search Clarification, source: ACM},
  abstract = {As a cornerstone of modern information access, search engines have become indispensable in everyday life. With the rapid advancements in AI and natural language processing (NLP) technologies, particularly large language models (LLMs), search engines have evolved to support more intuitive and intelligent interactions between users and systems. Conversational search, an emerging paradigm for next-generation search engines, leverages natural language dialogue to facilitate complex and precise information retrieval, thus attracting significant attention. Unlike traditional keyword-based search engines, conversational search systems enhance user experience by supporting intricate queries, maintaining context over multi-turn interactions, and providing robust information integration and processing capabilities. Key components such as query reformulation, search clarification, conversational retrieval, and response generation work in unison to enable these sophisticated interactions. In this survey, we explore the recent advancements and potential future directions in conversational search, examining the critical modules that constitute a conversational search system. We highlight the integration of LLMs in enhancing these systems and discuss the challenges and opportunities that lie ahead in this dynamic field. Additionally, we provide insights into real-world applications and robust evaluations of current conversational search systems, aiming to guide future research and development in conversational search.},
  issn = {1046-8188},
  month = {sep},
}

@article{wang_comprehensive_2025,
  title = {A {Comprehensive},
  author = {Wang, Fali and Zhang, Zhiwei and Zhang, Xianren and Wu, Zongyu and Mo, TzuHao and Lu, Qiuhao and Wang, Wanjing and Li, Rui and Xu, Junjie and Tang, Xianfeng and He, Qi and Ma, Yao and Huang, Ming and Wang, Suhang},
  year = {2025},
  doi = {10.1145/3768165},
  url = {https://doi.org/10.1145/3768165},
  journal = {ACM Trans. Intell. Syst. Technol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Domain-specific Models, On-Device LLMs, Small Language Models, Trustworthiness, source: ACM},
  abstract = {Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: .},
  annote = {Just Accepted},
  issn = {2157-6904},
  month = {sep},
}

@article{joel_survey_2025,
  title = {A {Survey},
  author = {Joel, Sathvik and Wu, Jie and Fard, Fatemeh},
  year = {2025},
  doi = {10.1145/3770084},
  url = {https://doi.org/10.1145/3770084},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {code generation, domain-specific languages (DSLs), Large language models, low-resource programming languages (LRPLs), systematic literature review, source: ACM},
  abstract = {Large Language Models (LLMs) have shown remarkable capabilities in code generation for popular programming languages. However, their performance in Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a critical challenge. This gap affects millions of developers - with Rust alone having 3.5 million users - who are currently unable to fully leverage LLM capabilities. LRPLs and DSLs face unique challenges, including severe data scarcity and, for DSLs, highly specialized syntax and semantics that are poorly represented in general-purpose datasets. Addressing these challenges is crucial as LRPLs and DSLs significantly enhance development efficiency in specialized domains and applications, including financial and scientific works. While several surveys on LLMs for software engineering and code exist, none comprehensively address the challenges and opportunities specific to LRPLs and DSLs. Our survey fills this gap by providing a systematic review of the current state, methodologies, and challenges in leveraging LLMs for code generation in LRPL and DSL. We filtered 111 papers from over 27,000 published studies from 2020 – 2024 to understand the capabilities and limitations of LLMs in these specialized domains. We also expanded our literature search to include 5 recent papers from 2024 – 2025. We report LLMs used, benchmarks, and metrics to evaluate code generation in LRPLs and DSLs, as well as strategies used to enhance LLM performance, and the collected datasets and curation methods in this context.We identified four main evaluation techniques used in the literature, along with several metrics to assess code generation in LRPL and DSL. We categorized the methods used for LLM improvement into six main groups and summarized the novel methods and architectures proposed by the researchers. We also classified different approaches used for data collection and preparation. While different techniques, metrics, and datasets are used, there is a lack of a standard approach and a benchmark dataset to evaluate code generation in several LRPLs and DSLs. We discuss several distinctions of the studied approaches with the ones used in high-resource programming languages (HRPLs), as well as several challenges unique to these languages, especially DSLs. The challenges stem from the scarcity of data, the unique requirements, and specialized domains, which often need expertise guidelines or domain-specific tools. Accordingly, we provide insights into different research opportunities for the studied aspects. This survey serves as a comprehensive resource for researchers and practitioners working at the intersection of LLMs, software engineering, and specialized programming languages, providing a foundation for future advancements in LRPL and DSL code generation. A GitHub repository was created to organize the papers of this survey at .},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {oct},
}

@article{zhang_survey_2025-2,
  title = {A {Survey},
  author = {Zhang, Lingzhe and Jia, Tong and Jia, Mengxi and Wu, Yifan and Liu, Aiwei and Yang, Yong and Wu, Zhonghai and Hu, Xuming and Yu, Philip and Li, Ying},
  year = {2025},
  doi = {10.1145/3746635},
  url = {https://doi.org/10.1145/3746635},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AIOps, anomaly detection, assisted remediation, failure perception, incident report, Large language model, logs, metrics, root cause analysis, time series, source: ACM},
  abstract = {As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention. However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain. We analyzed 183 research articles published between January 2020 and December 2024 to answer four key research questions (RQs). In RQ1, we examine the diverse failure data sources utilized, including advanced LLM-based processing techniques for legacy data and the incorporation of new data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks, highlighting the emergence of novel tasks and the publication trends across these tasks. RQ3 investigates the various LLM-based methods applied to address AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to assess LLM-integrated AIOps approaches. Based on our findings, we discuss the state-of-the-art advancements and trends, identify gaps in existing research, and propose promising directions for future exploration.},
  issn = {0360-0300},
  month = {sep},
}

@article{bui_systematic_2024,
  title = {A {Systematic},
  author = {Bui, Minh-Thanh and Boffa, Matteo and Valentim, Rodolfo Vieira and Navarro, Jose Manuel and Chen, Fuxing and Bao, Xiaosheng and Houidi, Zied Ben and Rossi, Dario},
  year = {2024},
  doi = {10.1145/3696379},
  url = {https://doi.org/10.1145/3696379},
  journal = {Proc. ACM Netw.},
  volume = {2},
  number = {CoNEXT4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {computing methodologies, firewalls, intrusion detection systems, machine learning, natural language processing, security and privacy, source: ACM},
  abstract = {We explore the capabilities of Large Language Models (LLMs) to assist or substitute devices (i.e., firewalls) and humans (i.e., security experts) respectively in the detection and analysis of security incidents. We leverage transformer-based technologies, from relatively small to foundational sizes, to address the problem of correctly identifying the attack severity (and accessorily identifying and explaining the attack type). We contrast a broad range of LLM techniques (prompting, retrieval augmented generation, and fine-tuning of several models) using state-of-the-art machine learning models as a baseline. Using proprietary data from commercial deployment, our study provides an unbiased picture of the strengths and weaknesses of LLM for intrusion detection.},
  month = {nov},
}

@article{pan_survey_2025,
  title = {A {Survey},
  author = {Pan, Jingyu and Zhou, Guanglei and Chang, Chen-Chia and Jacobson, Isaac and Hu, Jiang and Chen, Yiran},
  year = {2025},
  doi = {10.1145/3715324},
  url = {https://doi.org/10.1145/3715324},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {electronic design automation, Large language models, machine learning, source: ACM},
  abstract = {Within the rapidly evolving domain of Electronic Design Automation (EDA), Large Language Models (LLMs) have emerged as transformative technologies, offering unprecedented capabilities for optimizing and automating various aspects of electronic design. This survey provides a comprehensive exploration of LLM applications in EDA, focusing on advancements in model architectures, the implications of varying model sizes, and innovative customization techniques that enable tailored analytical insights. By examining the intersection of LLM capabilities and EDA requirements, the article highlights the significant impact these models have on extracting nuanced understandings from complex datasets. Furthermore, it addresses the challenges and opportunities in integrating LLMs into EDA workflows, paving the way for future research and application in this dynamic field. Through this detailed analysis, the survey aims to offer valuable insights to professionals in the EDA industry, AI researchers, and anyone interested in the convergence of advanced AI technologies and electronic design.},
  issn = {1084-4309},
  month = {feb},
}

@article{shi_survey_2025,
  title = {A {Survey},
  author = {Shi, Liang and Tang, Zhengju and Zhang, Nan and Zhang, Xiaotong and Yang, Zhi},
  year = {2025},
  doi = {10.1145/3737873},
  url = {https://doi.org/10.1145/3737873},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {fine-tuning, Large language models, prompt engineering, Text-to-SQL, source: ACM},
  abstract = {With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field.},
  issn = {0360-0300},
  month = {sep},
}

@article{qin_ai-based_2025,
  title = {{AI},
  author = {Qin, Peinuan and Zhu, Zicheng and Yamashita, Naomi and Yang, Yitian and Suga, Keita and Lee, Yi-Chieh},
  year = {2025},
  doi = {10.1145/3757455},
  url = {https://doi.org/10.1145/3757455},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI-mediated communication, multilingual communication, non-native speakers, real-time, speaking support},
  abstract = {Non-native speakers (NNSs) often face speaking challenges in real-time multilingual communication, such as struggling to articulate their thoughts. To address this issue, we developed an AI-based speaking assistant (AISA) that provides speaking references for NNSs based on their input queries, task background, and conversation history. To explore NNSs' interaction with AISA and its impact on NNSs' speaking during real-time multilingual communication, we conducted a mixed-method study involving a within-subject experiment and follow-up interviews. In the experiment, two native speakers (NSs) and one NNS formed a team (31 teams in total) and completed two collaborative tasks-one with access to the AISA and one without. Overall, our study revealed four types of AISA input patterns among NNSs, each reflecting different levels of effort and language preferences. Although AISA did not improve NNSs' speaking competence, follow-up interviews revealed that it helped improve the logical flow and depth of their speech. Moreover, the additional multitasking introduced by AISA, such as entering and reviewing system output, potentially elevated NNSs' workload and anxiety. Based on these observations, we discuss the pros and cons of implementing tools to assist NNS in real-time multilingual communication and offer design recommendations.},
  month = {oct},
}

@article{naveed_comprehensive_2025,
  title = {A {Comprehensive},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  year = {2025},
  doi = {10.1145/3744746},
  url = {https://doi.org/10.1145/3744746},
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {16},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Augmented LLMs, chatGPT, Large Language Models, LLM Benchmarking, LLM training, LLMs, Multimodal LLMs, source: ACM},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multimodal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive, informative summaries of the existing works to advance the LLM research.},
  issn = {2157-6904},
  month = {aug},
}

@article{israelsen_good_2025,
  title = {“{A},
  author = {Israelsen, Brett and Ahmed, Nisar R. and Aitken, Matthew and Frew, Eric W. and Lawrence, Dale A. and Argrow, Brian M.},
  year = {2025},
  doi = {10.1145/3732794},
  url = {https://doi.org/10.1145/3732794},
  journal = {J. Hum.-Robot Interact.},
  volume = {14},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {autonomous robots, human-autonomy interaction, Markov decision processes, probabilistic models, proficiency assessment, source: ACM},
  abstract = {How can intelligent machines assess their competency to complete a task? This question has come into focus for autonomous systems that algorithmically make decisions under uncertainty. We argue that machine self-confidence—a form of meta-reasoning based on self-assessments of system knowledge about the state of the world, itself, and ability to reason about and execute tasks—leads to many computable and useful competency indicators for such agents. This article presents our body of work, so far, on this concept in the form of the Factorized Machine Self-Confidence (FaMSeC) framework, which holistically considers several major factors driving competency in algorithmic decision-making: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are derived via “problem-solving statistics” embedded in Markov Decision Process solvers and related approaches. These statistics come from evaluating probabilistic exceedance margins in relation to certain outcomes and associated competency standards specified by an evaluator. Once designed, and evaluated, the statistics can be easily incorporated into autonomous agents and serve as indicators of competency. We include detailed descriptions and examples for Markov Decision Process agents and show how outcome assessment and solver quality factors can be found for a range of tasking contexts through novel use of meta-utility functions, behavior simulations, and surrogate prediction models. Numerical evaluations are performed to demonstrate that FaMSeC indicators perform as desired (references to human subject studies beyond the scope of this article are provided).},
  month = {jul},
}

@article{jiang_survey_2025,
  title = {A {Survey},
  author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  year = {2025},
  doi = {10.1145/3747588},
  url = {https://doi.org/10.1145/3747588},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Code Generation, Code Large Language Models, Large Language Models, source: ACM},
  abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and provide a quantitative and qualitative comparative analysis of experimental results of code LLMs, sourced from their original papers to ensure a fair comparison on the HumanEval, MBPP, and BigCodeBench benchmarks, across various levels of difficulty and types of programming tasks, to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page () to continuously document and disseminate the most recent advances in the field.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {jul},
}

@article{chau_is_2025,
  title = {An {IS},
  author = {Chau, Michael and Xu, Jennifer},
  year = {2025},
  doi = {10.1145/3713032},
  url = {https://doi.org/10.1145/3713032},
  journal = {ACM Trans. Manage. Inf. Syst.},
  volume = {16},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {artificial intelligence, business and management, information systems research, Large language models, source: ACM},
  abstract = {Large language models have been advancing very rapidly and are making substantial impacts on all areas of business and management. We review the development of large language models and their applications in business and management, and identify the major issues and challenges faced by both practitioners and researchers. Based on our review, we propose an agenda for information systems researchers on large language models and discuss some of the potential directions for future research. Lastly, we present the articles in the special issue as exemplary research on large language models and discuss their implications.},
  issn = {2158-656X},
  month = {feb},
}

@article{zhang_systematic_2025,
  title = {A {Systematic},
  author = {Zhang, Haopeng and Yu, Philip S. and Zhang, Jiawei},
  year = {2025},
  doi = {10.1145/3731445},
  url = {https://doi.org/10.1145/3731445},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {11},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {dataset, deep learning, large language model, Summarization, source: ACM},
  abstract = {Text summarization research has undergone several significant transformations with the advent of deep neural networks, pre-trained language models (PLMs), and recent large language models (LLMs). This survey thus provides a comprehensive review of the research progress and evolution in text summarization through the lens of these paradigm shifts. It is organized into two main parts: (1) a detailed overview of datasets, evaluation metrics, and summarization methods before the LLM era, encompassing traditional statistical methods, deep learning approaches, and PLM fine-tuning techniques, and (2) the first detailed examination of recent advancements in benchmarking, modeling, and evaluating summarization in the LLM era. By synthesizing existing literature and presenting a cohesive overview, this survey also discusses research trends, open challenges, and proposes promising research directions in summarization, aiming to guide researchers through the evolving landscape of summarization research.},
  issn = {0360-0300},
  month = {jun},
}

@article{fu_ai_2025,
  title = {{AI},
  author = {Fu, Michael and Pasuksmit, Jirat and Tantithamthavorn, Chakkrit},
  year = {2025},
  doi = {10.1145/3712190},
  url = {https://doi.org/10.1145/3712190},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {34},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI Security, Artificial Intelligence, Deep Learning, DevOps, DevSecOps, Machine Learning, Supply Chain Security, Vulnerability, source: ACM},
  abstract = {DevOps has emerged as one of the most rapidly evolving software development paradigms. With the growing concerns surrounding security in software systems, the DevSecOps paradigm has gained prominence, urging practitioners to incorporate security practices seamlessly into the DevOps workflow. However, integrating security into the DevOps workflow can impact agility and impede delivery speed. Recently, the advancement of artificial intelligence (AI) has revolutionized automation in various software domains, including software security. AI-driven security approaches, particularly those leveraging machine learning or deep learning, hold promise in automating security workflows. They have the potential to reduce manual efforts and can be incorporated into DevOps practices to support consistent delivery speed while aligning with the principles of the DevSecOps paradigm. This article seeks to contribute to the critical intersection of AI and DevSecOps by presenting a comprehensive landscape of AI-driven security techniques applicable to DevOps and identifying avenues for enhancing security, trust, and efficiency in software development processes. We analyzed 99 research papers spanning from 2017 to 2023. Specifically, we address two key research questions (RQs). In RQ1, we identified 12 security tasks associated with the DevSecOps process and reviewed existing AI-driven security approaches, the problems they addressed, and the 65 benchmarks used to evaluate those approaches. Drawing insights from our findings, in RQ2, we discussed state-of-the-art AI-driven security approaches, highlighted 15 challenges in existing research, and proposed 15 corresponding avenues for future opportunities.},
  issn = {1049-331X},
  month = {apr},
}

@article{yu_survey_2022,
  title = {A {Survey},
  author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  year = {2022},
  doi = {10.1145/3512467},
  url = {https://doi.org/10.1145/3512467},
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {11s},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Knowledge-enhanced Methods, Natural language generation, source: ACM},
  abstract = {The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
  issn = {0360-0300},
  month = {nov},
}

@article{barbosa_cost-effective_2025,
  title = {A {Cost},
  author = {Barbosa, Juliana Silva and Gondhali, Ulhas and Petrossian, Gohar and Sharma, Kinshuk and Chakraborty, Sunandan and Jacquet, Jennifer and Freire, Juliana},
  year = {2025},
  doi = {10.1145/3725256},
  url = {https://doi.org/10.1145/3725256},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {large language models, pseudo-labeling, wildlife, source: ACM},
  abstract = {Wildlife trafficking remains a critical global issue, significantly impacting biodiversity, ecological stability, and public health. Despite efforts to combat this illicit trade, the rise of e-commerce platforms has made it easier to sell wildlife products, putting new pressure on wild populations of endangered and threatened species. The use of these platforms also opens a new opportunity: as criminals sell wildlife products online, they leave digital traces of their activity that can provide insights into trafficking activities as well as how they can be disrupted. The challenge lies in finding these traces. Online marketplaces publish ads for a plethora of products, and identifying ads for wildlife-related products is like finding a needle in a haystack. Learning classifiers can automate ad identification, but creating them requires costly, time-consuming data labeling that hinders support for diverse ads and research questions. This paper addresses a critical challenge in the data science pipeline for wildlife trafficking analytics: generating quality labeled data for classifiers that select relevant data. While large language models (LLMs) can directly label advertisements, doing so at scale is prohibitively expensive. We propose a cost-effective strategy that leverages LLMs to generate pseudo labels for a small sample of the data and uses these labels to create specialized classification models. Our novel method automatically gathers diverse and representative samples to be labeled while minimizing the labeling costs. Our experimental evaluation shows that our classifiers achieve up to 95\% F1 score, outperforming LLMs at a lower cost. We present real use cases that demonstrate the effectiveness of our approach in enabling analyses of different aspects of wildlife trafficking.},
  month = {jun},
}

@article{li_structure-aware_2025,
  title = {A {Structure},
  author = {Li, Wei and Li, Yi and Shao, Yanqiu and Bi, Mengxi},
  year = {2025},
  doi = {10.1145/3725733},
  url = {https://doi.org/10.1145/3725733},
  journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  volume = {24},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Annotation alignment, Chinese classics, community detection, source: ACM},
  abstract = {Throughout the long history of China, ideologists have annotated classical texts, providing a valuable resource for scholarly study. Many of these annotations are presented as entire paragraphs, each sentence of which must be linked to the corresponding classical sentences, also in paragraph form. However, there has been a lack of definitions and datasets for this specific task. In this article, we propose a definition for the task of annotation alignment of Chinese classics and present a corresponding dataset. We observe that directly matching an annotation sentence to a classical sentence may encounter challenges due to limitations in classical Chinese semantic models. Furthermore, an annotation sentence may not be directly related to the target classical sentence but may instead be more closely associated with other neighboring annotation sentences. Based on these observations, we propose a semantic relevance graph-based approach that leverages task-specific structural features. Our proposed method achieves a macro precision of 82.17 and a micro precision of 86.55, significantly surpassing all baseline models including large language models, which attests to the effectiveness of our approach.},
  issn = {2375-4699},
  month = {apr},
}

@article{prakash_all_2024,
  title = {All in {One},
  author = {Prakash, Yash and Nayak, Akshay Kolgar and Sunkara, Mohan and Jayarathna, Sampath and Lee, Hae-Na and Ashok, Vikas},
  year = {2024},
  doi = {10.1145/3664639},
  url = {https://doi.org/10.1145/3664639},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {EICS},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Blind, Online shopping, Screen reader, Visual impairment, Web usability, source: ACM},
  abstract = {Perusing web data items such as shopping products is a core online user activity. To prevent information overload, the content associated with data items is typically dispersed across multiple webpage sections over multiple web pages. However, such content distribution manifests an unintended side effect of significantly increasing the interaction burden for blind users, since navigating to-and-fro between different sections in different pages is tedious and cumbersome with their screen readers. While existing works have proposed methods for the context of a single webpage, solutions enabling usable access to content distributed across multiple webpages are few and far between. In this paper, we present InstaFetch, a browser extension that dynamically generates an alternative screen reader-friendly user interface in real-time, which blind users can leverage to almost instantly access different item-related information such as description, full specification, and user reviews, all in one place, without having to tediously navigate to different sections in different webpages. Moreover, InstaFetch also supports natural language queries about any item, a feature blind users can exploit to quickly obtain desired information, thereby avoiding manually trudging through reams of text. In a study with 14 blind users, we observed that the participants needed significantly lesser time to peruse data items with InstaFetch, than with a state-of-the-art solution.},
  month = {jun},
}

@article{ibrahimzada_alphatrans_2025,
  title = {{AlphaTrans},
  author = {Ibrahimzada, Ali Reza and Ke, Kaiyao and Pawagi, Mrigank and Abid, Muhammad Salman and Pan, Rangeet and Sinha, Saurabh and Jabbarvand, Reyhaneh},
  year = {2025},
  doi = {10.1145/3729379},
  url = {https://doi.org/10.1145/3729379},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Neuro-Symbolic Code Translation and Validation, source: ACM},
  abstract = {Code translation transforms programs from one programming language (PL) to another. One prominent use case is application modernization to enhance maintainability and reliability. Several rule-based transpilers have been designed to automate code translation between different pairs of PLs. However, the rules can become obsolete as the PLs evolve and cannot generalize to other PLs. Recent studies have explored the automation of code translation using Large Language Models (LLMs). One key observation is that such techniques may work well for crafted benchmarks but fail to generalize to the scale and complexity of real-world projects with inter- and intra-class dependencies, custom types, PL-specific features, etc. We propose AlphaTrans, a neuro-symbolic approach to automate repository-level code translation. AlphaTrans translates both source and test code, and employs multiple levels of validation to ensure the translation preserves the functionality of the source program. To break down the problem for LLMs, AlphaTrans leverages program analysis to decompose the program into fragments and translates them in the reverse call order. We leveraged AlphaTrans to translate ten real-world open-source projects consisting of ⟨836, 8575, 2719⟩ (application and test) classes, (application and test) methods, and unit tests. AlphaTrans breaks down these projects into 17874 fragments and translates the entire repository. 96.40\% of the translated fragments are syntactically correct, and AlphaTrans validates the translations’ runtime behavior and functional correctness for 27.03\% and 25.14\% of the application method fragments. On average, integrated translation and validation takes 34 hours (min=3, max=121) to translate a project, showing its scalability in practice. For the syntactically or semantically incorrect translations, AlphaTrans generates a report including existing translation, stack trace, test errors, or assertion failures. We provided these artifacts to two developers to fix the translation bugs in four projects. They fixed the issues in 20.1 hours on average (5.5 hours for the smallest and 34 hours for the largest project) and achieved all passing tests. Without AlphaTrans, translating and validating such big projects could take weeks, if not months.},
  month = {jun},
}

@article{srba_survey_2025,
  title = {A {Survey},
  author = {Srba, Ivan and Razuvayevskaya, Olesya and Leite, João A. and Moro, Robert and Schlicht, Ipek Baris and Tonelli, Sara and García, Francisco Moreno and Lottmann, Santiago Barrio and Teyssou, Denis and Porcellini, Valentin and Scarton, Carolina and Bontcheva, Kalina and Bielikova, Maria},
  year = {2025},
  doi = {10.1145/3770077},
  url = {https://doi.org/10.1145/3770077},
  journal = {ACM Trans. Intell. Syst. Technol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Credibility Assessment, Credibility Signals, Large Language Models, Literature Survey, Natural Language Processing, NLP, source: ACM},
  abstract = {In the age of social media and generative AI, the ability to automatically assess the credibility of online content has become increasingly critical, complementing traditional approaches to false information detection. Credibility assessment relies on aggregating diverse credibility signals – small units of information, such as content subjectivity, bias, or a presence of persuasion techniques – into a final credibility label/score. However, current research in automatic credibility assessment and credibility signals detection remains highly fragmented, with many signals studied in isolation and lacking integration. Notably, there is a scarcity of approaches that detect and aggregate multiple credibility signals simultaneously. These challenges are further exacerbated by the absence of a comprehensive and up-to-date overview of research works that connects these research efforts under a common framework and identifies shared trends, challenges, and open problems. In this survey, we address this gap by presenting a systematic and comprehensive literature review of 175 research papers, focusing on textual credibility signals within the field of Natural Language Processing (NLP), which undergoes a rapid transformation due to advancements in Large Language Models (LLMs). While positioning the NLP research into the the broader multidisciplinary landscape, we examine both automatic credibility assessment methods as well as the detection of nine categories of credibility signals. We provide an in-depth analysis of three key categories: 1) factuality, subjectivity and bias, 2) persuasion techniques and logical fallacies, and 3) check-worthy and fact-checked claims. In addition to summarising existing methods, datasets, and tools, we outline future research direction and emerging opportunities, with particular attention to evolving challenges posed by generative AI.},
  annote = {Just Accepted},
  issn = {2157-6904},
  month = {sep},
}

@article{koh_empirical_2022,
  title = {An {Empirical},
  author = {Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui},
  year = {2022},
  doi = {10.1145/3545176},
  url = {https://doi.org/10.1145/3545176},
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {8},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {datasets, Document summarization, language models, neural networks, transformer, source: ACM},
  abstract = {Long documents such as academic articles and business reports have been the standard format to detail out important issues and complicated subjects that require extra attention. An automatic summarization system that can effectively condense long documents into short and concise texts to encapsulate the most important information would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural architectures, significant research efforts have been made to advance automatic text summarization systems, and numerous studies on the challenges of extending these systems to the long document domain have emerged. In this survey, we provide a comprehensive overview of the research on long document summarization and a systematic evaluation across the three principal components of its research setting: benchmark datasets, summarization models, and evaluation metrics. For each component, we organize the literature within the context of long document summarization and conduct an empirical analysis to broaden the perspective on current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in this rapidly growing field.},
  issn = {0360-0300},
  month = {dec},
}

@article{schoeffer_ai_2025,
  title = {{AI},
  author = {Schoeffer, Jakob and Jakubik, Johannes and Vössing, Michael and Kühl, Niklas and Satzger, Gerhard},
  year = {2025},
  doi = {10.1613/jair.1.15873},
  url = {https://doi.org/10.1613/jair.1.15873},
  journal = {J. Artif. Int. Res.},
  volume = {82},
  pages = {471--501},
  note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
  keywords = {source: ACM},
  abstract = {In AI-assisted decision-making, a central promise of having a human-in-the-loop is that they should be able to complement the AI system by overriding its wrong recommendations. In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice. Different ways of relying on AI recommendations have immediate, yet distinct, implications for decision quality. Unfortunately, reliance and decision quality are often inappropriately conflated in the current literature on AI-assisted decision-making. In this work, we disentangle and formalize the relationship between reliance and decision quality, and we characterize the conditions under which human-AI complementarity is achievable. To illustrate how reliance and decision quality relate to one another, we propose a visual framework and demonstrate its usefulness for interpreting empirical findings, including the effects of interventions like explanations. Overall, our research highlights the importance of distinguishing between reliance behavior and decision quality in AI-assisted decision-making.},
  issn = {1076-9757},
  month = {apr},
}

@article{ye_gradual_2023,
  title = {A {Gradual},
  author = {Ye, Wenjia and Toro, Matías and Olmedo, Federico},
  year = {2023},
  doi = {10.1145/3586036},
  url = {https://doi.org/10.1145/3586036},
  journal = {Proc. ACM Program. Lang.},
  volume = {7},
  number = {OOPSLA1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Gradual Typing, Probabilistic Lambda Calculus, Type Systems, source: ACM},
  abstract = {Probabilistic programming languages have recently gained a lot of attention, in particular due to their applications in domains such as machine learning and differential privacy. To establish invariants of interest, many such languages include some form of static checking in the form of type systems. However, adopting such a type discipline can be cumbersome or overly conservative. Gradual typing addresses this problem by supporting a smooth transition between static and dynamic checking, and has been successfully applied for languages with different constructs and type abstractions. Nevertheless, its benefits have never been explored in the context of probabilistic languages. In this work, we present and formalize GPLC, a gradual source probabilistic lambda calculus. GPLC includes a binary probabilistic choice operator and allows programmers to gradually introduce/remove static type–and probability–annotations. The static semantics of GPLC heavily relies on the notion of probabilistic couplings, as required for defining several relations, such as consistency, precision, and consistent transitivity. The dynamic semantics of GPLC is given via elaboration to the target language TPLC, which features a distribution-based semantics interpreting programs as probability distributions over final values. Regarding the language metatheory, we establish that TPLC–and therefore also GPLC–is type safe and satisfies two of the so-called refined criteria for gradual languages, namely, that it is a conservative extension of a fully static variant and that it satisfies the gradual guarantee, behaving smoothly with respect to type precision.},
  month = {apr},
}

@article{metzger_user_2024,
  title = {A {User},
  author = {Metzger, Andreas and Laufer, Jan and Feit, Felix and Pohl, Klaus},
  year = {2024},
  doi = {10.1145/3666005},
  url = {https://doi.org/10.1145/3666005},
  journal = {ACM Trans. Auton. Adapt. Syst.},
  volume = {19},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {adaptive system, debugging, explainability, interpretability, machine learning, reinforcement learning, source: ACM},
  abstract = {Online reinforcement learning (RL) is increasingly used for realizing adaptive systems in the presence of design time uncertainty because Online RL can leverage data only available at run time. With Deep RL gaining interest, the learned knowledge is no longer represented explicitly but hidden in the parameterization of the underlying artificial neural network. For a human, it thus becomes practically impossible to understand the decision-making of Deep RL, which makes it difficult for (1) software engineers to perform debugging, (2) system providers to comply with relevant legal frameworks, and (3) system users to build trust. The explainable RL technique XRL-DINE, introduced in earlier work, provides insights into why certain decisions were made at important time steps. Here, we perform an empirical user study concerning XRL-DINE involving 73 software engineers split into treatment and control groups. The treatment group is given access to XRL-DINE, while the control group is not. We analyze (1) the participants’ performance in answering concrete questions related to the decision-making of Deep RL, (2) the participants’ self-assessed confidence in giving the right answers, (3) the perceived usefulness and ease of use of XRL-DINE, and (4) the concrete usage of the XRL-DINE dashboard.},
  issn = {1556-4665},
  month = {sep},
}

@article{baumer_algorithmic_2024,
  title = {Algorithmic {Subjectivities},
  author = {Baumer, Eric P. S. and Taylor, Alex S. and Brubaker, Jed R. and McGee, Micki},
  year = {2024},
  doi = {10.1145/3660344},
  url = {https://doi.org/10.1145/3660344},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {31},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {algorithms, reflective HCI, Subjectivity, source: ACM},
  abstract = {This article considers how subjectivities are enlivened in algorithmic systems. We first review related literature to clarify how we see “subjectivities” as emerging through a tangled web of processes and actors. We then offer two case studies exemplifying the emergence of algorithmic subjectivities: one involving computational topic modeling of blogs written by parents with children on the autism spectrum and one involving algorithmic moderation of social media content. Drawing on these case studies, we then articulate a series of qualities that characterizes algorithmic subjectivities. We also compare and contrast these qualities with a number of related concepts from prior literature to articulate how algorithmic subjectivities constitute a novel theoretical contribution, as well as how it offers a focal lens for future empirical investigation and for design. In short, this article points out how certain worlds are being made and/or being made possible via algorithmic systems, and it asks Human–Computer Interaction (HCI) to consider what other worlds might be possible.},
  issn = {1073-0516},
  month = {aug},
}

@article{cai_after_2021,
  title = {After {Violation},
  author = {Cai, Jie and Wohn, Donghee Yvette},
  year = {2021},
  doi = {10.1145/3479554},
  url = {https://doi.org/10.1145/3479554},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {5},
  number = {CSCW2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {content moderation, live streaming, profiling, volunteer moderator},
  abstract = {Content moderation is an essential part of online community health and governance. While much of extant research is centered on what happens to the content, moderation also involves the management of violators. This study focuses on how moderators (mods) make decisions about their actions after the violation takes place but before the sanction by examining how they "profile" the violators. Through observations and interviews with volunteer mods on Twitch, we found that mods engage in a complex process of collaborative evidence collection and profile violators into different categories to decide the type and extent of punishment. Mods consider violators' characteristics as well as behavioral history and violation context before taking moderation action. The main purpose of the profiling was to avoid excessive punishment and aim to integrate violators more into the community. We discuss the contributions of profiling to moderation practice and suggest design mechanisms to facilitate mods' profiling processes.},
  month = {oct},
}

@article{meylan_study_2020,
  title = {A {Study},
  author = {Meylan, Alexandre and Cherubini, Mauro and Chapuis, Bertil and Humbert, Mathias and Bilogrevic, Igor and Huguenin, Kévin},
  year = {2020},
  doi = {10.1145/3410154},
  url = {https://doi.org/10.1145/3410154},
  journal = {ACM Trans. Priv. Secur.},
  volume = {24},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Checksums, integrity, security, usability, web downloads, source: ACM},
  abstract = {App stores provide access to millions of different programs that users can download on their computers. Developers can also make their programs available for download on their websites and host the program files either directly on their website or on third-party platforms, such as mirrors. In the latter case, as users download the software without any vetting from the developers, they should take the necessary precautions to ensure that it is authentic. One way to accomplish this is to check that the published file’s integrity verification code—the checksum—matches that (if provided) of the downloaded file. To date, however, there is little evidence to suggest that such a process is effective. Even worse, very few usability studies about it exist.In this article, we provide the first comprehensive study that assesses the usability and effectiveness of the manual checksum verification process. First, by means of an in-situ experiment with 40 participants and eye-tracking technology, we show that the process is cumbersome and error-prone. Second, after a 4-month-long in-the-wild experiment with 134 participants, we demonstrate how our proposed solution—a Chrome extension that verifies checksums automatically—significantly reduces human errors, improves coverage, and has only limited impact on usability. It also confirms that, sadly, only a tiny minority of websites that link to executable files in our sample provide checksums (0.01\%), which is a strong call to action for web standards bodies, service providers, and content creators to increase the use of file integrity verification on their properties.},
  issn = {2471-2566},
  month = {sep},
}

@article{hamed_comparison_2025,
  title = {A {Comparison},
  author = {Hamed, Naeima and Rana, Omer and Orozco-terWengel, Pablo and Goossens, Benoît and Perera, Charith},
  year = {2025},
  doi = {10.1145/3705863},
  url = {https://doi.org/10.1145/3705863},
  journal = {J. Data and Information Quality},
  volume = {17},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Data integration, Data platforms, FAIR Open Data principles, Urban and non-urban data observatories, source: ACM},
  abstract = {Open Data Observatories refer to online platforms that provide real-time and historical data for a particular application context, e.g., urban/non-urban environments or a specific application domain. They are generally developed to facilitate collaboration within one or more communities through reusable datasets, analysis tools, and interactive visualizations. Open Data Observatories collect and integrate various data from multiple disparate data sources—some providing mechanisms to support real-time data capture and ingest. Data types can include sensor data (soil, weather, traffic, pollution levels) and satellite imagery. Data sources can include Open Data providers, interconnected devices, and services offered through the Internet of Things. The continually increasing volume and variety of such data require timely integration, management, and analysis, yet presented in a way that end-users can easily understand. Data released for open access preserve their value and enable a more in-depth understanding of real-world choices. This survey compares 13 Open Data Observatories and their data management approaches—investigating their aims, design, and types of data. We conclude with research challenges that influence the implementation of these observatories, outlining some strengths and limitations for each one and recommending areas for improvement. Our goal is to identify best practices learned from the selected observatories to aid the development of new Open Data Observatories.},
  issn = {1936-1955},
  month = {mar},
}

@article{irrera_novel_2023,
  title = {A {Novel},
  author = {Irrera, Ornella and Mannocci, Andrea and Manghi, Paolo and Silvello, Gianmaria},
  year = {2023},
  doi = {10.1145/3597310},
  url = {https://doi.org/10.1145/3597310},
  journal = {J. Data and Information Quality},
  volume = {15},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {data curation, data enrichment, datasets, open science, Scholarly knowledge graphs, source: ACM},
  abstract = {In the last decade, scholarly graphs became fundamental to storing and managing scholarly knowledge in a structured and machine-readable way. Methods and tools for discovery and impact assessment of science rely on such graphs and their quality to serve scientists, policymakers, and publishers. Since research data became very important in scholarly communication, scholarly graphs started including dataset metadata and their relationships to publications. Such graphs are the foundations for Open Science investigations, data-article publishing workflows, discovery, and assessment indicators. However, due to the heterogeneity of practices (FAIRness is indeed in the making), they often lack the complete and reliable metadata necessary to perform accurate data analysis; e.g., dataset metadata is inaccurate, author names are not uniform, and the semantics of the relationships is unknown, ambiguous or incomplete.This work describes an open and curated scholarly graph we built and published as a training and test set for data discovery, data connection, author disambiguation, and link prediction tasks. Overall the graph contains 4,047 publications, 5,488 datasets, 22 software, 21,561 authors; 9,692 edges interconnect publications to datasets and software and are labeled with semantics that outline whether a publication is citing, referencing, documenting, supplementing another product.To ensure high-quality metadata and semantics, we relied on the information extracted from PDFs of the publications and the datasets and software webpages to curate and enrich nodes metadata and edges semantics. To the best of our knowledge, this is the first ever published resource, including publications and datasets with manually validated and curated metadata.},
  issn = {1936-1955},
  month = {aug},
}

@inproceedings{xu_generative_2024,
  title = {Generative {AI},
  author = {Xu, Anbang and Yu, Tan and Du, Min and Gundecha, Pritam and Guo, Yufan and Zhu, Xinliang and Wang, May and Li, Ping and Chen, Xinyun},
  year = {2024},
  doi = {10.1145/3627673.3680117},
  url = {https://doi.org/10.1145/3627673.3680117},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5599--5602},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {enterprise application, generation, rag, retrieval},
  abstract = {This workshop introduces generative AI applications for enterprise, with a focus on retrieval-augmented generation (RAG) systems. Generative AI is a field of artificial intelligence that can create new content and solve complex problems. RAG systems are a novel generative AI technique that combines information retrieval with text generation to generate rich and diverse responses. RAG systems can leverage enterprise data, which is often specific, structured, and dynamic, to provide customized solutions for various domains. However, enterprise data also poses challenges such as scalability, security, and data quality. This workshop convenes researchers and practitioners to explore RAG and other generative AI systems in real-world enterprise scenarios, fostering knowledge exchange, collaboration, and identification of future directions. Relevant to the CIKM community, the workshop intersects with core areas of data science and machine learning, offering potential benefits across various domains.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{feng_response_2025,
  title = {Response {Quality},
  author = {Feng, Naihe and Sui, Yi and Hou, Shiyi and Cresswell, Jesse C. and Wu, Ga},
  year = {2025},
  doi = {10.1145/3726302.3730244},
  url = {https://doi.org/10.1145/3726302.3730244},
  booktitle = {Proceedings of the 48th {International},
  pages = {2832--2836},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {conformal prediction, retrieval augmented generation, source: Scopus},
  abstract = {Existing research on Retrieval-Augmented Generation (RAG) primarily focuses on improving overall question-answering accuracy, often overlooking the quality of sub-claims within generated responses. Recent methods that attempt to improve RAG trustworthiness, such as through auto-evaluation metrics, lack probabilistic guarantees or require ground truth answers. To address these limitations, we propose Conformal-RAG, a novel framework inspired by recent applications of conformal prediction (CP) on large language models (LLMs). Conformal-RAG leverages CP and internal information from the RAG mechanism to offer statistical guarantees on response quality. It ensures group-conditional coverage spanning multiple sub-domains without requiring manual labelling of conformal sets, making it suitable for complex RAG applications. Compared to existing RAG auto-evaluation methods, Conformal-RAG offers statistical guarantees on the quality of refined sub-claims, ensuring response reliability without the need for ground truth answers. Additionally, our experiments demonstrate that by leveraging information from the RAG system, Conformal-RAG retains up to 60\% more high-quality sub-claims from the response compared to direct applications of CP to LLMs, while maintaining the same reliability guarantee.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{prahlad_personalizing_2025,
  title = {Personalizing {Large},
  author = {Prahlad, D. and Lee, C. and Kim, D. and Kim, H.},
  year = {2025},
  doi = {10.1145/3701716.3715473},
  url = {https://doi.org/10.1145/3701716.3715473},
  booktitle = {Companion {Proceedings},
  journal = {… Proceedings of the ACM on Web …},
  pages = {1259--1263},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, knowledge graph, large language model, personalization, retrieval augmented generation},
  abstract = {… timely, factual, and personalized information fed to the LLM. … (RAG) using knowledge graphs (KGs) to assist the LLM in … of storing continuously updated factual information in a …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@article{salemi_evaluating_2024,
  title = {Evaluating retrieval quality in retrieval-augmented generation},
  author = {Salemi, A. and Zamani, H.},
  year = {2024},
  doi = {10.1145/3626772.3657957},
  url = {https://doi.org/10.1145/3626772.3657957},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th International ACM SIGIR …},
  pages = {2395--2400},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, evaluation, retrieval quality, retrieval-augmented generation, source: ACM},
  abstract = {… and the downstream performance of RAG. In this paper, … RAG systems, where we apply the LLM in RAG system on each document in the retrieval result list individually and use the LLM’…},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{jin_flashrag_2025,
  title = {{FlashRAG},
  author = {Jin, Jiajie and Zhu, Yutao and Dou, Zhicheng and Dong, Guanting and Yang, Xinyu and Zhang, Chenghao and Zhao, Tong and Yang, Zhao and Wen, Ji-Rong},
  year = {2025},
  doi = {10.1145/3701716.3715313},
  url = {https://doi.org/10.1145/3701716.3715313},
  booktitle = {Companion {Proceedings},
  pages = {737--740},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {efficient, rag, research, retrieval augmented generation, toolkit},
  abstract = {With the advent of large language models (LLMs) and multimodal large language models (MLLMs), the potential of retrieval-augmented generation (RAG) has attracted considerable research attention. However, the absence of a standardized framework for implementation, coupled with the inherently complex RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. In response to this challenge, we develop FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing and comparing existing RAG methods and developing their own algorithms within a unified framework. Our toolkit has implemented 16 advanced RAG methods and gathered and organized 38 benchmark datasets. It has various features, including a customizable modular framework, a rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{xu_rallrec_2025,
  title = {{RALLRec},
  author = {Xu, Jian and Luo, Sichun and Chen, Xiangyu and Huang, Haoming and Hou, Hanxu and Song, Linqi},
  year = {2025},
  doi = {10.1145/3701716.3715508},
  url = {https://doi.org/10.1145/3701716.3715508},
  booktitle = {Companion {Proceedings},
  pages = {1436--1440},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {large language model, recommender system, retrieval-augmented generation},
  abstract = {Large Language Models (LLMs) have been integrated into recommendation systems to enhance user behavior comprehension. The Retrieval Augmented Generation (RAG) technique is further incorporated into these systems to retrieve more relevant items and improve system performance. However, existing RAG methods rely primarily on textual semantics and often fail to incorporate the most relevant items, limiting the effectiveness of the systems.In this paper, we propose Representation learning for retrieval-Augmented Large Language model Recommendation (RALLRec). Specifically, we enhance textual semantics by prompting LLMs to generate more detailed item descriptions, followed by joint representation learning of textual and collaborative semantics, which are extracted by the LLM and recommendation models, respectively. Considering the potential time-varying characteristics of user interest, a simple yet effective reranking method is further introduced to capture the dynamics of user preference. We conducted extensive experiments on three real-world datasets, and the evaluation results validated the effectiveness of our method. Code is made public at https://github.com/JianXu95/RALLRec.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{wang_r3ag_2024,
  title = {{R3AG},
  author = {Wang, Zihan and Ge, Xuri and Jose, Joemon M. and Yu, Haitao and Ma, Weizhi and Ren, Zhaochun and Xin, Xin},
  year = {2024},
  doi = {10.1145/3673791.3698435},
  url = {https://doi.org/10.1145/3673791.3698435},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {307--310},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {information retrieval, large lan- guage models, reliability, retrieval-augmented generation, source: ACM},
  abstract = {Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks.To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@article{sudhi_rag-ex_2024,
  title = {Rag-ex: {A},
  author = {Sudhi, V. and Bhat, S. R. and Rudat, M. and Teucher, R.},
  year = {2024},
  doi = {10.1145/3626772.3657660},
  url = {https://doi.org/10.1145/3626772.3657660},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {2776--2780},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, explainability, large language models, retrieval augmented generation},
  abstract = {… trust and confidence of end users in LLM-based applications, including Retrieval Augmented Generation (RAG… hint at the accuracy of LLM generation in the RAG system in most cases. …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{lin_raccoon_2025,
  title = {{RACCOON},
  author = {Lin, Jonathan and Joshi, Aditya and Paik, Hye-young and Doung, Tri Dung and Gurdasani, Deepti},
  year = {2025},
  doi = {10.1145/3701716.3715501},
  url = {https://doi.org/10.1145/3701716.3715501},
  booktitle = {Companion {Proceedings},
  pages = {1123--1127},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {geocoding, large language models, location extraction, news articles, rag, retrieval-augmented generation},
  abstract = {Geocoding involves automatic extraction of location coordinates of incidents reported in news articles, and can be used for epidemic intelligence or disaster management. This paper introduces Retrieval-Augmented Coordinate Capture Of Online News articles (RACCOON), an open-source geocoding approach that extracts geolocations from news articles. RACCOON uses a retrieval-augmented generation (RAG) approach where candidate locations and associated information are retrieved in the form of context from a location database, and a prompt containing the retrieved context, location mentions and news articles is fed to an LLM to generate the location coordinates. Our evaluation on three datasets, two underlying LLMs, three baselines and several ablation tests based on the components of RACCOON demonstrate the utility of RACCOON. To the best of our knowledge, RACCOON is the first RAG-based approach for geocoding using pre-trained LLMs.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@article{xu_retrieval-augmented_2024,
  title = {Retrieval-augmented generation with knowledge graphs for customer service question answering},
  author = {Xu, Z. and Cruz, M. J. and Guevara, M. and Wang, T. and {...},
  year = {2024},
  doi = {10.1145/3626772.3661370},
  url = {https://doi.org/10.1145/3626772.3661370},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {2905--2909},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, knowledge graph, large language model, question answering, retrieval-augmented generation, source: ACM},
  abstract = {… copies bear this notice and the full citation on the first page. … LLM-based customer service question answering system that seamlessly integrates retrieval-augmented generation (RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{kaiser_preference-based_2025,
  title = {Preference-based {Learning},
  author = {Kaiser, Magdalena and Weikum, Gerhard},
  year = {2025},
  doi = {10.1145/3701716.3715544},
  url = {https://doi.org/10.1145/3701716.3715544},
  booktitle = {Companion {Proceedings},
  pages = {1053--1057},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {conversational question answering, preference-based learning, retrieval augmented generation},
  abstract = {Conversational Question Answering (ConvQA) involves multiple subtasks, i) to understand incomplete questions in their context, ii) to retrieve relevant information, and iii) to generate answers. This work presents PRAISE, a pipeline-based approach for ConvQA that trains LLM adapters for each of the three subtasks. As labeled training data for individual subtasks is unavailable in practice, PRAISE learns from its own generations using the final answering performance as feedback signal without human intervention and treats intermediate information, like relevant evidence, as weakly labeled data. We apply Direct Preference Optimization by contrasting successful and unsuccessful samples for each subtask. In our experiments, we show the effectiveness of this training paradigm: PRAISE shows improvements per subtask and achieves new state-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5 percentage points increase in precision over baselines.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{xu_rlg-rag_2025,
  title = {{RLG},
  author = {Xu, Kehan and Zhang, Kun and Huang, Wei and Li, Jingyuan and Wang, Yuanzhuo},
  year = {2025},
  doi = {10.1145/3701716.3715554},
  url = {https://doi.org/10.1145/3701716.3715554},
  booktitle = {Companion {Proceedings},
  pages = {1450--1454},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge evaluation, knowledge retrieval, reasoning graph, retrieval-augmented generation},
  abstract = {The knowledge retrieval, integration, and evaluation processes in the RAG method lack the guidance of reasoning logic, leading to ongoing challenges in maintaining factual consistency. To address these issues, this paper proposes the RLG-RAG framework, which constructs a reasoning graph based on user queries to guide the knowledge retrieval, integration, and evaluation processes. By fully representing the reasoning logic of RAG, RLG-RAG dynamically models and integrates knowledge relationships during retrieval and defines a precise scope of relevant knowledge through sufficiency evaluation. This reduces inference-irrelevant knowledge that large language models may obtain. Experimental analyses on accuracy, factual consistency, and robustness demonstrate that RLG-RAG resists interference and provides accurate, factually consistent answers. The project URL is https://doi.org/10.5281/zenodo.14852250.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{yang_cascadercg_2025,
  title = {{CascadeRCG},
  author = {Yang, Di and Zhu, Jingwei and Wu, Haihong and Tan, Minghuan and Li, Chengming and Yang, Min},
  year = {2025},
  doi = {10.1145/3701716.3715466},
  url = {https://doi.org/10.1145/3701716.3715466},
  booktitle = {Companion {Proceedings},
  pages = {1465--1469},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {LLM, NLP, online mental health support, RAG},
  abstract = {Online mental health support(OMHS) plays a crucial role in promoting well-being, but the shortage of mental health professionals necessitates automated systems to address complex care needs. While large language models (LLMs) are widely adopted, they often fall short in OMHS settings due to the complexity and ambiguity of the questions posed. Additionally, providing accurate answers requires extensive knowledge, which LLMs may lack, leading to responses that often lack depth, professionalism, and critical detail. To address these limitations, we introduce a new task tailored to OMHS scenarios, focusing on enhancing the professionalism and knowledgeability of generated responses. Furthermore, we propose a comprehensive benchmark designed to systematically evaluate the quality of responses. Building on these foundations, we propose the CascadeRCG framework, an optimized approach based on Retrieval-Augmented Generation (RAG). This framework first employs a knowledge management strategy, then introduces a two-stage cross-iterative Retrieval mechanism and a Clustering-then-summarizing module, followed by the final Generation stage. Experimental results on both single-turn and multi-turn psychological dialogue datasets, compared to other RAG-based baselines across different LLMs, show significant improvements in response professionalism and knowledge depth. This enhancement in response quality provides an effective methodology and strategy for further improving OMHS systems. Our code is available at https://github.com/CAS-SIAT-XinHai/CascadeRCG.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhou_navigating_2025,
  title = {Navigating {Generative},
  author = {Zhou, Yujia and Ji, Wei and Ge, Xuri and Ai, Qingyao and Jose, Joemon M. and Liu, Yiqun and Do, Hyo Jin and Feldman, Molly Q and He, Jessica and Hwang, Angel Hsing-Chi and Kim, Seyun},
  year = {2025},
  doi = {10.1145/3707640.3729212},
  url = {https://doi.org/10.1145/3707640.3729212},
  booktitle = {Adjunct {Proceedings},
  pages = {4176--4179},
  publisher = {Association for Computing Machinery},
  keywords = {Accountability, Attribution, Authorship, Co-creation, Disclosure, Generative AI, Ownership, Responsibility},
  abstract = {The increasing integration of generative AI into work has amplified issues of disclosure, ownership, and accountability, including whether and how to acknowledge AI use, who owns AI-generated or co-created work, and who is accountable for risks. In response, governments, organizations, and researchers are introducing new policies, guidelines, and methods for enhanced transparency. However, the complex interplay between multiple stakeholders and technologies, coupled with growing AI agency, continues to spark debates about ownership and accountability of co-created work, leading to open questions about whether, when, and how to disclose and attribute human-AI co-created work. To address these emergent issues, this workshop aims to gather interdisciplinary researchers, practitioners, and experts to discuss key questions from law, technology, design, and HCI research standpoints, with the ultimate goal of promoting responsible generative AI use for work.},
  address = {New York, NY, USA},
  series = {{CHIWORK},
  isbn = {979-8-4007-1397-2},
}

@inproceedings{siro_llm4eval_2025,
  title = {{LLM4Eval},
  author = {Siro, Clemencia and Rahmani, Hossein A. and Aliannejadi, Mohammad and Craswell, Nick and Clarke, Charles L. A. and Faggioli, Guglielmo and Mitra, Bhaskar and Thomas, Paul and Yilmaz, Emine},
  year = {2025},
  doi = {10.1145/3726302.3730367},
  url = {https://doi.org/10.1145/3726302.3730367},
  booktitle = {Proceedings of the 48th {International},
  pages = {4188--4191},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {automated evaluation, generative models, large language models},
  abstract = {Large language models (LLMs) have demonstrated increasing task-solving abilities not present in smaller models. Utilizing the capabilities and responsibilities of LLMs for automated evaluation (LLM4Eval) has recently attracted considerable attention in multiple research communities. Building on the success of previous workshops, which established foundations in automated judgments and RAG evaluation, this third iteration aims to address emerging challenges as IR systems become increasingly personalized and interactive. The main goal of the third LLM4Eval workshop is to bring together researchers from industry and academia to explore three critical areas: the evaluation of personalized IR systems while maintaining fairness, the boundaries between automated and human assessment in subjective scenarios, and evaluation methodologies for systems that combine multiple IR paradigms (search, recommendations, and dialogue). By examining these challenges, we seek to understand how evaluation approaches can evolve to match the sophistication of modern IR applications. The format of the workshop is interactive, including roundtable discussion sessions, fostering dialogue about the future of IR evaluation while avoiding one-sided discussions. This is the third iteration of the workshop series, following successful events at SIGIR 2024 and WSDM 2025, with the first iteration attracting over 50 participants.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{huang_paracoder_2025,
  title = {{ParaCoder},
  author = {Huang, Xiaowen and Zhang, Xu and Tao, Lvfang and Mao, Renjie and Zhou, Nan and Zhu, Wenxi and Deng, Minwen and Meng, Jintao and Wei, Yanjie and Zhou, Amelie Chi and Wang, Bingqiang and Feng, Shengzhong},
  year = {2025},
  doi = {10.1145/3711708.3723442},
  url = {https://doi.org/10.1145/3711708.3723442},
  booktitle = {Proceedings of the 1st {FastCode},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  note = {event-place: The Westin Las Vegas Hotel \&amp; Spa, Las Vegas, NV, USA},
  keywords = {code generation, LLM, parallelization},
  abstract = {High-performance parallel code generation is a complex and fascinating area in computer science that focuses on producing code that executes as quickly and efficiently as possible. In our paper, we designed a new architecture for parallel code generation agent with 4 inter-connected components of LLM—Memory, Planning, Tools and Action. It also incooperated with two techniques: data augmentation, prompting and retrieval-augmented editing to improve the performance of the parallel codes. Data augmentation is implemented by extracting and processing PIE dataset, and also synthesis dataset generated by LLM models with ParEval benchmark. Finally planning-oriented prompting, code verification and retrieval augmented editing are used to promote the actual performance of the LLM generated code. The evaluation results confirm that a rough speedup of 6.06X and 5.13X are achieved using Qwen2.5-Coder-7B-Instruct, Qwen2.5-Coder-14B-Instruct LLM models.},
  address = {New York, NY, USA},
  series = {{FCPC},
  isbn = {979-8-4007-1446-7},
}

@inproceedings{rahmani_llm4eval_2024,
  title = {{LLM4Eval},
  author = {Rahmani, Hossein A. and Siro, Clemencia and Aliannejadi, Mohammad and Craswell, Nick and Clarke, Charles L. A. and Faggioli, Guglielmo and Mitra, Bhaskar and Thomas, Paul and Yilmaz, Emine},
  year = {2024},
  doi = {10.1145/3626772.3657992},
  url = {https://doi.org/10.1145/3626772.3657992},
  booktitle = {Proceedings of the 47th {International},
  pages = {3040--3043},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {automated evaluation, generative models, large language models},
  abstract = {Large language models (LLMs) have demonstrated increasing task-solving abilities not present in smaller models. Utilizing the capabilities and responsibilities of LLMs for automated evaluation (LLM4Eval) has recently attracted considerable attention in multiple research communities. For instance, LLM4Eval models have been studied in the context of automated judgments, natural language generation, and retrieval augmented generation systems. We believe that the information retrieval community can significantly contribute to this growing research area by designing, implementing, analyzing, and evaluating various aspects of LLMs with applications to LLM4Eval tasks. The main goal of LLM4Eval workshop is to bring together researchers from industry and academia to discuss various aspects of LLMs for evaluation in information retrieval, including automated judgments, retrieval-augmented generation pipeline evaluation, altering human evaluation, robustness, and trustworthiness of LLMs for evaluation in addition to their impact on real-world applications. We also plan to run an automated judgment challenge prior to the workshop, where participants will be asked to generate labels for a given dataset while maximising correlation with human judgments. The format of the workshop is interactive, including roundtable and keynote sessions and tends to avoid the one-sided dialogue of a mini-conference.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{vizgirda_socialgenpod_2024,
  title = {{SocialGenPod},
  author = {Vizgirda, Vidminas and Zhao, Rui and Goel, Naman},
  year = {2024},
  doi = {10.1145/3589335.3651251},
  url = {https://doi.org/10.1145/3589335.3651251},
  booktitle = {Companion {Proceedings},
  pages = {1067--1070},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {decentralised web, privacy, retrieval augmented generation, solid, source: ACM},
  abstract = {We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid - a decentralised Web specification - to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0172-6},
}

@inproceedings{zhou_img2loc_2024,
  title = {{Img2Loc},
  author = {Zhou, Zhongliang and Zhang, Jielu and Guan, Zihan and Hu, Mengxuan and Lao, Ni and Mu, Lan and Li, Sheng and Mai, Gengchen},
  year = {2024},
  doi = {10.1145/3626772.3657673},
  url = {https://doi.org/10.1145/3626772.3657673},
  booktitle = {Proceedings of the 47th {International},
  pages = {2749--2754},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {image localization, large multi-modality models, vector database, source: ACM},
  abstract = {Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval. Traditional methods typically employ either classification-dividing the Earth's surface into grid cells and classifying images accordingly, or retrieval-identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models (LMMs) like GPT-4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training. A video demonstration of the system can be accessed via this link https://drive.google.com/file/d/16A6A-mc7AyUoKHRH3\_WBRToRC13sn7tU/view?usp=sharing},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{frobe_large_2025,
  title = {Large {Language},
  author = {Fröbe, Maik and Parry, Andrew and Schlatt, Ferdinand and MacAvaney, Sean and Stein, Benno and Potthast, Martin and Hagen, Matthias},
  year = {2025},
  doi = {10.1145/3726302.3730218},
  url = {https://doi.org/10.1145/3726302.3730218},
  booktitle = {Proceedings of the 48th {International},
  pages = {2858--2863},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {inter-annotator agreement, ir evaluation, reproducibility},
  abstract = {Relevance judgments can differ between assessors, but previous work has shown that such disagreements have little impact on the effectiveness rankings of retrieval systems. This applies to disagreements between humans as well as between human and large language model (LLM) assessors. However, the agreement between different LLM assessors has not yet been systematically investigated. To close this gap, we compare eight LLM assessors on the TREC DL tracks and the retrieval task of the RAG track with each other and with human assessors. We find that the agreement between LLM assessors is higher than between LLMs and humans and, importantly, that LLM assessors favor retrieval systems that use LLMs in their ranking decisions: our analyses with 30-50 retrieval systems show that the system rankings obtained by LLM assessors overestimate LLM-based re-rankers by 9 to 17 positions on average.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhu_finir_2025,
  title = {{FinIR},
  author = {Zhu, Fengbin and Ma, Yunshan and Feng, Fuli and Wang, Chao and Luan, Huanbo and Ye, Guangnan and Zhang, Shuo and Mehta, Dhagash and Chen, Pingping and Xiang, Bing and Chua, Tat-Seng},
  year = {2025},
  doi = {10.1145/3726302.3730366},
  url = {https://doi.org/10.1145/3726302.3730366},
  booktitle = {Proceedings of the 48th {International},
  pages = {4184--4187},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {bing xiang, chao wang, dhagash mehta, fengbin zhu, fuli feng, guangnan ye, huanbo luan, pingping chen, shuo zhang, tat-seng chua, yunshan ma},
  abstract = {Recent advancements in Generative AI, such as Large Language Models (LLMs), have demonstrated remarkable success across various general tasks. Extensive studies have explored leveraging generative models in finance, but significant challenges persist. This half-day workshop explores potential approaches and research directions to address these challenges by equipping generative models with advanced Information Retrieval (IR) models. Specifically, this workshop seeks to provide a platform for discussing innovative ideas that facilitate the advancement of IR technology to enrich generative models in finance from four key perspectives: (i) financial IR techniques (ii) financial IR benchmarking and evaluation (iii) financial systems and agents/assistants (iv) and trustworthiness, privacy and security when applying financial IR and generative models. This workshop aims to deepen understanding, accelerate progress, and support the advancement of IR technology to enhance generative models to address financial challenges.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{lajewska_ginger_2025,
  title = {{GINGER},
  author = {Łajewska, Weronika and Balog, Krisztian},
  year = {2025},
  doi = {10.1145/3726302.3730166},
  url = {https://doi.org/10.1145/3726302.3730166},
  booktitle = {Proceedings of the 48th {International},
  pages = {2723--2727},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {grounding, retrieval-augmented generation, source attribution, source: ACM},
  abstract = {Retrieval-augmented generation (RAG) faces challenges related to factual correctness, source attribution, and response completeness. To address them, we propose a modular pipeline for grounded response generation that operates on information nuggets - minimal, atomic units of relevant information extracted from retrieved documents. The multistage pipeline encompasses nugget detection, clustering, ranking, top cluster summarization, and fluency enhancement. It guarantees grounding in specific facts, facilitates source attribution, and ensures maximum information inclusion within length constraints. Experiments on the TREC RAG'24 dataset, using the AutoNuggetizer framework, demonstrate that GINGER achieves state-of-the-art performance on this benchmark.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{rogers_what_2024,
  title = {What {Should},
  author = {Rogers, Kantwon and Webber, Reiden John Allen and Gorostiaga Zubizarreta, Geronimo and Melo Cruz, Arthur and Chen, Shengkang and Arkin, Ronald C. and Borenstein, Jason and Wagner, Alan R.},
  year = {2024},
  doi = {10.1145/3610978.3640752},
  url = {https://doi.org/10.1145/3610978.3640752},
  booktitle = {Companion of the 2024 {ACM},
  pages = {906--910},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boulder, CO, USA},
  keywords = {deception, ethical dilemmas, human-robot interaction, LLM},
  abstract = {This study compares human ethical judgments with Large Language Models (LLMs) on robotic deception in various scenarios. Surveying human participants and querying LLMs, we presented ethical dilemmas in high-risk and low-risk contexts. Findings reveal alignment between humans and LLMs in high-risk scenarios, prioritizing safety, but notable divergences in low-risk situations, reflecting challenges in AI development to accurately capture human social nuances and moral expectations.},
  address = {New York, NY, USA},
  series = {{HRI},
  isbn = {979-8-4007-0323-2},
}

@inproceedings{thakur_assessing_2025,
  title = {Assessing {Support},
  author = {Thakur, Nandan and Pradeep, Ronak and Upadhyay, Shivani and Campos, Daniel and Craswell, Nick and Soboroff, Ian and Dang, Hoa Trang and Lin, Jimmy},
  year = {2025},
  doi = {10.1145/3726302.3730165},
  url = {https://doi.org/10.1145/3726302.3730165},
  booktitle = {Proceedings of the 48th {International},
  pages = {2759--2763},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {llm judge, retrieval-augmented generation, support evaluation, source: Scopus},
  abstract = {Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing ”ground truth”. A crucial factor in RAG evaluation is ”support”, or whether the information in the cited documents supports the answer. We conducted a comparative study of submissions to the TREC 2024 RAG Track, evaluating an automatic LLM judge (GPT-4o) against human judges for support assessment. We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions. Our results indicate good agreement between human and GPT-4o predictions. Further analysis of the disagreements shows that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment. We provide a qualitative analysis of human and GPT-4o errors to help guide future evaluations.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{belikova_data-efficient_2025,
  title = {Data-efficient {Meta},
  author = {Belikova, Julia and Polev, Konstantin and Parchiev, Rauf and Simakov, Dmitry},
  year = {2025},
  doi = {10.1145/3726302.3731969},
  url = {https://doi.org/10.1145/3726302.3731969},
  booktitle = {Proceedings of the 48th {International},
  pages = {4385--4389},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {data efficiency, hallucination detection, model probing, question-answering, retrieval-augmented generation, source: Scopus},
  abstract = {Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly deployed in industry applications, yet their reliability remains hampered by challenges in detecting hallucinations. While supervised state-of-the-art (SOTA) methods that leverage LLM hidden states-such as activation tracing and representation analysis-show promise, their dependence on extensively annotated datasets limits scalability in real-world applications. This paper addresses the critical bottleneck of data annotation by investigating the feasibility of reducing training data requirements for two SOTA hallucination detection frameworks: Lookback Lens, which analyzes attention head dynamics, and probing-based approaches, which decode internal model representations. We propose a methodology combining efficient classification algorithms with dimensionality reduction techniques to minimize sample size demands while maintaining competitive performance. Evaluations on standardized question-answering RAG benchmarks show that our approach achieves performance comparable to strong proprietary LLM-based baselines with only 250 training samples. These results highlight the potential of lightweight, data-efficient paradigms for industrial deployment, particularly in annotation-constrained scenarios.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{arabzadeh_ir-rag_2025,
  title = {{IR},
  author = {Arabzadeh, Negar and Chen, Ziheng and Petroni, Fabio and Siciliano, Federico and Silvestri, Fabrizio and Trappolini, Giovanni},
  year = {2025},
  doi = {10.1145/3726302.3730362},
  url = {https://doi.org/10.1145/3726302.3730362},
  booktitle = {Proceedings of the 48th {International},
  pages = {4168--4171},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {generative models, neural databases, retrieval augmented generation, source: ACM},
  abstract = {In recent years, Retrieval-Augmented Generation (RAG) systems have become a cornerstone of artificial intelligence, attracting considerable attention in a variety of fields. By integrating the strengths of information retrieval and generative models, these systems have shown immense potential to push the boundaries of machine learning applications. Nevertheless, RAG systems still face significant challenges and offer ample room for advancement and innovation.This workshop aims to highlight the central role of information retrieval within RAG frameworks, which we believe has often been overshadowed by the emphasis on the generative components. While the generative models are integral to these systems, the quality and effectiveness of the retrieval mechanism is equally critical, as it has a direct impact on the overall system performance and outcomes. We invite papers that rethink and prioritise the fundamental aspects of RAG systems, particularly in strengthening the information retrieval component. Through this workshop, we aim to gain deeper insights into how improved retrieval methods can enhance the performance and reliability of RAG systems. The event will bring together leading experts, researchers and practitioners to provide a collaborative platform for exchanging ideas, sharing results and fostering innovation. Our aim is to stimulate research and discussion that reaffirms the essential role of information retrieval in shaping the next generation of generative systems.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1592-1},
  series = {{SIGIR},
}

@inproceedings{azimi_towards_2025,
  title = {Towards an {Energy},
  author = {Azimi, Zoha and Farahani, Reza and Timmerer, Christian and Prodan, Radu},
  year = {2025},
  doi = {10.1145/3715675.3715835},
  url = {https://doi.org/10.1145/3715675.3715835},
  booktitle = {Proceedings of the 4th {Mile},
  pages = {89--90},
  publisher = {Association for Computing Machinery},
  note = {event-place: Denver, CO, USA},
  keywords = {Energy Efficiency, FFmpeg, LLM, Retrieval-Augmented Generation, Video Processing},
  abstract = {Large language models (LLMs), the backbone of generative artificial intelligence (AI) like ChatGPT, have become more widely integrated in different fields, including multimedia. The rising number of conversational queries on such platforms now emits as much CO2 as everyday activities, leading to an exponential growth of energy consumption and underscoring urgent sustainability challenges. This short paper introduces an energy-aware LLM-based video processing tool. Employing open-source LLM models and techniques like fine-tuning and Retrieval-Augmented Generation (RAG), this tool recommends video processing commands and executes them in an energy-aware manner. Preliminary results show that it achieves reduced energy consumption per prompt compared to baselines.},
  address = {New York, NY, USA},
  series = {{MHV},
  isbn = {979-8-4007-1488-7},
}

@inproceedings{avula_measuring_2025,
  title = {Measuring the {Fairness},
  author = {Avula, Sandeep and Lee, Chia-Jung and Zhang, Rongting and Murdock, Vanessa},
  year = {2025},
  doi = {10.1145/3726302.3730230},
  url = {https://doi.org/10.1145/3726302.3730230},
  booktitle = {Proceedings of the 48th {International},
  pages = {2994--2998},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {fairness, information retrieval, rag, retrieval augmented generation},
  abstract = {In this paper, we investigate the problem of quantifying fairness in Retrieval-Augmented Generation (RAG) systems, particularly for complex cognitive tasks that go beyond factual question-answering. While RAG systems have demonstrated effectiveness in information extraction tasks, their fairness implications for cognitively complex tasks - including ideation, content creation, and analytical reasoning - remain under-explored. We propose a novel evaluation framework that extends IR fairness metrics by incorporating centrality-based measures to account for influence of retrieved documents on generated output beyond ranking. Our framework evaluates RAG systems across various cognitive dimensions using two ranking approaches: lexical (BM25) and dense (BGE), and language models of varying sizes. Our findings provide insights into: (1) the propagation of fairness disparities from retrieval to generation phases, and (2) the variation in system performance across different cognitive dimensions.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{li_neutronrag_2025,
  title = {{NeutronRAG},
  author = {Li, Peizheng and Chen, Chaoyi and Yuan, Hao and Fu, Zhenbo and Shen, Hang and Yang, Xinbo and Wang, Qiange and Ai, Xin and Zhang, Yanfeng and Wen, Yingyou and Yu, Ge},
  year = {2025},
  doi = {10.1145/3722212.3725119},
  url = {https://doi.org/10.1145/3722212.3725119},
  booktitle = {Companion of the 2025 {International},
  pages = {163--166},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {GraphRAG, HybridRAG, retrieval-augmented generation, VectorRAG},
  abstract = {Retrieval-Augmented Generation (RAG) has been widely used to enhance the generation quality of large language models (LLMs), particularly in domain-specific tasks. As application requirements become increasingly diverse, various RAG methods have been proposed to optimize the retrieval and generation quality of different task scenarios, such as VectorRAG, GraphRAG, and HybridRAG. However, RAG faces two key challenges in these retrieval methods: evaluating different retrieval methods and optimizing parameter configurations. First, different retrieval methods show different performances. How to uniformly compare and analyze the advantages and disadvantages of these retrieval methods remains an open research problem. Second, the effectiveness of RAG is highly sensitive to key parameter configurations. Optimizing these parameters is challenging due to the complexity of the parameter space and the difficulty in identifying the sources of errors in the generated responses. Existing RAG tools typically use a single retrieval method, lacking analytical capabilities and multi-strategy support. To address these challenges, we introduce NeutronRAG, a demonstration of understanding the effectiveness of RAG from a data retrieval perspective. NeutronRAG supports hybrid retrieval strategies and helps researchers iteratively refine RAG configuration to improve retrieval and generation quality through systematic analysis, visual feedback, and parameter adjustment advice. It facilitates data-driven decisions to enhance LLM generation quality while exploring effective retrieval strategies in RAG systems. The web app is available at: https://github.com/iDC-NEU/NeutronRAG.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{lee_rag-enhanced_2025,
  title = {{RAG},
  author = {Lee, Hsiu-Hung and Chen, Chung-Chi and Yen, An-Zi},
  year = {2025},
  doi = {10.1145/3701716.3715520},
  url = {https://doi.org/10.1145/3701716.3715520},
  booktitle = {Companion {Proceedings},
  journal = {arXiv preprint arXiv …},
  pages = {1096--1099},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {evidence recommendation, financial advisory, retrieval augmented generation, source: Google Scholar},
  abstract = {The complexity of legal documents, particularly in financial legal disputes, poses significant challenges for both experts and automated systems. This study introduces a system leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to recommend key evidence in financial advisor dispute cases. Unlike traditional legal AI tasks focused on outcome prediction, our approach emphasizes supporting judicial reasoning by recommending key evidence relevant to adjudication. We constructed a dataset of 371 annotated cases from Taiwan, spanning 25 years, including claims, judicial opinions, and final judgments, with annotations highlighting key evidence. Using RAG, our system retrieves and generates evidence recommendations grounded in analogous past cases while maintaining temporal and contextual consistency. This methodology enhances judicial efficiency and supports equitable legal decision-making by streamlining the recommendation of critical evidence.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{jokinen_exploring_2024,
  title = {Exploring a {Japanese},
  author = {Jokinen, Kristiina and Wilcock, Graham},
  year = {2024},
  doi = {10.1145/3610978.3640622},
  url = {https://doi.org/10.1145/3610978.3640622},
  booktitle = {Companion of the 2024 {ACM},
  pages = {578--582},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boulder, CO, USA},
  keywords = {cypher query language, generative AI, graph databases, Japanese cooking, knowledge graphs, large language models, retrieval augmented generation, semantic search, social robots},
  abstract = {The paper describes ongoing work applying Generative AI to a real world application. We use Retrieval Augmented Generation and other GenAI tools that combine large language models with Neo4j knowledge graphs. These tools help a robot to chat in English about Japanese cooking using a knowledge base that is in Japanese.},
  address = {New York, NY, USA},
  series = {{HRI},
  isbn = {979-8-4007-0323-2},
}

@inproceedings{petroni_ir-rag_2024,
  title = {{IR},
  author = {Petroni, Fabio and Siciliano, Federico and Silvestri, Fabrizio and Trappolini, Giovanni},
  year = {2024},
  doi = {10.1145/3626772.3657984},
  url = {https://doi.org/10.1145/3626772.3657984},
  booktitle = {Proceedings of the 47th {International},
  pages = {3036--3039},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {source: ACM},
  abstract = {In recent years, Retrieval Augmented Generation (RAG) systems have emerged as a pivotal component in the field of artificial intelligence, gaining significant attention and importance across various domains. These systems, which combine the strengths of information retrieval and generative models, have shown promise in enhancing the capabilities and performance of machine learning applications. However, despite their growing prominence, RAG systems are not without their limitations and continue to be in need of exploration and improvement. This workshop seeks to focus on the critical aspect of information retrieval and its integral role within RAG frameworks. We argue that current efforts have undervalued the role of Information Retrieval (IR) in the RAG and have concentrated their attention on the generative part. As the cornerstone of these systems, IR's effectiveness dramatically influences the overall performance and outcomes of RAG models. We call for papers that will seek to revisit and emphasize the fundamental principles underpinning RAG systems. At the end of the workshop, we aim to have a clearer understanding of how robust information retrieval mechanisms can significantly enhance the capabilities of RAG systems. The workshop will serve as a platform for experts, researchers, and practitioners. We intend to foster discussions, share insights, and encourage research that underscores the vital role of Information Retrieval in the future of generative systems.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0431-4},
  series = {{SIGIR},
}

@inproceedings{alrabie_towards_2025,
  title = {Towards {Human},
  author = {Alrabie, Lina and Andolina, Salvatore},
  year = {2025},
  doi = {10.1145/3750069.3750103},
  url = {https://doi.org/10.1145/3750069.3750103},
  booktitle = {Proceedings of the 16th {Biannual},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial intelligence, HCAI, Human-centered AI, Large Language Models, LLM, Retrieval Augmented Generation, Software testing},
  abstract = {This study examines the integration of a Retrieval-Augmented Generation (RAG)–based AI testing assistant within an Italian public administration, focusing on its practical benefits and challenges. Qualitative feedback from 15 software testers reveals that while the assistant improves testing efficiency and supports knowledge retrieval, challenges regarding accuracy, consistency, and transparency reduce user trust and effectiveness. Participants highlighted the need for enhanced data curation, improved interaction, and finer-grained control over the system, indicating areas for future improvements to the assistant and its surrounding workflows. The findings emphasize the importance of adopting a human-centered approach in the design and integration of AI solutions in the public sector, offering valuable insights for the ongoing development of human-centered RAG research.},
  address = {New York, NY, USA},
  series = {{CHItaly},
  isbn = {979-8-4007-2102-1},
}

@inproceedings{yang_explaingen_2025,
  title = {{ExplainGen},
  author = {Yang, Zhicheng and Jia, Xinle and Jiang, Xiaopeng},
  year = {2025},
  doi = {10.1145/3722570.3726897},
  url = {https://doi.org/10.1145/3722570.3726897},
  booktitle = {Proceedings of the 3rd {International},
  pages = {120--123},
  publisher = {Association for Computing Machinery},
  note = {event-place: Irvine, CA, USA},
  keywords = {Human-centered Computing, Large Language Model, Misinformation, source: Scopus},
  abstract = {While LLMs show promise in identifying misinformation, they often struggle with context-dependent cases and may even reinforce falsehoods due to biases in their training data. Instead of making final decisions on misinformation, we propose leveraging LLMs as human-centered assistants to generate context-based explanations that support human judgment in combating misinformation. This paper introduces ExplainGen, an LLM-based web app designed to provide fact-grounded explanations for assessing the credibility of statements. Due to limited transparency in LLM training data and the scarcity of high-quality fact-checking datasets, we scrape data from various domains and combine them with publicly available fact-checking instructions to fine-tune ExplainGen. Our evaluation shows that ExplainGen generates well-supported explanations that outperform the baseline models. As future work, we plan to conduct survey-based experiments to evaluate the effectiveness of ExplainGen for human decision-making, and incorporate retrieval-augmented generation (RAG) to reduce hallucinations of ExplainGen LLM. Our source code and datasets are available on our GitHub page: https://github.com/Accuracy98/ExplainGen.},
  address = {New York, NY, USA},
  series = {{HumanSys},
  isbn = {979-8-4007-1609-6},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{chu_chatasd_2024,
  title = {{ChatASD},
  author = {Chu, Lei and Wu, Hongyan and Pan, Yi},
  year = {2024},
  doi = {10.1145/3698587.3701538},
  url = {https://doi.org/10.1145/3698587.3701538},
  booktitle = {Proceedings of the 15th {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Shenzhen, China},
  keywords = {Autism, Knowledge Graph, LLM, Question-and-Answer System, Retrieval-Augmented Generation},
  abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by developmental delays, communication difficulties, repetitive behaviors, and restricted interests. Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, particularly in providing personalized question-and-answer(Q\&amp;A) services, making them well-suited for constructing dialogue engines for autism Q\&amp;A systems. However, general LLMs often lack integrated autism knowledge during training, limiting their professional competency in autism consultation. Additionally, the automatic evaluation of scientific accuracy in autism medical knowledge Q\&amp;A remains underexplored. To address this gap, we propose ChatASD, an autism knowledge Q\&amp;A framework based on Graph Retrieval-Augmented Generation (GraphRAG) technology. This framework leverages LLMs and retrieves relevant information from medical literature to generate an autism knowledge graph, employing a combination of global and community queries to produce reliable responses. Compared to traditional methods, ChatASD effectively addresses the sparse distribution of autism knowledge, providing more accurate and comprehensive answersAutomatic efficacy evaluations and competitive experiments on system responses indicate our approach significantly improves reliability of autism-related professional knowledge queries.},
  address = {New York, NY, USA},
  series = {{BCB},
  isbn = {979-8-4007-1302-6},
}

@inproceedings{bendel_animal_2024,
  title = {The {Animal},
  author = {Bendel, Oliver and Zbinden, Nick},
  year = {2024},
  doi = {10.1145/3702336.3702347},
  url = {https://doi.org/10.1145/3702336.3702347},
  booktitle = {Proceedings of the {International},
  publisher = {Association for Computing Machinery},
  keywords = {Animal Body Language, Animal-Computer Interaction, Artificial Intelligence, Generative AI, GPT, Multimodal Large Language Model, source: ACM},
  abstract = {Generative AI has become widespread since 2022. Technical advancements have resulted in multimodal large language models and other AI models that generate, analyze, and evaluate texts, images, and sounds. Such capabilities can be helpful in encounters between humans and animals. For example, apps with generative AI on a smartphone can be used to assess the body language and behavior of animals – e.g., during a walk or hike – and provide a recommendation for human behavior. It is often useful to take into account the animal's environment and situation. The apps can help people to avert approaches and attacks, and thus also protect animals. In “The Animal Whisperer Project”, three apps were developed as prototypes based on the multimodal large language model GPT-4 from OpenAI from the beginning to mid-2024. Three specific GPTs resulted: the Cow Whisperer, the Horse Whisperer, and the Dog Whisperer. All three showed impressive capabilities after the first prompt engineering. These were improved by implementing information from expert interviews and adding labeled images of animals and other materials. AI-based apps for interpreting body language, behavior, and the overall situation can apparently be created today, without much effort, in a low-budget project. However, turning them into products would certainly raise questions, such as liability in the event of accidents.},
  address = {New York, NY, USA},
  series = {{ACI},
  isbn = {979-8-4007-1175-6},
}

@inproceedings{sokol_ventana_2025,
  title = {Ventana a la {Verdad},
  author = {Sokol, Anna and Sisk, Matthew L. and Alvarez, Josefina Echavarría and Chawla, Nitesh},
  year = {2025},
  doi = {10.1145/3701551.3704123},
  url = {https://doi.org/10.1145/3701551.3704123},
  booktitle = {Proceedings of the {Eighteenth},
  pages = {1052--1055},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hannover, Germany},
  keywords = {conversational ai, generative ai, large language models, retrieval-augmented generation, source: ACM, source: Scopus},
  abstract = {We present Ventana a la Verdad, a chatbot designed to make the Clar- ification Archive and the reports of the Colombian Truth Commis- sion [6] more accessible to a wider audience. These archives contain a wealth of documents, interviews, and testimonies from Colom- bia's armed conflict, but navigating them can be challenging due to their volume and complexity. Using existing large language models (LLMs) and natural language processing techniques, our chatbot allows users to interact with the archives through natural language queries, receiving relevant and contextually appropriate responses. In the sensitive context of peace and reconciliation, where misin- formation or hallucinations can have significant adverse effects, ensuring the accuracy and reliability of information is paramount. This tool aims to facilitate better understanding and engagement with historical content, supporting educational and research efforts. We discuss the development of the chatbot, the challenges encoun- tered, and its potential impact on making the Colombian Truth Commission's archives more accessible. The chatbot is available by link here: http://ventanaverdad.lucyapps.net:1337/},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1329-3},
  series = {{WSDM},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{lu_taxonomy_2024,
  title = {A {Taxonomy},
  author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Liu, Yue and Xing, Zhenchang and Whittle, Jon},
  year = {2024},
  doi = {10.1145/3644815.3644956},
  url = {https://doi.org/10.1145/3644815.3644956},
  booktitle = {Proceedings of the {IEEE},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {foundation model, generative AI, large language model, LLM, responsible AI, software architecture, taxonomy},
  abstract = {Large language model (LLM) based chatbots, such as ChatGPT, have attracted huge interest in foundation models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. However, the architecture design of foundation model based systems has not yet been systematically explored. There is limited understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and system design options. Our taxonomy comprises three categories: the pretraining and adaptation of foundation models, the architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy can serve as concrete guidance for designing foundation model based systems.},
  address = {New York, NY, USA},
  series = {{CAIN},
  isbn = {979-8-4007-0591-5},
}

@inproceedings{chen_behaviorally_2025,
  title = {Behaviorally {Aligned},
  author = {Chen, Qiurui and Niforatos, Evangelos and Kortuem, Gerd},
  year = {2025},
  doi = {10.1145/3743049.3748567},
  url = {https://doi.org/10.1145/3743049.3748567},
  booktitle = {Proceedings of the {Mensch},
  pages = {494--502},
  publisher = {Association for Computing Machinery},
  keywords = {Chatbot, Education, Human-Computer Interaction, Industrial Design, Retrieval-Augmented Generation},
  abstract = {Retrieval-Augmented Generation (RAG) chatbots show promise in educational settings, yet their application in industrial design, with its iterative and reflective workflows, remains underexplored. This study investigates how master’s students in industrial design perceive the effectiveness of a RAG chatbot in supporting their graduation projects. We developed a chatbot prototype trained on 132 industrial design theses (2021–2023), employing semantic search, multimodal capabilities, and stage-specific guidance, and evaluated it through a mixed-methods approach involving a quantitative question-ranking task (n=7) and a qualitative focus group (n=4). Findings indicate strong performance for practical, early-stage queries but highlight issues with irrelevant corpus results, verbose outputs, and underused features, with five key themes emerging: corpus relevance, output reliability, interaction clarity, multimodal support, and experience-oriented learning. These results inform design guidelines for behaviorally aligned RAG chatbots, enhancing support for critical thinking and process navigation in industrial design education.},
  address = {New York, NY, USA},
  series = {{MuC},
  isbn = {979-8-4007-1582-2},
}

@inproceedings{liu_treatrag_2025,
  title = {{TreatRAG},
  author = {Liu, Chao-Chin and Yao, Hao-Ren and Chang, Der-Chen and Frieder, Ophir},
  year = {2025},
  doi = {10.1145/3705328.3748022},
  url = {https://doi.org/10.1145/3705328.3748022},
  booktitle = {Proceedings of the {Nineteenth},
  pages = {690--695},
  publisher = {Association for Computing Machinery},
  keywords = {Large Language Models, Medical Digital Twins, Medication Recommendation, Retrieval-Augmented Generation},
  abstract = {Medication recommendation is a critical function of clinical decision support systems, directly influencing patient safety and treatment efficacy. While large language models (LLMs) show promise in clinical tasks such as summarization and question answering, their ability to make accurate treatment predictions remains limited, in part, due to their lack of specialized medical knowledge and exposure to real-world patient data. We introduce TreatRAG, an interpretable, model-agnostic retrieval-augmented generation (RAG) framework aimed at early-stage development to enhance medication recommendation accuracy using publicly available clinical data; thus, TreatRAG forms a critical foundational step toward future clinical validation and domain expert involvement. TreatRAG retrieves similar patient cases, i.e., so called "digital twins", using interpretable N-gram Jaccard similarity and augments the input prompt to ground LLM predictions in real clinical scenarios. We evaluate our framework on the MIMIC-IV dataset using BioGPT, BioMistral, Phi3, and Flan-T5. TreatRAG-enhanced BioGPT improves its F1-score from 0.14 to 0.34, BioMistral from 0.22 to 0.54, Phi-3 from 0.09 to 0.16, and Flan-T5 from 0.23 to 0.30, while also lowering, often significantly, the hallucination rate. Our model-agnostic framework offers a flexible, effective, and interpretable solution to advance the reliability of LLMs in clinical decision support.},
  address = {New York, NY, USA},
  series = {{RecSys},
  isbn = {979-8-4007-1364-4},
}

@inproceedings{zachariah_multi-model_2025,
  title = {Multi-{Model},
  author = {Zachariah, Arun George and Praveen, Varun and Ochoa, Samuel and Sriram, Parthasarathy},
  year = {2025},
  doi = {10.1145/3701716.3715188},
  url = {https://doi.org/10.1145/3701716.3715188},
  booktitle = {Companion {Proceedings},
  pages = {2943--2946},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {ai systems, retrieval-augmented generation, visual question answering},
  abstract = {This paper introduces a novel task-agnostic system for visual question answering, designed to address the limitations of existing visual-language models in scenarios requiring fine-grained reasoning and contextual understanding. By integrating computer vision, natural language processing, and retrieval-augmented generation, the system dynamically selects models and interprets diverse visual inputs without necessitating task-specific retraining. It leverages scene graph extraction and a model multiplexer to process visual content contextually, while an instruction execution engine transforms multimodal data into actionable insights. The system emphasizes explainability through annotated outputs and textual responses, enhancing user trust and comprehension. Its modular design ensures scalability and adaptability, enabling seamless incorporation of new models and datasets for evolving applications. This work represents a significant step forward in visual question answering, offering a transparent and versatile solution tailored to real-world complexities. The complete source code can be found at https://github.com/NVIDIA/Multi-Model-Workflows.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{portaz_harmonizing_2024,
  title = {Harmonizing {Ethical},
  author = {Portaz, Miguel and Manjarres, Angeles and Santos, Olga C.},
  year = {2024},
  doi = {10.1145/3631700.3664900},
  url = {https://doi.org/10.1145/3631700.3664900},
  booktitle = {Adjunct {Proceedings},
  pages = {380--385},
  publisher = {Association for Computing Machinery},
  note = {event-place: Cagliari, Italy},
  keywords = {Collaborative Learning, Ethics, Human-Centered, Hybrid Intelligence, Intelligent Psychomotor Systems, Retrieval Augmented Generation, XAI},
  abstract = {As the demand for personalized and adaptive learning experiences increase, there is a urgent need for providing effective feedback mechanisms within critical systems, such as in psychomotor learning systems. This proposal introduces an approach for the integration of retrieval-augmented generation tools to provide comprehensive and insightful feedback to users. By combining the strengths of retrieval-based techniques and generative models, these tools offer the potential to enhance learning outcomes by delivering tailored feedback that is both informative and engaging. The proposal also emphasises the importance of incorporating explainability and transparency concepts. Following the hybrid intelligence paradigm it is possible to ensure that the feedback provided by these tools is not only accurate but also understandable to humans. This approach fosters trust and promotes a deeper understanding of the psychomotor learning process, empowering users and facilitators to make informed decisions about the psychomotor learning path. The hybrid intelligence paradigm, which combines the strengths of both human and artificial intelligence, plays a crucial role in the deployment of these solutions. By taking advantage of the cognitive capabilities of human experts alongside the computational power of artificial intelligence algorithms, it is possible to offer personalised feedback that takes into account both technical accuracy and pedagogical effectiveness. Through these collaborative efforts it is also possible to create learning environments that are inclusive, adaptable, and beneficial to lifelong learning. In conclusion, this proposal introduces retrieval-augmented generation tools for providing feedback in psychomotor learning systems, which represents a significant step towards in its personalization, and whose ethical implications align with the new regulations on the implementation of intelligent technologies in critical systems.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-0466-6},
}

@inproceedings{esposito_leveraging_2024,
  title = {Leveraging {Large},
  author = {Esposito, Matteo and Palagiano, Francesco},
  year = {2024},
  doi = {10.1145/3661167.3661226},
  url = {https://doi.org/10.1145/3661167.3661226},
  booktitle = {Proceedings of the 28th {International},
  pages = {442--445},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salerno, Italy},
  keywords = {Analysis, Fine-Tuning, Generative AI, Human Experts, Large Language Model, LLM, Management, Preliminary, Risk, Security, Standards},
  abstract = {Preliminary security risk analysis (PSRA) provides a quick approach to identify, evaluate, and propose remediation to potential risks in specific scenarios. The extensive expertise required for an effective PSRA and the substantial textual-related tasks hinders quick assessments in mission-critical contexts, where timely and prompt actions are essential. The speed and accuracy of human experts in PSRA significantly impact response time. A large language model can quickly summarise information in less time than a human. To our knowledge, no prior study has explored the capabilities of fine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of FTM in assisting practitioners in PSRA. We manually curated 141 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the proficiency of the FTM versus seven human experts. Within the industrial context, our approach has proven successful in reducing errors in PSRA, hastening security risk detection, and minimizing false positives and negatives. This translates to cost savings for the company by averting unnecessary expenses associated with implementing unwarranted countermeasures. Therefore, experts can focus on more comprehensive risk analysis, leveraging LLMs for an effective preliminary assessment within a condensed timeframe.},
  address = {New York, NY, USA},
  series = {{EASE},
  isbn = {979-8-4007-1701-7},
}

@inproceedings{lee_llm-driven_2024,
  title = {An {LLM},
  author = {Lee, Junhee and Kang, SungJoo and Ko, In-Young},
  year = {2024},
  doi = {10.1145/3704440.3704778},
  url = {https://doi.org/10.1145/3704440.3704778},
  booktitle = {Proceedings of the 25th {International},
  pages = {9--10},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, Hong Kong},
  keywords = {Dynamic Generation, IaC, LLM, source: Scopus},
  abstract = {This paper proposes a Large Language Model (LLM)-driven framework for generation of Infrastructure as Code (IaC) in dynamic environments. While IaC simplifies infrastructure management, static templates are often inadequate. Leveraging recent LLM advancements, we design and integrate requirements refinement, retrieval-augmented generation (RAG), and code ensemble methods to improve IaC code accuracy. Preliminary results show that, despite challenges(syntax errors and hallucinations), manual adjustments enable executable code, suggesting potential for dynamic generation.},
  address = {New York, NY, USA},
  series = {Middleware '24},
  isbn = {979-8-4007-1354-5},
  annote = {Cited by: 2},
}

@inproceedings{spitzer_looking_2024,
  title = {Looking {Through},
  author = {Spitzer, Philipp and Celis, Sebastian and Martin, Dominik and Kühl, Niklas and Satzger, Gerhard},
  year = {2024},
  doi = {10.1145/3670653.3677488},
  url = {https://doi.org/10.1145/3670653.3677488},
  booktitle = {Proceedings of {Mensch},
  pages = {566--570},
  publisher = {Association for Computing Machinery},
  note = {event-place: Karlsruhe, Germany},
  keywords = {Artificial Intelligence, Explainable AI, Human-Computer Interaction, Large Language Models},
  abstract = {As AI becomes more powerful, it also becomes more complex. Traditionally, eXplainable AI (XAI) is used to make these models more transparent and interpretable to decision-makers. However, research shows that decision-makers can lack the ability to properly interpret XAI techniques. Large language models (LLMs) offer a solution to this challenge by providing natural language text in combination with XAI techniques to provide more understandable explanations. However, previous work has only explored this approach for inherently interpretable models–an understanding of how LLMs can assist decision-makers when using deep learning models is lacking. To fill this gap, we investigate how different augmentation strategies of LLMs assist decision-makers in interacting with deep learning models. We evaluate the satisfaction and preferences of decision-makers through a user study. Overall, our results provide first insights into how LLMs support decision-makers in interacting with deep learning models and open future avenues to continue this endeavor.},
  address = {New York, NY, USA},
  series = {{MuC},
  isbn = {979-8-4007-0998-2},
}

@inproceedings{ye_boosting_2024,
  title = {Boosting {Conversational},
  author = {Ye, Linhao and Lei, Zhikai and Yin, Jianghao and Chen, Qin and Zhou, Jie and He, Liang},
  year = {2024},
  doi = {10.1145/3626772.3657980},
  url = {https://doi.org/10.1145/3626772.3657980},
  booktitle = {Proceedings of the 47th {International},
  pages = {2301--2305},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {conversational question answering, large language models, retrieval-augmented generation},
  abstract = {Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models(LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level RAG (ConvRAG) approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA). In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-the-art baselines. Moreover, we also release a Chinese CQA dataset with new features including reformulated question, extracted keyword, retrieved paragraphs and their helpfulness, which facilitates further researches in RAG enhanced CQA.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{recio_abad_automatic_2025,
  title = {Automatic {Proof},
  author = {Recio Abad, Juan Carlos and Saborido, Rubén and Chicano, Francisco},
  year = {2025},
  doi = {10.1145/3696630.3728705},
  url = {https://doi.org/10.1145/3696630.3728705},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1679--1682},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {formal methods, isabelle/HOL, large language models, software verification},
  abstract = {Software formal verification is an important and challenging tool to ensure software correctness. It requires tools to formulate and verify mathematical theorems. One example of such a tool is Isabelle/HOL, a proof assistant that supports higher-order logic (HOL), enabling rigorous mathematical reasoning that could be useful for formal verification in software and hardware systems. However, the use of these tools requires a human expert on theorem proving in many cases. With the advent of LLMs (Large Language Models) a research line emerged to use them for proof inference. This paper investigates the effectiveness of math LLMs, reasoner LLMs, and hybrid math-reasoner LLMs in Isabelle/HOL proof inference, evaluating their capabilities across three configurations: base models without domain-specific training, fine-tuned variants adapted to the formal proof domain, and systems based on Retrieval Augmented Generation (RAG). The significance of this comparison extends beyond theoretical interest, potentially informing the development of hybrid systems that leverage the complementary strengths of all approaches.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{krueger_towards_2025,
  title = {Towards a {LLM},
  author = {Krueger, Evan and Carpenter, Taylor and Leach, Kevin and Weimer, James},
  year = {2025},
  doi = {10.1145/3696630.3731673},
  url = {https://doi.org/10.1145/3696630.3731673},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1538--1539},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {LLM-based design, software requirements},
  abstract = {This position paper motivates the development of a large language model (LLM) -based system for creating comprehensive, testable product requirements documents (PRDs). The intention is to guide innovators who lack formal product management experience— clinicians, researchers, academics, and other specialists—in producing specifications detailed enough for successful contracting with engineering or manufacturing partners. The LLM would be augmented with a retrieval-augmented generation (RAG) component and set up in a conversational manner, mitigating issues that arise when under-specified requirements are handed to contract firms.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{chen_llm-powered_2025,
  title = {{LLM},
  author = {Chen, Hao and Du, Lun and Chen, Xu and Ma, Xiaojun and Zhang, Jiang},
  year = {2025},
  doi = {10.1145/3701716.3715450},
  url = {https://doi.org/10.1145/3701716.3715450},
  booktitle = {Companion {Proceedings},
  pages = {903--906},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {graph analytics, heterogeneous information network, large language model},
  abstract = {Most existing Knowledge Base Question Answering methods focus primarily on retrieving factual information, leaving more complex, analysis-driven tasks relatively unexplored. However, real-world queries often involve graph-based computations such as degree calculation or community detection, which require more advanced reasoning. In this paper, we introduce LLM4GraphAna, a Large Language Model-based approach designed to handle these challenging, analysis-focused queries within the KBQA framework. By integrating Function Orchestration and Parameterization, LLM4GraphAna can invoke our well-defined functions to perform graph analytics. Experimental results demonstrate that our method significantly improves performance on analysis-intensive questions.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{soman_observations_2024,
  title = {Observations on {LLMs},
  author = {Soman, Sumit and H. G., Ranjani},
  year = {2024},
  doi = {10.1145/3639856.3639892},
  url = {https://doi.org/10.1145/3639856.3639892},
  booktitle = {Proceedings of the {Third},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bangalore, India},
  keywords = {Bard, Chatbot, ChatGPT, Enterprise Wireless., Generative AI, GPT3.5, GPT4, Large Language Models, LLaMA, Telecom},
  abstract = {The landscape for building conversational interfaces (chatbots) has witnessed a paradigm shift with recent developments in generative Artificial Intelligence (AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and GPT4), Google’s Bard, Large Language Model Meta AI (LLaMA), among others. In this paper, we analyze capabilities and limitations of incorporating such models in conversational interfaces for the telecommunication domain, specifically for enterprise wireless products and services. Using Cradlepoint’s publicly available data for our experiments, we present a comparative analysis of the responses from such models for multiple use-cases including domain adaptation for terminology and product taxonomy, context continuity, robustness to input perturbations and errors. We believe this evaluation would provide useful insights to data scientists engaged in building customized conversational interfaces for domain-specific requirements.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1649-2},
}

@inproceedings{zhu_multimodal_2024,
  title = {Multimodal {Representation},
  author = {Zhu, Xinliang and Dhua, Arnab and Gray, Douglas and Yalniz, I. Zeki and Yu, Tan and Elhoseiny, Mohamed and Plummer, Bryan},
  year = {2024},
  doi = {10.1145/3626772.3657987},
  url = {https://doi.org/10.1145/3626772.3657987},
  booktitle = {Proceedings of the 47th {International},
  pages = {3047--3050},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {large language model, multimodal large language model, multimodal representation, multimodal retrieval, vision language modeling},
  abstract = {Multimodal data is available in many applications like e-commerce production listings, social media posts and short videos. However, existing algorithms dealing with those types of data still focus on uni-modal representation learning by vision-language alignment and cross-modal retrieval. In this workshop, we target to bring a new retrieval problem where both queries and documents are multimodal. With the popularity of vision language modeling, large language models (LLMs), retrieval augmented generation (RAG), and multimodal LLM, we see a lot of new opportunities for multimodal representation and retrieval tasks. This event will be a comprehensive half-day workshop focusing on the subject of multimodal representation and retrieval. The agenda includes keynote speeches, oral presentations, and an interactive panel discussion.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@article{salminen_using_2024,
  title = {Using {Cipherbot},
  author = {Salminen, J. and Jung, S. and Medina, J. and Aldous, K. and {...},
  year = {2024},
  doi = {10.1145/3657604.3664690},
  url = {https://doi.org/10.1145/3657604.3664690},
  booktitle = {Proceedings of the {Eleventh},
  journal = {Proceedings of the …},
  pages = {279--283},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, cipherbot, generative AI, LLMs, student interaction, source: ACM},
  abstract = {… hallucination [8] whereupon the LLM “invents” factually incorrect answers. The particular approach Cipherbot applies is retrieval-augmented generation (RAG… for the LLM to generate its …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {L@{S},
  isbn = {979-8-4007-0633-2},
}

@inproceedings{fung_automatic_2024,
  title = {Automatic {Feedback},
  author = {Fung, Sze Ching Evelyn and Wong, Man Fai and Tan, Chee Wei},
  year = {2024},
  doi = {10.1145/3657604.3664673},
  url = {https://doi.org/10.1145/3657604.3664673},
  booktitle = {Proceedings of the {Eleventh},
  pages = {255--258},
  publisher = {Association for Computing Machinery},
  note = {event-place: Atlanta, GA, USA},
  keywords = {source: ACM},
  abstract = {Since data science is traditionally an advanced field taught at the college or university level, introducing its concepts to K-12 students can present unique learning challenges. As educational environments increasingly adopt data science curricula for K-12 students, the need for scalable, personalized teaching tools becomes critical. While the integration of large language models (LLMs) in educational environments offers significant potential for scalability and automation, it is important to note that the generated language output may not always be highly suitable for K-12 students. In this paper, we introduce the DSRAG, a novel educational automatic feedback generation framework that leverages Retrieval-Augmented Generation (RAG) and cloud-based LLMs to provide automated and personalized feedback for K-12 students engaged in data science education. DSRAG employs Langchain question-answering and RAG systems to manage educational content and generate feedback on the top of GPT-4. We also demonstrate the framework's capability to simplify complex concepts and align its responses to be pedagogically appropriate and understandable for K-12 students.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0633-2},
  series = {L@{S},
}

@inproceedings{emerson_codedocs_2025,
  title = {{CodeDocs},
  author = {Emerson, Amanda and Meehan, Tim and Rogers, Matt and Cowen, William and Darabos, Christian},
  year = {2025},
  doi = {10.1145/3708035.3736102},
  url = {https://doi.org/10.1145/3708035.3736102},
  booktitle = {Practice and {Experience},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, Collaboration, GitHub, Large Language Models, Research Computing, Retrieval-Augmented Generation, Software Documentation},
  abstract = {Managing and understanding large-scale software repositories remains a challenge in research software development. This paper presents CodeDocs, an AI-powered tool that automates the generation of structured documentation for GitHub repositories using Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). The tool leverages Dartmouth’s Dartmouth Chat interface and the LangChain\_Dartmouth Python package to scan repository structures, extract meaningful code insights and generate a README.md file to improve usability, maintainability, and collaboration by providing structured project documentation that supports shared understanding across teams. We evaluate the tool’s performance against manually curated documentation and discuss its implications for research computing. It should be noted that this approach can be implemented using the standard LangChain Python package and any LLM, making it accessible beyond Dartmouth’s infrastructure.},
  address = {New York, NY, USA},
  series = {{PEARC},
  isbn = {979-8-4007-1398-9},
}

@inproceedings{carmel_liverag_2025,
  title = {The {LiveRAG},
  author = {Carmel, David and Filice, Simone and Horowitz, Guy and Maarek, Yoelle and Somekh, Oren and Tavory, Ran},
  year = {2025},
  doi = {10.1145/3726302.3733591},
  url = {https://doi.org/10.1145/3726302.3733591},
  booktitle = {Proceedings of the 48th {International},
  pages = {4199--4201},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {evaluation, llm, rag, source: Scopus},
  abstract = {The LiveRAG Challenge at SIGIR 2025 provides a competitive platform for advancing Retrieval-Augmented Generation (RAG) technologies. Participants from academia and industry have been invited to build a RAG-based question answering system using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal is to enable fair, focused comparisons on retrieval and prompting strategies. During the Live Challenge Day, the competing teams must provide answers and supportive information to 500 unseen questions within a strict two-hour window. Evaluation is conducted in two stages: automated LLM-as-a-judge scoring mechanism for correctness and faithfulness, followed by a manual review of top ranked submissions. The winners will be announced and prizes awarded during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{sun_d-bot_2025,
  title = {D-{Bot},
  author = {Sun, Zhaoyan and Zhou, Xuanhe and Wu, Jianming and Zhou, Wei and Li, Guoliang},
  year = {2025},
  doi = {10.1145/3722212.3725091},
  url = {https://doi.org/10.1145/3722212.3725091},
  booktitle = {Companion of the 2025 {International},
  pages = {235--238},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {database diagnosis, database system, large language model},
  abstract = {Database administrators (DBAs) play an important role in maintaining the high performance and high availability of database systems. However, database diagnosis by DBAs often takes tedious efforts and time, which is insufficient for the large amount of database instances (e.g., millions of instances on the cloud). Besides, existing diagnosis tools built with rules and small-scale learned models lack the capability for flexible reasoning and report generation, and thus can only serve as ”one small piece of a bigger puzzle”. Recently, Large language models (LLMs) have exhibited superiority in natural language understanding, reasoning and generation, positioning them as a potential comprehensive copilot to DBAs to address these limitations. Thus, we introduce D-Bot, an LLM-powered DBA copilot that can automatically acquire pertinent knowledge from diagnostic documents, interact with users for self-refinement, and generate reasonable and well-founded diagnosis reports (i.e., specifying root causes, solutions, and references). We will show that D-Bot can effectively automate database diagnosis in real scenarios, even for complex anomalies.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{wang_andromeda_2025,
  title = {Andromeda: {Debugging},
  author = {Wang, Pengyi and Chen, Sibei and Fan, Ju and Wu, Bin and Tang, Nan and Tan, Jian},
  year = {2025},
  doi = {10.1145/3722212.3725080},
  url = {https://doi.org/10.1145/3722212.3725080},
  booktitle = {Companion of the 2025 {International},
  pages = {243--246},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {database performance debugging, retrieval-augmented generation},
  abstract = {Debugging performance issues in a database management system (DBMS) is tedious and challenging, even for experienced database administrators (DBAs). Thus, with the rapid advancement of large language models (LLMs), developing an LLM-powered co-pilot to assist or even replace DBAs by automatically diagnosing issues and generating recommendations for resolution presents a promising direction. However, directly prompting LLMs for DBMS performance debugging often yields either generic or irrelevant responses, as LLMs may lack both domain knowledge in performance debugging and a deep understanding of DBMS internals. In this paper, we introduce Andromeda, a retrieval-augmented, LLM-powered system for automatic DBMS performance debugging. Andromeda enables users to pose natural language questions about various performance issues and provides context-aware and actionable recommendations for resolution. To achieve this, Andromeda first retrieves relevant evidence from multiple sources, including historical questions, troubleshooting manuals, DBMS telemetry data, and execution logs. It then leverages effective training strategies to adapt an open-source LLM, empowering it to diagnose and resolve issues based on the retrieved evidence. We have developed a web application for Andromeda and demonstrated its effectiveness in debugging real-world DBMS performance issues.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{zhang_sortinghat_2025,
  title = {{SortingHat},
  author = {Zhang, Yifan and Zhao, Xinkui and Wang, Zuxin and Zhou, Zhengyi and Cheng, Guanjie and Deng, Shuiguang and Yin, Jianwei},
  year = {2025},
  doi = {10.1145/3701716.3715199},
  url = {https://doi.org/10.1145/3701716.3715199},
  booktitle = {Companion {Proceedings},
  pages = {2951--2954},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {digital human, education, large language models, multi agent reinforcement learning, retrieval augmented generation, source: ACM},
  abstract = {Operating Systems (OS) courses are among the most challenging in computer science education due to the complexity of internal structures and the diversity of running environments. Traditional teaching methods often fail to address the diverse backgrounds, learning speeds, and practical needs of students. To tackle these challenges, we present SortingHat, a personalized digital teaching assistant tailored specifically for OS education. SortingHat integrates advanced AI technologies, including a retrieval-augmented generation (RAG) framework and multi-agent reinforcement learning (MARL), to deliver adaptive, scalable, and effective educational support. SortingHat features a 3D digital human interface powered by large language models (LLMs) to provide personalized, empathetic, and context-aware guidance. It generates tailored exercises based on each student's learning history and academic performance, reinforcing weak areas and challenging advanced concepts. Additionally, the system incorporates a robust evaluation pipeline that ensures fair, consistent, and unbiased grading of student submissions while delivering personalized, actionable feedback for improvement. By combining personalized guidance, adaptive content creation, and automated assessment, SortingHat transforms OS education into an engaging, immersive, and scalable experience.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{kopkow_high_2025,
  title = {High {Expectations},
  author = {Kopkow, Alina and Bangerl, Mia Magdalena and Pammer-Schindler, Viktoria},
  year = {2025},
  doi = {10.1145/3743049.3748576},
  url = {https://doi.org/10.1145/3743049.3748576},
  booktitle = {Proceedings of the {Mensch},
  pages = {637--647},
  publisher = {Association for Computing Machinery},
  keywords = {GenAI Literacy, Generative AI, LLMs, SME},
  abstract = {Professionals in small and medium-sized enterprises (SMEs) have high expectations towards GenAI technologies. They are a large and particular target user group and therefore interesting for research on (designing) GenAI technologies for the workplace due to SMEs’ typical characteristics of high agility but low resources for innovation. In this paper, we present a survey study with 37 professionals working in Austrian SMEs on their use of GenAI, their plans and concerns for future use, and their GenAI literacy. Our findings show that GenAI is used frequently, and primarily for simple tasks such as content generation and search, despite concerns about data security and professionals’ limited confidence in usage. The GenAI literacy of the participants varied and was generally low. Good design that addresses the needs of SMEs for productivity in key tasks, ease-of-use, and trustworthiness, while improving GenAI literacy and supporting long-term human learning, will be needed to fulfill expectations and facilitate effective and sustainable usage of GenAI in SMEs.},
  address = {New York, NY, USA},
  series = {{MuC},
  isbn = {979-8-4007-1582-2},
}

@inproceedings{khurana_table_2025,
  title = {Table {Retrieval},
  author = {Khurana, Udayan and Suneja, Sahil and Samulowitz, Horst},
  year = {2025},
  doi = {10.1145/3701716.3715558},
  url = {https://doi.org/10.1145/3701716.3715558},
  booktitle = {Companion {Proceedings},
  pages = {1072--1076},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {generative ai, large language models, semantic similarity, structured data search, table retrieval},
  abstract = {Searching for relevant tables in response to a textual phrase or a question is an important task for large tabular data repositories, such as relational databases, CSV files in data lakes, etc. It is somewhat different from the problem of web document search because the subjects of search are tables rather than documents, while the query remains textual. In this paper, we explore a novel technique for table search on large repositories using natural language queries. It is based on a generative methodology that aims to maximize the semantic connection between the query and the resulting tables. Unlike traditional keyword search approaches, our technique can find the needed tables more effectively through deeper semantic concept discovery rather than simply searching for exact keyword matches. Additionally, our technique supports natural language queries rather than plain keyword queries. In this paper, we describe the core ideas, implementation, and effectiveness of our method using two different benchmarks with diverse queries.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{schmidt_detecting_2024,
  title = {Detecting {Generated},
  author = {Schmidt, Sebastian and Zelch, Ines and Bevendorff, Janek and Stein, Benno and Hagen, Matthias and Potthast, Martin},
  year = {2024},
  doi = {10.1145/3589335.3651489},
  url = {https://doi.org/10.1145/3589335.3651489},
  booktitle = {Companion {Proceedings},
  pages = {722--725},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {advertising, llm, retrieval-augmented generation},
  abstract = {Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate responses to queries. It is only a small step to also let the same technology insert ads within the generated responses - instead of separately placing ads next to a response. Inserted ads would be reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. Considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models, users of conversational search engines may very well be confronted with generated native ads in the near future. In this paper, we thus take a first step to investigate whether LLMs can also be used as a countermeasure, i.e., to block generated native ads. We compile the Webis Generated Native Ads 2024 dataset of queries and generated responses with automatically inserted ads, and evaluate whether LLMs or fine-tuned sentence transformers can detect the ads. In our experiments, the investigated LLMs struggle with the task but sentence transformers achieve precision and recall values above 0.9.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0172-6},
}

@inproceedings{ardimento_enhancing_2024,
  title = {Enhancing {Software},
  author = {Ardimento, Pasquale and Bernardi, Mario Luca and Cimitile, Marta and Scalera, Michele},
  year = {2024},
  doi = {10.1145/3652620.3687776},
  url = {https://doi.org/10.1145/3652620.3687776},
  booktitle = {Proceedings of the {ACM},
  pages = {103--106},
  publisher = {Association for Computing Machinery},
  note = {event-place: Linz, Austria},
  keywords = {education, generative AI, model-driven software engineering, scaffolding, software modelling, UML},
  abstract = {This study introduces an innovative AI-powered scaffolding approach aimed at enhancing software modeling learning through UML diagrams. The focus of this research is on defining the principles and functions comprising the scaffolding. Leveraging recent advancements in generative AI, our approach provides a structured educational framework to improve comprehension and proficiency in modeling concepts. We present the initial implementation of the scaffolding, specifically highlighting the feedback function. By integrating theoretical insights with practical applications, this study seeks to advance Model-Driven Software Engineering education and underscores the potential of AI in enhancing instructional methodologies.},
  address = {New York, NY, USA},
  series = {{MODELS},
  isbn = {979-8-4007-0622-6},
}

@inproceedings{li_enhancing_2024,
  title = {Enhancing {LLM},
  author = {Li, Yichen and Peng, Yun and Huo, Yintong and Lyu, Michael R.},
  year = {2024},
  doi = {10.1145/3643795.3648392},
  url = {https://doi.org/10.1145/3643795.3648392},
  booktitle = {Proceedings of the 1st {International},
  pages = {70--74},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {code generation, large language model},
  abstract = {Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information available in IDEs to enhance the capabilities of LLMs of repository-level code completion. We conducted preliminary experiments to validate the performance of IDECoder and observed that this synergy represents a promising trend for future exploration.},
  address = {New York, NY, USA},
  series = {{LLM4Code},
  isbn = {979-8-4007-0579-3},
}

@inproceedings{saha_roy_evidence_2025,
  title = {Evidence {Contextualization},
  author = {Saha Roy, Rishiraj and Schlotthauer, Joel and Hinze, Chris and Foltyn, Andreas and Hahn, Luzian and Kuech, Fabian},
  year = {2025},
  doi = {10.1145/3701551.3704126},
  url = {https://doi.org/10.1145/3701551.3704126},
  booktitle = {Proceedings of the {Eighteenth},
  pages = {1040--1043},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hannover, Germany},
  keywords = {conversations, large language models, question answering, source: ACM},
  abstract = {Retrieval Augmented Generation (RAG) works as a backbone for interacting with an enterprise's own data via Conversational Question Answering (ConvQA). In a RAG system, a retriever fetches passages from a collection in response to a question, which are then included in the prompt of a large language model (LLM) for generating a natural language (NL) answer. However, several RAG systems today suffer from two shortcomings: (i) retrieved passages usually contain their raw text and lack appropriate document context, negatively impacting both retrieval and answering quality; and (ii) attribution strategies that explain answer generation typically rely only on similarity between the answer and the retrieved passages, thereby only generating plausible but not causal explanations. In this work, we demonstrate RAGonite, a RAG system that remedies the above concerns by: (i) contextualizing evidence with source metadata and surrounding text; and (ii) computing counterfactual attribution, a causal explanation approach where the contribution of an evidence to an answer is determined by the similarity of the original response to the answer obtained by removing that evidence. To evaluate our proposals, we release a new benchmark ConfQuestions: it has 300 hand-created conversational questions, each in English and German, coupled with ground truth URLs, completed questions, and answers from 215 public Confluence pages. These documents are typical of enterprise wiki spaces with heterogeneous elements. Experiments with RAGonite on ConfQuestions show the viability of our ideas: contextualization improves RAG performance, and counterfactual explanations outperform standard attribution.},
  address = {New York, NY, USA},
  series = {{WSDM},
  isbn = {979-8-4007-1329-3},
}

@inproceedings{wu_clear_2025,
  title = {{CLEAR},
  author = {Wu, Yutao and Ran, Kun and Liu, Ming and Cardilini, Adam P. A. and Nurwidyantoro, Arif and Liu, Xiao},
  year = {2025},
  doi = {10.1145/3701716.3715170},
  url = {https://doi.org/10.1145/3701716.3715170},
  booktitle = {Companion {Proceedings},
  pages = {2927--2930},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {actionable climate insights, large language model, natural language processing, report generation},
  abstract = {Web platforms play a critical role in climate change communication. However, despite facing increasing climate issues, residents especially those living in rural areas still struggle to access and interpret climate policies, while local governments lack structured feedback on policy effectiveness and community needs. To bridge this dual communication challenge, we present CLEAR (Climate PoLicy rEtrieval and summA Rization), a system powered by Large Language Models (LLMs) which can analyze resident queries and provide customized responses addressing their climate concerns based on relevant local government policy documents. Our system leverages a fine-tuned Llama-3.2-3B to decompose natural queries into structured components to enable precise information retrieval and summarization. The workflow of the CLEAR system consists of: (1) semantic query analysis to identify resident location and climate concerns, (2) intelligent retrieval from a curated dataset of authoritative policy documents, and (3) multi-modal policy summarization with geospatial context. When there is no corresponding policy for the specific climate concern, the CLEAR system can help to generate contextualized feedback to relevant local governments. In this paper, through real-world implementation and demonstration, we show how CLEAR can effectively bridge the information gap between Australian rural communities and local governments through LLM-based policy retrieval and summarization. Our source code is available at https://github.com/wuyoscar/CLEAR.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{felemban_exaggeration-based_2025,
  title = {Exaggeration-based {Fake},
  author = {Felemban, Abdullah and Ghaleb, Mustafa and Felemban, Muhamad},
  year = {2025},
  doi = {10.1145/3733813.3764365},
  url = {https://doi.org/10.1145/3733813.3764365},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {1--4},
  publisher = {Association for Computing Machinery},
  keywords = {Cybersecurity, Fake news, LLM, Misinformation detection},
  abstract = {We address the challenge of detecting exaggeration in cybersecurity tweets on X, where misinformation spreads rapidly. Our novel framework uses local Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) to gather evidence and assess tweets’ rhetorical intensity, offering graded exaggeration scores. Validated through a human study and a pilot that matches LLM results with human labels, this work lays the groundwork for improved misinformation detection tools.},
  address = {New York, NY, USA},
  series = {{3D},
  isbn = {979-8-4007-1902-8},
}

@inproceedings{buzzi_towards_2025,
  title = {Towards {Empowering},
  author = {Buzzi, Marina and Leporini, Barbara and Lo Duca, Angelica and Punzo, Veronica and Rotelli, Daniela},
  year = {2025},
  doi = {10.1145/3733155.3736797},
  url = {https://doi.org/10.1145/3733155.3736797},
  booktitle = {Proceedings of the 18th {ACM},
  pages = {212--215},
  publisher = {Association for Computing Machinery},
  keywords = {Accessibility, Education, Generative AI, Special needs, STEM},
  abstract = {Supporting students with special needs requires adaptive, personalized, and continuously updated educational strategies. Traditional static approaches often fail to capture the complexity and evolution of students’ learning trajectories. In this paper, we propose a dynamic-context AI-based digital platform designed to empower teachers by monitoring student progress, suggesting tailored strategies, and adapting learning materials based on real-time observations and assessments. The system also assists in the compilation and iterative refinement of the IEP (Individualised Educational Plan), offering continuous feedback through a digital co-teacher (chatbot). By providing the AI with an evolving context, the platform aims to enable more accurate and responsive educational support. A theoretical use case illustrates how the system can help teachers adjust objectives and expectations when students encounter unforeseen difficulties, fostering more inclusive and effective learning experiences.},
  address = {New York, NY, USA},
  series = {{PETRA},
  isbn = {979-8-4007-1402-3},
}

@inproceedings{biancini_multiple-choice_2024,
  title = {Multiple-{Choice},
  author = {Biancini, Giorgio and Ferrato, Alessio and Limongelli, Carla},
  year = {2024},
  doi = {10.1145/3631700.3665233},
  url = {https://doi.org/10.1145/3631700.3665233},
  booktitle = {Adjunct {Proceedings},
  pages = {584--590},
  publisher = {Association for Computing Machinery},
  note = {event-place: Cagliari, Italy},
  keywords = {Generative AI, LLMs, Multiple Choice Question},
  abstract = {Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test’s source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-0466-6},
}

@inproceedings{varga_inquiry_2025,
  title = {Inquiry {Assistant},
  author = {Varga, István and Yamashita, Yuta},
  year = {2025},
  doi = {10.1145/3726302.3731956},
  url = {https://doi.org/10.1145/3726302.3731956},
  booktitle = {Proceedings of the 48th {International},
  pages = {4319--4323},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {automated customer support, automated dialogue management, conversational agent, knowledge graph generation, source: Scopus},
  abstract = {Businesses are increasingly overwhelmed by inquiries related to their services or products. Relying on human agents to handle inquiries via email results in higher costs and delayed responses, contributing to customer dissatisfaction. In response to these challenges, this pilot study leverages advancements in Large Language Models (LLMs) by proposing a fully automated method for generating a knowledge graph from unstructured data in help pages, which is then utilized to power a fully automated dialogue management system. By transitioning to a chat-based approach, our method aims to handle ambiguous, incomplete, or nonspecific inquiries more effectively and enhance customer satisfaction with tailored, natural responses. We also implement explicit safeguards to improve intent identification and prevent response hallucinations. We validate our proposal in the hotel industry, demonstrating that our knowledge graph based AI agent outperforms the baseline Retrieval-Augmented Generation (RAG) model in accuracy while facilitating more natural and coherent dialogues.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{zhu_bridging_2025,
  title = {Bridging the {Gender},
  author = {Zhu, Wangda and Xing, Wanli and Lyu, Bailing and Li, Chenglu and Zhang, Fan and Li, Hai},
  year = {2025},
  doi = {10.1145/3706468.3706539},
  url = {https://doi.org/10.1145/3706468.3706539},
  booktitle = {Proceedings of the 15th {International},
  pages = {918--923},
  publisher = {Association for Computing Machinery},
  keywords = {Gender gap, Generative AI, Learning outcomes, Math story},
  abstract = {Addressing the gender gap in K-12 math education is essential for providing equitable learning opportunities, as historical disparities in engagement, performance, and confidence between male and female students in mathematics are often linked to educational biases. Integrating Generative AI (GAI) into math education shows promise for bridging the gender gap in K12 math learning. This study proposes an innovative pedagogy and platform that enables students to create math stories powered by GAI, enhancing their conceptual understanding of key mathematical ideas. The platform was implemented in two K5 schools to evaluate its effectiveness and mechanism (N = 86). Pre- and post-intervention surveys and usage logs indicated significant improvements in students’ learning outcomes regarding Math Question (MQ) skills and Math Story (MS) skills. Bayes SEM further modeled the mechanism: students’ creating math stories powered by GAI significantly improves MS, which further improves MQ. We further found female students were significantly more engaged in creating stories on this platform and gained more improvement on MQ than male students. The results suggest that AI-powered math story creation can be an effective tool for deepening students’ mathematical learning outcomes and has the potential to mitigate the gender gap.},
  address = {New York, NY, USA},
  series = {{LAK},
  isbn = {979-8-4007-0701-8},
}

@inproceedings{lai_graphyour_2025,
  title = {Graphy'our {Data},
  author = {Lai, Longbin and Luo, Changwei and Lou, Yunkai and Ju, Mingchen and Yang, Zhengyi},
  year = {2025},
  doi = {10.1145/3722212.3725106},
  url = {https://doi.org/10.1145/3722212.3725106},
  booktitle = {Companion of the 2025 {International},
  pages = {147--150},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {document analysis, graph model, large language model, source: ACM},
  abstract = {While Large Language Models (LLMs) excel at single-document queries and conversational workflows, they struggle with progressively exploring, analyzing, and synthesizing large unstructured document sets, such as in literature surveys. We address this challenge – termed Progressive Document Investigation – by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw documents into a graph, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers, demonstrating how Graphy facilitates the literature-survey scenario, with video available at https://youtu.be/uM4nzkAdGlM.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1564-8},
  series = {{SIGMOD},
}

@article{huly_old_2024,
  title = {Old ir methods meet rag},
  author = {Huly, O. and Pogrebinsky, I. and Carmel, D. and Kurland, O. and {...},
  year = {2024},
  doi = {10.1145/3626772.3657935},
  url = {https://doi.org/10.1145/3626772.3657935},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {2559--2563},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, source: ACM},
  abstract = {… reduce hallucinations in the generated content [23]. RAG improves the quality of the LLM … of sparse methods for RAG-LLM, is inferior by far to other sparse methods: MRF and RM3. …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{chan_dont_2025,
  title = {Don't {Do},
  author = {Chan, Brian J. and Chen, Chao-Ting and Cheng, Jui-Hung and Huang, Hen-Hsen},
  year = {2025},
  doi = {10.1145/3701716.3715490},
  url = {https://doi.org/10.1145/3701716.3715490},
  booktitle = {Companion {Proceedings},
  pages = {893--897},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {source: ACM},
  abstract = {Retrieval-augmented generation (RAG) has gained traction as a powerful approach for enhancing language models by integrating external knowledge sources. However, RAG introduces challenges such as retrieval latency, potential errors in document selection, and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable size, into the LLM's extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement traditional RAG pipelines. These findings suggest that, for certain applications, particularly those with a constrained knowledge base, CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1331-6},
  series = {{WWW},
}

@inproceedings{niu_part_2025,
  title = {{PaRT},
  author = {Niu, Zihan and Xie, Zheyong and Cao, Shaosheng and Lu, Chonggang and Ye, Zheyu and Xu, Tong and Liu, Zuozhu and Gao, Yan and Chen, Jia and Xu, Zhe and Wu, Yi and Hu, Yao},
  year = {2025},
  doi = {10.1145/3726302.3731946},
  url = {https://doi.org/10.1145/3726302.3731946},
  booktitle = {Proceedings of the 48th {International},
  pages = {4269--4274},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {llm, rag, social chatbot},
  abstract = {Social chatbots have become essential companions in daily scenarios ranging from emotional support to personal interaction. However, conventional chatbots with passive response mechanisms usually rely on users to initiate or sustain dialogues by bringing up new topics, resulting in diminished engagement and shortened dialogue duration. In this paper, we present PaRT, a novel framework enabling context-aware proactive dialogues for social chatbots through personalized real-time retrieval and generation. Specifically, PaRT first integrates user profiles and dialogue context into a large language model (LLM), which is initially prompted to refine user queries and recognize underlying intents for the upcoming conversation. Guided by refined intents, the LLM generates personalized dialogue topics as targeted queries to retrieve relevant passages from RedNote. Finally, we prompt LLMs with summarized passages to generate knowledge-grounded and engagement-optimized responses. Our approach has been running stably in a real-world production environment for more than 30 days, achieving a 21.77\% improvement in the average duration of dialogues.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_storyverse_2024,
  title = {{StoryVerse},
  author = {Wang, Yi and Zhou, Qian and Ledo, David},
  year = {2024},
  doi = {10.1145/3649921.3656987},
  url = {https://doi.org/10.1145/3649921.3656987},
  booktitle = {Proceedings of the 19th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: Worcester, MA, USA},
  keywords = {Character Simulation, Generative AI, Large Language Models, Narrative Planning, Video games},
  abstract = {Automated plot generation for games enhances the player’s experience by providing rich and immersive narrative experience. Recent advancements use Large Language Models (LLMs) to drive the behavior of virtual characters, allowing plots to emerge from interactions between characters and their environments. However, the emergent nature of such decentralized plot generation makes it difficult for authors to direct plot progression. We propose a novel plot creation workflow that mediates between a writer’s authorial intent and the emergent behaviors from LLM-driven character simulations, through a novel authorial structure called “abstract acts”. Writers create high-level plot outlines which are transformed into character actions via an LLM-based narrative planning process, based on the game world state. This results in narratives co-created by the author, the simulated characters, and the player. We present StoryVerse as a proof-of-concept system to demonstrate the workflow, and showcase its versatility across various stories and game environments.},
  address = {New York, NY, USA},
  series = {{FDG},
  isbn = {979-8-4007-0955-5},
}

@article{rome__2024,
  title = {" {Ask},
  author = {Rome, S. and Chen, T. and Tang, R. and Zhou, L. and Ture, F.},
  year = {2024},
  doi = {10.1145/3626772.3661345},
  url = {https://doi.org/10.1145/3626772.3661345},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {2827--2831},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, assistive ai, customer care, llm, rag, reranking, vector db},
  abstract = {… a typical RAG … ], our Citation Rail was accomplished by prompting the LLM to cite its sources in a specific manner (cf, Figure 1) combined with a post processing step where the citations …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{juan_co-trained_2025,
  title = {Co-{Trained},
  author = {Juan, Yining and Chen, Chung-Chi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
  year = {2025},
  doi = {10.1145/3701716.3715524},
  url = {https://doi.org/10.1145/3701716.3715524},
  booktitle = {Companion {Proceedings},
  pages = {1048--1052},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {earnings calls, presentation preparation, question generation, retrieval-augmented generation},
  abstract = {In diverse professional environments, ranging from academic conferences to corporate earnings calls, the ability to anticipate audience questions stands paramount. Traditional methods, which rely on manual assessment of an audience's background, interests, and subject knowledge, often fall short-particularly when facing large or heterogeneous groups, leading to imprecision and inefficiency. While NLP has made strides in text-based question generation, its primary focus remains on academic settings, leaving the intricate challenges of professional domains, especially earnings call conferences, underserved. Addressing this gap, our paper pioneers the multi-question generation (MQG) task specifically designed for earnings call contexts. Our methodology involves an exhaustive collection of earnings call transcripts and a novel annotation technique to classify potential questions. Furthermore, we introduce a retriever-enhanced strategy to extract relevant information. With a core aim of generating a spectrum of potential questions that analysts might pose, we derive these directly from earnings call content. Empirical evaluations underscore our approach's edge, revealing notable excellence in the accuracy, consistency, and perplexity of the questions generated.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{he_low-cost_2025,
  title = {Low-{Cost},
  author = {He, Shanxiu and Xie, Wentai and Qiao, Yifan and Carlson, Parker and Yang, Tao},
  year = {2025},
  doi = {10.1145/3726302.3730227},
  url = {https://doi.org/10.1145/3726302.3730227},
  booktitle = {Proceedings of the 48th {International},
  pages = {2987--2993},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {efficiency, hybrid search, large language model},
  abstract = {Low-cost retrieval is crucial for document search on resource-limited computing platforms. This paper presents a staged sparse-to-dense retrieval framework that substitutes expensive dense query encoding with a dense pseudo-query (DPQ), an approximation derived solely from sparse retrieval results. DPQ scheme employs a simple, rank-aware weighting to combine corresponding dense representations of top sparse results, providing an opportunity to efficiently leverage an expensive but expressive LLM or BERT-based dense model without requiring GPUs. The evaluation demonstrates that DPQ-based retrieval runs fast on an affordable platform and outperforms several low-cost baselines in zero-shot retrieval.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhang_knowledge-based_2025,
  title = {Knowledge-{Based},
  author = {Zhang, Yiran and Li, Ruiyin and Liang, Peng and Sun, Weisong and Liu, Yang},
  year = {2025},
  doi = {10.1145/3696630.3728493},
  url = {https://doi.org/10.1145/3696630.3728493},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {530--534},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {large language model, multi-agent system, software architecture},
  abstract = {Architecture design is a critical step in software development. However, creating a high-quality architecture is often costly due to the significant need for human expertise and manual effort. Recently, agents built upon Large Language Models (LLMs) have achieved remarkable success in various software engineering tasks. Despite this progress, the use of agents to automate the architecture design process remains largely unexplored. To address this gap, we envision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework. MAAD uses agents to simulate human roles in the traditional software architecture design process, thereby automating the design process. To empower these agents, MAAD incorporates knowledge extracted from three key sources: 1) existing system designs, 2) authoritative literature, and 3) architecture experts. By envisioning the MAAD framework, we aim to advance the full automation of application-level system development.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{bai_insight_2025,
  title = {Insight {Agents},
  author = {Bai, Jincheng and Zhang, Zhenyu and Zhang, Jennifer and Zhu, Jason},
  year = {2025},
  doi = {10.1145/3726302.3731959},
  url = {https://doi.org/10.1145/3726302.3731959},
  booktitle = {Proceedings of the 48th {International},
  pages = {4335--4339},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {agentic workflow, information retrieval, llm, rag},
  abstract = {Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this new LLM-backed end-to-end agentic workflow designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 89.5\% based on human evaluation, with latency of P90 below 15s.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{sarmah_towards_2024,
  title = {Towards reducing hallucination in extracting information from financial reports using {Large},
  author = {Sarmah, Bhaskarjit and Mehta, Dhagash and Pasquali, Stefano and Zhu, Tianjie},
  year = {2024},
  doi = {10.1145/3639856.3639895},
  url = {https://doi.org/10.1145/3639856.3639895},
  booktitle = {Proceedings of the {Third},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bangalore, India},
  keywords = {Earning Call Transcripts, Financial Markets, Large Language Models, Natural Language Processing, source: Scopus},
  abstract = {For a financial analyst, the question and answer (Q\&amp;A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\&amp;A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy—transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\&amp;A systems, and empirically demonstrate superiority of our method.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1649-2},
  annote = {Cited by: 5},
}

@inproceedings{sun_docs2kg_2025,
  title = {{Docs2KG},
  author = {Sun, Qiang and Luo, Yuanyi and Zhang, Wenxiao and Li, Sirui and Li, Jichunyang and Niu, Kai and Kong, Xiangrui and Liu, Wei},
  year = {2025},
  doi = {10.1145/3701716.3715309},
  url = {https://doi.org/10.1145/3701716.3715309},
  booktitle = {Companion {Proceedings},
  pages = {801--804},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {heterogeneous data, knowledge graph, unstructured data},
  abstract = {Enterprises generate vast amounts of unstructured documents, posing challenges for knowledge extraction and representation. Large language models (LLMs) offer strong potential for processing such data but struggle with factual accuracy and provenance. Knowledge graphs (KGs) provide a structured framework to address these limitations [6], yet constructing high-quality KGs from heterogeneous data remains a challenge. To address this issue, we present Docs2KG, a modular framework to build high-quality KGs from diverse unstructured documents. We first employs state-of-the-art document processing techniques to extract textual content, tabular data, and figures. The extracted information is then unified into a multifaceted KG with three aspects: (1) a Layout KG capturing document structural hierarchies, (2) a Metadata KG preserving document properties, and (3) a Semantic KG representing domain-specific entities and relationships. Docs2KG supports multiple construction paradigms for Semantic KG: ontology-based approaches, hybrid NLP pipelines with LLM verification, LLM-guided ontology generation, and specialized models for named entity recognition, event extraction, and causal relationship identification to enhance semantic coverage and accuracy. A key feature of Docs2KG is its human-in-the-loop verification interface, enabling iterative quality assessment and refinement of the resulting KGs. Docs2KG is openly available at https://docs2kg.ai4wa.com, with the aim of advancing knowledge graph construction research and accelerating enterprise applications through high-quality knowledge graph construction.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{ugarte_astral_2025,
  title = {{ASTRAL},
  author = {Ugarte, Miriam and Valle, Pablo and Parejo, José Antonio and Segura, Sergio and Arrieta, Aitor},
  year = {2025},
  doi = {10.1145/3713081.3731733},
  url = {https://doi.org/10.1145/3713081.3731733},
  booktitle = {Proceedings of the 34th {ACM},
  pages = {31--35},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {automated testing, LLMs, safety},
  abstract = {In this paper, we present ASTRAL, a tool that automates the generation and execution of test inputs (i.e., prompts) to evaluate the safety of Large Language Models (LLMs). ASTRAL consists of three microservice modules. The first is a test generator, which employs a novel black-box coverage criterion to create balanced and diverse unsafe test inputs across a wide range of safety categories and linguistic characteristics (e.g., different writing styles and persuasion techniques). Additionally, the test generator incorporates an LLM-based approach that leverages Retrieval-Augmented Generation (RAG), few-shot prompting strategies, and web browsing to produce up-to-date test inputs. The second module is the test executor, which runs the generated test inputs on the LLM under test. Finally, the test evaluator acts an oracle to assess the execution outputs to identify unsafe responses, enabling a fully automated LLM testing process.},
  address = {New York, NY, USA},
  series = {{ISSTA},
  isbn = {979-8-4007-1474-0},
}

@inproceedings{ma_fine-tuning_2024,
  title = {Fine-{Tuning},
  author = {Ma, Xueguang and Wang, Liang and Yang, Nan and Wei, Furu and Lin, Jimmy},
  year = {2024},
  doi = {10.1145/3626772.3657951},
  url = {https://doi.org/10.1145/3626772.3657951},
  booktitle = {Proceedings of the 47th {International},
  pages = {2421--2425},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {dense retrieval, large language model, reranker},
  abstract = {While large language models (LLMs) have shown impressive NLP capabilities, existing IR applications mainly focus on prompting LLMs to generate query expansions or generating permutations for listwise reranking. In this study, we leverage LLMs directly to serve as components in the widely used multi-stage text ranking pipeline. Specifically, we fine-tune the open-source LLaMA-2 model as a dense retriever (repLLaMA) and a pointwise reranker (rankLLaMA). This is performed for both passage and document retrieval tasks using the MS MARCO training data. Our study shows that finetuned LLM retrieval models outperform smaller models. They are more effective and exhibit greater generalizability, requiring only a straightforward training strategy. Moreover, our pipeline allows for the fine-tuning of LLMs at each stage of a multi-stage retrieval pipeline. This demonstrates the strong potential for optimizing LLMs to enhance a variety of retrieval tasks. Furthermore, as LLMs are naturally pre-trained with longer contexts, they can directly represent longer documents. This eliminates the need for heuristic segmenting and pooling strategies to rank long documents. On the MS MARCO and BEIR datasets, our repLLaMA-rankLLaMA pipeline demonstrates a high level of effectiveness.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{shen_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Shen, Lei and Zhao, Kang and Jin, Zhipeng and Tao, Wen and Yang, Yi and Han, Cong and Li, Shuanglong and Cai, Zhongmin and Liu, Lin},
  year = {2025},
  doi = {10.1145/3726302.3731957},
  url = {https://doi.org/10.1145/3726302.3731957},
  booktitle = {Proceedings of the 48th {International},
  pages = {4324--4328},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {image captioning, multimodal large language model, search advertising, text-to-image generation},
  abstract = {Recent advancements in generative artificial intelligence are driving a significant transformation in information retrieval and content generation, creating substantial opportunities for online advertising. Text-to-image generation technology has become increasingly prevalent in advertising content production, demonstrating promising performance improvements in terms of semantic relevance and visual appeal. However, existing models often suffer from inadequate representation of entity concepts, such as prominent product brands and recognizable landmarks. This inherent limitation subsequently leads to notable deficiencies in brand tonality, industry-specific relevance, and market adaptability of the generated advertising content. To address this challenge, we propose a multimodal ad content generation framework specifically engineered for online advertising system, particularly focused on resolving the deficiency in entity concepts. Our framework is comprised of two phases: first, an image captioning module with entity-aware learning based on multimodal large language model, leveraging retrieval-augmented techniques to incorporate entity concepts into image descriptions; second, a text-to-image diffusion model refined on image-text pairs enriched with entity concepts to facilitate entity-grounded image generation. Extensive experiments validate the effectiveness of our framework, demonstrating superior performance in both image captioning and image generation compared to existing methods, particularly in the accuracy of depiction of relevant entities in advertising images. Moreover, the deployment of the framework in the system primary traffic of Baidu Search Ads, has brought significant enhancements to advertisement revenue for both advertisers and the platform.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhang_modelgalaxy_2024,
  title = {{ModelGalaxy},
  author = {Zhang, Wenling and Li, Yixiao and Li, Zhaotian and Sun, Hailong and Gao, Xiang and Liu, Xudong},
  year = {2024},
  doi = {10.1145/3626772.3657676},
  url = {https://doi.org/10.1145/3626772.3657676},
  booktitle = {Proceedings of the 47th {International},
  pages = {2771--2775},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {large language model, meta-learning, model retrieval},
  abstract = {With the growing number of available machine learning models and the emergence of model-sharing platforms, model reuse has become a significant approach to harnessing the power of artificial intelligence. One of the key issues to realizing model reuse resides in efficiently and accurately finding the target models that meet user needs from a model repository. However, the existing popular model-sharing platforms (e.g., Hugging Face) mainly support model retrieval based on model name matching and task filtering. If not familiar with the platform or specific models, users may suffer from low retrieval efficiency and a less user-friendly interaction experience. To address these issues, we have developed ModelGalaxy, a versatile model retrieval platform supporting multiple model retrieval methods, including keyword-based search, dataset-based search, and user-task-centric search. Moreover, ModelGalaxy leverages the power of large language models to provide users with easily retrieving and using models. Our source code is available at https://github.com/zwl906711886/ModelGalaxy.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@article{ngom_mallet_2024,
  title = {Mallet: {Sql},
  author = {Ngom, A. L. and Kraska, T.},
  year = {2024},
  doi = {10.1145/3663742.3663973},
  url = {https://doi.org/10.1145/3663742.3663973},
  booktitle = {Proceedings of the {Seventh},
  journal = {Proceedings of the Seventh International Workshop …},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, source: ACM},
  abstract = {… • We perform RAG over system documentation and over expertise from … the LLM produced a hallucination. Whenever such a hallucination is detected, we iteratively re-prompt the LLM in …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{aiDM},
  isbn = {979-8-4007-0680-6},
}

@inproceedings{frey_pots_2025,
  title = {{POTS},
  author = {Frey, Johannes and Ferraz, Lucas and Hofer, Marvin},
  year = {2025},
  doi = {10.1145/3701716.3715194},
  url = {https://doi.org/10.1145/3701716.3715194},
  booktitle = {Companion {Proceedings},
  pages = {2831--2834},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {graph retrieval augmented generation, llms, ontology, ontology retrieval, ontology terms embedding, owl, semantic search, terminology lookup service},
  abstract = {We present a novel microservice-based system, that facilitates a polyparadigmatic ontology term search (leveraging semantic search via vector embeddings, keyword search, and attribute filters). The search index strategy intends to preserve important semantic aspects of the ontological context of a term (selected attributes and term relationships) using structured search fields and multilevel vector spaces assembling hyper-level vector spaces. The flexible, yet simple query API allows fine-grained search requests based on a combination of fuzzy and exact filters. The architecture is based on a highly automatable and flexible Docker Compose setup strategy. While deploying the system for a local ontology is only one command away, the setup also allows ingesting a configurable subset of over 1,800 published ontologies in over 12,000 versions via DBpedia Archivo.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{van_schaik_field_2024,
  title = {A {Field},
  author = {van Schaik, Tempest A. and Pugh, Brittany},
  year = {2024},
  doi = {10.1145/3626772.3661346},
  url = {https://doi.org/10.1145/3626772.3661346},
  booktitle = {Proceedings of the 47th {International},
  pages = {2832--2836},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {evaluation metrics, llms, offline evaluation, summarization},
  abstract = {Large Language models (LLMs) are rapidly being adopted for tasks such as text summarization, in a wide range of industries. This has driven the need for scalable, automatic, reliable, and cost-effective methods to evaluate the quality of LLM-generated text. What is meant by evaluating an LLM is not yet well defined and there are widely different expectations about what kind of information evaluation will produce. Evaluation methods that were developed for traditional Natural Language Processing (NLP) tasks (before the rise of LLMs) remain applicable but are not sufficient for capturing high-level semantic qualities of summaries. Emerging evaluation methods that use LLMs to evaluate LLM-output, appear to be powerful but lacking in reliability. New elements of LLM generated text that were not an element of previous NLP tasks, such as the artifacts of hallucination, need to be considered. We outline the different types of LLM evaluation currently used in the literature but focus on offline, system-level evaluation of the text generated by LLMs. Evaluating LLM-generated summaries is a complex and fast-evolving area, and we propose strategies for applying evaluation methods to avoid common pitfalls. Despite having promising strategies for evaluating LLM summaries, we highlight some open challenges that remain.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{buchmann_white-box_2024,
  title = {White-box {LLM},
  author = {Buchmann, Thomas and Peinl, René and Schwägerl, Felix},
  year = {2024},
  doi = {10.1145/3652620.3687803},
  url = {https://doi.org/10.1145/3652620.3687803},
  booktitle = {Proceedings of the {ACM},
  pages = {556--560},
  publisher = {Association for Computing Machinery},
  note = {event-place: Linz, Austria},
  keywords = {artificial intelligence, large language models, low-code, model-driven engineering, semiformal},
  abstract = {Low-code development (LCD) platforms promise to empower citizen developers to define core domain models and rules for business applications. However, as domain rules grow complex, LCD platforms may fail to do so effectively. Generative AI, driven by large language models (LLMs), offers source code generation from natural language but suffers from its non-deterministic black-box nature and limited explainability. Therefore, rather than having LLMs generate entire applications from single prompts, we advocate for a white-box approach allowing citizen developers to specify domain models semi-formally, attaching constraints and operations as natural language annotations. These annotations are fed incrementally into an LLM contextualized with the generated application stub. This results in deterministic and better explainable generation of static application components, while offering citizen developers an appropriate level of abstraction. We report on a case study in manufacturing execution systems, where the implementation of the approach provides first insights.},
  address = {New York, NY, USA},
  series = {{MODELS},
  isbn = {979-8-4007-0622-6},
}

@inproceedings{venkatakrishnan_semantic_2024,
  title = {Semantic interlinking of {Immigration},
  author = {Venkatakrishnan, Radhakrishnan and Tanyildizi, Emrah and Canbaz, M. Abdullah},
  year = {2024},
  doi = {10.1145/3589335.3651557},
  url = {https://doi.org/10.1145/3589335.3651557},
  booktitle = {Companion {Proceedings},
  pages = {605--608},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {data restructuring, document processing, information retrieval, knowledge graphs, large language models, legal tech, source: ACM, source: Scopus},
  abstract = {The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0172-6},
  annote = {Cited by: 6},
}

@inproceedings{tian_template-based_2025,
  title = {Template-{Based},
  author = {Tian, Yong-En and Tang, Yu-Chien and Wang, Kuang-Da and Yen, An-Zi and Peng, Wen-Chih},
  year = {2025},
  doi = {10.1145/3726302.3730253},
  url = {https://doi.org/10.1145/3726302.3730253},
  booktitle = {Proceedings of the 48th {International},
  pages = {2706--2710},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {agentic framework, decomposed prompting, large language model, template-based financial report generation, source: ACM},
  abstract = {Tailoring structured financial reports from companies' earnings releases is crucial for understanding financial performance and has been widely adopted in real-world analytics. However, existing summarization methods often generate broad, high-level summaries, which may lack the precision and detail required for financial reports that typically focus on specific, structured sections. While Large Language Models (LLMs) hold promise, generating reports adhering to predefined multi-section templates remains challenging. This paper investigates two LLM-based approaches popular in industry for generating templated financial reports: an agentic information retrieval (IR) framework and a decomposed IR approach, namely AgenticIR and DecomposedIR. The AgenticIR utilizes collaborative agents prompted with the full template. In contrast, the DecomposedIR approach applies a prompt chaining workflow to break down the template and reframe each section as a query answered by the LLM using the earnings release. To quantitatively assess the generated reports, we evaluated both methods in two scenarios: one using a financial dataset without direct human references, and another with a weather-domain dataset featuring expert-written reports. Experimental results show that while AgenticIR may excel in orchestrating tasks and generating concise reports through agent collaboration, DecomposedIR statistically significantly outperforms AgenticIR approach in providing broader and more detailed coverage in both scenarios, offering reflection on the utilization of the agentic framework in real-world applications.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_paperping_2025,
  title = {{PaperPing},
  author = {Wang, Ruotong and Zhou, Xinyi and Qiu, Lin and Chang, Joseph Chee and Bragg, Jonathan and Zhang, Amy X.},
  year = {2025},
  doi = {10.1145/3715070.3757230},
  url = {https://doi.org/10.1145/3715070.3757230},
  booktitle = {Companion {Publication},
  pages = {532--535},
  publisher = {Association for Computing Machinery},
  keywords = {AI agent, group communication, large language models, recommender systems, retrieval augmented generation},
  abstract = {AI agents are entering group spaces to facilitate or contribute to conversations. However, without an understanding of group context, these contributions can be irrelevant or even intrusive. In this demo, we introduce a socially-aware AI agent that proactively contributes to research group chats by posting context-aware messages that contain paper recommendations and contextualized explanations. To achieve this, PaperPing instantiates Social-RAG\&nbsp;[14]. It retrieves relevant interactions in the group context, builds a social knowledge base, and extracts social signals to contextualize the generated messages. PaperPing has been deployed in 18 channels as part of a user study for at least 3 months, reaching 500+ researchers. As part of the demo, we will demonstrate PaperPing in a Slack workspace open to all CSCW attendees during and after the conference, allowing researchers to explore the interactions in real settings with the side benefits of fostering collaborative learning within the community.},
  address = {New York, NY, USA},
  series = {{CSCW},
  isbn = {979-8-4007-1480-1},
}

@inproceedings{zhao_ontology-guided_2025,
  title = {Ontology-{Guided},
  author = {Zhao, Mengyue and Nokleby, Matthew and Shen, Bo and Dong, Wenbo and Pachauri, Deepti and Yang, Andrew},
  year = {2025},
  doi = {10.1145/3726302.3731964},
  url = {https://doi.org/10.1145/3726302.3731964},
  booktitle = {Proceedings of the 48th {International},
  pages = {4360--4364},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {hybrid search, knowledge graph, large language model, multi-hop reasoning, natural language processing, ontology-driven retrieval, store fulfillment., text-to-cypher},
  abstract = {Answering complex queries in store fulfillment, such as ”What percentage of employee-assigned actions remain unresolved?” or ”How many worklists for a specific product type were completed within a timeframe at each location?” requires precise, multi-hop reasoning across datasets with varying granularities. This paper introduces an ontology-based knowledge graph (KG) approach integrated with a structured text-to-Cypher generation pipeline, enabling accurate retrieval for such queries. Benchmarking against a robust hybrid search baseline combining BM25 and semantic search, our method demonstrates superior performance in addressing multi-hop and cross-granularity questions. Leveraging a KG schema designed to capture intricate relationships (e.g. (OrderLineItem)-[:INVOLVES\_ACTION]-\&gt;(Action)-[:INVOLVES]-\&gt;(BatchProcess)-[:IS\_COMPLETED\_AT]-\&gt;(Location)), we reveal universal patterns for constructing and querying highly relational data. This work highlights the transformative potential of ontology-driven KGs to improve reasoning, data aggregation, and decision-making, with broader implications for any domain requiring structured, multi-relational data analysis.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhang_automating_2024,
  title = {Automating {Geospatial},
  author = {Zhang, Qianheng and Gao, Song},
  year = {2024},
  doi = {10.1145/3678717.3695760},
  url = {https://doi.org/10.1145/3678717.3695760},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {715--716},
  publisher = {Association for Computing Machinery},
  note = {event-place: Atlanta, GA, USA},
  keywords = {automate workflow, GeoAI, GIS, LLM, Prompt engineering, source: ACM},
  abstract = {The field of Geospatial Artificial Intelligence (GeoAI) has significantly impacted domain applications such as urban analytics, environmental monitoring, and disaster management. While powerful geoprocessing tools in geographic information systems (GIS) like ArcGIS Pro are available, automating these workflows with Python scripting using AI chatbots remains a challenge, especially for non-expert users. This study investigates whether ChatGPT-4 can automate GIS workflows by generating ArcPy functions based on structured instructions. We tested prompt engineering's ability on helping large language models (LLMs) understand spatial data and GIS workflows. The overall task success rate reaches 80.5\%. It is a valid and easy to implement approach for domain scientists who want to use ArcPy to automate their workflows.},
  address = {New York, NY, USA},
  series = {{SIGSPATIAL},
  isbn = {979-8-4007-1107-7},
}

@inproceedings{zhou_dataset_2025,
  title = {Dataset for {Industrial},
  author = {Zhou, Yan and Zhou, Baifan and Li, Huajian and Lyu, Qianhang and Qu, Yuanwei and Waaler, Arild and Yu, Ingrid C.},
  year = {2025},
  doi = {10.1145/3701716.3715310},
  url = {https://doi.org/10.1145/3701716.3715310},
  booktitle = {Companion {Proceedings},
  pages = {825--828},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {dataset generation, industrial dataset resource, question answering with explanation, system information, source: ACM},
  abstract = {The digital and green transition under Industry 4.0 has accelerated the adoption of AI in industries such as manufacturing, energy, and mining. Question Answering with Explanation (QAE), as a way of human interaction with AI, is crucial for enhancing transparency and trust in high-stakes industrial applications. However, industrial QAE remains underexplored due to the lack of publicly available, high-quality datasets, hindered by the need for expert effort and corporate restrictions. To this end, we introduce PANDAX ( https://doi.org/10.5281/zenodo.14510798 ), the first open-source industrial QAE dataset, and SEG, a scalable method for generating high-quality QAE datasets using LLMs. PANDAX focuses on three key topics of industrial system information: partonomy, functionality, and parameters, across critical domains such as green technology and cooling systems. SEG ensures scalability and quality through ensemble generation, majority voting, expert ranking, etc. The human evaluation validates PANDAX's high quality, positioning it as a valuable resource for advancing QAE techniques, benchmarking language technologies, and supporting research in explainable AI for industrial systems.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{diaz-de-arcaya_towards_2024,
  title = {Towards the self-healing of {Infrastructure},
  author = {Diaz-De-Arcaya, Josu and López-De-Armentia, Juan and Zárate, Gorka and Torre-Bastida, Ana I.},
  year = {2024},
  doi = {10.1145/3643788.3648014},
  url = {https://doi.org/10.1145/3643788.3648014},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {22--25},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {automated patching, IaC, infrastructure as code, large language models, LLMs, self-healing},
  abstract = {The generalization of the use of cloud computing and edge computing solutions in industry requires innovative techniques to keep up with the complexity of these scenarios. In particular, the large heterogeneity of the infrastructural devices and the myriad of services offered by the various private and cloud providers represent a challenge. Infrastructure as Code (IaC) technologies have been adopted to reduce the complexity of these scenarios, but even IaC technologies have their drawbacks, as the errors resulting from their use often combine the complexities of the underlying layers and require a high level of expertise. In this regard, the recent upsurge of Large Language Models represents an opportunity as they are able to tackle different problems. In this article, we aspire to shed light on the automated patching of IaC projects with the help of LLMs. We evaluate the suitability of this hypothesis by using a well-known LLM that is able to solve all the scenarios we envisioned and assess the possibility of doing the same with smaller, offline LLMs, which could lead to the use of these technologies in resource-constrained environments, such as edge computing.},
  address = {New York, NY, USA},
  series = {{APR},
  isbn = {979-8-4007-0577-9},
}

@inproceedings{zhao_best_2025,
  title = {Best practice for supply chain in {LLM},
  author = {Zhao, Shengming and Wang, Jiawei},
  year = {2025},
  doi = {10.1145/3713081.3731748},
  url = {https://doi.org/10.1145/3713081.3731748},
  booktitle = {Proceedings of the 34th {ACM},
  pages = {174--177},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {large language models, medical system, software best practice, software supply chain},
  abstract = {The application of large language models in medical applications is crucial for enhancing diagnostic accuracy, improving patient communication, and boosting healthcare efficiency. Their ability to process vast amounts of data, generate concise information, and automate tasks positions LLM applications as transformative tools. Recent studies and real-world examples underscore the importance of delivering secure and responsible LLM-assisted applications. In this manuscript, we outline our intention of uncovering best software practices in the supply chain of LLM medical applications.},
  address = {New York, NY, USA},
  series = {{ISSTA},
  isbn = {979-8-4007-1474-0},
}

@inproceedings{galitsky_truth-o-meter_2024,
  title = {Truth-{O},
  author = {Galitsky, Boris and Chernyavskiy, Anton and Ilvovsky, Dmitry},
  year = {2024},
  doi = {10.1145/3626772.3657679},
  url = {https://doi.org/10.1145/3626772.3657679},
  booktitle = {Proceedings of the 47th {International},
  pages = {2817--2821},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {evidence retrieval, fact-checking, hallucinations detection, llms},
  abstract = {Large Language Models (LLM) often produce text with incorrect facts and hallucinations. To address this issue, we developed a fact-checking system Truth-O-Meter12 which verifies LLM results on the Internet and other sources of information to detect wrong claims/facts and proposes corrections for them. NLP and reasoning techniques such as Abstract Meaning Representation and syntactic alignment are applied to match hallucinating sentences with truthful ones. To handle inconsistent sources while fact-checking, we rely on argumentation analysis in the form of defeasible logic programming, selecting the most authoritative source. Our evaluation shows that LLM content can be substantially improved for factual correctness and meaningfulness on an industrial scale.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{zinjad_resumeflow_2024,
  title = {{ResumeFlow},
  author = {Zinjad, Saurabh Bhausaheb and Bhattacharjee, Amrita and Bhilegaonkar, Amey and Liu, Huan},
  year = {2024},
  doi = {10.1145/3626772.3657680},
  url = {https://doi.org/10.1145/3626772.3657680},
  booktitle = {Proceedings of the 47th {International},
  pages = {2781--2785},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {ai persona, automated resume generation, information extraction, large language models, personalization, prompt engineering},
  abstract = {Crafting the ideal, job-specific resume is a challenging task for many job applicants, especially for early-career applicants. While it is highly recommended that applicants tailor their resume to the specific role they are applying for, manually tailoring resumes to job descriptions and role-specific requirements is often (1) extremely time-consuming, and (2) prone to human errors. Furthermore, performing such a tailoring step at scale while applying to several roles may result in a lack of quality of the edited resumes. To tackle this problem, in this demo paper, we propose ResumeFlow: a Large Language Model (LLM) aided tool that enables an end user to simply provide their detailed resume and the desired job posting, and obtain a personalized resume specifically tailored to that specific job posting in the matter of a few seconds. Our proposed pipeline leverages the language understanding and information extraction capabilities of state-of-the-art LLMs such as OpenAI's GPT-4 and Google's Gemini, in order to (1) extract details from a job description, (2) extract role-specific details from the user-provided resume, and then (3) use these to refine and generate a role-specific resume for the user. Our easy-to-use tool leverages the user-chosen LLM in a completely off-the-shelf manner, thus requiring no fine-tuning. We demonstrate the effectiveness of our tool via a https://www.youtube.com/watch?v=Agl7ugyu1N4 and propose novel task-specific evaluation metrics to control for alignment and hallucination. Our tool is available at https://job-aligned-resume.streamlit.app.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{bayer_towards_2025,
  title = {Towards {A},
  author = {Bayer, Robert and Robroek, Ties and Tözün, Pinar},
  year = {2025},
  doi = {10.1145/3719159.3721223},
  url = {https://doi.org/10.1145/3719159.3721223},
  booktitle = {Proceedings of the 3rd {International},
  pages = {23--26},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {Benchmarking, Deep Learning, Edge Computing},
  abstract = {Machine learning (ML) benchmarks are crucial for evaluating the performance, efficiency, and scalability of ML systems, especially as the adoption of complex ML pipelines, such as retrieval-augmented generation (RAG), continues to grow. These pipelines introduce intricate execution graphs that require more advanced benchmarking approaches. Additionally, collocating workloads can improve resource efficiency but may introduce contention challenges that must be carefully managed. Detailed insights into resource utilization are necessary for effective collocation and optimized edge deployments. However, existing benchmarking frameworks often fail to capture these critical aspects.We introduce a modular end-to-end ML benchmarking framework designed to address these gaps. Our framework emphasizes modularity and reusability by enabling reusable pipeline stages, facilitating flexible benchmarking across diverse ML workflows. It supports complex workloads and measures their end-to-end performance. The workloads can be collocated, with the framework providing insights into resource utilization and contention between the concurrent workloads.},
  address = {New York, NY, USA},
  series = {{TDIS},
  isbn = {979-8-4007-1526-6},
}

@inproceedings{yu_biasnavi_2025,
  title = {{BiasNavi},
  author = {Yu, Junliang and Huynh, Jay Thai Duong and Fan, Shaoyang and Demartini, Gianluca and Chen, Tong and Yin, Hongzhi and Sadiq, Shazia},
  year = {2025},
  doi = {10.1145/3701716.3715169},
  url = {https://doi.org/10.1145/3701716.3715169},
  booktitle = {Companion {Proceedings},
  pages = {2939--2942},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {agent, bias management, data management, large language models},
  abstract = {Bias in datasets undermines the fairness, transparency, and reliability of AI systems, presenting critical challenges across applications. Existing tools for managing data bias often remain inaccessible to non-experts or struggle with the complexities of domain-specific datasets. In this work, we introduce BiasNavi, an large language models (LLM)-empowered toolkit for data bias management. BiasNavi features an autonomous agent that seamlessly integrates with modules for bias identification, measurement, surfacing, and adaptation, reasoning over data and interactions to adaptively guide users through the bias management pipeline. With intuitive and personalized interfaces, BiasNavi empowers users to customize their workflows and address data bias effectively. A case study with the COMPAS dataset demonstrates how BiasNavi, by leveraging the advanced reasoning capabilities of LLMs, democratizes responsible AI practices, making bias management both accessible and effective for users with varying levels of expertise. BiasNavi is available at: https://github.com/CIRES-Hub/BiasNavi.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhao_checkguard_2024,
  title = {{CheckGuard},
  author = {Zhao, Fei and Chen, Jiawen and Huang, Bin and Zhang, Chengcui and Warner, Gary},
  year = {2024},
  doi = {10.1145/3627673.3679155},
  url = {https://doi.org/10.1145/3627673.3679155},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5425--5429},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {check fraud detection, cross-modal generation, machine learning, multi-modal large language model, stolen check},
  abstract = {The prevalence of check fraud, particularly with stolen checks sold on platforms such as Telegram, creates significant challenges for both individuals and financial institutions. This underscores the urgent need for innovative solutions to detecting and preventing such fraud on social media platforms. While deep learning techniques show great promise in detecting objects and extracting information from images, their effectiveness in addressing check fraud is hindered by the lack of comprehensive, open-source, large training datasets specifically for check information extraction. To bridge this gap, this paper introduces "CheckGuard," a large labeled image-to-text cross-modal dataset designed for check information extraction. CheckGuard comprises over 7,000 real-world stolen check image segments from more than 15 financial institutions, featuring a variety of check styles and layouts. These segments have been manually labeled, resulting in over 50,000 samples across seven key elements: Drawer, Payee, Amount, Date, Drawee, Routing Number, and Check Number. This dataset supports various tasks such as visual question answering (VQA) on checks and check image captioning. Our paper details the rigorous data collecting, cleaning, and annotation processes that make CheckGuard a valuable resource for researchers in check fraud detection, machine learning, and multimodal large language models (MLLMs). We not only benchmark state-of-the-art (SOTA) methods on this dataset to assess their performance but also explore potential enhancements. Our application of parameter-efficient fine-tuning (PEFT) techniques on the SOTA MLLMs demonstrates significant performance improvements, providing valuable insights and practical approaches for enhancing model efficacy on this task. As an evolving project, CheckGuard will continue to be updated with new data, enhancing its utility and driving further advancements in the field. Our PEFT-based MLLM code is available at: https://github.com/feizhao19/CheckGuard. For data access, researchers are required to contact the authors directly.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{lyu_towards_2024,
  title = {Towards {Advancing},
  author = {Lyu, Hanjia},
  year = {2024},
  doi = {10.1145/3627673.3680270},
  url = {https://doi.org/10.1145/3627673.3680270},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5459--5462},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {content understanding, personalization, recommendation, representation learning, user understanding},
  abstract = {In the realm of personalized recommendation systems, accurately capturing user preferences and item characteristics is important for delivering relevant and satisfying recommendations. This study introduces innovative approaches using Large Language Models (LLMs) to generate detailed textual descriptions that enhance both user and item representations. We propose a dual strategy: for user representation, we employ supervised fine-tuning coupled with Retrieval-Augmented Generation (RAG) to keep the model current with dynamic user preferences; for item representation, we leverage the extensive knowledge base of LLMs to enrich item descriptions and infer traits from user interactions. These methods promise a deeper, more nuanced understanding of both users and items, potentially leading to superior recommendation accuracy. We adopt a rigorous evaluation methodology, ensuring the reliability of our results and the effectiveness of our proposed system. This paper discusses these methodologies, presents our preliminary findings, and highlights the potential of text-augmented profiles in advancing recommendation systems.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{urban_demonstrating_2024,
  title = {Demonstrating {CAESURA},
  author = {Urban, Matthias and Binnig, Carsten},
  year = {2024},
  doi = {10.1145/3626246.3654732},
  url = {https://doi.org/10.1145/3626246.3654732},
  booktitle = {Companion of the 2024 {International},
  pages = {472--475},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago AA, Chile},
  keywords = {large language models, multi-modal, query planning},
  abstract = {In many domains, multi-modal data takes an important role and modern question-answering systems based on LLMs allow users to query this data using simple natural language queries. Retrieval Augmented Generation (RAG) is a recent approach that extends Large Language Models (LLM) with database technology to enable such multi-modal QA systems. In RAG, relevant data is first retrieved from a vector database and then fed into an LLM that computes the query result. However, RAG-based approaches have severe issues, such as regarding efficiency and scalability, since LLMs have high inference costs and can only process limited amounts of data. Therefore, in this demo paper, we propose CAESURA, a database-first approach that extends databases with LLMs. The main idea is that CAESURA utilizes the reasoning capabilities of LLMs to translate natural language queries into execution plans. Using such execution plans allows CAESURA to process multi-modal data outside the LLM using query operators and optimization strategies that are footed in scalable query execution strategies of databases. Our demo allows users to experience CAESURA on two example datasets containing tables, texts, and images1.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-0422-2},
}

@inproceedings{nadel_enabling_2024,
  title = {Enabling access to large-language models ({LLMs},
  author = {Nadel, Peter and Maloney, Delilah and Monahan, Kyle},
  year = {2024},
  doi = {10.1145/3626203.3670577},
  url = {https://doi.org/10.1145/3626203.3670577},
  booktitle = {Practice and {Experience},
  publisher = {Association for Computing Machinery},
  note = {event-place: Providence, RI, USA},
  keywords = {High-Performance Computing (HPC), Large-Language Models (LLMs), Open OnDemand (OOD)},
  abstract = {The use of language models, particularly large-language models (LLMs), have been increasingly popular and can be transformative in higher education, by both enabling novel research approaches and providing instructional opportunities for skills needed in data science and engineering. However, running these LLMs traditionally requires access to advanced hardware resources and technical knowledge. To better provide a platform for experimenting with LLMs for users of all skill levels, we developed the Tufts Technology Services (TTS) LLM-Hub, a series of example Jupyter notebooks served through Tufts Open OnDemand (OOD) to setup, configure, and run LLMs automatically. The TTS LLM-Hub enabled quick access to running LLMs, while reducing barriers to compute and enabling users to chat with an LLM in just four clicks. We have used these platforms for support of advanced data science courses, and to enable research computing at Tufts.},
  address = {New York, NY, USA},
  series = {{PEARC},
  isbn = {979-8-4007-0419-2},
}

@inproceedings{colombo_leveraging_2024,
  title = {Leveraging {Knowledge},
  author = {Colombo, Andrea},
  year = {2024},
  doi = {10.1145/3627673.3680268},
  url = {https://doi.org/10.1145/3627673.3680268},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5443--5446},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {graphrag, knowledge graph, large language models, laws, legislative systems},
  abstract = {Knowledge Graphs (KGs) have been used to organize large datasets into structured, interconnected information, enhancing data analytics across various fields. In the legislative context, one potential natural application of KGs is modeling the intricate set of interconnections that link laws and their articles with each other and the broader legislative context.At the same time, the rise of large language models (LLMs) such as GPT has opened new opportunities in legal applications, such as text generation and document drafting. Despite their potential, the use of LLMs in legislative contexts is critical since it requires the absence of hallucinations and reliance on up-to-date information, as new laws are published on a daily basis.This work investigates how Legislative Knowledge Graphs and LLMs can synergize and support legislative processes. We address three key questions: the benefits of using KGs for legislative systems, how LLM can support legislative activities by ensuring an accurate output, and how we can allow non-technical users to use such technologies in their activities. To this aim, we develop Legis AI Platform, an interactive platform focused on Italian legislation that enhances the possibility of conducting legislative analysis and that aims to support lawmaking activities.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{kuchenbuch_smart_2025,
  title = {Smart {Grid},
  author = {Kuchenbuch, René and Lehnhoff, Sebastian and Sauer, Jürgen},
  year = {2025},
  doi = {10.1145/3679240.3734601},
  url = {https://doi.org/10.1145/3679240.3734601},
  booktitle = {Proceedings of the 16th {ACM},
  pages = {495--504},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, IEC 62559 Use Cases, Natural Language Processing, Requirement Engineering, Smart Grid Architecture Model},
  abstract = {The IEC 62559 Use Case Methodology and Smart Grid Architecture Model (SGAM) Framework are crucial for fostering a shared understanding in the development of ICT-based power systems and their components within energy-related Requirements Engineering. However, discrepancies in interpretation among diverse stakeholders often diminish the quality of IEC 62559 Use Cases and SGAM Models, potentially leading to errors and costly setbacks in later project stages. This paper introduces the Smart Grid Assistive AI in Requirements Engineering (SGAAIRE), an intelligent system that leverages Large Language Models to enhance the quality of IEC 62559 Use Case descriptions and SGAM Models. Through a demonstration scenario featuring built-in quality defects, SGAAIRE is shown to effectively address common issues as an extension to a Use Case Management Repository. The proposed system supports the improvement of IEC 62559 Use Cases and SGAM Models, enabling the development of third-party tools aimed at advancing research in AI for energy-related requirements management.},
  address = {New York, NY, USA},
  series = {E-{Energy},
  isbn = {979-8-4007-1125-1},
}

@inproceedings{kim_cyberally_2025,
  title = {{CyberAlly},
  author = {Kim, Minjune and Wang, Jeff and Moore, Kristen and Goel, Diksha and Wang, Derui and Mohsin, Ahmad and Ibrahim, Ahmed and Doss, Robin and Camtepe, Seyit and Janicke, Helge},
  year = {2025},
  doi = {10.1145/3701716.3715171},
  url = {https://doi.org/10.1145/3701716.3715171},
  booktitle = {Companion {Proceedings},
  pages = {2851--2854},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {augmenting cyber defence, cyber incident response, human ai teaming},
  abstract = {The increasing frequency and sophistication of cyberattacks demand innovative approaches to strengthen defense capabilities. Training on live infrastructure poses significant risks to organizations, making secure, isolated cyber ranges an essential tool for conducting Red vs. Blue Team training events. These events enable security teams to refine their skills without impacting operational environments. While such training provides a strong foundation, the ever-evolving nature of cyber threats necessitates additional support for effective defense. To address this challenge, we introduce CyberAlly, a knowledge graph-enhanced AI assistant designed to enhance the efficiency and effectiveness of Blue Teams during incident response. Integrated into our cyber range alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks Blue Team actions, and suggests tailored mitigation recommendations based on insights from prior Red vs. Blue Team exercises. This demonstration highlights the feasibility and impact of CyberAlly in augmenting incident response and equipping defenders to tackle evolving threats with greater precision and confidence.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{khatua_evaluating_2025,
  title = {Evaluating {LLMs},
  author = {Khatua, Aparup and Kalmbach, Tobias and Mitra, Prasenjit and Sikdar, Sandipan},
  year = {2025},
  doi = {10.1145/3726302.3730190},
  url = {https://doi.org/10.1145/3726302.3730190},
  booktitle = {Proceedings of the 48th {International},
  pages = {2941--2945},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {source: ACM},
  abstract = {While LLMs have achieved impressive performance across various tasks, one under-explored area is evaluating their ability to follow instructions provided in the prompt when generating responses. In the context of question-answering (QA) tasks, a crucial research gap is whether LLMs prioritize their own parametric knowledge or the context provided in the prompt when generating an answer. Ignoring prompts, even when explicitly instructed to follow them, may adversely affect performance and potentially lead to unintended consequences. Additionally, LLMs should be self-reflective (i.e., LLMs should recognize when their knowledge is inadequate) and avoid hallucinations in such scenarios. To address our research question, we propose Oedipus, an evaluation framework to evaluate LLMs' ability to follow prompts. We further note that such abilities could also be influenced by contamination (i.e., exposure to datasets during training) and parametric knowledge. Consequently, we develop a novel QA dataset with four types of contexts- correct, masked, noisy, and absurd contexts with recent questions that LLMs are unlikely to have encountered in pre-training data or corpus and cannot be answered from parametric knowledge. We evaluate eight LLMs through our proposed evaluation framework and observe that LLMs often fail to follow instructions correctly and are not self-reflective.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1592-1},
  series = {{SIGIR},
}

@inproceedings{dieing_traditional_2025,
  title = {Traditional and {Deep},
  author = {Dieing, Thilo Ignaz},
  year = {2025},
  doi = {10.1145/3743049.3748572},
  url = {https://doi.org/10.1145/3743049.3748572},
  booktitle = {Proceedings of the {Mensch},
  pages = {749--754},
  publisher = {Association for Computing Machinery},
  keywords = {CAVAA, deep learning, Intent Classification, machine learning, NLP, safeguarding chatbots},
  abstract = {To enhance interactive political education, chatbots like Conversational Agent Voting Advice Applications (CAVAAs) are beginning to be powered by LLMs. While these systems offer open-domain conversational capabilities, they also pose risks when handling critical prompts involving toxic language or sensitive topics. This paper proposes intent classification as a method to safeguard political chatbots by distinguishing between safe and unsafe prompts. Using data from four international CAVAA studies and AI-generated inputs, four models — Random Forest, SVM, a DNN, and a CNN — were trained on fastText embeddings. The SVM achieved the highest F1 Macro of 0.955, outperforming all others. An explainability analysis confirmed that models learned meaningful indicators of critical intent. The findings support intent classification as a viable safety mechanism for political chatbots, while emphasizing the need for larger, real-world datasets, ethical considerations, and the need for scholars to further discuss what should be considered unsafe in this context.},
  address = {New York, NY, USA},
  series = {{MuC},
  isbn = {979-8-4007-1582-2},
}

@inproceedings{ranaut_text_2025,
  title = {Text {Obsoleteness},
  author = {Ranaut, Rishav and Saha, Sriparna and Jatowt, Adam and Gupta, Manish},
  year = {2025},
  doi = {10.1145/3726302.3730254},
  url = {https://doi.org/10.1145/3726302.3730254},
  booktitle = {Proceedings of the 48th {International},
  pages = {2827--2831},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {llm, multitask learning, text obsoleteness},
  abstract = {Maintaining accurate and up-to-date information is a persistent challenge for large-scale knowledge repositories, where outdated content can compromise their value. In this paper, we present a Multitask learning framework that uses Large Language Models (LLMs) for two tasks: semantic update detection and semantic update necessity prediction. The update detection task identifies obsoleteness by comparing older and newer text versions, while the update necessity prediction task determines whether an update is required based on a given context. To support these tasks, we curate a specialized dataset from Wikipedia called SEMUPDATES, focusing on frequently updated articles. Our experiments with five LLMs across four datasets in zero-shot, few-shot, and fine-tuned settings demonstrate that fine-tuning significantly enhances performance. In the multitask learning setup, Qwen delivers the best overall performance, while Mistral achieves the highest accuracy on individual tasks when fine-tuned separately. However, the performance differences across models are not substantial, suggesting that multiple LLMs can be effectively adapted for content update automation. These findings highlight the potential of LLMs in detecting and predicting obsolescence, providing a scalable solution for maintaining the timeliness of digital knowledge repositories.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{dewan_llm-driven_2025,
  title = {{LLM},
  author = {Dewan, Mouly and Liu, Jiqun and Shah, Chirag},
  year = {2025},
  doi = {10.1145/3726302.3730223},
  url = {https://doi.org/10.1145/3726302.3730223},
  booktitle = {Proceedings of the 48th {International},
  pages = {3055--3059},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information retrieval, llm evaluation, usefulness judgment, source: ACM},
  abstract = {In the information retrieval (IR) domain, evaluation plays a crucial role in optimizing search experiences and supporting diverse user intents. In the recent LLM era, research has been conducted to automate document relevance labels. These labels have traditionally been assigned by crowd-sourced workers, a process that is both time consuming and costly. This study focuses on LLM-generated usefulness labels, a crucial evaluation metric that considers the user's search intents and task objectives, an aspect where relevance falls short. Our experiment utilizes task-level, query-level, and document-level features along with user search behavior signals, which are essential in defining the usefulness of a document. Our research finds that (i) pre-trained LLMs can generate moderate usefulness labels by understanding the comprehensive search task session, and (ii) pre-trained LLMs perform better judgment in short search sessions when provided with search session contexts. Furthermore, we investigate whether LLMs can capture the unique divergence between relevance and usefulness, along with conducting an ablation study to identify the most critical metrics for accurate usefulness label generation. In conclusion, this work explores LLM-generated usefulness labels by evaluating critical metrics and optimizing for practicality in real-world settings.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{capari_sciencedirect_2024,
  title = {{ScienceDirect},
  author = {Capari, Artemis and Azarbonyad, Hosein and Tsatsaronis, Georgios and Afzal, Zubair and Dunham, Judson},
  year = {2024},
  doi = {10.1145/3626772.3661353},
  url = {https://doi.org/10.1145/3626772.3661353},
  booktitle = {Proceedings of the 47th {International},
  pages = {2976--2980},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {knowledge acquisition information retrieval, passage retrieval, scientific document processing},
  abstract = {From undergraduate students to renowned scholars, everyone occasionally encounters unknown concepts within their field of interest, especially when reading scientific articles. ScienceDirectTopic Pages (TP) are intended to facilitate learning and to provide users with a structured overview of sources to deepen their knowledge about such unfamiliar topics. Our free service provides insight into a vast set of technical topics across 20 different scientific domains. Designed to emulate the natural flow of learning, TPs are embedded within millions of articles so that users can click on unfamiliar concepts they come across whilst reading an article. This redirects the user to a TP, consisting of a definition of the concept, which provides the user with a basic understanding of the concept. The TP further presents a collection of relevant snippets extracted from books and review articles published by ScienceDirect for users interested in references and more detailed explanations and applications of the concept. Finally, a set of related topics is provided to extend the user's knowledge even further. To build TPs, we utilize various information retrieval methods across our product. We retrieve the most relevant snippets for each topic/concept using a semantic search model fine-tuned on our scientific database. We further leverage the power of Retrieval Augmented Generation to generate reliable definitions on the topics sourced from ScienceDirect's content. To retrieve a list of relevant concepts for each topic, we use the co-occurrence statistics of concepts within books and articles.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{isahagian_publish-subscribe_2023,
  title = {Publish-subscribe with large language models: {Improving},
  author = {Isahagian, Vatche and Muthusamy, Vinod and Slominski, Aleksander},
  year = {2023},
  doi = {10.1145/3626564.3629099},
  url = {https://doi.org/10.1145/3626564.3629099},
  booktitle = {Proceedings of the 24th {International},
  pages = {29--30},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bologna, Italy},
  keywords = {large language models, LLM, publish-subscribe},
  abstract = {Large Language Models (LLMs) have raised the expectations of users to be able to use natural language (NL) to interact with computer systems such as publish/subscribe systems. We introduce a design that extends existing pub/sub paradigms by adding support for NL queries to subscribe to publications of unstructured text messages. To make the system easier for users our system can also customize the notifications, using LLMs to generate targeted NL answers from the matching publications.},
  address = {New York, NY, USA},
  series = {Middleware '23},
  isbn = {979-8-4007-0429-1},
}

@inproceedings{abrami_vr-parlexplorer_2025,
  title = {{VR},
  author = {Abrami, Giuseppe and Bundan, Daniel and Manolis, Chrisowaladis and Mehler, Alexander},
  year = {2025},
  doi = {10.1145/3720553.3746672},
  url = {https://doi.org/10.1145/3720553.3746672},
  booktitle = {Proceedings of the 36th {ACM},
  pages = {177--183},
  publisher = {Association for Computing Machinery},
  keywords = {Chatbot, Hypertext, Parliamentary Speeches, RAG, VR-Interaction},
  abstract = {The enhanced visualization and interaction with information in collaborative VR environments enabled by chatbots is currently rather limited. To fill this gap and create a concrete application that combines spatial and virtual concepts of hypertext systems based on the use of LLMs, we present VR-ParlExplorer as a system for virtualizing plenary debates that allows users to interact with virtual members of parliament through chatbots. VR-ParlExplorer is implemented as a Plugin for Va.Si.Li-Lab to enable immersion in the dynamics of communication in parliamentary debates. The paper describes the functionality of VR-ParlExplorer and discusses specifics of the use case it addresses.},
  address = {New York, NY, USA},
  series = {{HT},
  isbn = {979-8-4007-1534-1},
}

@inproceedings{jiang_bridging_2024,
  title = {Bridging {Dictionary},
  author = {Jiang, Hang and Beeferman, Doug and Brannon, William and Heyward, Andrew and Roy, Deb},
  year = {2024},
  doi = {10.1145/3678884.3681820},
  url = {https://doi.org/10.1145/3678884.3681820},
  booktitle = {Companion {Publication},
  pages = {79--82},
  publisher = {Association for Computing Machinery},
  note = {event-place: San Jose, Costa Rica},
  keywords = {civic media, computational journalism, natural language processing, social polarization, text analysis, trust and human agency, source: ACM},
  abstract = {Words often carry different meanings for people from diverse backgrounds. Today's era of social polarization demands that we choose words carefully to prevent miscommunication, especially in political communication and journalism. To address this issue, we introduce the Bridging Dictionary, an interactive tool designed to illuminate how words are perceived by people with different political views. The Bridging Dictionary includes a static, printable document featuring 796 terms with summaries generated by a large language model. These summaries highlight how the terms are used distinctively by Republicans and Democrats. Additionally, the Bridging Dictionary offers an interactive interface that lets users explore selected words, visualizing their frequency, sentiment, summaries, and examples across political divides. We present a use case for journalists and emphasize the importance of human agency and trust in further enhancing this tool. The deployed version of Bridging Dictionary is available at https://dictionary.ccc-mit.org/.},
  address = {New York, NY, USA},
  series = {{CSCW},
  isbn = {979-8-4007-1114-5},
}

@inproceedings{parikh_echoguide_2024,
  title = {{EchoGuide},
  author = {Parikh, Vineet and Mahmud, Saif and Agarwal, Devansh and Li, Ke and Guimbretière, François and Zhang, Cheng},
  year = {2024},
  doi = {10.1145/3675095.3676611},
  url = {https://doi.org/10.1145/3675095.3676611},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {40--47},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {acoustic sensing, activity recognition, eating detection, foundation models},
  abstract = {Self-recording eating behaviors is a step towards a healthy lifestyle recommended by many health professionals. However, the current practice of manually recording eating activities using paper records or smartphone apps is often unsustainable and inaccurate. Smart glasses have emerged as a promising wearable form factor for tracking eating behaviors, but existing systems primarily identify when eating occurs without capturing details of the eating activities (E.g., what is being eaten). In this paper, we present EchoGuide, an application and system pipeline that leverages low-power active acoustic sensing to guide head-mounted cameras to capture egocentric videos, enabling efficient and detailed analysis of eating activities. By combining active acoustic sensing for eating detection with video captioning models and large-scale language models for retrieval augmentation, EchoGuide intelligently clips and analyzes videos to create concise, relevant activity records on eating. We evaluated EchoGuide with 9 participants in naturalistic settings involving eating activities, demonstrating high-quality summarization and significant reductions in video data needed, paving the way for practical, scalable eating activity tracking.},
  address = {New York, NY, USA},
  series = {{ISWC},
  isbn = {979-8-4007-1059-9},
}

@inproceedings{jia_agentir_2025,
  title = {{AgentIR},
  author = {Jia, Pengyue and Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
  year = {2025},
  doi = {10.1145/3726302.3730365},
  url = {https://doi.org/10.1145/3726302.3730365},
  booktitle = {Proceedings of the 48th {International},
  pages = {4180--4183},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {agent-based information retrieval, drl, llm},
  abstract = {Information retrieval (IR) systems are essential in modern society, aiding users to efficiently locate relevant information through query expansion, document retrieval, ranking, and re-ranking. User feedback from ranked outputs forms a dynamic interaction loop with IR systems, which can be modeled as either one-time or sequential decision-making problems. Over the past decade, deep reinforcement learning (DRL) has emerged as a promising approach to decision-making, leveraging the high model capacity of deep learning for complex tasks. While significant research has explored the application of DRL to IR tasks, several fundamental challenges remain underexplored, including the underlying information theory in DRL settings, the limitations of reinforcement learning methods for industrial IR applications, and the simulation of DRL-based IR systems. Concurrently, the advent of large language models (LLMs) has introduced new opportunities for optimizing and simulating IR systems. Building on the success of the Agent-based IR Workshop at SIGIR 2024, we propose hosting the second Agent-based IR Workshop at SIGIR 2025. This workshop will continue to provide a platform for researchers and practitioners from academia and industry to present cutting-edge advances in DRL-based and LLM-based IR systems from an agent-based perspective. By building on the foundation laid in the first workshop, the 2025 edition aims to delve deeper into emerging research challenges, foster collaborations, and explore innovative applications. Through engaging discussions and insightful presentations, the workshop seeks to further expand the boundaries of IR research and solidify its role as a premier venue for advancing agent-based IR systems.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{cheng_sqlord_2025,
  title = {{SQLord},
  author = {Cheng, Song and Cheng, Qiannan and Jin, Linbo and Yi, Lei and Zhang, Guannan},
  year = {2025},
  doi = {10.1145/3701716.3715541},
  url = {https://doi.org/10.1145/3701716.3715541},
  booktitle = {Companion {Proceedings},
  pages = {919--923},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {llm, text to sql, workflow generation},
  abstract = {Transforming natural language into SQL queries (NL2SQL) is crucial for data-driven business applications. Existing frameworks, trained on open-source datasets, struggle with complex business logic and lack domain-specific data for fine-tuning. Additionally, evaluation methods often require annotated data and executable database environments, which are scarce in real-world scenarios. To address these challenges, we propose SQLord, an enterprise-level NL2SQL framework. First, SQLord introduces a data reverse generation approach to convert raw SQL statements into annotated data for supervised fine-tuning (SFT). Second, it proposes a decomposition method for complex queries using an automated workflow generator. Additionally, SQLord features a comprehensive GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios. Offline tests significantly outperform state-of-the-art baselines, and online accuracy consistently exceeds 90, highlighting SQLord's advantages and effectiveness in complex real-world scenarios. SQLord has been successfully applied across multiple scenarios on the world's largest B2B e-commerce platform.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{fleshman_re-adaptir_2025,
  title = {{RE},
  author = {Fleshman, William and Van Durme, Benjamin},
  year = {2025},
  doi = {10.1145/3726302.3730240},
  url = {https://doi.org/10.1145/3726302.3730240},
  booktitle = {Proceedings of the 48th {International},
  pages = {2632--2636},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {adapter-tuning, fine-tuning, llm, lora, neural ir},
  abstract = {Large language models (LLMs) fine-tuned for text-retrieval have demonstrated state-of-the-art results across several information retrieval (IR) benchmarks. However, supervised training for improving these models requires numerous labeled examples, which are generally unavailable or expensive to acquire. In this work, we explore the effectiveness of extending reverse engineered adaptation to the context of information retrieval (RE-AdaptIR). We use RE-AdaptIR to improve LLM-based IR models using only unlabeled data. We demonstrate improved performance in both training domains and in zero-shot domains where the models have seen no queries. We analyze performance changes in various fine-tuning scenarios and offer findings of immediate use to IR practitioners.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{arabzadeh_human-ai_2025,
  title = {A {Human},
  author = {Arabzadeh, Negar and Clarke, Charles L.A.},
  year = {2025},
  doi = {10.1145/3726302.3730159},
  url = {https://doi.org/10.1145/3726302.3730159},
  booktitle = {Proceedings of the 48th {International},
  pages = {2784--2788},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {evaluation, large language models, relevance judgments, source: ACM},
  abstract = {Large Language Models (LLMs) are increasingly used to automate relevance judgments for information retrieval (IR) tasks, often demonstrating agreement with human labels that approaches inter-human agreement. To assess the robustness and reliability of LLM-based relevance judgments, we systematically investigate impact of prompt sensitivity on the task. We collected prompts for relevance assessment from 15 human experts and 15 LLMs across three tasks-binary, graded, and pairwise-yielding 90 prompts in total. We compare LLM-generated labels with TREC official human labels using Cohen's κ and pairwise agreement measures. In addition, we compare human- and LLM-generated prompts and analyze differences among different LLMs as judges. We release all data and prompts at https://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{di_sipio_use_2024,
  title = {On the use of {LLMs},
  author = {Di Sipio, Claudio and Rubei, Riccardo and Di Rocco, Juri and Di Ruscio, Davide and Iovino, Ludovico},
  year = {2024},
  doi = {10.1145/3652620.3687808},
  url = {https://doi.org/10.1145/3652620.3687808},
  booktitle = {Proceedings of the {ACM},
  pages = {596--601},
  publisher = {Association for Computing Machinery},
  note = {event-place: Linz, Austria},
  keywords = {domain-specific languages, large language models, model driven engineering, source: ACM},
  abstract = {In Model-Driven Engineering (MDE), domain-specific modeling languages (DSMLs) play a key role to model systems within specific application domains. Creating DSMLs is a complex, iterative process requiring input from both domain specialists and technical experts. This process often involves developing language artifacts, including syntax, semantics, and supporting environments, to gather feedback and achieve consensus among stakeholders.To facilitate the interaction between technical experts and domain specialists in the creation of new DSMLs, we propose using large language models for the specific task of supporting the requirement elicitation of language semantics. Our approach aims to reduce the time needed to develop proof-of-concept implementations, facilitating quicker agreement on the language's intended functions. Once consensus is reached, traditional technologies can be employed to develop the semantics of the agreed language. This method aims to mitigate potential misunderstandings that can arise during interactions between technical specialists and domain experts. As an initial investigation of this idea, we explore the model mutation problem as a case study. We developed a custom GPT model, named MuTaGENe, designed to support the definition of model mutations and tested it against the existing Wodel language.},
  address = {New York, NY, USA},
  series = {{MODELS},
  isbn = {979-8-4007-0622-6},
}

@inproceedings{ferrato_exploring_2025,
  title = {Exploring the {Potential},
  author = {Ferrato, Alessio and Limongelli, Carla and Gasparetti, Fabio and Sansonetti, Giuseppe and Micarelli, Alessandro},
  year = {2025},
  doi = {10.1145/3708319.3733648},
  url = {https://doi.org/10.1145/3708319.3733648},
  booktitle = {Adjunct {Proceedings},
  pages = {432--436},
  publisher = {Association for Computing Machinery},
  keywords = {Cultural Heritage, Large Language Models, Personalization, Visual Question Answering},
  abstract = {This paper investigates the application of a Multimodal Large Language Model to enhance visitor experiences in cultural heritage settings through Visual Question Answering (VQA) and Contextual Question Answering (CQA). We evaluate the zero-shot capabilities of LLaVA-7b (Large Language and Vision Assistant) on QA using the AQUA dataset. We assess how effectively it can answer questions about artwork, visual content, and contextual information through three experimental approaches. Our findings reveal that LLaVA demonstrates promising performance on visual questions, outperforming previous baselines but facing challenges with questions requiring contextual understanding. The selective knowledge integration approach showed the best overall performance, suggesting an efficient knowledge retrieval systems could enhance performance. Moreover, we show how to exploit such models to provide correct personalized answers using a well-established visitor model.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1399-6},
}

@inproceedings{kemper_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Kemper, Sara and Cui, Justin and Dicarlantonio, Kai and Lin, Kathy and Tang, Danjie and Korikov, Anton and Sanner, Scott},
  year = {2024},
  doi = {10.1145/3626772.3657670},
  url = {https://doi.org/10.1145/3626772.3657670},
  booktitle = {Proceedings of the 47th {International},
  pages = {2786--2790},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {conversational recommendation, dialogue state tracking, llm},
  abstract = {Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., "I'm watching my weight”). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., "classy joint for a date”). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{wheeler_procedural_2025,
  title = {Procedural {Memory},
  author = {Wheeler, Schaun and Jeunen, Olivier},
  year = {2025},
  doi = {10.1145/3708319.3734172},
  url = {https://doi.org/10.1145/3708319.3734172},
  booktitle = {Adjunct {Proceedings},
  pages = {360--364},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Large Language Models (LLMs) represent a landmark achievement in Artificial Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks such as text generation, code completion, and conversational coherence. These capabilities stem from their architecture, which mirrors human procedural memory—the brain’s ability to automate repetitive, pattern-driven tasks through practice. However, as LLMs are increasingly deployed in real-world applications, it becomes impossible to ignore their limitations operating in complex, unpredictable environments. This paper argues that LLMs, while transformative, are fundamentally constrained by their reliance on procedural memory. To create agents capable of navigating “wicked” learning environments—where rules shift, feedback is ambiguous, and novelty is the norm—we must augment LLMs with semantic memory and associative learning systems. By adopting a modular architecture that decouples these cognitive functions, we can bridge the gap between narrow procedural expertise and the adaptive intelligence required for real-world problem-solving.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1399-6},
}

@inproceedings{zhang_ai_2024,
  title = {{AI},
  author = {Zhang, Yongfeng and Liu, Zhiwei and Wen, Qingsong and Pang, Linsey and Liu, Wei and Yu, Philip S.},
  year = {2024},
  doi = {10.1145/3627673.3680120},
  url = {https://doi.org/10.1145/3627673.3680120},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5605--5607},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {ai agent, information retrieval, large language models (llms), recommender systems},
  abstract = {The field of information retrieval has significantly transformed with the integration of AI technologies. AI agents, especially those leveraging LLMs and vast computational power, have revolutionized information retrieval, processing, and presentation. LLM agents, with advanced memory, reasoning, and planning capabilities, can perform complex tasks, engage in coherent conversations, and provide personalized responses. Despite these advancements, challenges such as ensuring relevance and accuracy, mitigating biases, providing real-time responses, and maintaining data security remain. This workshop aims to explore these challenges, share innovative solutions, and discuss future directions. It will provide a platform to bring together researchers, practitioners to discuss the latest theoretical advancements and practical implementations of AI agents in information retrieval. Topics include AI in search, recommendation, and personalization systems. By gathering a diverse group of experts, the workshop seeks to deepen the understanding of AI agents in information retrieval, advance the field, and enhance its societal impact. Participants will gain insights into cutting-edge research, emerging trends, and foster knowledge exchange and collaboration within the community.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{zhang_agentfm_2025,
  title = {{AgentFM},
  author = {Zhang, Lingzhe and Zhai, Yunpeng and Jia, Tong and Huang, Xiaosong and Duan, Chiming and Li, Ying},
  year = {2025},
  doi = {10.1145/3696630.3728492},
  url = {https://doi.org/10.1145/3696630.3728492},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {525--529},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {distributed databases, failure management, multi agents, source: ACM},
  abstract = {Distributed databases are critical infrastructures for today's large-scale software systems, making effective failure management essential to ensure software availability. However, existing approaches often overlook the role distinctions within distributed databases and rely on small-scale models with limited generalization capabilities. In this paper, we conduct a preliminary empirical study to emphasize the unique significance of different roles. Building on this insight, we propose AgentFM, a role-aware failure management framework for distributed databases powered by LLM-driven multi-agents. AgentFM addresses failure management by considering system roles, data roles, and task roles, with a meta-agent orchestrating these components. Preliminary evaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and open new directions for further research.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{hillebrand_improving_2023,
  title = {Improving {Zero},
  author = {Hillebrand, Lars and Berger, Armin and Deußer, Tobias and Dilmaghani, Tim and Khaled, Mohamed and Kliem, Bernd and Loitz, Rüdiger and Pielka, Maren and Leonhard, David and Bauckhage, Christian and Sifa, Rafet},
  year = {2023},
  doi = {10.1145/3573128.3609344},
  url = {https://doi.org/10.1145/3573128.3609344},
  booktitle = {Proceedings of the {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Limerick, Ireland},
  keywords = {Large Language Models, Recommender System, Text Matching},
  abstract = {Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.},
  address = {New York, NY, USA},
  series = {{DocEng},
  isbn = {979-8-4007-0027-9},
}

@inproceedings{kejriwal_commonsense_2025,
  title = {Commonsense {AI},
  author = {Kejriwal, Mayank and McGuinness, Deborah L. and Lieberman, Henry},
  year = {2025},
  doi = {10.1145/3701716.3716841},
  url = {https://doi.org/10.1145/3701716.3716841},
  booktitle = {Companion {Proceedings},
  pages = {837--840},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {conceptnet, cyc, llms, machine common sense},
  abstract = {Machine common sense (MCS)-the challenge of enabling computers to grasp everyday human knowledge-has been a grand challenge in Artificial Intelligence (AI) since the 1950s. While recent advances in large language models have led to impressive progress, there is still no consensus on how much common sense today's AI actually possesses. In this brief review, we revisit the historical development of MCS in the context of the Web, examining how the Web's evolution-from early knowledge representation efforts to knowledge graphs, the Semantic Web, and crowdsourcing-has shaped MCS research. We argue that key breakthroughs in Web technologies were instrumental in addressing longstanding challenges of scale and coverage in commonsense reasoning. At the same time, MCS research has influenced the development of core Web applications, including intelligent agents, plausibility-based reasoning, and robust evaluation of black-box AI systems.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{luo_oneke_2025,
  title = {{OneKE},
  author = {Luo, Yujie and Ru, Xiangyuan and Liu, Kangwei and Yuan, Lin and Sun, Mengshu and Zhang, Ningyu and Liang, Lei and Zhang, Zhiqiang and Zhou, Jun and Wei, Lanning and Zheng, Da and Wang, Haofen and Chen, Huajun},
  year = {2025},
  doi = {10.1145/3701716.3715189},
  url = {https://doi.org/10.1145/3701716.3715189},
  booktitle = {Companion {Proceedings},
  pages = {2871--2874},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {information extraction, large language models, natural language processing, source: ACM},
  abstract = {We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE's efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. The code can be accessed on both GitHub1 and Zenodo2M, along with a video demonstration3.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{xu_geo-llava_2024,
  title = {Geo-{LLaVA},
  author = {Xu, Shihao and Luo, Yiyang and Shi, Wei},
  year = {2024},
  doi = {10.1145/3688866.3689124},
  url = {https://doi.org/10.1145/3688866.3689124},
  booktitle = {Proceedings of the 2nd {Workshop},
  pages = {11--15},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {geometry problem solving, in-context learning, large multimodal model, rag},
  abstract = {Geometry mathematics problems pose significant challenges for large language models (LLMs) because they involve visual elements and spatial reasoning. Current methods primarily rely on symbolic character awareness to address these problems. Considering geometry problem solving is a relatively nascent field with limited suitable datasets and currently almost no work on solid geometry problem solving, we collect a geometry question-answer dataset by sourcing geometric data from Chinese high school education websites, referred to as GeoMath. It contains solid geometry questions and answers with accurate reasoning steps as compensation for existing plane geometry datasets. Additionally, we propose a Large Multi-modal Model (LMM) framework named Geo-LLaVA, which incorporates retrieval augmentation with supervised fine-tuning (SFT) in the training stage, called meta-training, and employs in-context learning (ICL) during inference to improve performance. Our fine-tuned model with ICL attains the state-of-the-art performance of 65.25\% and 42.36\% on selected questions of the GeoQA dataset and GeoMath dataset respectively with proper inference steps. Notably, our model initially endows the ability to solve solid geometry problems and supports the generation of reasonable solid geometry picture descriptions and problem-solving steps. Our research sets the stage for further exploration of LLMs in multi-modal math problem-solving, particularly in geometry math problems.},
  address = {New York, NY, USA},
  series = {{LGM3A},
  isbn = {979-8-4007-1193-0},
}

@inproceedings{garcia_mesa_scalable_2025,
  title = {Scalable {Patent},
  author = {García Mesa, Juan José},
  year = {2025},
  doi = {10.1145/3708035.3736071},
  url = {https://doi.org/10.1145/3708035.3736071},
  booktitle = {Practice and {Experience},
  publisher = {Association for Computing Machinery},
  keywords = {function calling, Large language models, patent classification},
  abstract = {User requests often foment technological innovation, prompting transformative change across research and functional areas. At Arizona State University, a collaboration with their intellectual property management services led to the development of an automated tool that addresses the growing need for efficient patent classification and analysis. The system leverages large language models integrated with function calling to retrieve, clean, and analyze patent data from the US Patent and Trademark Office data portal. Key functionalities include extracting university-assigned patents, classifying them using a streamlined version of the official code hierarchy, and executing targeted queries such as retrieving detailed patent information or identifying patents related to specific subject matters. This work contributes to more efficient patent management processes and serves as a blueprint for applying similar methodologies to related problems. This approach combines conventional data analysis with dynamic function calling, mitigating common artificial intelligence challenges such as hallucinations and providing accurate outputs.},
  address = {New York, NY, USA},
  series = {{PEARC},
  isbn = {979-8-4007-1398-9},
}

@inproceedings{zeng_combining_2024,
  title = {Combining {Large},
  author = {Zeng, Xia and La Barbera, David and Roitero, Kevin and Zubiaga, Arkaitz and Mizzaro, Stefano},
  year = {2024},
  doi = {10.1145/3626772.3657965},
  url = {https://doi.org/10.1145/3626772.3657965},
  booktitle = {Proceedings of the 47th {International},
  pages = {2332--2336},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {ai-crowdsourcing integration, llm, misinformation detection},
  abstract = {Research on misinformation detection has primarily focused either on furthering Artificial Intelligence (AI) for automated detection or on studying humans' ability to deliver an effective crowdsourced solution. Each of these directions however shows different benefits. This motivates our work to study hybrid human-AI approaches jointly leveraging the potential of large language models and crowdsourcing, which is understudied to date. We propose novel combination strategies Model First, Worker First, and Meta Vote, which we evaluate along with baseline methods such as mean, median, hard- and soft-voting. Using 120 statements from the PolitiFact dataset, and a combination of state-of-the-art AI models and crowdsourced assessments, we evaluate the effectiveness of these combination strategies. Results suggest that the effectiveness varies with scales granularity, and that combining AI and human judgments enhances truthfulness assessments' effectiveness and robustness.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{wang_wildlifelookup_2025,
  title = {{WildlifeLookup},
  author = {Wang, Xiangqi and Yang, Tianyu and Rohr, Jason and Scheffers, Brett and Chawla, Nitesh and Zhang, Xiangliang},
  year = {2025},
  doi = {10.1145/3701551.3704121},
  url = {https://doi.org/10.1145/3701551.3704121},
  booktitle = {Proceedings of the {Eighteenth},
  pages = {1064--1067},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hannover, Germany},
  keywords = {large language models (llms), retrieval-augment generation (rag), wildlife knowledge network},
  abstract = {Wildlife management is increasingly reliant on data-driven insights to address the impacts of climate change on species and ecosystems. However, the complexity of accessing and querying large, multimodal datasets often limits the ability of non-technical users, such as wildlife managers and conservationists, to make informed decisions. To address this challenge, we present WildlifeLookup, a public accessible, intelligent chatbot designed to facilitate natural language interaction with a novel knowledge graph (KN-Wildlife) that houses critical wildlife and environmental data. WildlifeLookup simplifies access to species distributions, habitat interactions, and climate-related events by converting user queries into precise graph queries, reducing the technical barriers for end users. The chatbot WildlifeLookup is available at https://oknbot.ngrok.dev/},
  address = {New York, NY, USA},
  series = {{WSDM},
  isbn = {979-8-4007-1329-3},
}

@inproceedings{pathiyan_cherumanal_towards_2024,
  title = {Towards {Investigating},
  author = {Pathiyan Cherumanal, Sachin and Scholer, Falk and Trippas, Johanne R and Spina, Damiano},
  year = {2024},
  doi = {10.1145/3686215.3690156},
  url = {https://doi.org/10.1145/3686215.3690156},
  booktitle = {Companion {Proceedings},
  pages = {61--66},
  publisher = {Association for Computing Machinery},
  note = {event-place: San Jose, Costa Rica},
  keywords = {audio output, bias, conversational search, information retrieval},
  abstract = {Voice-based systems like Amazon Alexa, Google Assistant, and Apple Siri, along with the growing popularity of OpenAI’s ChatGPT and Microsoft’s Copilot, serve diverse populations, including visually impaired and low-literacy communities. This reflects a shift in user expectations from traditional search to more interactive question-answering models. However, presenting information effectively in voice-only channels remains challenging due to their linear nature. This limitation can impact the presentation of complex queries involving controversial topics with multiple perspectives. Failing to present diverse viewpoints may perpetuate or introduce biases and affect user attitudes. Balancing information load and addressing biases is crucial in designing a fair and effective voice-based system. To address this, we (i) review how biases and user attitude changes have been studied in screen-based web search, (ii) address challenges in studying these changes in voice-based settings like SCS, (iii) outline research questions, and (iv) propose an experimental setup with variables, data, and instruments to explore biases in a voice-based setting like Spoken Conversational Search.},
  address = {New York, NY, USA},
  series = {{ICMI},
  isbn = {979-8-4007-0463-5},
}

@inproceedings{zhang_edu-values_2025,
  title = {Edu-{Values},
  author = {Zhang, Peiyi and Zhang, Yazhou and Wang, Bo and Rong, Lu and Tiwari, Prayag and Qin, Jing},
  year = {2025},
  doi = {10.1145/3701716.3715518},
  url = {https://doi.org/10.1145/3701716.3715518},
  booktitle = {Companion {Proceedings},
  pages = {1519--1523},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {educational benchmark, large language models, values alignment, source: ACM},
  abstract = {In this paper, we present Edu-Values, the first Chinese education values evaluation benchmark that includes seven core values: professional philosophy, teachers' professional ethics, education laws and regulations, cultural literacy, educational knowledge and skills, basic competencies and subject knowledge. We meticulously design 1,418 questions, covering multiple-choice, multi-modal question answering, subjective analysis, adversarial prompts, and Chinese traditional culture (short answer) questions. We conduct human feedback based automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight three main findings: (1) due to differences in educational culture, Chinese LLMs outperform English LLMs, with Qwen 2 ranking the first with a score of 81.37; (2) LLMs often struggle with teachers' professional ethics and professional philosophy; (3) leveraging Edu-Values to build an external knowledge repository for RAG significantly improves LLMs' alignment. This demonstrates the effectiveness of the proposed benchmark.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{liu_multimodal_2025,
  title = {Multimodal {Intent},
  author = {Liu, Junwen and Huang, Haikuan and Huang, Gang and Ge, Shuang and Hu, Jinlian},
  year = {2025},
  doi = {10.1145/3701716.3719642},
  url = {https://doi.org/10.1145/3701716.3719642},
  booktitle = {Companion {Proceedings},
  pages = {3008--3011},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {data augmentation, e-commerce customer service, model fusion, multimodal intent recognition, RAG, vision-language models},
  abstract = {The "Multimodal Dialogue System Intent Recognition Challenge", jointly organized by Alibaba Taobao and Tmall Group and the World Wide Web Conference (WWW), focuses on multimodal intent recognition in e-commerce scenarios, aiming to address technical challenges in joint understanding of images and text for customer service applications. During the preliminary round, over 1,500 teams participated, with 11 advancing to the semi-finals and 9 ultimately presenting in the final. Innovative approaches from participants centered on three key directions: multimodal data augmentation (e.g., synthetic sample generation, image-text co-augmentation), model optimization (discriminative fine-tuning, model soup fusion, etc.), and prompt engineering. Significantly, the top three teams elevated the weighted F1-score from a baseline of 0.78 to above 0.9. This improvement was achieved by incorporating a diverse set of techniques, including but not limited to vision-language models, structure-aware retrieval, and hierarchical label optimization. The competition outcomes validate the potential of lightweight models in data-scarce scenarios and provide open-source technical pathways for applying multimodal large language models to e-commerce customer service. These advancements drive progress in fine-grained semantic comprehension, domain adaptation, and efficient inference, offering valuable insights for the industrial deployment of intelligent customer service systems.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{ni_reliable_2024,
  title = {Reliable {Knowledge},
  author = {Ni, Bo},
  year = {2024},
  doi = {10.1145/3627673.3680266},
  url = {https://doi.org/10.1145/3627673.3680266},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5463--5466},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {knowledge graph, question answering, trustworthy AI, uncertainty quantification},
  abstract = {Recently, Knowledge Graphs (KGs) have been successfully coupled with Large Language Models (LLMs) to mitigate their hallucinations and enhance their reasoning capability, e.g., KG-based retrieval-augmented framework for question-answering. However, current KG-LLM frameworks lack rigorous uncertainty estimation, limiting their reliable deployment in high-stake applications where the cost of errors is significant. To address this crucial gap, we propose a new trustworthy KG-LLM framework, UaG(\&lt;u\&gt;U\&lt;/u\&gt;ncertainty \&lt;u\&gt;A\&lt;/u\&gt;ware \&lt;u\&gt;G\&lt;/u\&gt;raph Reasoning), which incorporates uncertainty quantification into the KG-LLM framework. We design an uncertainty-aware multi-step reasoning framework that leverages conformal prediction to provide a theoretical guarantee on the prediction set. To manage the error rate of the multi-step process, we additionally introduce an error rate control module to adjust the error rate within the individual components. Our preliminary results demonstrate that UaG can achieve the desired theoretical coverage while maintaining a reasonable prediction set size.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{rashedul_hasan_muse_2025,
  title = {{MUSE},
  author = {Rashedul Hasan, Mohammad},
  year = {2025},
  doi = {10.1145/3716553.3750735},
  url = {https://doi.org/10.1145/3716553.3750735},
  booktitle = {Proceedings of the 27th {International},
  pages = {699--705},
  publisher = {Association for Computing Machinery},
  keywords = {cognitive psychology, human experience modeling, longitudinal data, multimodal learning, neuroscience, psychoanalysis, vision-language model},
  abstract = {Realizing truly capable personal AI for health and education requires effectively modeling complex longitudinal experiential (LE) data. Unlike standard datasets, LE data from human experience is inherently multifaceted, dynamic, and contextual. Current AI approaches struggle with this complexity due to three critical gaps rooted in differences from human cognition: insufficient multimodal processing, lack of generative perception, and inadequate symbolic order contextualization. Drawing upon interdisciplinary insights, our blue sky vision, the MUSE (Multimodal, generative, and Symbolic framEwork), proposes to bridge these gaps. By integrating multimodal representations addressing LE data’s structure, generative simulation capturing “what-if” dynamics, and symbolic order grounding for context, MUSE aims for a profound, contextually aware understanding of individual experience, essential for robust inference and enabling advanced personal AI applications.},
  address = {New York, NY, USA},
  series = {{ICMI},
  isbn = {979-8-4007-1499-3},
}

@inproceedings{vedula_question_2024,
  title = {Question {Suggestion},
  author = {Vedula, Nikhita and Rokhlenko, Oleg and Malmasi, Shervin},
  year = {2024},
  doi = {10.1145/3626772.3661371},
  url = {https://doi.org/10.1145/3626772.3661371},
  booktitle = {Proceedings of the 47th {International},
  pages = {2960--2964},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {conversational shopping assistants, product question suggestion},
  abstract = {Digital assistants have become ubiquitous in e-commerce applications, following the recent advancements in Information Retrieval (IR), Natural Language Processing (NLP) and Generative Artificial Intelligence (AI). However, customers are often unsure or unaware of how to effectively converse with these assistants to meet their shopping needs. In this work, we emphasize the importance of providing customers a fast, easy to use, and natural way to interact with conversational shopping assistants. We propose a framework that employs Large Language Models (LLMs) to automatically generate contextual, useful, answerable, fluent and diverse questions about products, via in-context learning and supervised fine-tuning. Recommending these questions to customers as helpful suggestions or hints to both start and continue a conversation can result in a smoother and faster shopping experience with reduced conversation overhead and friction. We perform extensive offline evaluations, and discuss in detail about potential customer impact, and the type, length and latency of our generated product questions if incorporated into a real-world shopping assistant.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{petruzzelli_recommending_2024,
  title = {Recommending {Healthy},
  author = {Petruzzelli, Alessandro and Musto, Cataldo and Di Carlo, Michele Ciro and Tempesta, Giovanni and Semeraro, Giovanni},
  year = {2024},
  doi = {10.1145/3640457.3688193},
  url = {https://doi.org/10.1145/3640457.3688193},
  booktitle = {Proceedings of the 18th {ACM},
  pages = {1057--1061},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bari, Italy},
  keywords = {Food Recommendation, Health-aware Recommender Systems, Large Language Models, Sustainability},
  abstract = {Given the rising global concerns about healthy nutrition and environmental sustainability, individuals need more and more support in making good choices concerning their daily meals. To this end, in this paper we introduce HeaSE, a framework for Healthy And Sustainable Eating. Given an input recipe, HeaSE identifies healthier and more sustainable meals by exploiting retrieval techniques and large language models. The framework works in two steps. First, it uses food retrieval strategies based on macro-nutrient information to identify candidate alternative meals. This ensures that the substitutions maintain a similar nutritional profile. Next, HeaSE employs large language models to re-rank these potential replacements while considering factors beyond just nutrition, such as the recipe’s environmental impact. In the experimental evaluation, we showed the capabilities of LLMs in identifying more sustainable and healthier alternatives within a set of candidate options. This highlights the potential of these models to guide users towards food choices that are both nutritious and environmentally responsible.},
  address = {New York, NY, USA},
  series = {{RecSys},
  isbn = {979-8-4007-0505-2},
}

@inproceedings{benedict_gen-ir_2024,
  title = {Gen-{IR},
  author = {Bénédict, Gabriel and Zhang, Ruqing and Metzler, Donald and Yates, Andrew and Jiang, Ziyan},
  year = {2024},
  doi = {10.1145/3626772.3657982},
  url = {https://doi.org/10.1145/3626772.3657982},
  booktitle = {Proceedings of the 47th {International},
  pages = {3029--3032},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {generative models, information retrieval, large language models},
  abstract = {Generative information retrieval (Gen-IR) is a fast-growing interdisciplinary research area that investigates how to leverage advances in generative Artificial Intelligence (AI) to improve information retrieval systems. Gen-IR has attracted interest from the information retrieval, natural language processing, and machine learning communities, among others. Since the dawn of Gen-IR last year, there has been an explosion of Gen-IR systems that have launched and are now widely used. Interest in this area across academia and industry is only expected to continue to grow as new research challenges and application opportunities arise. The goal of this proposed workshop, The Second Workshop on Generative Information Retrieval (Gen-IR @ SIGIR 2024) is to provide an interactive venue for exploring a broad range of foundational and applied Gen-IR research. The workshop will focus on tasks such as generative document retrieval, grounded answer generation, generative recommendation, and generative knowledge graphs, all through the lens of model training, model behavior, and broader issues. The workshop will be highly interactive, favoring panel discussions, poster sessions, and roundtable discussions over one-sided keynotes and paper talks.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{luo_evolutionagent_2025,
  title = {{EvolutionAgent},
  author = {Luo, Junhui and Song, Li and Sun, Ranrun},
  year = {2025},
  doi = {10.1145/3701716.3719226},
  url = {https://doi.org/10.1145/3701716.3719226},
  booktitle = {Companion {Proceedings},
  pages = {2973--2977},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {closed-loop adaptation, llm agent, llm-based simulation, self-evolving framework},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in simulating user behavior, offering significant potential for user modeling, behavior analysis, interest matching, and the extraction of unstructured features. However, the process of user simulation necessitates the identification of both unstructured and structured features, as well as the formulation of multi-stages workflow, which remains a challenging and labor-intensive task. To address this, we propose EvolutionAgent , a novel framework grounded in a discretized material repository, which employs self-reflective and evolutionary mechanisms to automate the search for unstructured features and the generation of simulation workflow. This framework establishes an efficient and robust simulation mechanism, achieving state-of-the-art performance across three distinct datasets. Notably, EvolutionAgent exhibits exceptional robustness, even in scenarios with limited historical data for users and products, underscoring its adaptability and reliability.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{kapuriya_exploring_2025,
  title = {Exploring the {Role},
  author = {Kapuriya, Janak and Kaushik, Manit and Ganguly, Debasis and Bhatia, Sumit},
  year = {2025},
  doi = {10.1145/3726302.3730194},
  url = {https://doi.org/10.1145/3726302.3730194},
  booktitle = {Proceedings of the 48th {International},
  pages = {2962--2966},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {diversity, in-context learning, large language models},
  abstract = {In-Context Learning (ICL) has gained prominence due to its ability to perform tasks without requiring extensive training data and its robustness to noisy labels. A typical ICL workflow involves selecting localized examples relevant to a given input using sparse or dense embedding-based similarity functions. However, relying solely on similarity-based selection may introduce topical biases in the retrieved contexts, potentially leading to suboptimal downstream performance. We posit that reranking the retrieved context to enhance topical diversity can improve downstream task performance. To achieve this, we leverage maximum marginal relevance (MMR) which balances topical similarity with inter-example diversity. Our experimental results demonstrate that diversifying the selected examples leads to consistent improvements in downstream performance across various context sizes and similarity functions. The implementation of our approach is made available at https://github.com/janak11111/Diverse-ICL.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_mememo_2024,
  title = {{MeMemo},
  author = {Wang, Zijie J. and Chau, Duen Horng},
  year = {2024},
  doi = {10.1145/3626772.3657662},
  url = {https://doi.org/10.1145/3626772.3657662},
  booktitle = {Proceedings of the 47th {International},
  pages = {2765--2770},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {large language models, neural information retrieval, on-device, source: Scopus},
  abstract = {Retrieval-augmented text generation (RAG) addresses the common limitations of large language models (LLMs), such as hallucination, by retrieving information from an updatable external knowledge base. However, existing approaches often require dedicated backend servers for data storage and retrieval, thereby limiting their applicability in use cases that require strict data privacy, such as personal finance, education, and medicine. To address the pressing need for client-side dense retrieval, we introduce MeMemo, the first open-source JavaScript toolkit that adapts the state-of-the-art approximate nearest neighbor search technique HNSW to browser environments. Developed with modern and native Web technologies, such as IndexedDB and Web Workers, our toolkit leverages client-side hardware capabilities to enable researchers and developers to efficiently search through millions of high-dimensional vectors in the browser. MeMemo enables exciting new design and research opportunities, such as private and personalized content creation and interactive prototyping, as demonstrated in our example application RAG Playground. Reflecting on our work, we discuss the opportunities and challenges for on-device dense retrieval. MeMemo is available at https://github.com/poloclub/mememo.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
  annote = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{li_graph-augmented_2025,
  title = {Graph-{Augmented},
  author = {Li, Siyuan and Du, Yongping and Li, Mingyang},
  year = {2025},
  doi = {10.1145/3726302.3730203},
  url = {https://doi.org/10.1145/3726302.3730203},
  booktitle = {Proceedings of the 48th {International},
  pages = {2700--2705},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {constraint-aware filtering, graph-augmented retrieval, memory-driven reasoning, multihop question answering},
  abstract = {Addressing multi-hop reasoning of complex query effectively is a challenging task in information retrieval field. It demands the ability to retrieve and integrate dispersed knowledge across multiple documents dynamically while maintaining coherence in multi-step reasoning process. This study addresses these challenges with three primary contributions. It explores the integrating of large language models with graph-augmented retrieval methods for complex multihop reasoning. Moreover, the Memory-Driven Chain-of-Reasoning strategy is introduced, leveraging the memory of historical queries and results to optimize multi-step reasoning dynamically. Additionally, the Constraint-Aware Filtering in Chunked Window strategy is developed to improve retrieval precision by partitioning and filtering large retrieval windows based on query constraints. The experiments on public benchmarks indicate that our method substantially outperforms competitive approaches, achieving up to 13.8\% and 14.0\% improvements in EM and F1 on HotpotQA, 9.4\% and 12.8\% on MuSiQue, and 6.4\% and 3.2\% on 2WikiMultiHopQA, respectively.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{kamran_vision_2024,
  title = {Vision {Paper},
  author = {Kamran, Parnian and Devanbu, Premkumar and Stanford, Caleb},
  year = {2024},
  doi = {10.1145/3691621.3694932},
  url = {https://doi.org/10.1145/3691621.3694932},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {35--42},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {dafny, formal verification, large language models, program synthesis, proof-carrying code, source: ACM},
  abstract = {Code completions produced by today's large language models (LLMs) offer no formal guarantees. We propose proof-carrying code completions (PC3). In this paradigm, a high-resourced entity (the LLM provided by the server) must provide a code completion together with a proof of a chosen safety property which can be independently checked by a low-resourced entity (the user). In order to provide safety proofs without requiring the user to write specifications in formal logic, we statically generate preconditions for all dangerous function calls (i.e., functions that may violate the safety property) which must be proved by the LLM.To demonstrate the main ideas, we provide a prototype implementation in the program verification language Dafny, and a case study focusing on file system vulnerabilities. Unlike Python code generated by GPT-4, Dafny code generated by PC3 provably avoids a common weakness related to path traversal (CWE-35), using a single generation attempt (k = 1) and a modest number of tokens (3, 350). Our tool is available as an open source repository at https://github.com/DavisPL/PCCC.},
  address = {New York, NY, USA},
  series = {{ASEW},
  isbn = {979-8-4007-1249-4},
}

@inproceedings{gawin_navigating_2025,
  title = {Navigating {Semantic},
  author = {Gawin, Cole and Sun, Yidan and Kejriwal, Mayank},
  year = {2025},
  doi = {10.1145/3701716.3715472},
  url = {https://doi.org/10.1145/3701716.3715472},
  booktitle = {Companion {Proceedings},
  pages = {971--975},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {abstract common sense, conceptnet, llm prompting},
  abstract = {Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-4o-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{wang_international_2024,
  title = {International {Workshop},
  author = {Wang, Shuai and Zhuang, Shengyao and Koopman, Bevan and Zuccon, Guido and Cui, Xiquan and Dave, Vachik and Su, Yi and Al Jadda, Khalifeh and Kumar, Srijan and McAuley, Julian and Ye, Tao and Guo, Stephen and Huyen, Chip},
  year = {2024},
  doi = {10.1145/3627673.3679083},
  url = {https://doi.org/10.1145/3627673.3679083},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5580--5583},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {artificial intelligence, foundation model, gen ai, llm, recommender system, source: ACM},
  abstract = {Recommender system (RecSys) plays important roles in helping users navigate, discover, and consume massive and highly-dynamic information. Today, many RecSys solutions deployed in the real world rely on categorical user-profiles and/or pre-calculated recommendation actions that stay static during a user session. However, recent trends suggest that RecSys need to model user intent in real time and constantly adapt to meet user needs at the moment or change user behavior in situ. There are three primary drivers for this emerging need of online adaptation. First, in order to meet the increasing demand for a better personalized experience, the personalization dimensions and space will grow larger and larger. It would not be feasible to pre-compute recommended actions for all personalization scenarios beyond a certain scale. Second, in many settings the system does not have user prior history to leverage. Estimating user intent in real time is the only feasible way to personalize. As various consumer privacy laws tighten, it is foreseeable that many businesses will reduce their reliance on static user profiles. Therefore, it makes the modeling of user intent in real time an important research topic. Third, a user's intent often changes within a session and between sessions, and user behavior could shift significantly during dramatic events. Therefore, it is important to investigate more on online and adaptive recommender system (OARS) that can adapt in real time to meet user needs and be robust against distribution shifts. Every year, the organizers survey the most important topics for OARS and propose a new workshop program. In light of the recent advancement of LLMs and foundation models in RecSys, in this new edition, we decide to formally add the new topic of foundation and LLM models in OARS. We will invite experts and papers in the field to facilitate its further advancement. Our workshop offers a focused discussion of the new study and application of OARS, and will bring together an interdisciplinary community of researchers and practitioners from both industry and academia to discuss on new topics in the area, grow a community, and push the direction forward.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{alzahrani_accessible_2025,
  title = {Accessible {AI},
  author = {Alzahrani, Nabeel},
  year = {2025},
  doi = {10.1145/3708035.3736048},
  url = {https://doi.org/10.1145/3708035.3736048},
  booktitle = {Practice and {Experience},
  publisher = {Association for Computing Machinery},
  keywords = {AI, Cyberinfrastructure, HPC, Training and Education},
  abstract = {High-performance computing (HPC) and artificial intelligence (AI) are increasingly critical in research and industry, yet opportunities to learn these skills remain limited for many students and educators. This paper introduces a modular curriculum comprising 14 free and open interactive workshops designed to train learners at all levels – from K-12 and community colleges to vocational programs, colleges, and universities – in essential HPC and AI concepts. The workshops are hosted on GitHub and delivered through web-based Jupyter notebooks, accessible on any device (including smartphones) via platforms like Google Colaboratory (Colab) with no software installation required. The curriculum is fully compatible with the Advanced Cyberinfrastructure Coordination Ecosystem: Services \&amp; Support (ACCESS), a U.S. National Science Foundation (NSF)-funded initiative that provides free access to advanced computing systems, such as HPC. Each workshop integrates surveys and interactive exercises to measure engagement and learning outcomes, including before-and-after knowledge checks to assess knowledge gains. A complete list of workshops and their associated learning outcomes is provided to support clarity and replicability. The curriculum has been pilot-tested with student research assistants at California State University, San Bernardino (CSUSB), yielding overwhelmingly positive feedback on its accessibility and effectiveness. Survey data collected from a broader group of 84 learners further demonstrates strong engagement, learning gains, and perceived inclusivity. The material is designed to be engaging, accessible, and easily integrated into existing courses, learning management systems, or online education platforms. By democratizing access to HPC and AI education, this work aims to broaden participation and prepare a diverse workforce with computational and analytical skills.},
  address = {New York, NY, USA},
  series = {{PEARC},
  isbn = {979-8-4007-1398-9},
}

@inproceedings{barzanji_expectations_2025,
  title = {Expectations and {Needs},
  author = {Barzanji, Chrakhan and Rosteck, Niclas and Lau, Annika and Rottmann, Sebastian and Loitsch, Claudia},
  year = {2025},
  doi = {10.1145/3743049.3748564},
  url = {https://doi.org/10.1145/3743049.3748564},
  booktitle = {Proceedings of the {Mensch},
  pages = {565--570},
  publisher = {Association for Computing Machinery},
  keywords = {AI-based Chatbot, Chatbot Design, Conversational Agents, Female STEM Students Needs, Trust And Acceptance, User Expectations, User Needs, User-Centered Design},
  abstract = {Digital tools in higher education often promise support but rarely reflect the everyday realities of their users. This paper presents insights into what female STEM students expect from a supportive academic chatbot, aiming to better understand their needs and how such tools can be meaningfully integrated into their educational routines. As part of a broader context analysis, unstructured interviews were conducted with 30 female STEM students regarding their wishes and needs. Through thematic analysis, 71 wishes were extracted and organized into four broader themes, which we discuss and form into guidelines for designing academic chatbots. By conducting the first female-only study asking their specific question, we aim to include an underrepresented user group in the design of future academic chatbot applications.},
  address = {New York, NY, USA},
  series = {{MuC},
  isbn = {979-8-4007-1582-2},
}

@inproceedings{gong_cosearchagent_2024,
  title = {{CoSearchAgent},
  author = {Gong, Peiyuan and Li, Jiamian and Mao, Jiaxin},
  year = {2024},
  doi = {10.1145/3626772.3657672},
  url = {https://doi.org/10.1145/3626772.3657672},
  booktitle = {Proceedings of the 47th {International},
  pages = {2729--2733},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {agents, collaborative search, large language models, source: ACM},
  abstract = {Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped with the capacity to understand the queries and context in multi-user conversations and the ability to search the Web for relevant information via APIs, CoSearchAgent can respond to user queries with answers grounded on the relevant search results. It can also ask clarifying questions when the information needs are unclear. The proposed CoSearchAgent is highly flexible and would be useful for supporting further research on collaborative search. The code and demo are accessible at https://github.com/pygongnlp/CoSearchAgent},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{piazza_which_2025,
  title = {“{Which},
  author = {Piazza, Alexander and Schacht, Sigurd and Herzog, Michael},
  year = {2025},
  doi = {10.1145/3708319.3733809},
  url = {https://doi.org/10.1145/3708319.3733809},
  booktitle = {Adjunct {Proceedings},
  pages = {425--428},
  publisher = {Association for Computing Machinery},
  keywords = {Explainable Recommender Systems, Large Language Models, Social Robots, Vocational Training Programs},
  abstract = {School students need to make decisions about their career paths after graduating. In Germany, students can choose between more than 300 vocational training programs, which can be overwhelming. Frequently, the students hesitate to talk with career counselors. The objective of this research is, therefore, to provide a recommendation system for school students to support their decision-making, which is based on their interests and provides recommendations with explanations based on a LLM. This system was developed with a social robot as the user interface to make it easy to use and appeal to the young target group. Based on user observations, preliminary findings indicate that the system is a valuable and engaging approach to support career counseling activities.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1399-6},
}

@inproceedings{gosmar_insight_2024,
  title = {Insight {AI},
  author = {Gosmar, Diego and Peretto, Elena and Coleman, Oita},
  year = {2024},
  doi = {10.1145/3661167.3661235},
  url = {https://doi.org/10.1145/3661167.3661235},
  booktitle = {Proceedings of the 28th {International},
  pages = {437--441},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salerno, Italy},
  keywords = {Artificial intelligence, Conversational ai, ethicalai, insightai, mentalhealth},
  abstract = {This paper presents an AI-based risk detection model (architectural framework) for real-time emotional support and risk assessment, addressing the rise in mental health issues among youth. The model leverages Insight AI (Sentiment and Emotional Analysis) to analyze synthetic utterance interactions. To design the initial version of the model, the Fundació Ajuda I Esperança customer experience has been considered: the analyzed service concerns the initial customer service (phase one) where particular focus has been paid to vulnerable youth and young adults from 14 to 25 years old. In particular, the emotional support chat that has managed - at the time of writing - more than 7,000 chat interactions, dealing with mental health problems such as depression anxiety, relationship. The findings emphasize ethical AI use in mental health services, promoting responsible deployment. Fundació Ajuda I Esperança is a no-profit organization providing help for people with emotional situations and in needs for psychological support like loneliness issues, self-harm and suicidal instincts.},
  address = {New York, NY, USA},
  series = {{EASE},
  isbn = {979-8-4007-1701-7},
}

@inproceedings{zhang_training_2025,
  title = {Training {Large},
  author = {Zhang, Yifan and Leach, Kevin},
  year = {2025},
  doi = {10.1145/3696630.3731662},
  url = {https://doi.org/10.1145/3696630.3731662},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1477--1478},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {compiler optimization, large language models},
  abstract = {LLVM Intermediate Representation (LLVM IR) serves as a crucial bridge between source code and machine code, yet its complexity presents challenges for both human developers and AI-driven analysis tools. We propose an approach to train Large Language Models (LLMs) to better comprehend and optimize LLVM IR by leveraging diverse optimization feedback. Instead of relying on a single optimization strategy like -Oz, our framework incorporates multiple types of optimizations, combining (1) static supervision from various compiler-defined passes with (2) interactive, feedback-driven refinement learning based on preferences (potentially using reinforcement learning). This dual-feedback mechanism is designed to enable LLMs to process and analyze LLVM IR more effectively, thereby facilitating downstream software engineering tasks such as code summarization, optimization recommendation, and vulnerability detection.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{xu_towards_2024,
  title = {Towards {Seamless},
  author = {Xu, Han},
  year = {2024},
  doi = {10.1145/3627673.3680275},
  url = {https://doi.org/10.1145/3627673.3680275},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5495--5498},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {information retrieval, natural language processing},
  abstract = {Integrating Large Language Models (LLMs) with external tools and APIs is essential for fields such as information retrieval and knowledge management. While LLMs have made significant strides, their effective integration with external APIs-essential for real-world applications-remains challenging. This paper introduces RESTful-Llama, a novel method designed to empower open-source LLMs to accurately convert natural language instructions into well-formed RESTful API calls. Moreover, RESTful-Llama utilizes DOC-Prompt, a newly proposed technique for generating fine-tuning datasets from publicly available API documentation. Initial experiments demonstrate that RESTful-Llama significantly enhances the accuracy of generated REST API requests.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{zhang_intelligent_2025,
  title = {Intelligent {Interaction},
  author = {Zhang, Kai and Gong, Weiyin and Liu, Qi and Lu, Junyu and Bao, Meikai and Liu, Xukai and Zhu, Linbo and Chen, Enhong},
  year = {2025},
  doi = {10.1145/3701716.3715182},
  url = {https://doi.org/10.1145/3701716.3715182},
  booktitle = {Companion {Proceedings},
  pages = {2947--2950},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {empathetic interaction, interactive platform, online education, virtual digital human},
  abstract = {The rapid advancement of artificial intelligence and virtual digital human technologies is ushering in transformative changes in education. Virtual digital humans, empowered by multimodal information fusion and natural language processing capabilities, offer a more engaging and humanized interaction experience for online learning. However, general-purpose AI models for digital teaching often face notable challenges: 1) They often lack deep understanding of users, overlooking personalized learning needs; 2) they may struggle with knowledge reasoning, leading to unreliable and outdated content; 3) their human-machine interactions can feel unnatural, lacking emotional resonance and empathetic support.To address these challenges, we developed an interaction platform focused on delivering personalized digital human services. This platform optimizes learner profiling, domain-specific data integration, and service frameworks to enhance the intelligence, personalization, and quality of digital teaching agents, effectively tackling issues related to shallow user understanding and unreliable reasoning. Through the integration of realistic digital human avatars and personalized interactive technology, the platform enables more natural and seamless human-machine interactions. Additionally, emotional speech synthesis and synchronized facial expression technology allow virtual teachers' facial expressions to dynamically align with their speech, enhancing expressiveness, vitality, and empathy, which together heighten users' sense of immersion and interaction quality. This development marks a significant step toward smarter, more personalized education.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{mulayim_large_2024,
  title = {Large {Language},
  author = {Mulayim, Ozan Baris and Paul, Lazlo and Pritoni, Marco and Prakash, Anand Krishnan and Sudarshan, Malavikha and Fierro, Gabe},
  year = {2024},
  doi = {10.1145/3671127.3698792},
  url = {https://doi.org/10.1145/3671127.3698792},
  booktitle = {Proceedings of the 11th {ACM},
  pages = {312--317},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hangzhou, China},
  keywords = {Knowledge Graphs, Large Language Models, Semantic Ontology},
  abstract = {Semantic ontologies offer a formalized, machine-readable framework for representing knowledge, enabling the structured description of complex systems. In the building domain, the adoption of ontologies like the Brick schema has transformed how buildings and their systems are modeled by providing a standardized, interoperable language. However, the complexity and the steep learning curve involved in developing and querying semantic models present substantial challenges, often requiring a workforce with specialized expertise. This paper builds on our experience in investigating how Large Language Models (LLMs) can help address these challenges, focusing on their role in constructing and querying of semantic models, particularly using the Brick Schema. Our study outlines the requirements and metrics for evaluating the scalability and effectiveness of LLM-based tools, while also discussing the current challenges and limitations in developing such tools. Ultimately, this paper aims to orient research efforts as various groups experiment with diverse techniques, while enabling more effective comparison of emerging solutions and fostering collaboration across the field.},
  address = {New York, NY, USA},
  series = {{BuildSys},
  isbn = {979-8-4007-0706-3},
}

@inproceedings{borg_creating_2024,
  title = {Creating {Virtual},
  author = {Borg, Alexander and Parodis, Ioannis and Skantze, Gabriel},
  year = {2024},
  doi = {10.1145/3610978.3640592},
  url = {https://doi.org/10.1145/3610978.3640592},
  booktitle = {Companion of the 2024 {ACM},
  pages = {273--277},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boulder, CO, USA},
  keywords = {human-robot interaction, large language models, robots in education, virtual patients},
  abstract = {This paper presents a virtual patient (VP) platform for medical education, combining a social robot, Furhat, with large language models (LLMs). Aimed at enhancing clinical reasoning (CR) training, particularly in rheumatology, this approach introduces more interactive and realistic patient simulations. The use of LLMs both for driving the dialogue, but also for the expression of emotions in the robot's face, as well as automatic analysis and generation of feedback to the student, is discussed. The platform's effectiveness was tested in a pilot study with 15 medical students, comparing it against a traditional semi-linear VP platform. The evaluation indicates a preference for the robot platform in terms of authenticity and learning effect. We conclude that this novel integration of a social robot and LLMs in VP simulations shows potential in medical education, offering a more engaging learning experience.},
  address = {New York, NY, USA},
  series = {{HRI},
  isbn = {979-8-4007-0323-2},
}

@inproceedings{parthasarathy_polymer_2025,
  title = {Polymer: {Development},
  author = {Parthasarathy, Dhasarathy and Yu, Yinan and Barr, Earl T.},
  year = {2025},
  doi = {10.1145/3696630.3728494},
  url = {https://doi.org/10.1145/3696630.3728494},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {535--539},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {large language models, software development automation},
  abstract = {Software development builds digital tools to automate processes, yet its initial phases, up to deployment, remain largely manual. There are two reasons: Development tasks are often under-specified and transitions between tasks usually require a translator. These reasons are mutually reinforcing: it makes little sense to specify tasks when you cannot connect them and writing a translator requires a specification. LLMs change this cost equation: they can handle under-specified systems and they excel at translation. Thus, they can act as skeleton keys that unlock the automation of tasks and transitions that were previously too expensive to automatically interlink. We introduce a recipe for writing development workflows as software (polymer) to further automate the initial phases of development. We show how adopting polymer at Volvo, a large automotive manufacturer, to automate testing saved 2–3 FTEs at the cost of two months to develop and deploy. We close with open challenges when polymerizing development workflows.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{frobe_reneuir_2024,
  title = {{ReNeuIR},
  author = {Fröbe, Maik and Mackenzie, Joel and Mitra, Bhaskar and Nardini, Franco Maria and Potthast, Martin},
  year = {2024},
  doi = {10.1145/3626772.3657994},
  url = {https://doi.org/10.1145/3626772.3657994},
  booktitle = {Proceedings of the 47th {International},
  pages = {3051--3054},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {algorithms, efficiency, neural ir, ranking, retrieval, sustainable ir},
  abstract = {The Information Retrieval (IR) community has a rich history of empirically measuring novel retrieval methods in terms of effectiveness and efficiency. However, as the search ecosystem is developing rapidly, comparatively little attention has been paid to evaluating efficiency in recent years, which raises the question of the cost-benefit ratio between effectiveness and efficiency. In this regard, it has become difficult to compare and contrast systems in an empirically fair way. Factors including hardware configurations, software versioning, experimental settings, and measurement methods all contribute to the difficulty of meaningfully comparing search systems, especially where efficiency is a key component of the evaluation. Furthermore, efficiency is no longer limited to time and space but has found new, challenging dimensions that stretch to resource, sample, and energy efficiency and have implications for users, researchers, and the environment. Examining algorithms and models through the lens of efficiency and its trade-off with effectiveness requires revisiting and establishing new standards and principles, from defining relevant concepts, to designing measures, to creating guidelines for making sense of the significance of findings. The third iteration of ReNeuIR aims to bring the community together to debate these questions and collaboratively test and improve a benchmarking framework for efficiency derived from the discussions of the first two iterations of this workshop. We provide a first prototype of this framework by organizing a shared task track focused on comparability and reproducibility at the workshop.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{shen_iotcoder_2024,
  title = {{IoTCoder},
  author = {Shen, Leming and Zheng, Yuanqing},
  year = {2024},
  doi = {10.1145/3636534.3697447},
  url = {https://doi.org/10.1145/3636534.3697447},
  booktitle = {Proceedings of the 30th {Annual},
  pages = {1647--1649},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington D.C., DC, USA},
  keywords = {IoT applications, large language models},
  abstract = {Existing code Large Language Models are primarily designed for generating simple and general algorithms but are not dedicated to IoT applications. To fill this gap, we present IoTCoder, a coding copilot specifically designed to synthesize programs for IoT application development. IoTCoder features three locally deployed small language models (SLMs): a Task Decomposition SLM that decomposes a complex IoT application into multiple tasks with detailed descriptions, a Requirement Transformation SLM that converts the decomposed tasks described in natural language to well-structured specifications, and a Modularized Code Generation SLM that generates modularized code based on the task specifications. Experiment results show that IoTCoder can synthesize programs adopting more IoT-specific algorithms and outperform state-of-the-art code LLMs in terms of both task accuracy (by more than 24.2\% on average) and memory usage (by less than 358.4 MB on average).},
  address = {New York, NY, USA},
  series = {{ACM},
  isbn = {979-8-4007-0489-5},
}

@inproceedings{oliveira_improving_2024,
  title = {Improving {VR},
  author = {Oliveira, Elisa Ayumi Masasi de and Silva, Diogo Fernandes Costa and Filho, Arlindo Rodrigues Galvão},
  year = {2024},
  doi = {10.1145/3691573.3691619},
  url = {https://doi.org/10.1145/3691573.3691619},
  booktitle = {Proceedings of the 26th {Symposium},
  pages = {289--293},
  publisher = {Association for Computing Machinery},
  note = {event-place: Manaus, Brazil},
  keywords = {3D Scene, Accessibility, Multimodal Large Language Models (MLLMs), Virtual Reality (VR)},
  abstract = {Advancements in Virtual Reality (VR) technology hold immense promise for enriching immersive experiences. Despite the advancements in VR technology, there remains a significant gap in addressing accessibility concerns, particularly in automatically providing descriptive information for VR scenes. This paper combines the potential of leveraging Multimodal Large Language Models (MLLMs) to automatically generate text descriptions for 360 VR scenes according to Speech-to-Text (STT) prompts. As a case study, we conduct experiments on educational settings in VR museums, improving dynamic experiences across various contexts. Despite minor challenges in adapting MLLMs to VR Scenes, the experiments demonstrate that they can generate descriptions with high quality. Our findings provide insights for enhancing VR experiences and ensuring accessibility to individuals with disabilities or diverse needs.},
  address = {New York, NY, USA},
  series = {{SVR},
  isbn = {979-8-4007-0979-1},
}

@inproceedings{young_gaqr_2024,
  title = {{GaQR},
  author = {Young, Oliver and Fan, Yixing and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Cheng, Xueqi},
  year = {2024},
  doi = {10.1145/3627673.3679930},
  url = {https://doi.org/10.1145/3627673.3679930},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4228--4232},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {generation-augmented retrieval, knowledge distillation, question rewriting},
  abstract = {Query understanding is an essential part in search systems to improve the recall. Unlike prior works focusing on word expansions, in this paper, we leverage the comprehension ability of LLM to generate detailed queries from a global semantic perspective. To this end, we introduce an efficient GaQR to reformulate a question into several queries using Chain of Thought (CoT) and make it more efficient through knowledge distillation. Specifically, we first prompt a teacher model to generate indicative queries by considering answer generation one step ahead. Then, we filter out low-quality queries by validating the effectiveness of all generated queries in retrieving useful passages. Finally, we distill a student rewriter based on the verified results to improve efficiency. Our experimental results demonstrate that the rewriter improves the retrieval performance by 3\% to 15\% on the Miracl and NFCorpus datasets and shows good generalisation ability across different retrieval methods. Moreover, the efficiency of the rewriter after knowledge distillation is improved by as much as 5 times. Code is available at https://github.com/youngbeauty250/GaQR.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{sun_integrated_2024,
  title = {An {Integrated},
  author = {Sun, Yiding and Wang, Feng and Zhu, Yutao and Zhao, Wayne Xin and Mao, Jiaxin},
  year = {2024},
  doi = {10.1145/3626772.3657671},
  url = {https://doi.org/10.1145/3626772.3657671},
  booktitle = {Proceedings of the 47th {International},
  pages = {2713--2718},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {data processing, data quality, large language models},
  abstract = {The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code and demonstration video are accessible on GitHub.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{chen_retrieving_2025,
  title = {Retrieving the {Right},
  author = {Chen, Szu-Ju and Jin, Jing and Wei, Sheng-Lun and Chen, Chien-Hung and Chen, Hsin-Hsi},
  year = {2025},
  doi = {10.1145/3726302.3730246},
  url = {https://doi.org/10.1145/3726302.3730246},
  booktitle = {Proceedings of the 48th {International},
  pages = {2951--2955},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information systems document filtering, information systems question answering},
  abstract = {Legal question answering requires accurate retrieval of relevant laws, yet the significant writing style gap between user queries and legal provisions poses a major challenge. Existing datasets and retrieval methods often struggle to capture the complexity of legal language, limiting retrieval effectiveness. In this study, we introduce the Legal Query-to-Provision Retrieval (LQPR) task and construct Query2Provision (Q2P), a dataset designed to enhance law retrieval by incorporating diverse case scenarios and linguistic structures representative of real-world legal inquiries. To address the style disparity, we propose a style translation approach that transforms informal user queries into a more formal legal tone and simplifies complex legal provisions for better alignment. Our experiments demonstrate that integrating writing style transformation significantly improves retrieval performance. The dataset is available at https://github.com/ntunlplab/Query2Provision},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhao_ai_2025,
  title = {{AI},
  author = {Zhao, Yanjie and Wang, Haoyu},
  year = {2025},
  doi = {10.1145/3696630.3730571},
  url = {https://doi.org/10.1145/3696630.3730571},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1768--1773},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {AI models, biological analogy, conceptual frameworks, large language models, machine learning, model architecture},
  abstract = {Artificial intelligence models, particularly large language models (LLMs), have become increasingly important and prevalent in modern computational systems. Their rapid development has created a need for more intuitive frameworks to understand their structure, function, and evolution. This paper introduces the "AI Model Genome" framework, a novel biological analogy that maps AI model components to their genomic counterparts. By drawing parallels between model architectures and species classification, training data and genetic material, and model parameters and gene sequences, we establish a comprehensive framework for conceptualizing model development. This biological perspective offers researchers and practitioners new insights into model design, modification strategies, and evolutionary pathways, potentially accelerating innovation in the field while providing a more accessible mental model for understanding complex AI systems.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{jia_vqlens_2025,
  title = {{VQLens},
  author = {Jia, Yansha and You, Zhengxin and Wang, Yujie and Shen, Qiaomu and Tang, Bo},
  year = {2025},
  doi = {10.1145/3722212.3725142},
  url = {https://doi.org/10.1145/3722212.3725142},
  booktitle = {Companion of the 2025 {International},
  pages = {267--270},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {query execution, vector database, visual analytics system},
  abstract = {With the increasing demand for high-speed similarity search over massive datasets, vector databases have become essential in modern data-intensive applications, particularly in LLMs and other AI-driven workloads. The growing complexity and scale of vector database operations demand more comprehensive evaluation methodologies. However, existing evaluation methodologies primarily rely on high-level information retrieval metrics (e.g., recall, accuracy) and system performance indicators (e.g., query latency, throughput), which fail to provide fine-grained insights. This limitation hinders the diagnosis of unexpected query results and the optimization of system performance. To bridge this gap, we present VQLens, an interactive visual analytics system for in-depth exploration of query execution traces and large-scale vector distributions. Specifically, VQLens (1) computes the overall data distribution and overlays query traces to reveal retrieval patterns, (2) visualizes query execution trace using multiple representations to enhance interpretability, and (3) enables interactive multi-view exploration, supporting both global pattern discovery and case-specific analysis. VQLens is open-source at https://github.com/DBGroup-SUSTech/VQLens.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{wang_i-guide_2025,
  title = {I-{GUIDE},
  author = {Wang, Shaowen and Baig, Furqan and Kang, Yunfan and Li, Erick and Michels, Alexander and Jaroenchai, Nattapon and Padmanabhan, Anand and Kumar, Arunesh and Kalyanam, Rajesh and S. Oller Smith, Noah and Song, X. Carol and Zhao, Lan and Manson, Steven and Ramamurthy, Mohan and Taghavikish, Sina and Tarboton, David},
  year = {2025},
  doi = {10.1145/3708035.3736108},
  url = {https://doi.org/10.1145/3708035.3736108},
  booktitle = {Practice and {Experience},
  publisher = {Association for Computing Machinery},
  keywords = {cyberinfrastructure, geospatial artificial intelligence (AI), science gateway},
  abstract = {The I-GUIDE Platform is a pioneering cyberinfrastructure environment designed to advance geospatial data-intensive convergence science and knowledge discovery across various domains. By integrating geospatial data, domain-specific knowledge elements, and open educational resources, the platform empowers researchers and practitioners to tackle complex scientific challenges cutting across multiple disciplines. This paper provides an overview of the motivation, architecture, principles, and applications and impacts of the platform, demonstrating how it fosters interdisciplinary collaboration and enables geospatial data-intensive convergence research and education.},
  address = {New York, NY, USA},
  series = {{PEARC},
  isbn = {979-8-4007-1398-9},
}

@inproceedings{xie_neighborhood-based_2024,
  title = {Neighborhood-{Based},
  author = {Xie, Zhouhang and Wu, Junda and Jeon, Hyunsik and He, Zhankui and Steck, Harald and Jha, Rahul and Liang, Dawen and Kallus, Nathan and Mcauley, Julian},
  year = {2024},
  doi = {10.1145/3640457.3688191},
  url = {https://doi.org/10.1145/3640457.3688191},
  booktitle = {Proceedings of the 18th {ACM},
  pages = {1045--1050},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bari, Italy},
  keywords = {source: ACM},
  abstract = {Conversational recommender systems (CRS) should understand users’ expressed interests, which are frequently semantically rich and knowledge-intensive. Prior works attempt to address this challenge by using external knowledge bases or parametric knowledge in large language models (LLMs). In this paper, we study a complementary solution, exploiting item knowledge in the training data. We hypothesize that many inference-time user requests can be answered by reusing popular crowd-written answers associated with similar training queries. Following this intuition, we define a class of neighborhood-based CRS that makes recommendations by identifying items commonly associated with similar training dialogue contexts. Experiments on Inspired, Redial, and Reddit-Movie benchmarks show our method outperforms state-of-the-art LLMs with 2 billion parameters, and offers on-par performance to 7 billion parameter models while using over 170 times less GPU memory. We also show neighborhood and model-based predictions can be combined to achieve further performance improvements1.},
  address = {New York, NY, USA},
  series = {{RecSys},
  isbn = {979-8-4007-0505-2},
}

@inproceedings{khatua_policies_2025,
  title = {Policies for {Refugee},
  author = {Khatua, Aparup},
  year = {2025},
  doi = {10.1145/3701716.3715580},
  url = {https://doi.org/10.1145/3701716.3715580},
  booktitle = {Companion {Proceedings},
  pages = {1062--1066},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {generative agents, mental health, policy simulations, refugees, source: ACM},
  abstract = {In recent times, mental health has attracted the attention of researchers, and the extant literature has explored digital platforms to identify and analyze individuals' expressions of distress on social media platforms, such as depressive tweets, to probe various aspects of users' mental health. This unidirectional perspective often overlooks the societal context that may impact mental health. In the context of refugees' mental health, we propose a novel dual-perspective approach by examining both self-reflective tweets expressing emotional distress and hostile tweets from host country citizens. We employ large language models to extract and summarize crucial concerns from vast unstructured user-generated data, using them as input for our agent-based simulations for policy intervention. The proposed simulation-based approach suggests comprehensive policies to address displacement-related trauma and societal hostility faced by refugees in host countries.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{kim_conversational_2025,
  title = {Conversational {Argument},
  author = {Kim, Kyusik and Ryu, Jeongwoo and Heo, Dongseok and Song, Hyungwoo and Oh, Changhoon and Suh, Bongwon},
  year = {2025},
  doi = {10.1145/3726302.3730175},
  url = {https://doi.org/10.1145/3726302.3730175},
  booktitle = {Proceedings of the 48th {International},
  pages = {2837--2842},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {argument search, conversational search, selective exposure},
  abstract = {Conversational argument search systems influence how users access diverse perspectives but are prone to selective exposure. To address this, we propose two strategies: an interface-level multi-agent framework that structures perspective presentation and an interaction-level questioning strategy that encourages deeper engagement. We evaluate these strategies through a 2 x 2 factorial user study, examining their impact on selective exposure. Results show that the multi-agent setup facilitates broader perspective comparison, while agent-initiated questioning fosters deeper reflection; together, they promote more balanced argument access. Based on these findings, we discuss conversational search systems to mitigate selective exposure by implementing multi-agent interactions and questioning mechanisms.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{sun_timelinekgqa_2025,
  title = {{TimelineKGQA},
  author = {Sun, Qiang and Li, Sirui and Huynh, Du and Reynolds, Mark and Liu, Wei},
  year = {2025},
  doi = {10.1145/3701716.3715308},
  url = {https://doi.org/10.1145/3701716.3715308},
  booktitle = {Companion {Proceedings},
  pages = {797--800},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge graph, question answering, temporal knowledge graph},
  abstract = {Question answering over temporal knowledge graphs (TKGs) is crucial for understanding evolving facts and relationships, yet its development is hindered by limited datasets and difficulties in generating custom QA pairs. We propose a novel categorization framework based on timeline-context relationships, along with TimelineKGQA, a universal temporal QA generator applicable to any TKGs. The code is available at: https://github.com/PascalSun/TimelineKGQA as an open source Python package.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhu_ospc_2024,
  title = {{OSPC},
  author = {Zhu, Chenxi and Gao, Haotian and Duan, Yuxiao and Hao, Guo and Luo, Minnan and Zhao, Xiang},
  year = {2024},
  doi = {10.1145/3589335.3665994},
  url = {https://doi.org/10.1145/3589335.3665994},
  booktitle = {Companion {Proceedings},
  pages = {1904--1907},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {meme, multi-modal, vision language models, zero-shot},
  abstract = {Harmful memes refer to the memes which contain social bias towards a certain group, such as gender, race, disabilities and so on. Detecting these harmful memes requires the model to have both visual and linguistic understanding of memes and some external knowledge about the culture and morality. This technical report summarises the fourth place solution of the Online Safety Prize Challenge, which enhances a visual language model (VLM) by integrating external Optical Character Recognition (OCR) tools, employs prompt engineering and manual resource management to optimize performance within limited resource constraints. This strategy achieved a score of 0.723/0.643(AUROC/ACC) in this challenge with one single submission},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0172-6},
}

@inproceedings{shi_scalable_2025,
  title = {Scalable {Overload},
  author = {Shi, Yang and Sun, Yiping and Du, Jiaolong and Zhong, Xiaocheng and Wang, Zhiyong and Hu, Yao},
  year = {2025},
  doi = {10.1145/3701716.3715576},
  url = {https://doi.org/10.1145/3701716.3715576},
  booktitle = {Companion {Proceedings},
  pages = {1303--1307},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {anns graph indexing, approximate nearest neighborhood search, data partitioning for distributed computing system, system and resource scalability},
  abstract = {Approximate Nearest Neighbor Search (ANNS) is essential for modern data-driven applications that require efficient retrieval of top-k results from massive vector databases. Although existing graph-based ANNS algorithms achieve a high recall rate on billion-scale datasets, their slow construction speed and limited scalability hinder their applicability to large-scale industrial scenarios. In this paper, we introduce SOGAIC, the first Scalable Overload-Aware Graph-Based ANNS Index Construction system tailored for ultra-large-scale vector databases: 1) We propose a dynamic data partitioning algorithm with overload constraints that adaptively introduces overlaps among subsets; 2) To enable efficient distributed subgraph construction, we employ an a load-balancing task scheduling framework combined with an agglomerative merging strategy; 3) Extensive experiments on various datasets demonstrate a reduction of 47.3\% in average construction time compared to existing methods. The proposed method has also been successfully deployed in a real-world industrial search engine, managing over 10 billion daily updated vectors and serving hundreds of millions of users.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhu_retrieval-augmented_2024,
  title = {Retrieval-augmented {Query},
  author = {Zhu, Peide and Li, Na and Zhao, Zhiming},
  year = {2024},
  doi = {10.1145/3589335.3651553},
  url = {https://doi.org/10.1145/3589335.3651553},
  booktitle = {Companion {Proceedings},
  pages = {907--910},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {heterogeneous research asset retrieval, large language models, query reformulation},
  abstract = {Discovering and reusing research assets such as datasets and computational notebooks is crucial for building research workflows in data-centric studies. The rapid growth of research assets in scientific communities provides scientists with great opportunities to enhance research efficacy but also poses significant challenges in finding suitable materials for specific tasks. Scientists, especially those focusing on cross-disciplinary research, often find it difficult to formulate effective queries to retrieve desired resources. Previous work has proposed query reformulation methods to increase the efficiency of research asset search. However, it relies on existent knowledge graphs and is constrained to computational notebooks only. As research assets utilized by data analytic workflows are in essence heterogeneous, i.e., of distinct kinds and from diversified sources, query reformulation methods in this regard should consider the relationship between different types of research assets. To address the above challenges, we propose a retrieval-augmented query reformulation method for heterogeneous research asset retrieval. It is developed in the context of a Notebook-based virtual research environment (VRE) and offers query reformulation services to other VRE components. We demonstrate the effectiveness of the proposed query reformulation service with experiments on dataset and notebook retrieval. Up till now, we have indexed 8,954 datasets and 18,158 notebooks. The experimental results show that the proposed service can create useful query suggestions.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0172-6},
}

@inproceedings{semenkin_context_2024,
  title = {Context {Composing},
  author = {Semenkin, Anton and Sokolov, Yaroslav and Vu, Evgeniia},
  year = {2024},
  doi = {10.1145/3643796.3648446},
  url = {https://doi.org/10.1145/3643796.3648446},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {15--17},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {artificial intelligence, code completion, context composing, integrated development environment, programming, prompt engineering, transformers, source: ACM},
  abstract = {Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.},
  address = {New York, NY, USA},
  series = {{IDE},
  isbn = {979-8-4007-0580-9},
}

@inproceedings{tsukuda_eliciting_2025,
  title = {Eliciting {Implicit},
  author = {Tsukuda, Kosetsu and Maruta, Atsuki and Kato, Makoto P. and Joho, Hideo},
  year = {2025},
  doi = {10.1145/3698204.3716461},
  url = {https://doi.org/10.1145/3698204.3716461},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {292--297},
  publisher = {Association for Computing Machinery},
  keywords = {E-commerce, information needs, taxonomy, think-aloud method},
  abstract = {In this paper, we elicit implicit information needs that arise during the process of deciding which products to purchase on e-commerce (EC) sites. We designed product purchase tasks to capture implicit information needs, and we conducted a user study to collect utterance data using a think-aloud method. By analyzing the utterances of participants during the tasks, we developed a taxonomy comprising five categories where people express preferences for products and 11 categories where people want to understand products. Our taxonomy includes implicit information needs that have not been captured in existing EC-related taxonomies (e.g., Preference for Subjective Attributes and Understanding Product Differences). We revealed the characteristics of each category of information need in terms of timing during the tasks: e.g., the information need of Understanding Product Range occurred very frequently in the early stage of a task. We also revealed the occurrence frequencies for different task types: e.g., the information needs of Preference for Objective Attributes, Understanding Product Range, and Understanding Terminology had a higher occurrence when purchasing products less frequently and at a higher cost than when purchasing products frequently at a relatively low cost. Our taxonomy could be used to further improve users’ purchasing processes on EC sites.},
  address = {New York, NY, USA},
  series = {{CHIIR},
  isbn = {979-8-4007-1290-6},
}

@inproceedings{jiang_gats_2024,
  title = {{GATS},
  author = {Jiang, Cong and Chen, Zhongde and Zhang, Bo and Ren, Yankun and Dong, Xin and Cheng, Lei and Yang, Xinxing and Li, Longfei and Zhou, Jun and Mo, Linjian},
  year = {2024},
  doi = {10.1145/3626772.3661372},
  url = {https://doi.org/10.1145/3626772.3661372},
  booktitle = {Proceedings of the 47th {International},
  pages = {2920--2924},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {audience targeting, large language models, multi-task fine-tuning, online advertising},
  abstract = {This paper presents GATS (\&lt;u\&gt;G\&lt;/u\&gt;enerative \&lt;u\&gt;A\&lt;/u\&gt;udience \&lt;u\&gt;T\&lt;/u\&gt;argeting \&lt;u\&gt;S\&lt;/u\&gt; ystem for Online Advertising), a new framework using large language models (LLMs) to improve audience targeting in online advertising. GATS overcomes the shortcomings of rule-based, look-alike, and graph-based methods by facilitating flexible and interpretable audience criteria expression. The framework integrates intent recognition, knowledge mining, and Data Management Platform (DMP) mapping to translate advertiser demands into actionable user tags and correlate them within a DMP. A small, white-box model called LightGATS (base on QWen-14B), fine-tuned with a high-quality LLM corpus, ensures the framework's safety and efficiency, operating within a scalable hybrid online-offline architecture. GATS's effectiveness is validated through extensive experiments, marking a significant advancement in audience targeting technology.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{carlson_dynamic_2025,
  title = {Dynamic {Superblock},
  author = {Carlson, Parker and Xie, Wentai and He, Shanxiu and Yang, Tao},
  year = {2025},
  doi = {10.1145/3726302.3730183},
  url = {https://doi.org/10.1145/3726302.3730183},
  booktitle = {Proceedings of the 48th {International},
  pages = {3004--3009},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {dynamic pruning, efficiency, learned sparse retrieval},
  abstract = {This paper proposes superblock pruning (SP) during top-k online document retrieval for learned sparse representations. SP structures the sparse index as a set of superblocks on a sequence of document blocks and conducts a superblock-level selection to decide if some superblocks can be pruned before visiting their child blocks. SP generalizes the previous flat block or cluster-based pruning, allowing the early detection of groups of documents that cannot or are less likely to appear in the final top-k list. SP can accelerate sparse retrieval in a rank-safe or approximate manner under a high-relevance competitiveness constraint. Our experiments show that the proposed scheme significantly outperforms state-of-the-art baselines on MS MARCO passages on a single-threaded CPU.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhong_doctopus_2025,
  title = {Doctopus: {A},
  author = {Zhong, Yuanhao and Deng, Yuhao and Chai, Chengliang and Gu, Ruixin and Yuan, Ye and Wang, Guoren and Cao, Lei},
  year = {2025},
  doi = {10.1145/3722212.3725103},
  url = {https://doi.org/10.1145/3722212.3725103},
  booktitle = {Companion of the 2025 {International},
  pages = {275--278},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {cost optimization, information extraction, query enhancement},
  abstract = {Extracting structured data from unstructured documents is essential for applications like analytical SQL queries and decision-making. Strategies such as pre-trained language models (PLMs) can be employed, but they often fall short in quality. Large language models (LLMs) have shown effectiveness in attribute extraction but are costly, making them impractical for large-scale document sets. To best trade off quality and cost, we present Doctopus, a system designed for accurate and cost-effective attribute extraction. Overall, Doctopus combines LLMs with non-LLM strategies to achieve an optimal quality-cost balance. First, the system employs an index-based approach to efficiently identify and process only relevant chunks. Afterwards, it further estimates the quality of multiple strategies for each attribute. Finally, based on the cost and estimated quality, Doctopus dynamically selects the optimal strategies through budget-aware optimization. With a real-life scenario, we demonstrate that Doctopus allows users to extract attributes accurately and affordably. The corresponding video is available at https://youtu.be/Cxl\_PfvZY10?si=NYoHt2SyD9KHqd6V.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{liu_robust-ir_2025,
  title = {Robust-{IR},
  author = {Liu, Yu-An and Nachimovsky, Haya and Zhang, Ruqing and Kurland, Oren and Guo, Jiafeng and Tennenholtz, Moshe},
  year = {2025},
  doi = {10.1145/3726302.3730355},
  url = {https://doi.org/10.1145/3726302.3730355},
  booktitle = {Proceedings of the 48th {International},
  pages = {4142--4145},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {competitive search, information retrieval, robustness},
  abstract = {With the advancement of information retrieval (IR) technologies, robustness is increasingly attracting attention. When deploying technology into practice, we consider not only its average performance under normal conditions but, more importantly, its ability to maintain functionality across a variety of exceptional situations. In recent years, the research on IR robustness covers theory, evaluation, methodology, and application, and all of them show a growing trend. The purpose of this workshop is to systematize the latest results of each research aspect, to foster comprehensive communication within this niche domain while also bridging robust IR research with the broader community, and to promote further future development of robust IR. To avoid the one-sided talk of mini-conferences, this workshop adopts a highly interactive format, including round-table and panel discussion sessions, to encourage active participation and meaningful exchange among attendees.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{su_parametric_2025,
  title = {Parametric retrieval augmented generation},
  author = {Su, W. and Tang, Y. and Ai, Q. and Yan, J. and Wang, C. and Wang, H. and {...},
  year = {2025},
  doi = {10.1145/3726302.3729957},
  url = {https://doi.org/10.1145/3726302.3729957},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th …},
  pages = {1240--1250},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, large language model, retrieval augmented generation},
  abstract = {… documents into the input context of the LLM, which we refer to as the in… To this end, we introduce Parametric RAG, a new RAG … encourages the LLM to internalize the factual details in the …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{afzal_towards_2025,
  title = {Towards {Optimizing},
  author = {Afzal, Anum and Vladika, Juraj and Fazlija, Gentrit and Staradubets, Andrei and Matthes, Florian},
  year = {2025},
  doi = {10.1145/3711542.3711575},
  url = {https://doi.org/10.1145/3711542.3711575},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {250--257},
  publisher = {Association for Computing Machinery},
  keywords = {Benchmark, Large Language Models, Retrieval Augmented Generation},
  abstract = {Given the growing trend of many organizations integrating Retrieval Augmented Generation (RAG) into their operations, we assess RAG on domain-specific data and test state-of-the-art models across various optimization techniques. We incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble Retriever, and In-Context-Learning, to enhance the functionality and performance in the academic domain. We focus on data retrieval, specifically targeting various study programs at a large technical university. We additionally introduce a novel evaluation approach, the RAG Confusion Matrix designed to assess the effectiveness of various configurations within the RAG framework. By exploring the integration of both open-source (e.g., Llama2, Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer valuable insights into the application and optimization of RAG frameworks in domain-specific contexts. Our experiments show a significant performance increase when including multi-query in the retrieval phase.},
  address = {New York, NY, USA},
  series = {{NLPIR},
  isbn = {979-8-4007-1738-3},
}

@inproceedings{wallat_correctness_2025,
  title = {Correctness is not {Faithfulness},
  author = {Wallat, Jonas and Heuss, Maria and Rijke, Maarten de and Anand, Avishek},
  year = {2025},
  doi = {10.1145/3731120.3744592},
  url = {https://doi.org/10.1145/3731120.3744592},
  booktitle = {Proceedings of the 2025 {International},
  pages = {22--32},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {attributions, faithfulness, interpretability, large language models, retrieval-augmented generation, self-explanations, source: ACM, source: Scopus},
  abstract = {Large language models (LLMs) have transformed information retrieval through chat interfaces, but their hallucination tendencies pose significant risks. While Retrieval Augmented Generation (RAG) with citations has emerged as a solution by allowing users to verify responses through source attribution, current evaluation approaches focus primarily on citation correctness - whether cited documents support the corresponding statements. This is insufficient and we introduce citation faithfulness - whether the model's reliance on cited documents is genuine rather than post-rationalized to fit pre-existing knowledge. Our contributions are threefold: (i) we introduce coherent notions of attribution and introduce the concept of citation faithfulness; (ii) we propose desiderata for citations beyond correctness and accuracy needed for trustworthy systems; and (iii) we emphasize evaluating citation faithfulness by studying post-rationalization. Through experimentation, we reveal prevalent post-rationalization issues, finding that up to 57\% of citations lack faithfulness. This undermines reliable attribution and may result in misplaced trust, highlighting a critical gap in current LLM-based IR systems. We demonstrate why both citation correctness and faithfulness must be considered when deploying LLMs in IR applications, contributing to a broader discussion of building more reliable and transparent information access systems.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
  annote = {Cited by: 0},
}

@inproceedings{merker_axioms_2025,
  title = {Axioms for {Retrieval},
  author = {Merker, Jan Heinrich and Fröbe, Maik and Stein, Benno and Potthast, Martin and Hagen, Matthias},
  year = {2025},
  doi = {10.1145/3731120.3744601},
  url = {https://doi.org/10.1145/3731120.3744601},
  booktitle = {Proceedings of the 2025 {International},
  pages = {67--77},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {preferences, retrieval axioms, retrieval-augmented generation},
  abstract = {Information retrieval axioms are formalized constraints that good retrieval models should fulfill, e.g., to rank documents higher that contain the query terms more often. Over the last decades, more than 25 such axioms have been proposed and used to analyze, to improve, or to explain retrieval models. However, those axioms were meant for document ranking scenarios and thus do not directly fit the new scenario of retrieval-augmented generation systems (RAG). To close this gap, we rethink retrieval axioms for RAG. First, we transfer the underlying ideas of as many of the traditional axioms as possible to the new RAG setting (18 axioms can be transferred), and second, we suggest and formalize 11 new axioms to capture utility aspects of RAG answers. In experiments on the TREC 2024 RAG track data and on the Webis-CrowdRAG-25 corpus, we show that the new axioms more accurately capture automated and human RAG preferences than the transferred traditional axioms. Furthermore, we illustrate practical applications for inspecting preferences of language models and for aiding human preference judgments.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@article{dong_understand_2025,
  title = {Understand what {LLM},
  author = {Dong, G. and Zhu, Y. and Zhang, C. and Wang, Z. and Wen, J. R. and {...},
  year = {2025},
  doi = {10.1145/3696410.3714717},
  url = {https://doi.org/10.1145/3696410.3714717},
  booktitle = {Proceedings of the {ACM},
  journal = {Proceedings of the ACM …},
  pages = {4206--4225},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, large language model, retrieval-augmented generation},
  abstract = {… ) has effectively mitigated the hallucination problem of large language models (LLMs). How… developing a reliable RAG system. To address this issue, we propose DPARAG, a universal …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@article{jiang_rago_2025,
  title = {Rago: {Systematic},
  author = {Jiang, W. and Subramanian, S. and Graves, C. and Alonso, G. and {...},
  year = {2025},
  doi = {10.1145/3695053.3731093},
  url = {https://doi.org/10.1145/3695053.3731093},
  booktitle = {Proceedings of the 52nd {Annual},
  journal = {Proceedings of the …},
  pages = {974--989},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Computer Architecture, Computer System, Large Language Model, Performance Optimization, Retrieval-Augmented Generation},
  abstract = {… LLM-only systems often struggle to achieve high factual accuracy or providing up-todate … Our baseline is an extension of LLM-only systems, where additional RAG components are …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ISCA},
  isbn = {979-8-4007-1261-6},
}

@inproceedings{song_urag_2025,
  title = {{URAG},
  author = {Song, Yulun and Yan, Long and Qin, Lina and Wang, Gongju and Huang, Xingru and Hu, Luzhe and Liu, Weixin},
  year = {2025},
  doi = {10.1145/3708657.3708763},
  url = {https://doi.org/10.1145/3708657.3708763},
  booktitle = {Proceedings of the 2024 10th {International},
  pages = {660--667},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {Accuracy and comprehensiveness in knowledge retrieval, Multi-source heterogeneous data, Unified Retrieval-Augmented Generation (URAG), source: Scopus},
  abstract = {To address the issues of insufficient retrieval capabilities and "hallucinations" in responses generated by large models, this paper proposes a knowledge question-answering framework based on Unified Retrieval-Augmented Generation (URAG). The framework integrates three retrieval mechanisms—keyword retrieval, vector retrieval, and graph retrieval—enabling efficient and high-quality utilization of massive, multi-source, and heterogeneous data. It effectively overcomes the limitations of a single retrieval pathway. Experimental results demonstrate that the URAG framework excels across various task scenarios, enhancing the accuracy and comprehensiveness of knowledge retrieval. Its advantage is particularly evident when dealing with multi-dimensional and multi-layered information. In conclusion, the Unified Retrieval-Augmented Generation technique offers a new method for improving the performance of intelligent question-answering systems, with broad application prospects and research value. This study provides valuable insights into understanding RAG (Retrieval-Augmented Generation) technology and its application in large language models.},
  address = {New York, NY, USA},
  series = {{ICCIP},
  isbn = {979-8-4007-1744-4},
  annote = {Cited by: 0},
}

@inproceedings{wu_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Wu, Yuxia and Liao, Lizi and Fang, Yuan},
  year = {2025},
  doi = {10.1145/3726302.3729958},
  url = {https://doi.org/10.1145/3726302.3729958},
  booktitle = {Proceedings of the 48th {International},
  pages = {1434--1443},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {dynamic graph modeling, graph neural networks, retrieval-augmented generation},
  abstract = {Modeling dynamic graphs, such as those found in social networks, recommendation systems, and e-commerce platforms, is crucial for capturing evolving relationships and delivering relevant insights over time. Traditional approaches primarily rely on graph neural networks with temporal components or sequence generation models, which often focus narrowly on the historical context of target nodes. This limitation restricts the ability to adapt to new and emerging patterns in dynamic graphs. To address this challenge, we propose a novel framework, Retrieval-Augmented Generation for Dy namic Graph modeling (RAG4DyG ), which enhances dynamic graph predictions by incorporating contextually and temporally relevant examples from broader graph structures. Our approach includes a time- and context-aware contrastive learning module to identify high-quality demonstrations and a graph fusion strategy to effectively integrate these examples with historical contexts. The proposed framework is designed to be effective in both transductive and inductive scenarios, ensuring adaptability to previously unseen nodes and evolving graph structures. Extensive experiments across multiple real-world datasets demonstrate the effectiveness of RAG4DyG in improving predictive accuracy and adaptability for dynamic graph modeling. The code and datasets are publicly available at https://github.com/YuxiaWu/RAG4DyG.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{liu_dragon_2025,
  title = {{DRAGON},
  author = {Liu, Shangyu and Zheng, Zhenzhe and Huang, Xiaoyao and Wu, Fan and Chen, Guihai and Wu, Jie},
  year = {2025},
  doi = {10.1145/3704413.3764419},
  url = {https://doi.org/10.1145/3704413.3764419},
  booktitle = {Proceedings of the {Twenty},
  pages = {221--230},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rice University, Houston, TX, USA},
  keywords = {device-cloud collaborative inference, large language model, retrieval-augmented generation, speculative aggregation},
  abstract = {Small language models (SLMs) support efficient deployments on resource-constrained edge devices, but their limited capacity compromises inference performance. Retrieval-augmented generation (RAG) is a promising solution to enhance model performance by integrating external databases, without requiring intensive on-device model retraining. However, large-scale public databases and user-specific private contextual documents are typically located on the cloud and the device, respectively, while existing RAG implementations are primarily centralized. To bridge this gap, we propose DRAGON, a distributed RAG framework to enhance on-device SLMs through both general and personal knowledge without the risk of leaking document privacy. Specifically, DRAGON decomposes multi-document RAG into multiple parallel token generation processes performed independently and locally on the cloud and the device, and employs a newly designed Speculative Aggregation, a dual-side speculative algorithm to avoid frequent output synchronization between the cloud and device. A new scheduling algorithm is further introduced to identify the optimal aggregation side based on real-time network conditions. Evaluations on real-world hardware testbed demonstrate a significant performance improvement of DRAGON—up to 1.9X greater gains over standalone SLM compared to the centralized RAG, substantial reduction in per-token latency, and negligible Time to First Token (TTFT) overhead.},
  address = {New York, NY, USA},
  series = {{MobiHoc},
  isbn = {979-8-4007-1353-8},
}

@inproceedings{wang_instructrag_2025,
  title = {{InstructRAG},
  author = {Wang, Zheng and Teo, Shu Xian and Chew, Jun Jie and Shi, Wei},
  year = {2025},
  doi = {10.1145/3726302.3730009},
  url = {https://doi.org/10.1145/3726302.3730009},
  booktitle = {Proceedings of the 48th {International},
  pages = {1413--1422},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {agent planning, large language model, retrieval-augmented generation, source: ACM},
  abstract = {Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought-action-observation (TAO) process to enhance LLM performance, but these approaches are often constrained by the LLMs' limited knowledge of complex tasks. Retrieval-augmented generation (RAG) offers new opportunities by leveraging external databases to ground generation in retrieved information. In this paper, we identify two key challenges (enlargability and transferability) in applying RAG to task planning. We propose InstructRAG, a novel solution within a multi-agent meta-reinforcement learning framework, to address these challenges. InstructRAG includes a graph to organize past instruction paths (sequences of correct actions), an RL-Agent with Reinforcement Learning to expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to improve task generalization for transferability. The two agents are trained end-to-end to optimize overall planning performance. Our experiments on four widely used task planning datasets demonstrate that InstructRAG significantly enhances performance and adapts efficiently to new tasks, achieving up to a 19.2\% improvement over the best existing approach.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{zhang_traceback_2025,
  title = {Traceback of poisoning attacks to retrieval-augmented generation},
  author = {Zhang, B. and Xin, H. and Fang, M. and Liu, Z. and Yi, B. and Li, T. and {...},
  year = {2025},
  doi = {10.1145/3696410.3714756},
  url = {https://doi.org/10.1145/3696410.3714756},
  booktitle = {Proceedings of the {ACM},
  journal = {Proceedings of the ACM …},
  pages = {2085--2097},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, poisoning attacks, retrieval-augmented generation, traceback, source: ACM},
  abstract = {… However, for the retriever and LLM, we consider a practical scenario where the RAG owner … : Improving LLM confidence in benign text: The core idea is to boost the LLM’s confidence in …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@article{wang_unveiling_2025,
  title = {Unveiling {Knowledge},
  author = {Wang, Y. and Ren, R. and Wang, Y. and Zhao, W. X. and Liu, J. and {...},
  year = {2025},
  doi = {10.1145/3726302.3730112},
  url = {https://doi.org/10.1145/3726302.3730112},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th …},
  pages = {1262--1271},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, knowledge utilization, large language models, retrieval-augmented generation, source: ACM},
  abstract = {… throughout the RAG process. At the microscopic level, we investigate the role of LLM modules in … and external information, and how it handles noisy information with factual errors. …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wu_time-sensitve_2024,
  title = {Time-{Sensitve},
  author = {Wu, Feifan and Liu, Lingyuan and He, Wentao and Liu, Ziqi and Zhang, Zhiqiang and Wang, Haofen and Wang, Meng},
  year = {2024},
  doi = {10.1145/3627673.3679800},
  url = {https://doi.org/10.1145/3627673.3679800},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {2544--2553},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {large language model, retrieval-augmented generation, retriever, supervised contrastive learning},
  abstract = {Retrieval-augmented generation (RAG) enhances large language models (LLMs) by accessing external data sources, offering a promising way to improve accuracy and reliability. Despite its potential, conventional retrievers encounter bias and flaws with time-sensitive queries. In this paper, a benchmark query dataset is constructed to retrieve documents containing time-evolving facts, and the results show that current embedding-based similarity-matching methods struggle to handle queries with explicit temporal constraints. Therefore, we propose a novel approach that integrates supervised contrastive learning with tailored negative sample pairs for temporal constraints to train the retriever of an RAG system, along with query-side fine-tuning and routing techniques. Experimental results show that our approach significantly enhances the retriever performance of time-sensitive queries while ensuring the effectiveness of general queries. We will make the code and dataset publicly available at https://github.com/suzhou-22/TS-Retriever.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@article{guerraoui_efficient_2025,
  title = {Efficient federated search for retrieval-augmented generation},
  author = {Guerraoui, R. and Kermarrec, A. M. and Petrescu, D. and {...},
  year = {2025},
  doi = {10.1145/3721146.3721942},
  url = {https://doi.org/10.1145/3721146.3721942},
  booktitle = {Proceedings of the 5th {Workshop},
  journal = {Proceedings of the 5th …},
  pages = {74--81},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, federated search, large language models, resource selection, retrieval-augmented generation, routing},
  abstract = {… RAG integrates LLM text generation with external information retrieval, enabling models to ground their responses in credible … corporates them into the LLM prompt before response gen…},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{EuroMLSys},
  isbn = {979-8-4007-1538-9},
}

@inproceedings{cheng_ragtrace_2025,
  title = {{RAGTrace},
  author = {Cheng, Sizhe and Li, Jiaping and Wang, Huanchen and Ma, Yuxin},
  year = {2025},
  doi = {10.1145/3746059.3747741},
  url = {https://doi.org/10.1145/3746059.3747741},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Evaluation, Knowledge Tracking, Retrieval-Augmented Generation},
  abstract = {Retrieval-Augmented Generation (RAG) systems have emerged as a promising solution to enhance large language models (LLMs) by integrating external knowledge retrieval with generative capabilities. While significant advancements have been made in improving retrieval accuracy and response quality, a critical challenge remains that the internal knowledge integration and retrieval-generation interactions in RAG workflows are largely opaque. This paper introduces RAGTrace, an interactive evaluation system designed to analyze retrieval and generation dynamics in RAG-based workflows. Informed by a comprehensive literature review and expert interviews, the system supports a multi-level analysis approach, ranging from high-level performance evaluation to fine-grained examination of retrieval relevance, generation fidelity, and cross-component interactions. Unlike conventional evaluation practices that focus on isolated retrieval or generation quality assessments, RAGTrace enables an integrated exploration of retrieval-generation relationships, allowing users to trace knowledge sources and identify potential failure cases. The system’s workflow allows users to build, evaluate, and iterate on retrieval processes tailored to their specific domains of interest. The effectiveness of the system is demonstrated through case studies and expert evaluations on real-world RAG applications.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{jiang_piperag_2025,
  title = {{PipeRAG},
  author = {Jiang, Wenqi and Zhang, Shuai and Han, Boran and Wang, Jie and Wang, Bernie and Kraska, Tim},
  year = {2025},
  doi = {10.1145/3690624.3709194},
  url = {https://doi.org/10.1145/3690624.3709194},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {589--600},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {computer systems, retrieval-augmented generation, vector search},
  abstract = {Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6× speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1245-6},
}

@inproceedings{li_knowtrace_2025,
  title = {{KnowTrace},
  author = {Li, Rui and Dai, Quanyu and Zhang, Zeyu and Chen, Xu and Dong, Zhenhua and Wen, Ji-Rong},
  year = {2025},
  doi = {10.1145/3711896.3737015},
  url = {https://doi.org/10.1145/3711896.3737015},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {1470--1480},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {large language models, retrieval-augmented generation, source: ACM},
  abstract = {Recent advances in retrieval-augmented generation (RAG) furnish large language models (LLMs) with iterative retrievals of relevant information to handle complex multi-hop questions. These methods typically alternate between LLM reasoning and retrieval to accumulate external information into the LLM's context. However, the ever-growing context inherently imposes an increasing burden on the LLM to perceive connections among critical information pieces, with futile reasoning steps further exacerbating this overload issue. In this paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the context overload and (2) bootstrap higher-quality multi-step reasoning. Instead of simply piling the retrieved contents, KnowTrace autonomously traces out desired knowledge triplets to organize a specific knowledge graph relevant to the input question. Such a structured workflow not only empowers the LLM with an intelligible context for inference, but also naturally inspires a reflective mechanism of knowledge backtracing to identify contributive LLM generations as process supervision data for self-bootstrapping. Extensive experiments show that KnowTrace consistently surpasses existing methods across three multi-hop question answering benchmarks, and the bootstrapped version further amplifies the gains.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@article{pinna_integration_2025,
  title = {Integration of {Retrieval},
  author = {Pinna, S. and Massa, S. M. and Fenu, M. and Casti, G. and {...},
  year = {2025},
  doi = {10.1145/3733155.3733192},
  url = {https://doi.org/10.1145/3733155.3733192},
  booktitle = {Proceedings of the 18th {ACM},
  journal = {Proceedings of the 18th …},
  pages = {277--284},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Differential diagnosis, e-Health, Large Language Models, Retrieval-Augmented Generation},
  abstract = {… RetrievalAugmented Generation (RAG) technique into a pre-trained Large Language Model (LLM… Future work suggests that integrating RAG will be crucial to mitigating the hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{PETRA},
  isbn = {979-8-4007-1402-3},
}

@inproceedings{jang_au-rag_2024,
  title = {{AU},
  author = {Jang, Jisoo and Li, Wen-Syan},
  year = {2024},
  doi = {10.1145/3673791.3698416},
  url = {https://doi.org/10.1145/3673791.3698416},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {2--11},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {source: ACM},
  abstract = {Retrieval Augmented Generation (RAG) has been effectively used to improve the accuracy of question-answering (Q\&amp;A) systems powered by Large Language Models (LLMs) by integrating local knowledge and more up-to-date content. However, traditional RAG methods, including those with re-ranking mechanisms, face challenges when dealing with large, frequently updated data sources or when accessing sources exclusively via APIs, as they require pre-encoding all content into embedding vectors. To address these limitations, we introduce Agent-based Universal RAG (AU-RAG), a novel approach that augments data sources with descriptive metadata, allowing an agent to dynamically search through diverse data pools. This agent-driven system can learn from examples to retrieve and consolidate data from various sources on the fly, functioning as a more flexible and adaptive RAG. We demonstrate AU-RAG's functionality with a financial analysis example and evaluate its performance using a multi-source QA dataset. The results show that AU-RAG performs comparably to RAG with re-ranking in data retrieval tasks while also demonstrating an enhanced ability to intelligently learn and access new data sources from examples, making it a robust solution for dynamic and complex information environments.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@article{bergman_leveraging_2025,
  title = {Leveraging {Approximate},
  author = {Bergman, S. A. and Ji, Z. and Kermarrec, A. M. and Petrescu, D. and {...},
  year = {2025},
  doi = {10.1145/3721146.3721941},
  url = {https://doi.org/10.1145/3721146.3721941},
  booktitle = {Proceedings of the 5th {Workshop},
  journal = {Proceedings of the 5th …},
  pages = {66--73},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, approximate caching, large language models, latency reduction, machine learning systems, neural information retrieval, query optimization, retrieval-augmented generation, vector databases},
  abstract = {… trust their outputs without extensive verification by domain experts [2, 4]. Retrieval-augmented generation (RAG… cache specifically designed for RAG-based LLM systems. By intercepting …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{EuroMLSys},
  isbn = {979-8-4007-1538-9},
}

@inproceedings{mahapatra_-storage_2025,
  title = {In-{Storage},
  author = {Mahapatra, Rohan and Santhanam, Harsha and Priebe, Christopher and Xu, Hanyang and Esmaeilzadeh, Hadi},
  year = {2025},
  doi = {10.1145/3695053.3731032},
  url = {https://doi.org/10.1145/3695053.3731032},
  booktitle = {Proceedings of the 52nd {Annual},
  pages = {450--466},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {In-Storage Acceleration, Large Language Models, LLM, RAG, Retrieval-Augmented Generation, Specialized Accelerators, source: Scopus},
  abstract = {Retrieval-augmented generation (RAG) services are rapidly gaining adoption in enterprise settings as they combine information retrieval systems (e.g., databases) with large language models (LLMs) to enhance response generation and reduce hallucinations. By augmenting an LLM’s fixed pre-trained knowledge with real-time information retrieval, RAG enables models to effectively extend their context to large knowledge bases by selectively retrieving only the most relevant information. As a result, RAG provides the effect of dynamic updates to the LLM’s knowledge without requiring expensive and time-consuming retraining. While some deployments keep the entire database in memory, RAG services are increasingly shifting toward persistent storage to accommodate ever-growing knowledge bases, enhance utility, and improve cost-efficiency. However, this transition fundamentally reshapes the system’s performance profile: empirical analysis reveals that the Search \&amp; Retrieval phase emerges as the dominant contributor to end-to-end latency. This phase typically involves (1) running a smaller language model to generate query embeddings, (2) executing similarity and relevance checks over varying data structures, and (3) performing frequent, long-latency accesses to persistent storage. To address this triad of challenges, we propose a metamorphic in-storage accelerator architecture that provides the necessary programmability to support diverse RAG algorithms, dynamic data structures, and varying computational patterns. The architecture also supports in-storage execution of smaller language models for query embedding generation while final LLM generation is executed on DGX A100 systems. Experimental results show up to 4.3 × and 1.5 × improvement in end-to-end throughput compared to conventional retrieval pipelines using Xeon CPUs with NVMe storage and A100 GPUs with DRAM, respectively.},
  address = {New York, NY, USA},
  series = {{ISCA},
  isbn = {979-8-4007-1261-6},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{simoni_morse_2025,
  title = {{MoRSE},
  author = {Simoni, M. and Saracino, A. and {VP},
  year = {2025},
  doi = {10.1145/3672608.3707898},
  url = {https://doi.org/10.1145/3672608.3707898},
  booktitle = {Proceedings of the 40th {ACM},
  journal = {Proceedings of the 40th ACM …},
  pages = {1213--1222},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, cyber threat intelligence, cybersecurity, large language model, retrieval augmented generation},
  abstract = {… These hallucinations often occur when the model lacks … to ensure the relevance of LLM responses. As a result, … We validated these results with the LLM as a Judge method [33], which …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SAC},
  isbn = {979-8-4007-0629-5},
}

@inproceedings{tu_robust_2025,
  title = {Robust {Fine},
  author = {Tu, Yiteng and Su, Weihang and Zhou, Yujia and Liu, Yiqun and Ai, Qingyao},
  year = {2025},
  doi = {10.1145/3726302.3730078},
  url = {https://doi.org/10.1145/3726302.3730078},
  booktitle = {Proceedings of the 48th {International},
  pages = {1272--1282},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {fine-tuning, retrieval augmented generation, robust},
  abstract = {Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base (i.e., the retrieval system). In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{gokdemir_hiperrag_2025,
  title = {{HiPerRAG},
  author = {Gokdemir, O. and Siebenschuh, C. and Brace, A. and Wells, A. and {...},
  year = {2025},
  doi = {10.1145/3732775.3733586},
  url = {https://doi.org/10.1145/3732775.3733586},
  booktitle = {Proceedings of the {Platform},
  journal = {Proceedings of the …},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, AI, HPC, large language models, metric learning, neural information retrieval, retrieval-augmented generation},
  abstract = {… hallucinations of LLMs through the integration of neural information retrieval with LLM-based … Quantifying relevance in this manner improves the factuality of LLM outputs by dynamically …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{PASC},
  isbn = {979-8-4007-1886-1},
}

@inproceedings{liu_mask-based_2025,
  title = {Mask-based {Membership},
  author = {Liu, Mingrui and Zhang, Sixiao and Long, Cheng},
  year = {2025},
  doi = {10.1145/3696410.3714771},
  url = {https://doi.org/10.1145/3696410.3714771},
  booktitle = {Proceedings of the {ACM},
  pages = {2894--2907},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {membership inference attacks, retrieval-augmented generation, source: ACM, source: Scopus},
  abstract = {Retrieval-Augmented Generation (RAG) has been an effective approach to mitigate hallucinations in large language models (LLMs) by incorporating up-to-date and domain-specific knowledge. Recently, there has been a trend of storing up-to-date or copyrighted data in RAG knowledge databases instead of using it for LLM training. This practice has raised concerns about Membership Inference Attacks (MIAs), which aim to detect if a specific target document is stored in the RAG system's knowledge database so as to protect the rights of data producers. While research has focused on enhancing the trustworthiness of RAG systems, existing MIAs for RAG systems remain largely insufficient. Previous work either relies solely on the RAG system's judgment or is easily influenced by other documents or the LLM's internal knowledge, which is unreliable and lacks explainability. To address these limitations, we propose a \&lt;u\&gt;M\&lt;/u\&gt;ask-\&lt;u\&gt;B\&lt;/u\&gt;ased Membership Inference \&lt;u\&gt;A\&lt;/u\&gt;ttacks (MBA) framework. Our framework first employs a masking algorithm that effectively masks a certain number of words in the target document. The masked text is then used to prompt the RAG system, and the RAG system is required to predict the mask values. If the target document appears in the knowledge database, the masked text will retrieve the complete target document as context, allowing for accurate mask prediction. Finally, we adopt a simple yet effective threshold-based method to infer the membership of target document by analyzing the accuracy of mask prediction. Our mask-based approach is more document-specific, making the RAG system's generation less susceptible to distractions from other documents or the LLM's internal knowledge. Extensive experiments demonstrate the effectiveness of our approach compared to existing baseline models.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{xu_p-rag_2024,
  title = {P-{RAG},
  author = {Xu, Weiye and Wang, Min and Zhou, Wengang and Li, Houqiang},
  year = {2024},
  doi = {10.1145/3664647.3680661},
  url = {https://doi.org/10.1145/3664647.3680661},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {6969--6978},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {embodied ai, large language model, progressive method, retrieval augmented generation, source: Scopus},
  abstract = {Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
  annote = {Cited by: 4},
}

@inproceedings{tang_boosting_2025,
  title = {Boosting {Retrieval},
  author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi},
  year = {2025},
  doi = {10.1145/3726302.3729907},
  url = {https://doi.org/10.1145/3726302.3729907},
  booktitle = {Proceedings of the 48th {International},
  pages = {2441--2451},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {generative retrieval, retrieval-augmented generation, source: Scopus},
  abstract = {Large language models (LLMs) have shown success in knowledge-intensive tasks, including closed-book question answering and entity linking. However, their susceptibility to hallucination undermines their reliability. Retrieval-augmented generation (RAG) partially addresses this issue by combining a retriever to locate relevant documents and a generator to produce responses grounded in the retrieved evidence. Despite its advantages, RAG faces challenges: (i) the structural gap between traditional dense retrievers and autoregressive generators, and (ii) limited generation performance due to insufficient contextual guidance returned by the retriever. To tackle these limitations, we propose MINT, a novel framework that enhances RAG by co-training Retrieval-augMented generatIon and geNeration-augmented reTrieval (GAR). MINT (i) bridges the gap between the retriever and generator using a unified encoder-decoder structure (ii) incorporates an iterative co-training strategy between RAG and GAR, enabling mutual enhancement through pseudo-samples generation, and (iii) introduces three heuristic inference strategies to generate relevant document identifiers and answers. We conduct an empirical study on the KILT benchmark, and MINT is found to yield significant improvements in both retrieval and generation tasks compared with prevailing baselines.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{li_oreo_2025,
  title = {Oreo: {A},
  author = {Li, Sha and Ramakrishnan, Naren},
  year = {2025},
  doi = {10.1145/3731120.3744590},
  url = {https://doi.org/10.1145/3731120.3744590},
  booktitle = {Proceedings of the 2025 {International},
  pages = {238--253},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {contrastive learning, prompt optimization, retrieval augmented generation, source: ACM},
  abstract = {Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporating external documents or chunks prior to generation. However, even improved retriever relevance can bring erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific, format. Through a three-stage training paradigm–comprising supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment–it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{pogrebinsky_enhancing_2025,
  title = {Enhancing {Retrieval},
  author = {Pogrebinsky, Idan and Carmel, David and Kurland, Oren},
  year = {2025},
  doi = {10.1145/3731120.3744610},
  url = {https://doi.org/10.1145/3731120.3744610},
  booktitle = {Proceedings of the 2025 {International},
  pages = {410--415},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {llm, query selection, rag, text completion, source: ACM},
  abstract = {In a Retrieval-Augmented Generation (RAG) system designed for the fundamental task of text completion, a query is derived from the prompt of a large language model (LLM) and is issued to an external resource. The retrieved content is then incorporated into the prompt to enhance text completion. Since prompts can be considerably long, it is common practice to use a short suffix of the prompt as the query. We empirically show, using a suite of oracle experiments, that this approach is often suboptimal with respect to other choices of a query from the prompt. This finding gives rise to a novel research challenge: identifying the optimal query for RAG in this setting. As an initial study, we propose a few query selection methods, some of which yield statistically significant improvements over using the prompt's suffix as a query.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{rau_context_2025,
  title = {Context {Embeddings},
  author = {Rau, David and Wang, Shuai and Déjean, Hervé and Clinchant, Stéphane and Kamps, Jaap},
  year = {2025},
  doi = {10.1145/3701551.3703527},
  url = {https://doi.org/10.1145/3701551.3703527},
  booktitle = {Proceedings of the {Eighteenth},
  pages = {493--502},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hannover, Germany},
  keywords = {context compression, llm, question-answering, retrieval-augmented generation},
  abstract = {Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge of LLMs by extending the input with external information. As a consequence, the contextual inputs to the model become much longer slowing down decoding time affecting the time a user has to wait for an answer. We address this challenge by presenting COCOM, an effective context compression method, reducing long contexts to only a handful of Context Embeddings, speeding up the generation time by a large margin. Our method allows for different compression rates, trading off decoding time for answer quality. Compared to earlier methods, COCOM allows for handling multiple contexts more effectively, significantly reducing decoding time for long inputs. Our method demonstrates an inference speed-up of up to 5.69 times while achieving higher performance compared to existing efficient context compression methods},
  address = {New York, NY, USA},
  series = {{WSDM},
  isbn = {979-8-4007-1329-3},
}

@inproceedings{li_lexrag_2025,
  title = {{LexRAG},
  author = {Li, Haitao and Chen, Yifan and YiRan, Hu and Ai, Qingyao and Chen, Junjie and Yang, Xiaoyu and Yang, Jianhui and Wu, Yueyue and Liu, Zeyang and Liu, Yiqun},
  year = {2025},
  doi = {10.1145/3726302.3730340},
  url = {https://doi.org/10.1145/3726302.3730340},
  booktitle = {Proceedings of the 48th {International},
  pages = {3606--3615},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {legal consultation conversation, multi-turn, retrieval-augmented generation},
  abstract = {Retrieval-augmented generation (RAG) has proven highly effective in improving large language models (LLMs) across various domains. However, there is no benchmark specifically designed to assess the effectiveness of RAG in the legal domain, which restricts progress in this area. To fill this gap, we propose LexRAG, the first benchmark to evaluate RAG systems for multi-turn legal consultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228 candidate legal articles. Each sample is annotated by legal experts and consists of five rounds of progressive questioning. LexRAG includes two key tasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of relevant legal articles based on multi-turn context. (2) Response generation, focusing on producing legally sound answers. To ensure reliable reproducibility, we develop LexiT, a legal RAG toolkit that provides a comprehensive implementation of RAG system components tailored for the legal domain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to enable detailed and effective assessment. Through experimental analysis of various LLMs and retrieval methods, we reveal the key limitations of existing RAG systems in handling legal consultation conversations. LexRAG establishes a new benchmark for the practical application of RAG systems in the legal domain, with its code and data available at https://github.com/CSHaitao/LexRAG.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{fuchs_lissa_2025,
  title = {{LiSSA},
  author = {Fuchß, Dominik and Hey, Tobias and Keim, Jan and Liu, Haoyu and Ewald, Niklas and Thirolf, Tobias and Koziolek, Anne},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00186},
  url = {https://doi.org/10.1109/icse55347.2025.00186},
  booktitle = {Proceedings of the {IEEE},
  pages = {1396--1408},
  publisher = {IEEE Press},
  keywords = {large language models, retrieval-augmented generation, traceability link recovery},
  abstract = {There are a multitude of software artifacts which need to be handled during the development and maintenance of a software system. These artifacts interrelate in multiple, complex ways. Therefore, many software engineering tasks are enabled — and even empowered — by a clear understanding of artifact interrelationships and also by the continued advancement of techniques for automated artifact linking.However, current approaches in automatic Traceability Link Recovery (TLR) target mostly the links between specific sets of artifacts, such as those between requirements and code. Fortunately, recent advancements in Large Language Models (LLMs) can enable TLR approaches to achieve broad applicability. Still, it is a nontrivial problem how to provide the LLMs with the specific information needed to perform TLR.In this paper, we present LiSSA, a framework that harnesses LLM performance and enhances them through Retrieval-Augmented Generation (RAG). We empirically evaluate LiSSA on three different TLR tasks, requirements to code, documentation to code, and architecture documentation to architecture models, and we compare our approach to state-of-the-art approaches.Our results show that the RAG-based approach can significantly outperform the state-of-the-art on the code-related tasks. However, further research is required to improve the performance of RAG-based approaches to be applicable in practice.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{liu_heterrag_2025,
  title = {{HeterRAG},
  author = {Liu, Chaoqiang and Liu, Haifeng and Chen, Dan and Huang, Yu and Zhang, Yi and Xiao, Wenjing and Liao, Xiaofei and Jin, Hai},
  year = {2025},
  doi = {10.1145/3695053.3731089},
  url = {https://doi.org/10.1145/3695053.3731089},
  booktitle = {Proceedings of the 52nd {Annual},
  pages = {884--898},
  publisher = {Association for Computing Machinery},
  keywords = {Approximate nearest neighbor search, DIMM, HBM, Large language models, Processing-in-memory, Retrieval-augmented generation},
  abstract = {By integrating external knowledge bases, Retrieval-augmented Generation (RAG) enhances natural language generation for knowledge-intensive scenarios and specialized domains, producing content that is both more informative and personalized. RAG systems typically consist of two fundamental stages: retrieval and generation. The retrieval stage experiences low bandwidth utilization due to its random and irregular memory access patterns. Meanwhile, the generation stage is also constrained by memory bandwidth limitations, which arise from involving a significant number of General Matrix-Vector Multiplications (GEMV) operations. These two stages collectively lead to memory bottlenecks within RAG systems. Recent efforts leverage HBM-based Processing-in-Memory (PIM) to accelerate conventional Large Language Models (LLMs). However, the retrieval stage incurs substantial storage overhead due to the need to maintain large-scale knowledge bases, resulting in a capacity bottleneck. Solely relying on HBM-based PIM in RAG is both costly and insufficient to meet the capacity demands. Fortunately, DIMM-based PIM provides a low-cost, high-capacity alternative that complements HBM. In this work, we propose HeterRAG, a novel heterogeneous PIM acceleration system for RAG. It combines HBM-based PIM and DIMM-based PIM to achieve high performance, energy efficiency, and low hardware cost. HeterRAG uses HBM-based PIM for the generation stage to meet bandwidth needs and DIMM-based PIM for the retrieval stage to satisfy memory capacity requirements. To further improve performance, HeterRAG incorporates three software–hardware co-optimization techniques: locality-aware retrieval, locality-aware generation, and fine-grained parallel pipelining. Experimental results demonstrate that, compared to RAG systems deployed on Intel Xeon CPUs and NVIDIA GPUs, HeterRAG achieves up to 26.5 × higher throughput, up to 27.6 × lower latency, and up to 2.8 × greater energy efficiency.},
  address = {New York, NY, USA},
  series = {{ISCA},
  isbn = {979-8-4007-1261-6},
}

@inproceedings{ruamsuk_enhancing_2025,
  title = {Enhancing {Retrieval},
  author = {Ruamsuk, Yanakorn and Mingkhwan, Anirach and Unger, Herwig},
  year = {2025},
  doi = {10.1145/3711542.3711558},
  url = {https://doi.org/10.1145/3711542.3711558},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {265--271},
  publisher = {Association for Computing Machinery},
  keywords = {Co-Occurrence Graph, Information Retrieval, Retrieval-Augmented Generation, Text-Representing Centroid, Vector Database},
  abstract = {This paper introduces a novel approach to enhance Retrieval-Augmented Generation (RAG) systems by integrating Text-Representing Centroid (TRC) methodology. Addressing the limitations of traditional vector databases, this method preserves structural relationships and adapts to content complexity, improving information retrieval efficiency and accuracy. Key contributions include advanced graph construction, relevance scoring algorithms, and extensive validation, with discussions on potential applications and future research. Empirical evidence demonstrates that TRC methods achieve a 75 percent success rate on 100 questions, outperforming traditional vector methods.},
  address = {New York, NY, USA},
  series = {{NLPIR},
  isbn = {979-8-4007-1738-3},
}

@inproceedings{zhang_exploring_2025,
  title = {Exploring {Efficient},
  author = {Zhang, Yining and Peng, Yinan and Tu, Chengying and Zhang, Zherui and Yan, Hongfei and Chen, Chong and Ma, Hao and Yang, Jia and Zhang, Yan and Liao, Rikun},
  year = {2025},
  doi = {10.1145/3677389.3702522},
  url = {https://doi.org/10.1145/3677389.3702522},
  booktitle = {Proceedings of the 24th {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, China},
  keywords = {document relevance, fine-tuning, research data, retrieval-augmented generation, source: Scopus},
  abstract = {Recent advances in large language models (LLM) have brought an explosive growth to chat-bot applications. Among them, retrieval-augmented generation, which provides extra context to make LLM capable of answering out-of-domain questions is becoming a popular method. However, naive implementation of RAG usually cannot reach ideal answer quality in complicated real-world scenarios. Researchers have proposed a number of methods to improve RAG, but many of them involves extra LLM calls which is too time-consuming for online application. In this paper, we explored practical techniques and designs in RAG that improve answers to user-satisfying quality while keeping the response latency at a moderate level in the scenario of a research data QA application in university. Our main findings include introducing a relevance judge with small-scale LLM for retrieved documents can effectively filter out less relevant ones, which can otherwise disrupt the generated answer greatly, and decomposing the generation task into multiple independent sub-tasks can reduce the chance of hallucination and also accelerates the generation. As for model performance, prompt engineering and fine-tuning (through learning from strong LLM) are effective yet simple ways to enhance answer quality. Our results and experience provide insights for building future real-world LLM applications.},
  address = {New York, NY, USA},
  series = {{JCDL},
  isbn = {979-8-4007-1093-3},
  annote = {Cited by: 0},
}

@article{hu_prompt_2024,
  title = {Prompt perturbation in retrieval-augmented generation based large language models},
  author = {Hu, Z. and Wang, C. and Shu, Y. and Paik, H. Y. and Zhu, L.},
  year = {2024},
  doi = {10.1145/3637528.3671932},
  url = {https://doi.org/10.1145/3637528.3671932},
  booktitle = {Proceedings of the 30th {ACM},
  journal = {Proceedings of the 30th ACM …},
  pages = {1119--1130},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, LLM, prompt attack, retrieval-augmented generation, robustness, source: ACM},
  abstract = {… The robustness of RAG needs to be carefully … LLM’s neuron activation and introduce methods to improve the robustness of RAG-based LLMs by detecting perturbations and factual …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{packowski_optimizing_2025,
  title = {Optimizing and {Evaluating},
  author = {Packowski, Sarah and Halilovic, Inge and Schlotfeldt, Jenifer and Smith, Trish},
  year = {2025},
  doi = {10.1145/3704137.3704181},
  url = {https://doi.org/10.1145/3704137.3704181},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {162--167},
  publisher = {Association for Computing Machinery},
  keywords = {Large language models, RAG, Retrieval-augmented generation},
  abstract = {Retrieval-augmented generation (RAG) is a popular technique for using large language models (LLMs) to build customer-support, question-answering solutions. In this paper, we share our team’s practical experience building and maintaining enterprise-scale RAG solutions that answer users’ questions about our software based on product documentation. Our experience has not always matched the most common patterns in the RAG literature. This paper focuses on solution strategies that are modular and model-agnostic. For example, our experience over the past few years - using different search methods and LLMs, and many knowledge base collections - has been that simple changes to the way we create knowledge base content can have a huge impact on our RAG solutions’ success. In this paper, we also discuss how we monitor and evaluate results. Common RAG benchmark evaluation techniques have not been useful for evaluating responses to novel user questions, so we have found a flexible, "human in the lead" approach is required.},
  address = {New York, NY, USA},
  series = {{ICAAI},
  isbn = {979-8-4007-1801-4},
}

@inproceedings{tan_pear_2025,
  title = {{PEAR},
  author = {Tan, Tao and Qian, Yining and Lv, Ang and Lin, Hongzhan and Wu, Songhao and Wang, Yongbo and Wang, Feng and Wu, Jingtong and Lu, Xin and Yan, Rui},
  year = {2025},
  doi = {10.1145/3696410.3714795},
  url = {https://doi.org/10.1145/3696410.3714795},
  booktitle = {Proceedings of the {ACM},
  pages = {1693--1702},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {context awareness, large language model, re-weighting attention heads, retrieval-augmented generation, source: ACM},
  abstract = {Large language models (LLMs) enhanced with retrieval-augmented generation (RAG) have introduced a new paradigm for web search. However, the limited context awareness of LLMs degrades their performance on RAG tasks. Existing methods to enhance context awareness are often inefficient, incurring time or memory overhead during inference, and many are tailored to specific position embeddings. In this paper, we propose Position-Embedding-Agnostic attention Re-weighting (PEAR), which enhances the context awareness of LLMs with zero inference overhead. Specifically, on a proxy task focused on context copying, we first detect heads which suppress the models' context awareness, thereby diminishing RAG performance. To weaken the impact of these heads, we re-weight their outputs with learnable coefficients. The LLM (with frozen parameters) is optimized by adjusting these coefficients to minimize loss on the proxy task. During inference, the optimized coefficients are fixed to re-weight these heads, regardless of the specific task at hand. Our proposed PEAR offers two major advantages over previous approaches: (1) It introduces zero additional inference overhead in terms of memory usage or inference time, while outperforming competitive baselines in accuracy and efficiency across various RAG tasks. (2) It is independent of position embedding algorithms, ensuring broader applicability. Our code is available at https://github.com/TTArch/PEAR-RAG.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@article{wu_lighter_2025,
  title = {Lighter and better: {Towards},
  author = {Wu, C. and Shao, N. and Liu, Z. and Xiao, S. and Li, C. and Zhang, C. and {...},
  year = {2025},
  doi = {10.1145/3701551.3703580},
  url = {https://doi.org/10.1145/3701551.3703580},
  booktitle = {Proceedings of the {Eighteenth},
  journal = {Proceedings of the …},
  pages = {271--280},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, large language models, retrieval augmented generation, source: ACM},
  abstract = {… As no modification is made to the LLM’s original parameters, we can optimize the performance in RAG … relevant knowledge, RAG significantly enhances the truthfulness and credibility of …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{WSDM},
  isbn = {979-8-4007-1329-3},
}

@article{zhu_emerge_2024,
  title = {Emerge: {Enhancing},
  author = {Zhu, Y. and Ren, C. and Wang, Z. and Zheng, X. and Xie, S. and Feng, J. and {...},
  year = {2024},
  doi = {10.1145/3627673.3679582},
  url = {https://doi.org/10.1145/3627673.3679582},
  booktitle = {Proceedings of the 33rd {ACM},
  journal = {Proceedings of the 33rd …},
  pages = {3549--3559},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, electronic health record, large language model, multimodal learning, retrieval-augmented generation, source: ACM},
  abstract = {… entity, the entities extracted by LLM have hallucination issues. Accurately … RAG-driven enhancement pipeline. “LM” denotes Language Model (basically BERT-based model), while “LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{liang_efficient_2025,
  title = {Efficient and verifiable responses using {Retrieval},
  author = {Liang, Henry and Zhou, Yu and Gurbani, Vijay K},
  year = {2025},
  doi = {10.1145/3703412.3703431},
  url = {https://doi.org/10.1145/3703412.3703431},
  booktitle = {Proceedings of the 4th {International},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {hybrid embedding, hybrid retrieval, LLM, RAG, RFP, source: Scopus},
  abstract = {The rise of large language models (LLMs) like ChatGPT has greatly enhanced the efficiency of everyday tasks through automation. However, the deployment of LLMs for tasks such as responding to Request-for-Proposals (RFPs) is hindered by deficiencies like hallucinations and lack of response provenance. For such tasks, the aim of an automated response is to generate precise answers that can still be quickly reviewed and corrected by a human; therefore it is critical to optimize the system such that relevant source document sections are identified for as many questions as possible, and all relevant contexts are attributed correctly; this makes LLMs alone insufficient for this task. We present an improved Retrieval Augmented Generation (RAG) architecture for automated RFP completion that enhances relevant content generation and significantly reduces manual effort in drafting responses. The proposed improvements are two-fold: we present a novel text embedding scheme that combines a dense contextual embedding with a sparse statistical embedding for document retrieval, and we improve on the provenance of the generated response by presenting an algorithm that accurately provides the document page numbers as references when generating the answers. The practical deployment of this solution highlights its potential for automatic RFP completion, as well as its ability to act as an architecture for applications in various domains with differing complexity levels, especially when efficiency, accuracy, and verifiable responses are paramount.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1161-9},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{barnett_seven_2024,
  title = {Seven failure points when engineering a retrieval augmented generation system},
  author = {Barnett, S. and Kurniawan, S. and Thudumu, S. and {...},
  year = {2024},
  doi = {10.1145/3644815.3644945},
  url = {https://doi.org/10.1145/3644815.3644945},
  booktitle = {Proceedings of the {IEEE},
  journal = {Proceedings of the …},
  pages = {194--199},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, case study, RAG, retrieval augmented generation, SE4AI},
  abstract = {… using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from … serving a fine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG) Systems that rely …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{CAIN},
  isbn = {979-8-4007-0591-5},
}

@inproceedings{difallah_wikirag_2025,
  title = {{WikiRAG},
  author = {Difallah, Djellel},
  year = {2025},
  doi = {10.1145/3711896.3737444},
  url = {https://doi.org/10.1145/3711896.3737444},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5391--5401},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {benchmarking, knowledge graph completion, large language models, retrieval augmented generation, wikidata, source: ACM},
  abstract = {Link prediction is an important task for knowledge graph completion and curation, and it has received significant attention from the research community. However, researchers often train and evaluate new models on small or outdated datasets that do not reflect the current state of knowledge, thereby disregarding new information and the rich textual content often linked to knowledge graphs. As a result, many opportunities to leverage these dimensions are missed. We introduce WikiRAG, a framework for knowledge completion and evaluation derived from Wikidata and Wikipedia, which enables research integrating retrieval techniques and large language models. Our framework combines the following contributions: (i) We revisit the Wikidata5M dataset by updating it to reflect the current state of Wikidata and providing automated tools for its periodic maintenance. (ii) We enrich the dataset with long-form textual content sourced from Wikipedia, enabling research that goes beyond traditional graph structures and shallow text methods toward dense retrieval techniques. (iii) We propose a simple yet effective baseline that leverages retrieval-augmented generation, demonstrating the utility of the dataset and integrating language model capabilities for link prediction. The revised dataset, coined Wikidata5M-RE, shows that the original graph grew by roughly 50\% in the number of edges, while 10\% of the edges have been removed. A comparative analysis of classic methods demonstrates that these changes can impact downstream task evaluation. Finally, our evaluation of WikiRAG's KGC method shows an improvement of up to 9\% in link prediction accuracy over state-of-the-art baselines, setting the stage for a new avenue in knowledge completion that uses deep information extraction. The source code, data, and other artifacts have been made available on the project website: https://github.com/colab-nyuad/WikiRAG},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{zhao_medrag_2025,
  title = {{MedRAG},
  author = {Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao, Chunyan},
  year = {2025},
  doi = {10.1145/3696410.3714782},
  url = {https://doi.org/10.1145/3696410.3714782},
  booktitle = {Proceedings of the {ACM},
  pages = {4442--4457},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {decision support, healthcare copilot, knowledge graph, large language models, retrieval-augmented generation},
  abstract = {Retrieval-augmented generation (RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations. MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates. Our code will be available at https://github.com/SNOWTEAM2023/MedRAG},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{soudani_fine_2024,
  title = {Fine {Tuning},
  author = {Soudani, Heydar and Kanoulas, Evangelos and Hasibi, Faegheh},
  year = {2024},
  doi = {10.1145/3673791.3698415},
  url = {https://doi.org/10.1145/3673791.3698415},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {12--22},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {data augmentation, fine tuning, retrieval augmented generation, source: ACM},
  abstract = {Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{wang_quantitative_2025,
  title = {Quantitative {Evaluation},
  author = {Wang, Kevin Shukang and Lawrence, Ramon},
  year = {2025},
  doi = {10.1145/3641554.3701917},
  url = {https://doi.org/10.1145/3641554.3701917},
  booktitle = {Proceedings of the 56th {ACM},
  pages = {1183--1189},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {artificial intelligence, human-in-the-loop, large language model, question answering, retrieval-augmented generation, source: ACM},
  abstract = {Generative artificial intelligence (GenAI) is transforming Computer Science education, and every instructor is reflecting on how AI will impact their courses. Instructors must determine how students may use AI for course activities and what AI systems they will support and encourage students to use. This task is challenging with the proliferation of large language models (LLMs) and related AI systems. The contribution of this work is an experimental evaluation of the performance of multiple open-source and commercial LLMs utilizing retrieval-augmented generation in answering questions for computer science courses and a cost-benefit analysis for instructors when determining what systems to use. A key factor is the time an instructor has to maintain their supported AI systems and the most effective activities for improving their performance. The paper offers recommendations for deploying, using, and enhancing AI in educational settings.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0531-1},
  series = {{SIGCSETS},
}

@inproceedings{dai_next-search_2025,
  title = {{NExT},
  author = {Dai, Sunhao and Wang, Wenjie and Pang, Liang and Xu, Jun and Ng, See-Kiong and Wen, Ji-Rong and Chua, Tat-Seng},
  year = {2025},
  doi = {10.1145/3726302.3730353},
  url = {https://doi.org/10.1145/3726302.3730353},
  booktitle = {Proceedings of the 48th {International},
  pages = {3922--3931},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {generative ai search, large language model, user feedback, source: ACM},
  abstract = {Generative AI search driven by large language models (LLMs) is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline-spanning query decomposition, document retrieval, and answer generation-yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop.To address this limitation, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages-such as refining query decomposition, rating retrieved documents, and editing initial generated responses-and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. As these feedback signals serve as valuable resources for refining the whole search pipeline, we also introduce a feedback store mechanism that encourages users to share and monetize their debugging efforts, further incentivizing participation. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{nandy_balancing_2025,
  title = {Balancing {Health},
  author = {Nandy, Gargi and Gupta, Srishti and Mohammad Afzali, Farhad and Peeples, Eric and Pilon, Betsy and Tsai, Chun-Hua},
  year = {2025},
  doi = {10.1145/3708319.3733709},
  url = {https://doi.org/10.1145/3708319.3733709},
  booktitle = {Adjunct {Proceedings},
  pages = {249--254},
  publisher = {Association for Computing Machinery},
  keywords = {chatbot, healthcare, HIE, large language models},
  abstract = {Family caregivers play a vital role in supporting children with chronic health conditions, such as neonates diagnosed with hypoxic-ischemic encephalopathy (HIE). However, navigating complex medical information can be overwhelming due to the quantity and quality of available literature. This study leverages Retrieval-Augmented Generation (RAG)-based Large Language Models (LLMs) to develop a chatbot that integrates peer-reviewed scientific literature and provides personalized, simplified summaries for caregivers. A user study involving six caregivers and five healthcare providers demonstrated the chatbot’s ability to enhance clarity, improve comprehension, and deliver essential medical information concisely. Our findings highlight the potential of RAG-based LLMs to enhance caregivers’ health literacy and support their information-seeking behavior, while also underscoring the importance of thoughtfully navigating the differing expectations of caregivers and healthcare providers regarding the type, depth, and presentation of medical information.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1399-6},
}

@inproceedings{dela_rosa_video-enriched_2025,
  title = {Video-{Enriched},
  author = {Dela Rosa, Kevin},
  year = {2025},
  doi = {10.1145/3701716.3716890},
  url = {https://doi.org/10.1145/3701716.3716890},
  booktitle = {Companion {Proceedings},
  pages = {1663--1667},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {agentic information retrieval, chatbots, multimodal retrieval},
  abstract = {In this work, we propose the use of ”aligned video captions” as an intelligent mechanism for integrating video content into retrieval augmented generation (RAG) based AI assistant systems. These captions serve as an efficient representation layer between videos and large language models (LLMs), describing both visual and audio content while requiring significantly less context window space compared to traditional frame sampling approaches. We demonstrate how this representation enables more effective agent-based retrieval and generation capabilities, with captions that can be dynamically adapted through targeted prompting or fine-tuning of the underlying models. Our empirical evaluation across multiple LLM configurations shows that this approach achieves comparable performance to direct video processing while being more computationally efficient and easier to reason about in downstream tasks. Notably, the approach shows particular strength in procedural content like How-To videos, where aligned captions significantly outperform speech-only baselines.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{yu_droidcoder_2024,
  title = {{DroidCoder},
  author = {Yu, Xinran and Li, Chun and Pan, Minxue and Li, Xuandong},
  year = {2024},
  doi = {10.1145/3691620.3695063},
  url = {https://doi.org/10.1145/3691620.3695063},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {681--693},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {Android, code completion, retrieval-augmented generation},
  abstract = {Android is the most popular mobile operating system. However, Android development requires extensive coding, especially for unique features such as lifecycle callbacks and UI widgets. Existing code completion methods typically utilize Retrieval-Augmented Generation (RAG) to provide contextual information for pre-trained code large language models (Code LLMs) to perform completion. Despite considerable progress in these methods, their effectiveness in Android development remains limited. This is because the features of Android development make it challenging for existing retrieval mechanisms to extract sufficient context effectively. In response, we propose DroidCoder, a novel Android code completion framework that employs Android development features and contextual information of code snippets to enrich RAG. It also incorporates a specifically designed loss function to fine-tune the model, enabling it to better utilize context-enhanced RAG for Android code completion. We evaluated our method on three base models and different types of applications, comparing it with two state-of-the-art code completion methods. The experimental results demonstrate that our method significantly outperforms the baselines at line-level and multi-line-level code completion and improves the quality of the completed code.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@article{xu_face4rag_2024,
  title = {Face4rag: {Factual},
  author = {Xu, Y. and Cai, T. and Jiang, J. and Song, X.},
  year = {2024},
  doi = {10.1145/3637528.3671656},
  url = {https://doi.org/10.1145/3637528.3671656},
  booktitle = {Proceedings of the 30th {ACM},
  journal = {Proceedings of the 30th ACM SIGKDD …},
  pages = {6083--6094},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, factual consistency evaluation, large language model},
  abstract = {… RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality … methods for factual inconsistency detection …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@article{jiao_pr-attack_2025,
  title = {Pr-attack: {Coordinated},
  author = {Jiao, Y. and Wang, X. and Yang, K.},
  year = {2025},
  doi = {10.1145/3726302.3730058},
  url = {https://doi.org/10.1145/3726302.3730058},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th International ACM …},
  pages = {656--667},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, bilevel optimization, large language models, retrieval-augmented generation},
  abstract = {… , thereby reducing hallucinations and enhancing the reliability of LLM outputs. RAG essentially consists of three key components [104], ie, knowledge database, retriever, and LLM. A …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhang_large-language_2025,
  title = {A large-language model-driven approach to chronic disease follow-up},
  author = {Zhang, Yang and Feng, Sidu and Zhai, Tianzhang and Pan, Rumo and Li, Jian},
  year = {2025},
  doi = {10.1145/3745034.3745043},
  url = {https://doi.org/10.1145/3745034.3745043},
  booktitle = {Proceedings of the 4th {International},
  pages = {50--55},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, Chronic Disease Management, Large Language Model, Personalized Follow-Up, Retrieval Augmented Generation},
  abstract = {Chronic disease follow-up is essential for adjusting treatment plans and improving patient outcomes, yet traditional methods often face challenges such as resource inefficiency, insufficient coverage, and poor personalization. Leveraging advancements in artificial intelligence, this study proposes a chronic disease follow-up approach driven by a large language model (LLM). The methodology includes fine-tuning a pre-trained LLM using chronic disease-specific datasets to enhance task-specific capabilities. Integration with Graph-based\&nbsp;Retrieval Augmented Generation (GraphRAG)technology enables structured and non-structed data utilization, improving retrieval and response accuracy. The system design incorporates personalized patient interaction, semantic similarity methods, and real-time knowledge updates to enhance compliance and efficiency. However, limitations such as the lack of comprehensive structured data and domain-specific constraints are noted, with future work aimed at refining knowledge graphs and optimizing workflows for vertical field applications. This study provides a novel framework for intelligent chronic disease management in the era of large models.},
  address = {New York, NY, USA},
  series = {{IC},
  isbn = {979-8-4007-1439-9},
}

@article{shi_retrieval_2025,
  title = {Retrieval augmented generation with collaborative filtering for personalized text generation},
  author = {Shi, T. and Xu, J. and Zhang, X. and Zang, X. and Zheng, K. and {...},
  year = {2025},
  doi = {10.1145/3726302.3730075},
  url = {https://doi.org/10.1145/3726302.3730075},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th …},
  pages = {1294--1304},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, han li, jun xu, kai zheng, teng shi, xiao zhang, xiaoxue zang, yang song, source: ACM},
  abstract = {… RAG for LLM personalization and identified the challenges: how to introduce collaborative information and how to retrieve documents that support personalized LLM … LLM hallucinations […},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{shawon_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Shawon, A. and Liscano, R. and Azim, A. and Sundaresan, V. and {...},
  year = {2025},
  doi = {10.1145/3680256.3721324},
  url = {https://doi.org/10.1145/3680256.3721324},
  booktitle = {Companion of the 16th {ACM},
  journal = {Companion of the 16th …},
  pages = {95--102},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, code smell, fine-tuning, large language model, lock contention, rag, refactored code recommendation},
  abstract = {… This process also augments the original prompt using fine-tuned LLM with the … LLM to remove the hallucination or wrong recommendation by checking or verifying from the external RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-1130-5},
}

@article{yang_im-rag_2024,
  title = {Im-rag: {Multi},
  author = {Yang, D. and Rao, J. and Chen, K. and Guo, X. and Zhang, Y. and {...},
  year = {2024},
  doi = {10.1145/3626772.3657760},
  url = {https://doi.org/10.1145/3626772.3657760},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {730--740},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, inner monologue, large language models, multi-round retrieval, question answering, retrieval augmented generation, source: ACM},
  abstract = {… LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG … by retrieving timely and relevant information, enhancing the factuality of responses. The …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{garetto_information_2025,
  title = {Information {Retrieval},
  author = {Garetto, Michele and Cornacchia, Alessandro and Galante, Franco and Leonardi, Emilio and Nordio, Alessandro and Tarable, Alberto},
  year = {2025},
  doi = {10.1145/3726302.3730008},
  url = {https://doi.org/10.1145/3726302.3730008},
  booktitle = {Proceedings of the 48th {International},
  pages = {602--612},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {automation bias, information quality, large language models, retrieval-augmented generation, stack exchange, web answering},
  abstract = {The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_evaluating_2025,
  title = {Evaluating {Sparse},
  author = {Wang, Yang and Dai, Guanlin and Ke, Song and Zheng, Chao},
  year = {2025},
  doi = {10.1145/3708657.3708747},
  url = {https://doi.org/10.1145/3708657.3708747},
  booktitle = {Proceedings of the 2024 10th {International},
  pages = {548--554},
  publisher = {Association for Computing Machinery},
  keywords = {evaluation, large-scale language model, retrieval-augmented generation},
  abstract = {With the rapid advancement of large-scale pre-trained models, their capabilities have found widespread applications across various fields, leading to increasing demand from individual users. However, personal users often face challenges such as concerns over data security and limitations in hardware resources, making the development and deployment of these models a complex task. Retrieval-Augmented Generation (RAG) systems offer a promising solution by enhancing a model’s ability to acquire knowledge through the integration of external knowledge bases, without the need for extensive training. Nevertheless, RAG systems employ a variety of retrieval algorithms, and different configurations can significantly impact both resource consumption and performance. As a result, choosing the right retrieval algorithm has become a critical area of research. This study evaluates and compares sparse and dense retrieval algorithms, aiming to identify how RAG system performance can be optimized under varying resource constraints and user requirements. The experimental results demonstrate that a well-matched combination of Large-scale Language Model and retrieval algorithm can maximize system performance under specific hardware conditions, offering new strategies for individual users to efficiently leverage large models.},
  address = {New York, NY, USA},
  series = {{ICCIP},
  isbn = {979-8-4007-1744-4},
}

@inproceedings{dong_decoupling_2025,
  title = {Decoupling {Knowledge},
  author = {Dong, Qian and Ai, Qingyao and Wang, Hongning and Liu, Yiding and Li, Haitao and Su, Weihang and Liu, Yiqun and Chua, Tat-Seng and Ma, Shaoping},
  year = {2025},
  doi = {10.1145/3696410.3714608},
  url = {https://doi.org/10.1145/3696410.3714608},
  booktitle = {Proceedings of the {ACM},
  pages = {4386--4395},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge injection, language model, retrieval augmented generation},
  abstract = {Retrieval-Augmented Generation (RAG) systems have become a crucial tool to augment large language models (LLMs) with external knowledge for better task performance. However, existing traditional RAG methods inject knowledge directly into the context, resulting in several limitations. First, these methods highly rely on the in-context learning capability of LLMs, which often leads to excessively long contexts. This is inefficient due to the quadratic complexity of self-attention, leading to significant increase in inference time. Second, the extended context and the nature of self-attention can cause the LLMs to lose important information in the context, thereby degrading the original capabilities of LLMs. Third, the effectiveness of knowledge injection is perturbed by the permutation of knowledge within the extended context, reducing the robustness of existing RAG methods. To tackle the above problems, we propose DecoupledRAG, a method that decouples external knowledge from the context within the RAG framework. Specifically, we introduce a cross-attention based method that injects retrieved knowledge directly into the inference process of LLM on the fly, without modifying its parameters or the input context, so that the external knowledge can be utilized robustly in a permutation-independent manner. To the best of our knowledge, this is the first work that explore how to utilize cross-attention to inject knowledge with low training cost in decoder-only LLM era. By leveraging cross-attention operation, DecoupledRAG enables seamless knowledge aggregation without creating extended context. Experimental results demonstrate that our method could achieve high efficiency while maintaining strong performance, which indicates that RAG frameworks have the potential to benefit further from more knowledge.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{zhang_domain-specific_2025,
  title = {A {Domain},
  author = {Zhang, Zhonghao and Yang, Xiaochen and Zeng, Changchang},
  year = {2025},
  doi = {10.1145/3718751.3718860},
  url = {https://doi.org/10.1145/3718751.3718860},
  booktitle = {Proceedings of the 2024 4th {International},
  pages = {682--687},
  publisher = {Association for Computing Machinery},
  keywords = {Civil Aviation Safety, Large Language Models (LLMs), Retrieval Augmented Generation (RAG), source: ACM},
  abstract = {This paper presents a domain-specific Retrieval Enhanced Generation (RAG) system designed for civil aviation security, to improve the quality of domain-specific query answers. Based on advanced Natural Language Processing (NLP) technology, this system combines large language models, embedding models and similarity search strategy, to optimize the accuracy, relevance and reliability of the generation. We evaluated the impact of different parameters (large language model, embedding model, and Top-k values) on the performance of the RAG system by adjusting them separately. In each set of experiments, we changed only one variable individually, while the others were kept fixed. This ensures that we can clearly observe the independent impact of each parameter change on the performance of the system. The results show that choosing an right large language model and top-k values will significantly improve the output quality, balancing between the accuracy of the answers and the comprehensiveness of the search. Though there's some limitations, such as too much reliance on vector similarity and the limitations of LLMs, the system still provides a tool to provide civil aviation practitioners with timely and accurate information. Future work will explore some improvements such as the use of graphical databases and the use of larger language models. This RAG system shows a step-forward in intelligent, data-driven decision-making in civil aviation safety, has great potential to improve response and safety management in emergency scenarios.},
  address = {New York, NY, USA},
  series = {{ICBAR},
  isbn = {979-8-4007-0975-3},
}

@article{wang_feb4rag_2024,
  title = {Feb4rag: {Evaluating},
  author = {Wang, S. and Khramtsova, E. and Zhuang, S. and {...},
  year = {2024},
  doi = {10.1145/3626772.3657853},
  url = {https://doi.org/10.1145/3626772.3657853},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {763--773},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, federated search, large language models (llms), retrieval augmented generation (rag), test collection.},
  abstract = {… unsuitable of typical RAG pipelines applications, we created prompts for an LLM to generate … These results enhance our confidence in the reliability of LLMbased relevance judgments, …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{cahoon_optimizing_2025,
  title = {Optimizing open-domain question answering with graph-based retrieval augmented generation},
  author = {Cahoon, Joyce and Singh, Prerna and Litombe, Nick and Larson, Jonathan and Trinh, Ha and Zhu, Yiwen and Mueller, Andreas and Psallidas, Fotis and Curino, Carlo},
  year = {2025},
  doi = {10.1145/3737412.3743489},
  url = {https://doi.org/10.1145/3737412.3743489},
  booktitle = {Proceedings of the 1st {Workshop},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {In this work, we benchmark various graph-based retrieval-augmented generation (RAG) systems across a broad spectrum of query types, including OLTP-style (fact-based) and OLAP-style (thematic) queries, to address the complex demands of open-domain question answering (QA). Traditional RAG methods often fall short in handling nuanced, multi-document synthesis tasks. By structuring knowledge as graphs, we can facilitate the retrieval of context that captures greater semantic depth and enhances language model operations. We explore graph-based RAG methodologies and introduce TREX, a novel, cost-effective alternative that combines graph-based indexing and vector-based retrieval techniques. Our benchmarking across four diverse datasets highlights the strengths of different RAG methodologies, demonstrates TREX’s ability to handle multiple open-domain QA types, and reveals the limitations of current evaluation methods. We publicly release these datasets to facilitate further research and benchmarking at https://github.com/microsoft/graphrag-benchmarking-datasets. Our findings underscore the potential of augmenting large language models with advanced retrieval capabilities and scalable graph-based AI solutions.},
  address = {New York, NY, USA},
  series = {{MIDAS},
  isbn = {979-8-4007-1960-8},
}

@article{pu_customized_2024,
  title = {Customized retrieval augmented generation and benchmarking for {EDA},
  author = {Pu, Y. and He, Z. and Qiu, T. and Wu, H. and Yu, B.},
  year = {2024},
  doi = {10.1145/3676536.3676730},
  url = {https://doi.org/10.1145/3676536.3676730},
  booktitle = {Proceedings of the 43rd {IEEE},
  journal = {Proceedings of the 43rd IEEE/ACM …},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, source: ACM},
  abstract = {… During the process of RAG, the robust reasoning and … the hallucination issue and improves the reliability of LLM … tool-related questions, applying existing RAG flows for EDA tool …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ICCAD},
  isbn = {979-8-4007-1077-3},
}

@inproceedings{zhu_collaborative_2025,
  title = {Collaborative {Retrieval},
  author = {Zhu, Yaochen and Wan, Chao and Steck, Harald and Liang, Dawen and Feng, Yesu and Kallus, Nathan and Li, Jundong},
  year = {2025},
  doi = {10.1145/3696410.3714908},
  url = {https://doi.org/10.1145/3696410.3714908},
  booktitle = {Proceedings of the {ACM},
  pages = {3323--3334},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {collaborative filtering, conversational recommender systems, large language models, llm, retrieval augmented generation},
  abstract = {Conversational recommender systems (CRS) aim to provide personalized recommendations via interactive dialogues with users. While large language models (LLMs) enhance CRS with their superior understanding of context-aware user preferences, they typically struggle to leverage behavioral data, which have proven to be important for classical collaborative filtering (CF)-based approaches. For this reason, we propose CRAG-Collaborative Retrieval Augmented Generation for LLM-based CRS. To the best of our knowledge, CRAG is the first approach that combines state-of-the-art LLMs with CF for conversational recommendations. Our experiments on two publicly available movie conversational recommendation datasets, i.e., a refined Reddit dataset (which we name Reddit-v2) as well as the Redial dataset, demonstrate the superior item coverage and recommendation performance of CRAG, compared to several CRS baselines. Moreover, we observe that the improvements are mainly due to better recommendation accuracy on recently released movies. The code and data are available at https://github.com/yaochenzhu/CRAG.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{gao_optimized_2025,
  title = {Optimized {Individual},
  author = {Gao, Yue},
  year = {2025},
  doi = {10.1145/3723366.3723380},
  url = {https://doi.org/10.1145/3723366.3723380},
  booktitle = {Proceedings of the 2024 4th {International},
  pages = {80--84},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {artificial intelligence, big data, encode combination, model hallucinations, query augment, source: Scopus},
  abstract = {As the era of big data arrives, enterprises have accumulated an increasing amount of data. How to effectively utilize cutting-edge artificial intelligence technologies to digitize massive amounts of information has become a significant challenge. Due to the exorbitant costs of training large language models (LLMs) from scratch, along with limitations such as the context window length of these models, user data privacy concerns, and model hallucinations, Retrieval-Augmented Generation (RAG) is increasingly being adopted in reality applications. In recent years, the development of large language models has led to their widespread application across industries. However, the inconsistency between the training objectives of large models and the retrieval tasks often results in severe hallucinations, particularly in very professional and personal domains, and then leads to incorrect responses. To solve this problem, RAG has been widely implemented. This paper delves into several aspects, including query augmentation, encoding combination, hybrid re-ranking, and filtering of final candidate documents, which presents a practical and effective RAG system. Extensive experiments were conducted on plant and resume datasets, with ablation studies performed for each module. Comparative analyses revealed significant improvements over existing methods, achieving an average enhancement of 4\% or more in real-world applications, and a notable 6\% improvement specifically in the resume dataset. The final experimental analysis substantiates the efficacy of this approach.},
  address = {New York, NY, USA},
  series = {{ISBDAI},
  isbn = {979-8-4007-1829-8},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{tupayachi_conversational_2025,
  title = {Conversational {Geographic},
  author = {Tupayachi, Jose and Li, Xueping},
  year = {2025},
  doi = {10.1145/3681772.3698217},
  url = {https://doi.org/10.1145/3681772.3698217},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {56--59},
  publisher = {Association for Computing Machinery},
  note = {event-place: Atlanta, GA, USA},
  keywords = {Geographical Information, Large Language Models, Question Answering, Retrieval Augmented Generation, source: ACM},
  abstract = {We present a pilot study exploring the potential of Large Language Models (LLMs) to interface with application programming interfaces through logical instructions, specifically within the domain of Geographic Question Answering for route optimization. This study employs a Continuous Retrieval-Augmented Generation approach combined with fine-tuned LLMs, featuring customized node-based storage and vector search retrieval. We also provide a comparative analysis of the method's effectiveness and adaptability in handling diverse textual queries.},
  address = {New York, NY, USA},
  series = {{IWCTS},
  isbn = {979-8-4007-1151-0},
}

@inproceedings{ehsan_explainable_2024,
  title = {Explainable {AI},
  author = {Ehsan, Upol and Riedl, Mark},
  year = {2024},
  doi = {10.1145/3686169.3686185},
  url = {https://doi.org/10.1145/3686169.3686185},
  booktitle = {Proceedings of the {Halfway},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santa Cruz, CA, USA},
  keywords = {Explainable AI, Generative AI, Large Language Models},
  abstract = {When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) “black-box” of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited. Especially when it comes to non-technical end-users. In this paper, we challenge the assumption of “opening” the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.},
  address = {New York, NY, USA},
  series = {{HttF},
  isbn = {979-8-4007-1042-1},
}

@inproceedings{yang_knowledge-enhanced_2025,
  title = {Knowledge-{Enhanced},
  author = {Yang, Da and Wang, Hongbo and Shao, Shanzhong and Liu, Shutian},
  year = {2025},
  doi = {10.1145/3716895.3716900},
  url = {https://doi.org/10.1145/3716895.3716900},
  booktitle = {Proceedings of the 5th {International},
  pages = {25--29},
  publisher = {Association for Computing Machinery},
  keywords = {knowledge graph (KG), large language model (LLM), Retrieval-Augmented Generation (RAG), subway maintenance},
  abstract = {To address the various challenges faced in training urban rail transit system maintenance personnel, this paper proposes a solution for developing a training system for subway maintenance personnel using knowledge graphs and a Retrieval-Augmented Generation (RAG)-enhanced large language model. The approach involves first creating a fine-tuning dataset from subway maintenance technical documents to fine-tune the large language model. This fine-tuned model then assists in constructing a subway maintenance knowledge graph. Concurrently, a vector database of subway maintenance knowledge is established. Finally, a question-answering system leveraging both the knowledge graph and the vector database as external knowledge sources is developed to support the training of subway maintenance personnel. Results demonstrate that this system can effectively enhance the learning efficiency of maintenance staff.},
  address = {New York, NY, USA},
  series = {{ICAICE},
  isbn = {979-8-4007-1800-7},
}

@inproceedings{wynn-williams_can_2025,
  title = {Can {Generative},
  author = {Wynn-Williams, Stephen and Tyrrell, Ryan and Pantelic, Vera and Lawford, Mark and Menghi, Claudio and Nalla, Phaneendra and Artail, Hassan},
  year = {2025},
  doi = {10.1145/3696630.3728568},
  url = {https://doi.org/10.1145/3696630.3728568},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {456--467},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {automotive software, generative AI, LLM, software testing},
  abstract = {Engineers need automated support for software testing. Generative AI is a novel technology for generating new content; however, its applicability for test case generation is still unclear. This work considers the following question: Can generative AI produce test cases in industrial software applications? We framed our question in the automotive domain. We performed our evaluation in collaboration with a large automotive manufacturer to assess to what extent generative AI can produce test cases (a.k.a. test scripts) from informal test case specifications. We considered 1) informal test case specifications defined in Rational Quality Manager, an industrial test management tool from IBM, and 2) executable test scripts specified as ecu.test packages supported by the ecu.test tool from Tracetronic. We used generative AI to produce the test scripts from the informal test case descriptions. Our results show that generative AI can produce correct or near-correct test scripts in a reasonable number of cases. We also analyzed the effects of prompt design, choice of generative AI model, and context accuracy on the effectiveness of our solution and reflected on our results.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@article{yao_cacheblend_2025,
  title = {{CacheBlend},
  author = {Yao, J. and Li, H. and Liu, Y. and Ray, S. and Cheng, Y. and Zhang, Q. and {...},
  year = {2025},
  doi = {10.1145/3689031.3696098},
  url = {https://doi.org/10.1145/3689031.3696098},
  booktitle = {Proceedings of the {Twentieth},
  journal = {Proceedings of the …},
  pages = {94--109},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, KV Cache, Large Language Models, Retrieval-Augmented-Generation, source: ACM},
  abstract = {… LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM … : when an LLM input contains multiple text …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{guo_efragembedding-finetuning_2024,
  title = {{EFRAG},
  author = {Guo, Jun and Huang, Jianye and Zhao, Zhichao and Chen, Jinming and Weng, Yuyou and Lin, Guoqing},
  year = {2024},
  doi = {10.1145/3689218.3689225},
  url = {https://doi.org/10.1145/3689218.3689225},
  booktitle = {Proceedings of the 2024 6th {International},
  pages = {21--27},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, Hong Kong},
  keywords = {Embedding-FineTuning, Power project, RAG},
  abstract = {This paper introduces an innovative method called EFRAG (Embedding-FineTuning Retrieval Augmented Generation for QA in power projects), which incorporates an external knowledge base as an additional information source to precisely answer questions related to power projects. The core of EFRAG lies in Embedding-FineTuning to enhance the model’s comprehension of user queries and project documents, thereby improving matching capability in power-related QA tasks and assisting the LLM in deriving accurate answers. This paper details the fundamental principles and implementation process of EFRAG and validates its effectiveness through a series of experiments. The experimental results indicate that EFRAG significantly improves the accuracy of QA results. The research findings not only provide a solution for enhancing power-related QA but also offer new insights into empowering the power industry through large language models.},
  address = {New York, NY, USA},
  series = {{PRIS},
  isbn = {979-8-4007-1825-0},
}

@inproceedings{yamanishi_tourmllm_2025,
  title = {{TourMLLM},
  author = {Yamanishi, Hiromasa and Xiao, Ling and Yamasaki, Toshihiko},
  year = {2025},
  doi = {10.1145/3731715.3733450},
  url = {https://doi.org/10.1145/3731715.3733450},
  booktitle = {Proceedings of the 2025 {International},
  pages = {1654--1663},
  publisher = {Association for Computing Machinery},
  note = {event-place: Chicago, IL, USA},
  keywords = {multimodal large language model, retrieval augmented generation, tourism},
  abstract = {Artificial Intelligence (AI) has shown significant potential in tourism, particularly in personalized recommendations, information provision, and user experience sharing. Recent advancements in multimodal large language models (MLLMs) have further enhanced AI-driven solutions. Although some MLLMs for tourism are proposed, existing models are constrained to a narrow range of tasks, limiting their effectiveness in providing information and personalization. Additionally, while some instruction tuning methods are proposed, effective learning methods that facilitate scaling for multi-task learning are still lacking in this domain. This paper proposes TourMLLM, a multimodal large language model designed to expand task coverage while improving training efficiency and accuracy. TourMLLM supports six key tasks, broadening its applications: landmark recognition, general review generation, conditional review generation, tourism recommendation, tourism image captioning with and without landmark names. To enhance adaptability and performance, we introduce task-adaptive retrieval-augmented instruction tuning and preference optimization strategies, allowing the model to handle diverse tourism-related tasks more effectively. Evaluation across six tasks demonstrates that TourMLLM outperforms GPT-4o in accuracy. The dataset and code are available online at https://github.com/HiromasaYamanishi/TourMLLM.},
  address = {New York, NY, USA},
  series = {{ICMR},
  isbn = {979-8-4007-1877-9},
}

@article{cai_forag_2024,
  title = {Forag: {Factuality},
  author = {Cai, T. and Tan, Z. and Song, X. and Sun, T. and Jiang, J. and Xu, Y. and {...},
  year = {2024},
  doi = {10.1145/3637528.3672065},
  url = {https://doi.org/10.1145/3637528.3672065},
  booktitle = {Proceedings of the 30th {ACM},
  journal = {Proceedings of the 30th …},
  pages = {199--210},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, fine-grained rlhf, rag},
  abstract = {… LLM, each containing a single piece of factual information. After the decomposition, we evaluate each subclaim individually. Since the decomposition using LLM … evaluate the factuality …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{maity_leveraging_2025,
  title = {Leveraging {In},
  author = {Maity, Subhankar and Deroy, Aniket and Sarkar, Sudeshna},
  year = {2025},
  doi = {10.1145/3734947.3734949},
  url = {https://doi.org/10.1145/3734947.3734949},
  booktitle = {Proceedings of the 16th {Annual},
  pages = {40--47},
  publisher = {Association for Computing Machinery},
  keywords = {Automatic Question Generation (AQG), In-Context Learning (ICL), Large Language Models (LLMs), Retrieval-Augmented Generation (RAG)},
  abstract = {Question generation in education is a time-consuming and cognitively demanding task, as it requires creating questions that are both contextually relevant and pedagogically sound. Current automated question generation methods often generate questions that are out of context. In this work, we explore advanced techniques for automated question generation in educational contexts, focusing on In-Context Learning (ICL), Retrieval-Augmented Generation (RAG), and a novel Hybrid Model that merges both methods. We implement GPT-4 for ICL using few-shot examples and BART with a retrieval module for RAG. The Hybrid Model combines RAG and ICL to address these issues and improve question quality. Evaluation is conducted using automated metrics, followed by human evaluation metrics. Our results show that both the ICL approach and the Hybrid Model consistently outperform other methods, including baseline models, by generating more contextually accurate and relevant questions.},
  address = {New York, NY, USA},
  series = {{FIRE},
  isbn = {979-8-4007-1318-7},
}

@inproceedings{huang_foodpuzzle_2025,
  title = {{FoodPuzzle},
  author = {Huang, Tenghao and Lee, Dong Hee and Sweeney, John and Shi, Jiatong and Steliotes, Emily and Lange, Matthew and May, Jonathan and Chen, Muhao},
  year = {2025},
  doi = {10.1145/3711896.3737384},
  url = {https://doi.org/10.1145/3711896.3737384},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5493--5504},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {agent, flavor science, in-context learning, large language models, retrieval-augmented generation},
  abstract = {Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address these challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. By leveraging their capacity to identify relevant evidence and reason within large context spaces, language model-backed agents can perform the labor-intensive tasks of flavor sourcing and understanding with enhanced efficiency and precision. To facilitate research in this area, we introduce the FoodPuzzle dataset, a challenging benchmark consisting of 978 food items and 1,766 flavor molecule profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{lu_retrieval-augmented_2025,
  title = {A {Retrieval},
  author = {Lu, Yanyan and Peng, Jiao and Xu, Xing and He, Yue and Li, Tao and Wei, Jie and Jing, Hongyu and Wang, Heqing and Xu, Bo and Song, Hui},
  year = {2025},
  doi = {10.1145/3705754.3705771},
  url = {https://doi.org/10.1145/3705754.3705771},
  booktitle = {Proceedings of the 2024 2nd {International},
  pages = {95--100},
  publisher = {Association for Computing Machinery},
  keywords = {hybrid search, language model, semantic screening, semantic segmentation},
  abstract = {Retrieval-augmented Generation has achieved significant success in improving the performance of large language models by utilizing external knowledge sources. However, despite its advancements, it still faces challenges in domain-specific structured documents such as the electric power industry, exhibiting low recall rates and response inaccuracies. To solve these issues, this paper designs a question-answering framework specifically for the electric power industry using the large language model. This study first analyzes the characteristics of documents in the electric power industry and proposes the Hierarchical Adaptive Semantic Segmentation(HASS) method, which improves the accuracy of responses and the precision of queries by subdividing knowledge points and integrating metadata. In the retrieval strategy, a Hybrid Search(HS) method is designed, combining the advantages of sparse and dense retrieval to improve the recall rate. To enable large language models to focus on relevant documents when generating responses, we propose the Irrelevant Document Filtering(IDF) method for minimizing the noise impact from irrelevant documents. Additionally, the model is designed with prompts and fine-tuned to further enhance its ability to utilize context and the accuracy of answer generation. Finally, due to the lack of publicly available datasets for document question-answering in the electricity sector, this work constructs a dataset manually for training and evaluation, comprising 1300 QA items covering several types of questions. Experimental results demonstrate that the methods proposed in this paper effectively improve the accuracy of LLMs in the electric power industry, with our method showing a 9\% improvement in accuracy over the traditional RAG approach.},
  address = {New York, NY, USA},
  series = {{CECCT},
  isbn = {979-8-4007-1019-3},
}

@inproceedings{tan_paths-over-graph_2025,
  title = {Paths-over-{Graph},
  author = {Tan, Xingyu and Wang, Xiaoyang and Liu, Qing and Xu, Xiwei and Yuan, Xin and Zhang, Wenjie},
  year = {2025},
  doi = {10.1145/3696410.3714892},
  url = {https://doi.org/10.1145/3696410.3714892},
  booktitle = {Proceedings of the {ACM},
  pages = {3505--3522},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge graph, knowledge graph question answering, large language models, retrieval-augmented generation, source: Scopus},
  abstract = {Large Language Models (LLMs) have achieved impressive results in various tasks but struggle with hallucination problems and lack of relevant knowledge, especially in deep complex reasoning and knowledge-intensive tasks. Knowledge Graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. However, existing KG-based LLM reasoning methods face challenges like handling multi-hop reasoning, multi-entity questions, and effectively utilizing graph structures. To address these issues, we propose Paths-over-Graph (PoG), a novel method that enhances LLM reasoning by integrating knowledge reasoning paths from KGs, improving the interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and multi-entity questions through a three-phase dynamic multi-hop path exploration, which combines the inherent knowledge of LLMs with factual knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant information from the graph exploration first and introduces efficient three-step pruning techniques that incorporate graph structures, LLM prompting, and a pre-trained language model (e.g., SBERT) to effectively narrow down the explored candidate paths. This ensures all reasoning paths contain highly relevant information captured from KGs, making the reasoning faithful and interpretable in problem-solving. PoG innovatively utilizes graph structure to prune the irrelevant noise and represents the first method to implement multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive experiments on five benchmark KGQA datasets demonstrate PoG outperforms the state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an average accuracy improvement of 18.9\%. Notably, PoG with GPT-3.5-Turbo surpasses ToG with GPT-4 by up to 23.9\%.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
  annote = {Cited by: 5; All Open Access; Gold Open Access},
}

@inproceedings{he_designminds_2025,
  title = {{DesignMinds},
  author = {He, Tianhao and Stanković, Andrija and Niforatos, Evangelos and Kortuem, Gerd},
  year = {2025},
  doi = {10.1145/3719160.3736633},
  url = {https://doi.org/10.1145/3719160.3736633},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Design Ideation, Designer-AI Collaboration, Eye-tracking, Generative AI, Large Language Model, Video-based Design, Vision Language Model},
  abstract = {Ideation is a critical component of video-based design (VBD), where videos serve as the primary medium for design exploration and inspiration. The emergence of generative AI offers considerable potential to enhance this process by streamlining video analysis and facilitating idea generation. In this paper, we present DesignMinds, a prototype that integrates a state-of-the-art Vision-Language Model (VLM) with a context-enhanced Large Language Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we conducted a between-subject study with 35 design practitioners, comparing its performance to a baseline condition. Our results demonstrate that DesignMinds significantly enhances the flexibility and originality of ideation, while also increasing task engagement. Importantly, the introduction of this technology did not negatively impact user experience, technology acceptance, or usability.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{wei_large_2025,
  title = {Large {Language},
  author = {Wei, Yunze and Chi, Kaiwen and Du, Shibo and Xie, Xiaohui and Geng, Ziyu and Han, Yuwei and Li, Zhen and Li, Zhanyou and Cui, Yong},
  year = {2025},
  doi = {10.1145/3744200.3744763},
  url = {https://doi.org/10.1145/3744200.3744763},
  booktitle = {Proceedings of the 2025 {Applied},
  pages = {32--38},
  publisher = {Association for Computing Machinery},
  note = {event-place: Madrid, Spain},
  keywords = {source: ACM},
  abstract = {Traditional network protocol testing methods face significant challenges in adapting to rapid protocol evolution. The challenges stem primarily from protocol specification analysis and customized code development for testing. To address this, we propose NeTestLLM, a Large Language Model (LLM)-powered framework that automates protocol testing through two key components: (1) a hybrid test case generator that extracts protocol specifications and produces high-coverage test cases, and (2) a retrieval-feedback-enhanced engine that translates natural language descriptions into executable code. Preliminary evaluations demonstrate that NeTestLLM achieves 94.1\% coverage on protocol specification understanding. A case study with commercial network equipment validates the practical effectiveness of our approach. Our work presents the first LLM-powered framework for automated network protocol testing to keep pace with the rapid evolution of network protocols and standards.},
  address = {New York, NY, USA},
  series = {{ANRW},
  isbn = {979-8-4007-2009-3},
}

@inproceedings{yang_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Yang, Shanglin and Zhu, Jialin and Wang, Jialin and Xu, Xiaohan and Shao, Zihang and Yao, Liwei and Zheng, Benchang and Huang, Hu},
  year = {2024},
  doi = {10.1145/3653081.3653102},
  url = {https://doi.org/10.1145/3653081.3653102},
  booktitle = {Proceedings of the 2023 5th {International},
  pages = {120--124},
  publisher = {Association for Computing Machinery},
  note = {event-place: Nanchang, China},
  keywords = {source: ACM, source: Scopus},
  abstract = {Large language models have demonstrated emergent intelligence and ability to handle a wide array of tasks. However, the reliability of these models in terms of factual accuracy and timely knowledge acquisition remains a challenge. Researchers explore the implementation of retrieval-augmented generation methods, aiming to enhance the authenticity and specificity in knowledge-intensive tasks. This paper discusses the practical application in industrial settings, particularly in assisting design personnel with navigating complex standards and quality manuals. Utilizing an open-source model with 6 billion parameters, the study employs quantization technology for local deployment, addressing computational challenges. The retrieval-augmented generation framework is analyzed, emphasizing the integration of document parsing, vector databases, and text embedding models. Experimental results compare models at different quantization levels, revealing trade-offs between response time, model size, and performance metrics. The findings suggest that 4-bit integer quantization is optimal for standard document retrieval and question-answering tasks, highlighting practical considerations for CPU inference. The paper concludes with insights into hyper-parameter tuning, model comparisons, and future optimizations for enhanced performance in edge device deployments of large language models.},
  address = {New York, NY, USA},
  series = {{IoTAAI},
  isbn = {979-8-4007-1648-5},
  annote = {Cited by: 1},
}

@inproceedings{li_g-refer_2025,
  title = {G-{Refer},
  author = {Li, Yuhan and Zhang, Xinni and Luo, Linhao and Chang, Heng and Ren, Yuxiang and King, Irwin and Li, Jia},
  year = {2025},
  doi = {10.1145/3696410.3714727},
  url = {https://doi.org/10.1145/3696410.3714727},
  booktitle = {Proceedings of the {ACM},
  pages = {240--251},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {explainable recommendation, graphrag, large language model},
  abstract = {Explainable recommendation has demonstrated significant advantages in informing users about the logic behind recommendations, thereby increasing system transparency, effectiveness, and trustworthiness. To provide personalized and interpretable explanations, existing works often combine the generation capabilities of large language models (LLMs) with collaborative filtering (CF) information. CF information extracted from the user-item interaction graph captures the user behaviors and preferences, which is crucial for providing informative explanations. However, due to the complexity of graph structure, effectively extracting the CF information from graphs still remains a challenge. Moreover, existing methods often struggle with the integration of extracted CF information with LLMs due to its implicit representation and the modality gap between graph structures and natural language explanations. To address these challenges, we propose G-Refer, a framework using Graph Retrieval-augmented large language models (LLMs) for explainable recommendation. Specifically, we first employ a hybrid graph retrieval mechanism to retrieve explicit CF signals from both structural and semantic perspectives. The retrieved CF information is explicitly formulated as human-understandable text by the proposed graph translation and accounts for the explanations generated by LLMs. To bridge the modality gap, we introduce knowledge pruning and retrieval-augmented fine-tuning to enhance the ability of LLMs to process and utilize the retrieved CF information to generate explanations. Extensive experiments show that G-Refer achieves superior performance compared with existing methods in both explainability and stability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@article{yang_addrllm_2025,
  title = {{AddrLLM},
  author = {Yang, Q. and Hong, Z. and Cao, D. and Wang, H. and Xie, Z. and He, T. and {...},
  year = {2025},
  doi = {10.1145/3690624.3709425},
  url = {https://doi.org/10.1145/3690624.3709425},
  booktitle = {Proceedings of the 31st {ACM},
  journal = {Proceedings of the 31st …},
  pages = {2756--2767},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, address rewriting, large language models, query reformulation},
  abstract = {… based on retrieval augmented large language model. We build a RAG framework specifically for … Not only does RAG help in improving the factual accuracy of answers, but it also allows …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1245-6},
}

@inproceedings{sarmah_hybridrag_2024,
  title = {{HybridRAG},
  author = {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and Rao, Rohan and Patel, Sunil and Pasquali, Stefano},
  year = {2024},
  doi = {10.1145/3677052.3698671},
  url = {https://doi.org/10.1145/3677052.3698671},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {608--616},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {source: ACM},
  abstract = {Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q\&amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q\&amp;A format, and hence provide a natural set of pairs of ground-truth Q\&amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{chen_i-card_2025,
  title = {I-{Card},
  author = {Chen, Liuqing and Cheang, Wengteng and Jiang, Zhaojun and Xu, Yuan and Cai, Zebin and Sun, Lingyun and Childs, Peter and Han, Ji and Hansen, Preben and Zuo, Haoyu},
  year = {2025},
  doi = {10.1145/3706598.3713934},
  url = {https://doi.org/10.1145/3706598.3713934},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {design cards, Design method, design method cards, design support tool, generative AI},
  abstract = {A design method card deck helps designers understand and provoke thinking by presenting each method in a simple format and allow designers to switch between methods seamlessly by maintaining the same simple format across the deck. However, recent observations have shown designers hesitate to use a card deck due to the lack of support, while other tools have provided identified support with generative AI. Through a formative study, we identified the specific support designers need when applying the design method cards and intentions in integrating generative AI. Accordingly, we developed the intelligent design method card deck, I-Card, which integrates generative AI to provide applicable design methods, design knowledge and data support, and interactive and dynamic support. A user study demonstrates that I-Card improved the design efficiency and applicability by offering personalized guidance, enhanced decision-making with comprehensive data generation and provided more design inspiration via interactive support.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{liu_aggregated_2025,
  title = {Aggregated {Knowledge},
  author = {Liu, Fengchen and Jung, Jordan and Feinstein, Wei and D'Ambrogia, Jeff and Jung, Gary},
  year = {2025},
  doi = {10.1145/3703412.3703434},
  url = {https://doi.org/10.1145/3703412.3703434},
  booktitle = {Proceedings of the 4th {International},
  publisher = {Association for Computing Machinery},
  keywords = {AWS Bedrock, Closed-Domain Question Answering, Domain-Specific Information Retrieval, Fine-tuning Language Models, GCP PaLM, Google Gemini-Pro, High Performance Computing, Large Language Models, Meta LLaMA, OpenAI GPT, Retrieval-Augmented Generation, source: ACM},
  abstract = {This paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1161-9},
}

@inproceedings{ordoumpozanis_generative_2025,
  title = {Generative {AI},
  author = {Ordoumpozanis, Kostas and Konstantakis, Markos and Zoi, Stavroula and Caridakis, George},
  year = {2025},
  doi = {10.1145/3749012.3749052},
  url = {https://doi.org/10.1145/3749012.3749052},
  booktitle = {Proceedings of the 3rd {International},
  pages = {39--47},
  publisher = {Association for Computing Machinery},
  keywords = {AI, Artificial Intelligence, Gen AI, Generative AI, UI, User Experience, User Interaction, User Interface, UX},
  abstract = {As artificial intelligence becomes an everyday presence across education, arts and creative technologies, and cultural heritage, the interaction between users and intelligent systems deserves critical examination. This submission presents a systematic review of 95 case studies, 64 in education, 14 in arts, and 17 in heritage — selected via a PRISMA-guided search and expert screening — to map how generative artificial intelligence is embedded at both the interface and interaction levels. We identify nine interface archetypes (e.g., conversational, adaptive dashboards, immersive environments interfaces), eight interaction patterns (e.g., conversing, collaborating, manipulating),and eight main user experience dimensions as observed in case studies. Our analysis further categorizes six modality-usage patterns—from text, image, audio, and video up to fully multi-modal workflows and distillsfour main categories of end-to-end application pipelines. Notably, only two studies were found to articulate design-phase guidelines, and limitations cluster around output quality, ethical risks, and a lack of longitudinal evaluations. We conclude with limitations observed and future research focused on explainability, participatory design, and sustained field deployments. This synthesis provides a foundation for researchers and practitioners seeking to harness generative artificial intelligence as a responsive, human-centered collaborator.},
  address = {New York, NY, USA},
  series = {{CHIGreece},
  isbn = {979-8-4007-1561-7},
}

@inproceedings{cao_ecc_2024,
  title = {{ECC},
  author = {Cao, Yupeng and Chen, Zhi and Pei, Qingyun and Lee, Nathan and Subbalakshmi, K. P. and Ndiaye, Papa Momar},
  year = {2024},
  doi = {10.1145/3677052.3698689},
  url = {https://doi.org/10.1145/3677052.3698689},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {257--265},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Earnings Conference Call Analysis, Large Language Model, Retrieval-Augmented Generation, Volatility forecasting},
  abstract = {In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock volatility is a critical challenge that has attracted both academics and investors. While previous studies have used multimodal deep learning-based models to obtain a general view of ECCs for volatility predicting, they often fail to capture detailed, complex information. Our research introduces a novel framework: ECC Analyzer, which utilizes large language models (LLMs) to extract richer, more predictive content from ECCs to aid the model’s prediction performance. We use the pre-trained large models to extract textual and audio features from ECCs and implement a hierarchical information extraction strategy to extract more fine-grained information. This strategy first extracts paragraph-level general information by summarizing the text and then extracts fine-grained focus sentences using Retrieval-Augmented Generation (RAG). These features are then fused through multimodal feature fusion to perform volatility prediction. Experimental results demonstrate that our model outperforms traditional analytical benchmarks, confirming the effectiveness of advanced LLM techniques in financial analysis.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{hellas_experiences_2024,
  title = {Experiences from {Integrating},
  author = {Hellas, Arto and Leinonen, Juho and Leppänen, Leo},
  year = {2024},
  doi = {10.1145/3649165.3690101},
  url = {https://doi.org/10.1145/3649165.3690101},
  booktitle = {Proceedings of the 2024 on {ACM},
  pages = {46--52},
  publisher = {Association for Computing Machinery},
  note = {event-place: Virtual Event, NC, USA},
  keywords = {source: ACM},
  abstract = {We provided students access to a state-of-the-art large language model (LLM) chatbot through the online materials of three university-level courses. One of the courses focused on software engineering with LLMs, while the two other courses were not directly related to LLMs. The chatbot used OpenAI GPT-4 without additional filters or system prompts. Our results suggest that only a minority of students engage with the chatbot in the courses that do not relate to LLMs. At the same time, unsurprisingly, nearly all students in the LLM-focused course leveraged the chatbot. In all courses, the majority of the chatbot usage came from a few superusers, whereas the majority of the students did not heavily use the chatbot even though it effectively provided free access to OpenAI's GPT-4 model (which would have otherwise required a paid subscription at the time of the study). We observe that in addition to students using the chatbot for course-specific purposes, many use the chatbot for their own purposes. Overall, our results suggest that the worst fears of educators – all students overrelying on chatbots – did not materialize. Finally, we discuss potential reasons for low usage, including the need for more tailored and scaffolded chatbot experiences targeted for specific types of use cases.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0598-4},
  series = {{SIGCSE},
}

@inproceedings{wang_colacare_2025,
  title = {{ColaCare},
  author = {Wang, Zixiang and Zhu, Yinghao and Zhao, Huiya and Zheng, Xiaochen and Sui, Dehao and Wang, Tianlong and Tang, Wen and Wang, Yasha and Harrison, Ewen and Pan, Chengwei and Gao, Junyi and Ma, Liantao},
  year = {2025},
  doi = {10.1145/3696410.3714877},
  url = {https://doi.org/10.1145/3696410.3714877},
  booktitle = {Proceedings of the {ACM},
  pages = {2250--2261},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {electronic health record, large language model, multi-agent, source: ACM, source: Scopus},
  abstract = {We introduce ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through multi-agent collaboration driven by Large Language Models (LLMs). Our approach seamlessly integrates domain-specific expert models with LLMs to bridge the gap between structured EHR data and text-based reasoning. Inspired by the Multidisciplinary Team (MDT) approach used in clinical settings, ColaCare employs two types of agents: DoctorAgents and a MetaAgent, which collaboratively analyze patient data. Expert models process and generate predictions from numerical EHR data, while LLM agents produce reasoning references and decision-making reports within the MDT-driven collaborative consultation framework. The MetaAgent orchestrates the discussion, facilitating consultations and evidence-based debates among DoctorAgents, simulating diverse expertise in clinical decision-making. We additionally incorporate the Merck Manual of Diagnosis and Therapy (MSD) medical guideline within a retrieval-augmented generation (RAG) module for medical evidence support, addressing the challenge of knowledge currency. Extensive experiments conducted on three EHR datasets demonstrate ColaCare's superior performance in clinical mortality outcome and readmission prediction tasks, underscoring its potential to revolutionize clinical decision support systems and advance personalized precision medicine. All code, case studies and a questionnaire are available at the project website: https://colacare.netlify.app.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
  annote = {Cited by: 1},
}

@article{qin_robust_2024,
  title = {Robust implementation of retrieval-augmented generation on edge-based computing-in-memory architectures},
  author = {Qin, R. and Yan, Z. and Zeng, D. and Jia, Z. and Liu, D. and Liu, J. and {...},
  year = {2024},
  doi = {10.1145/3676536.3676674},
  url = {https://doi.org/10.1145/3676536.3676674},
  booktitle = {Proceedings of the 43rd {IEEE},
  journal = {Proceedings of the 43rd …},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, source: ACM},
  abstract = {… LLM learning method, can improve the quality of the LLM-generated content without updating model parameters. However, the RAG-based LLM may … , the RAG performance for Citation …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ICCAD},
  isbn = {979-8-4007-1077-3},
}

@inproceedings{saketos_large_2024,
  title = {The {Large},
  author = {Saketos, Vasileios and Pantazi, Despina-Athanasia and Koubarakis, Manolis},
  year = {2024},
  doi = {10.1145/3688671.3688770},
  url = {https://doi.org/10.1145/3688671.3688770},
  booktitle = {Proceedings of the 13th {Hellenic},
  publisher = {Association for Computing Machinery},
  keywords = {Classification, Greek Legislation, Greek NLP Resources, Named Entity Recognition, Natural Language Processing, Pre-trained Language Models},
  abstract = {We develop four versions of GreekLegalRoBERTa, which are four large language models trained on Greek legal and nonlegal text. We show that our models surpass the performance of GreekLegalBERT, Greek- LegalBERT-v2, and GreekBERT in two tasks involving Greek legal documents: named entity recognition and multi-class legal topic classification. We view our work as a contribution to the study of domain-specific NLP tasks in low-resource languages, like Greek, using modern NLP techniques and methodologies.},
  address = {New York, NY, USA},
  series = {{SETN},
  isbn = {979-8-4007-0982-1},
}

@inproceedings{yu_stateful_2025,
  title = {Stateful {Large},
  author = {Yu, Lingfan and Lin, Jinkun and Li, Jinyang},
  year = {2025},
  doi = {10.1145/3689031.3696086},
  url = {https://doi.org/10.1145/3689031.3696086},
  booktitle = {Proceedings of the {Twentieth},
  pages = {144--158},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {Cache, LLM Serving, Multi-turn Conversations, source: ACM},
  abstract = {Large Language Models (LLMs) are wildly popular today and it is important to serve them efficiently. Existing LLM serving systems are stateless across requests. Consequently, when LLMs are used in the common setting of multi-turn conversations, a growing log of the conversation history must be processed alongside any request by the serving system at each turn, resulting in repeated processing.In this paper, we design Pensieve, a system optimized for multi-turn conversation LLM serving. Pensieve maintains the conversation state across requests by caching previously processed history to avoid duplicate processing. Pensieve's multi-tier caching strategy can utilize both GPU and CPU memory to efficiently store and retrieve cached data. Pensieve also generalizes the recent PagedAttention kernel to support attention between multiple input tokens with a GPU cache spread over non-contiguous memory. Our evaluation shows that Pensieve can achieve 1.14-3.0× the throughput of vLLM and TensorRT-LLM and significantly reduce latency.},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{xu_automating_2024,
  title = {Automating {Bibliometric},
  author = {Xu, Haowen and Li, Xueping and Tupayachi, Jose and Lian, Jianming Jamie and Omitaomu, Olufemi A.},
  year = {2024},
  doi = {10.1145/3681780.3697252},
  url = {https://doi.org/10.1145/3681780.3697252},
  booktitle = {Proceedings of the 2nd {ACM},
  pages = {43--49},
  publisher = {Association for Computing Machinery},
  note = {event-place: Atlanta, GA, USA},
  keywords = {Bibliometrics Analysis, Large Language Models, Retrieval-Augmented Generation, Transformers},
  abstract = {Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.},
  address = {New York, NY, USA},
  series = {{UrbanAI},
  isbn = {979-8-4007-1156-5},
}

@inproceedings{kocyigit_deceptilens_2025,
  title = {{DeceptiLens},
  author = {Kocyigit, Emre and Rossi, Arianna and Sergeeva, Anastasia and Negri Ribalta, Claudia and Farjami, Ali and Lenzini, Gabriele},
  year = {2025},
  doi = {10.1145/3715275.3732129},
  url = {https://doi.org/10.1145/3715275.3732129},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1942--1959},
  publisher = {Association for Computing Machinery},
  keywords = {dark patterns, deceptive design patterns, LLMs, multimodal LLMs},
  abstract = {To detect deceptive design patterns on UIs, traditional artificial intelligence models, such as machine learning, have limited coverage and a lack of multimodality. In contrast, the capabilities of Multimodal Large Language Model (MM-LLM) can achieve wider coverage with superior performance in the detection, while providing reasoning behind each decision. We propose and implement an MM-LLM-based approach (DeceptiLens) that analyzes UIs and assesses the presence of deceptive design patterns. We utilize Retrieval Augmented Generation (RAG) process in our design and task the model with capturing the deceptive patterns, classifying its category, e.g., false hierarchy, confirmshaming, etc., and explaining the reasoning behind the classifications by employing recent prompt engineering techniques, such as Chain-of-Thought (CoT). We first create a dataset by collecting UI screenshots from the literature and web sources and quantify the agreement between the model’s outputs and a few experts’ opinions. We additionally ask experts to gauge the transparency of the system’s explanations for its classifications in terms of recognized metrics of clarity, correctness, completeness, and verifiability. The results indicate that our approach is capable of capturing the deceptive patterns in UIs with high accuracy while providing clear, correct, complete, and verifiable justifications for its decisions. We additionally release two curated datasets, one with expert-labeled UIs with deceptive design patterns, and one with AI-based generated explanations. Lastly, we propose recommendations for future improvement of the approach in various contexts of use.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{hou_applied_2025,
  title = {Applied {Research},
  author = {Hou, Lei and Jia, Beixi and Xing, Chenguang and Chen, Zhaojiang and Du, Ziliang},
  year = {2025},
  doi = {10.1145/3732945.3732946},
  url = {https://doi.org/10.1145/3732945.3732946},
  booktitle = {Proceedings of the 2025 4th {International},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {Aviation Maintenance Automation, Knowledge Base Question Answering System, Large Language Models, Retrieval-Augmented Generation, source: ACM, source: Scopus},
  abstract = {In recent years, significant progress has been made in the application of LLMs in various specialized fields. However, in the field of aircraft maintenance, intelligent systems face significant challenges due to the high complexity and private data involved. This paper describes the Zhihang Aircraft Maintenance Assistant, an LLM-based intelligent system designed to assist in aircraft maintenance tasks. The system integrates domain-specific knowledge on various aspects of aircraft maintenance and builds a vectorized knowledge base through advanced semantic understanding. Using RAG (Retrieval Augmented Generation) technology, it extracts relevant knowledge from the database based on user input and generates accurate responses to assist maintenance personnel with tasks such as fault diagnosis and troubleshooting. By using a specialized maintenance knowledge base, the system solves the problem of hallucination and ensures greater reliability in generating responses. Zhihang Assistant also provides one-click generation of maintenance record orders and concise summaries of policies and regulations. These features improve compliance with standard operating procedures, streamline workflow efficiency and minimize errors caused by inaccurate information retrieval. Test results show that Zhihang Assistant's average scores for answer relevance, content fidelity and contextual relevance were 0.973, 0.807 and 0.917 respectively, demonstrating its ability to provide highly accurate, relevant and contextually appropriate answers based on real-world maintenance needs.},
  address = {New York, NY, USA},
  series = {{ISCCN},
  isbn = {979-8-4007-1520-4},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{michelon_large_2025,
  title = {Large {Language},
  author = {Michelon, François and Zhou, Yihong and Morstyn, Thomas},
  year = {2025},
  doi = {10.1145/3679240.3734586},
  url = {https://doi.org/10.1145/3679240.3734586},
  booktitle = {Proceedings of the 16th {ACM},
  pages = {590--602},
  publisher = {Association for Computing Machinery},
  keywords = {Demand-side Flexibility, Home Energy Management System, LLM, Parametrization, ReAct},
  abstract = {Home Energy Management Systems (HEMSs) help households tailor their electricity usage based on power system signals such as energy prices. This technology helps to reduce energy bills and offers greater demand-side flexibility that supports the power system stability. However, residents who lack a technical background may find it difficult to use HEMSs effectively, because HEMSs require well-formatted parameterization that reflects the characteristics of the energy resources, houses, and users’ needs. Recently, Large-Language Models (LLMs) have demonstrated an outstanding ability in language understanding. Motivated by this, we propose an LLM-based interface that interacts with users to understand and parameterize their “badly-formatted answers”, and then outputs well-formatted parameters to implement an HEMS. We further use Reason and Act method (ReAct) and few-shot prompting to enhance the LLM performance. Evaluating the interface performance requires multiple user–LLM interactions. To avoid the efforts in finding volunteer users and reduce the evaluation time, we additionally propose a method that uses another LLM to simulate users with varying expertise, ranging from knowledgeable to non-technical. By comprehensive evaluation, the proposed LLM-based HEMS interface achieves an average parameter retrieval accuracy of 88\%, outperforming benchmark models without ReAct and/or few-shot prompting.},
  address = {New York, NY, USA},
  series = {E-{Energy},
  isbn = {979-8-4007-1125-1},
}

@inproceedings{liu_can_2024,
  title = {Can {Small},
  author = {Liu, Suqing and Yu, Zezhu and Huang, Feiran and Bulbulia, Yousef and Bergen, Andreas and Liut, Michael},
  year = {2024},
  doi = {10.1145/3649217.3653554},
  url = {https://doi.org/10.1145/3649217.3653554},
  booktitle = {Proceedings of the 2024 on {Innovation},
  pages = {388--393},
  publisher = {Association for Computing Machinery},
  note = {event-place: Milan, Italy},
  keywords = {source: ACM},
  abstract = {Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system.To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s).We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0600-4},
  series = {{ITiCSE},
}

@inproceedings{xu_list-aware_2024,
  title = {List-aware {Reranking},
  author = {Xu, Shicheng and Pang, Liang and Xu, Jun and Shen, Huawei and Cheng, Xueqi},
  year = {2024},
  doi = {10.1145/3589334.3645336},
  url = {https://doi.org/10.1145/3589334.3645336},
  booktitle = {Proceedings of the {ACM},
  pages = {1330--1340},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {reranking, retrieval-augmented LLMs, truncation},
  abstract = {The results of information retrieval (IR) are usually presented in the form of a ranking list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates reranking and truncation via a generative paradigm based on an encoder-decoder architecture with novel loss functions for joint optimization to learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&amp;A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{yu_ic-cache_2025,
  title = {{IC},
  author = {Yu, Yifan and Gan, Yu and Sarda, Nikhil and Tsai, Lillian and Shen, Jiaming and Zhou, Yanqi and Krishnamurthy, Arvind and Lai, Fan and Levy, Hank and Culler, David},
  year = {2025},
  doi = {10.1145/3731569.3764829},
  url = {https://doi.org/10.1145/3731569.3764829},
  booktitle = {Proceedings of the {ACM},
  pages = {375--398},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Seoul, Republic of Korea},
  keywords = {cloud computing, large language models (LLMs), LLM serving, load balancing, quality-efficiency tradeoff, request routing, semantic caching, source: ACM},
  abstract = {Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 70\% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge transfer among requests. However, naively caching and reusing past responses leads to a big quality drop.In this paper, we introduce IC-Cache, a caching system that enables live LLM capability augmentation to improve serving efficiency: by leveraging historical request-response pairs from larger models as in-context examples, IC-Cache empowers small LLMs to imitate and even exceed the compositional abilities (e.g., reasoning) of their larger counterparts, enabling selective offloading of requests to reduce cost and latency. Achieving this live augmentation at scale introduces intricate trade-offs between response quality, latency, and system throughput. For a new request, IC-Cache efficiently selects similar, high-utility examples to prepend them to the new request's input. At scale, it adaptively routes requests across LLMs of varying capabilities, accounting for response quality and serving loads. IC-Cache employs a cost-aware cache replay mechanism that refines example quality offline to maximize online cache utility and efficiency. Evaluations on millions of realistic requests demonstrate that IC-Cache improves LLM serving throughput by 1.4–5.9x and reduces latency by 28–71\% without hurting response quality.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1870-0},
}

@inproceedings{wen_usb-rec_2025,
  title = {{USB},
  author = {Wen, Jianyu and Wang, Jingyun and Yan, Cilin and Cai, Jiayin and Jiang, Xiaolong and Zhang, Ying},
  year = {2025},
  doi = {10.1145/3705328.3748089},
  url = {https://doi.org/10.1145/3705328.3748089},
  booktitle = {Proceedings of the {Nineteenth},
  pages = {472--481},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational Recommendation, Large Language Model, Reinforcement Learning},
  abstract = {Recently, Large Language Models (LLMs) have been widely employed in Conversational Recommender Systems (CRSs). Unlike traditional language model approaches that focus on training, all existing LLMs-based approaches are mainly centered around how to leverage the summarization and analysis capabilities of LLMs while ignoring the issue of training. Therefore, in this work, we propose an integrated training-inference framework, User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs in conversational recommendation at the model level. Firstly, we design a LLM-based Preference Optimization (PO) dataset construction strategy for RL training, which helps the LLMs understand the strategies and methods in conversational recommendation. Secondly, we propose a Self-Enhancement Strategy (SES) at the inference stage to further exploit the conversational recommendation potential obtained from RL training. Extensive experiments on various datasets demonstrate that our method consistently outperforms previous state-of-the-art methods. Codes are available at .},
  address = {New York, NY, USA},
  series = {{RecSys},
  isbn = {979-8-4007-1364-4},
}

@inproceedings{ashaduzzaman_leveraging_2025,
  title = {Leveraging {Generative},
  author = {Ashaduzzaman, Md and Tsai, Chun-Hua},
  year = {2025},
  doi = {10.1145/3708319.3733696},
  url = {https://doi.org/10.1145/3708319.3733696},
  booktitle = {Adjunct {Proceedings},
  pages = {192--201},
  publisher = {Association for Computing Machinery},
  keywords = {AI-Chatbots, Follow-up questions., Large Language Models, Personalized recommendation explanations, User comprehension},
  abstract = {Generative AI, particularly Large Language Models (LLMs), has revolutionized human-computer interaction by enabling the generation of nuanced, human-like text. This presents new opportunities, especially in enhancing explainability for AI systems like recommender systems, a crucial factor for fostering user trust and engagement. LLM-powered AI-Chatbots can be leveraged to provide personalized explanations for recommendations. Although users often find these chatbot explanations helpful, they may not fully comprehend the content. Our research focuses on assessing how well users comprehend these explanations and identifying gaps in understanding. We also explore the key behavioral differences between users who effectively understand AI-generated explanations and those who do not. We designed a three-phase user study with 17 participants to explore these dynamics. The findings indicate that the clarity and usefulness of the explanations are contingent on the user asking relevant follow-up questions and having a motivation to learn. Comprehension also varies significantly based on users’ educational backgrounds.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1399-6},
}

@inproceedings{dou_design_2024,
  title = {Design and {Application},
  author = {Dou, Juhua and Zhao, Xuhua},
  year = {2024},
  doi = {10.1145/3691720.3691739},
  url = {https://doi.org/10.1145/3691720.3691739},
  booktitle = {Proceedings of the 2nd {International},
  pages = {111--115},
  publisher = {Association for Computing Machinery},
  note = {event-place: Shanghai, China},
  keywords = {source: ACM},
  abstract = {Aiming at the problems of inconvenient retrieval of teaching resources, untimely feedback, inaccurate questioning rate, and lack of learner privacy and security that exist in various online teaching resource platforms, it is proposed to construct a new college English teaching resource platform by utilizing the technology of Large Language Mode(LLM) and Retrieval-Augmented Generation(RAG). The platform builds a local large language model and a teaching resource repository for college English, which can intelligently identify learners' questions and queries, provide learners with real-time personalized interaction and accurate Q\&amp;A services, reduce teachers' workload in delivering resources and answering questions in real time, greatly improve the efficiency of using the resource repository, and create an intelligent resource environment for teaching quality improvement.},
  address = {New York, NY, USA},
  series = {{EKI},
  isbn = {979-8-4007-1023-0},
}

@inproceedings{kim_pimba_2025,
  title = {Pimba: {A},
  author = {Kim, Wonung and Lee, Yubin and Kim, Yoonsung and Hwang, Jinwoo and Oh, Seongryong and Jung, Jiyong and Huseynov, Aziz and Park, Woong Gyu and Park, Chang Hyun and Mahajan, Divya and Park, Jongse},
  year = {2025},
  doi = {10.1145/3725843.3756121},
  url = {https://doi.org/10.1145/3725843.3756121},
  booktitle = {Proceedings of the 58th {IEEE},
  pages = {292--307},
  publisher = {Association for Computing Machinery},
  keywords = {Heterogeneous system, Large Language Model (LLM), Linear Attention, Post-Transformer LLM, Processing-in-Memory (PIM), Recurrent Neural Network (RNN), State Space Model (SSM)},
  abstract = {Transformers are the driving force behind today’s Large Language Models (LLMs), serving as the foundation for their performance and versatility. Yet, their compute and memory costs grow with sequence length, posing scalability challenges for long-context inferencing. In response, the algorithm community is exploring alternative architectures—such as state space models (SSMs) (e.g., Mamba-2), linear attention, and recurrent neural networks (RNNs)—which we refer to as post-transformers. This shift presents a key challenge: building a serving system that efficiently supports not only emerging post-transformer LLMs but also existing transformer models within a unified framework. To address this challenge, we analyze the performance characteristics of transformer and post-transformer LLMs. Despite their algorithmic differences, both are largely bounded by memory bandwidth under batched inference—due to attention in transformers and state updates in post-transformers. Inspired by this finding, we propose Pimba, an accelerator solution that aims to address the memory bottleneck by jointly leveraging (1) Processing-in-Memory (PIM) paradigm and (2) LLM quantization. Further analyses suggest two additional insights: (1) state update operations, unlike attention, incur high hardware cost, making per-bank PIM acceleration inefficient, and (2) different low-precision arithmetic methods offer varying accuracy-area tradeoffs, while we identify Microsoft’s MX as a Pareto-optimal choice. Building on these insights, we design the architecture of Pimba as an array of State-update Processing Units (SPUs), each shared between two banks to enable interleaved access. Each SPU includes a State-update Processing Engine (SPE) that comprises element-wise multipliers and adders using MX-based quantized arithmetic, enabling efficient execution of state update and attention operations. Our evaluation shows that, compared to LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 4.1 × and 2.1 × higher generation throughput, respectively.},
  address = {New York, NY, USA},
  series = {{MICRO},
  isbn = {979-8-4007-1573-0},
}

@inproceedings{shah_-ta-da_2025,
  title = {From {To},
  author = {Shah, Chirag and White, Ryen W.},
  year = {2025},
  doi = {10.1145/3726302.3730352},
  url = {https://doi.org/10.1145/3726302.3730352},
  booktitle = {Proceedings of the 48th {International},
  pages = {3911--3921},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {agents, generative ai, modalities, tasks, source: ACM},
  abstract = {For decades, scholars have emphasized that tasks should be the central focus in Information Retrieval (IR). This point of view holds even more significance with the advent of Generative Artificial Intelligence (GenAI) models, which can, among other capabilities, understand natural language, engage in dialog with users, generate bespoke user interfaces, and power agents to help complete tasks. GenAI presents an unprecedented opportunity to finally realize the potential of tasks in IR, enhance task-focused retrieval and interaction, and create ”magical” task completion moments for users. In this paper, we explore the rationale and methodology behind this argument. Traditional IR systems support mostly simple tasks. The emergence of GenAI creates an opportunity for IR systems to help users achieve complex tasks and for the IR community to rekindle its interest and demonstrate leadership in this sizable and significant problem space. We underscore the pivotal role of tasks in IR and introduce new evidence supporting the notion that task-centric approaches, abstracted from specific modalities, represent the future of IR. Building on this foundation, we envision the development, utilization, and evaluation of next-generation IR systems. We propose a promising future where IR agents prioritize users, their tasks, and their situations. However, despite their potential to address task-focused and modality-independent IR, agents alone are insufficient. We propose a robust ecosystem around these agents that transcends traditional queries, questions, prompts, and modalities to address users' fundamental needs, tasks, and goals.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{sun_largepig_2025,
  title = {{LargePiG},
  author = {Sun, Zhongxiang and Si, Zihua and Zang, Xiaoxue and Zheng, Kai and Song, Yang and Zhang, Xiao and Xu, Jun},
  year = {2025},
  doi = {10.1145/3696410.3714800},
  url = {https://doi.org/10.1145/3696410.3714800},
  booktitle = {Proceedings of the {ACM},
  pages = {4766--4779},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {hallucination, pointer generator, query generation},
  abstract = {Recent research on query generation has focused on using Large Language Models (LLMs), which, despite achieving state-of-the-art performance, also introduce hallucination issues in generated queries. In this work, we categorize these issues into relevance hallucination and factuality hallucination, proposing a new typology for hallucinations arising from LLM-based query generation. We present an effective approach to decouple content from form in LLM-generated queries, preserving the factual knowledge extracted and integrated from inputs while leveraging the LLM's linguistic capabilities to construct syntactic structures, including function words. Specifically, we introduce a model-agnostic and training-free method that transforms the Large Language Model into a Pointer-Generator (LargePiG), where the pointer attention distribution utilizes the LLM's inherent attention weights, and the copy probability is derived from the difference between the vocabulary distribution in the model's high layers and the last layer. To validate the effectiveness of LargePiG, we constructed two datasets for assessing hallucination issues in query generation, covering both document and video scenarios. Empirical studies on various LLMs demonstrated LargePiG's superiority across both datasets. Additional experiments further verified that LargePiG reduces hallucination in large vision-language models and enhances the accuracy of document-based question-answering and factuality evaluation tasks. The source code and dataset are available at https://github.com/Jeryi-Sun/LargePiG.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{baughman_large_2024,
  title = {Large {Scale},
  author = {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul and Akay, Gozde and Feris, Rogerio and Johnson, Tony and Hammer, Stephen and Karlinsky, Leonid},
  year = {2024},
  doi = {10.1145/3637528.3671542},
  url = {https://doi.org/10.1145/3637528.3671542},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {4784--4792},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {applied computing, generative ai, large scale computing, neural networks, sports and entertainment},
  abstract = {We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the GRAMMY awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforementioned events, supporting 90 million fans around the world with 8 billion page views, continuously pushing the bounds on what is possible at the intersection of sports, entertainment, and AI.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{jayawardena_improving_2024,
  title = {Improving {Quality},
  author = {Jayawardena, Lasal and Yapa, Prasan},
  year = {2024},
  doi = {10.1145/3669754.3669784},
  url = {https://doi.org/10.1145/3669754.3669784},
  booktitle = {Proceedings of the 2024 10th {International},
  pages = {196--208},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bali Island, Indonesia},
  keywords = {Graph-based Knowledge, Large Language Models, Natural Language Processing, Paraphrase Generation, Sequence-to-Sequence Models},
  abstract = {Paraphrase generation is a fundamental area of research in Natural Language Processing (NLP) and Natural Language Generation (NLG), due to its sequence-to-sequence (Seq2Seq) nature. Paraphrasing, spanning across various domains, poses challenges for simpler model architectures due to the extensive knowledge required to generate paraphrases. The added constraint of generating diverse paraphrases further complicates the task for models trained on existing datasets. We present a methodology that leverages Graph-Based Retrieval Augmented Generation (G-RAG), capable of utilizing both entity and phrasal knowledge to address this issue. We demonstrate through experiments that this approach enables both complex models like Large Language models (LLMs) and smaller Seq2Seq models to generate more diverse paraphrases without compromising semantic similarity. Furthermore, this approach’s capacity to integrate domain-specific knowledge makes it particularly effective across different domains, enhancing its applicability in varied contexts. The results are further corroborated by human evaluation and extensive quantitative analysis focusing on semantic similarity, lexical diversity, syntactic diversity, and grammatical correctness to gauge high-quality paraphrases.},
  address = {New York, NY, USA},
  series = {{ICCAI},
  isbn = {979-8-4007-1705-5},
}

@inproceedings{dolata_development_2024,
  title = {Development in times of hype: {How},
  author = {Dolata, Mateusz and Lange, Norbert and Schwabe, Gerhard},
  year = {2024},
  doi = {10.1145/3597503.3639111},
  url = {https://doi.org/10.1145/3597503.3639111},
  booktitle = {Proceedings of the {IEEE},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {AI-based systems, challenges, fashion, freelancers, generative AI, hype, hype-induced SE, hype-SE, novelty, paradigm, product, qualitative research, SE for generative AI, SE4GenAI},
  abstract = {The rise of generative AI has led many companies to hire freelancers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with aspects they perceive as unique to generative AI such as unpredictability of its output, the occurrence of hallucinations, and the inconsistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token limits and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.},
  address = {New York, NY, USA},
  series = {{ICSE},
  isbn = {979-8-4007-0217-4},
}

@inproceedings{luz_enhancing_2025,
  title = {Enhancing {Emotional},
  author = {Luz, Jonas de Araújo, Jr and Pessoa, Rafael Fonseca and Ribeiro, Guadalupe Prado Saldanha and Lira, João Vitor Vieira and Rodrigues, Maria Andréia Formico},
  year = {2025},
  doi = {10.1145/3696630.3730554},
  url = {https://doi.org/10.1145/3696630.3730554},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1461--1468},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {artificial intelligence, facial expressions, game characters, LLMs, RAG, source: ACM},
  abstract = {Facial expressiveness is essential for immersive gaming, yet generating emotionally responsive 3D characters remains challenging. This paper presents an optimized generative AI framework that integrates OpenAI's LLMs with OpenFace to improve the mapping between blendshapes (or morph targets) and facial action units (standardized facial muscle movements defined by Facial Action Coding System) for dynamic facial expressions. The system, embedded in an original game scene, generates facial expressions based on interactive player dialogues. From a software engineering perspective, it features modular, scalable AI-driven animation pipelines that support adaptive emotion modeling and game engine integration. User evaluations demonstrate high realism, clear emotion recognition, and strong engagement, confirming the system's potential to enhance player interaction. Further refinements in blendshape mappings and real-time adjustments can enhance the differentiation of subtle emotions like Disgust and Contempt, improving the system's overall expressiveness.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{amri_approach_2024,
  title = {An {Approach},
  author = {Amri, Samir and Bani, Rkia and Bani, Saida},
  year = {2024},
  doi = {10.1145/3659677.3659736},
  url = {https://doi.org/10.1145/3659677.3659736},
  booktitle = {Proceedings of the 7th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: Meknes, AA, Morocco},
  keywords = {source: ACM},
  abstract = {This project tackles the challenge of advancing document analysis with generative AI techniques. It explores two main approaches:• Fine-Tuning and Retrieval Augmented Generation (RAG).• Fine-Tuning: This approach utilizes the BART model in conjunction with a specialized vector database. The Fine-Tuning phase involves a comprehensive process of data acquisition, cleaning, and processing. This phase provides valuable insights into the challenges and considerations involved in document analysis using Fine-Tuning.Retrieval Augmented Generation (RAG): This novel method leverages generative AI for contextual understanding and response generation. The RAG section delves into the objectives, methodology, and results achieved with this cutting-edge approach.A comparative analysis is then conducted to shed light on the distinct contributions of Fine-Tuning and RAG to document analysis. Furthermore, the project extends beyond AI models by developing an interactive User Interface (UI). This UI utilizes various technologies to ensure a seamless user experience. Key features include functionalities for file upload, error handling, responsive design, and smooth integration with the backend system.},
  address = {New York, NY, USA},
  series = {{NISS},
  isbn = {979-8-4007-0929-6},
}

@inproceedings{shin_planfitting_2025,
  title = {{PlanFitting},
  author = {Shin, Donghoon and Hsieh, Gary and Kim, Young-Ho},
  year = {2025},
  doi = {10.1145/3719160.3736607},
  url = {https://doi.org/10.1145/3719160.3736607},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Creating personalized and actionable exercise plans often requires iteration with experts, which can be costly and inaccessible to many individuals. This work explores the capabilities of Large Language Models (LLMs) in addressing these challenges. We present PlanFitting, an LLM-driven conversational agent that assists users in creating and refining personalized weekly exercise plans. By engaging users in free-form conversations, PlanFitting helps elicit users’ goals, availabilities, and potential obstacles, and enables individuals to generate personalized exercise plans aligned with established exercise guidelines. Our study—involving a user study, intrinsic evaluation, and expert evaluation—demonstrated PlanFitting’s ability to guide users to create tailored, actionable, and evidence-based plans. We discuss future design opportunities for LLM-driven conversational agents to create plans that better comply with exercise principles and accommodate personal constraints.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@article{tsai_rtlfixer_2024,
  title = {Rtlfixer: {Automatically},
  author = {Tsai, Y. D. and Liu, M. and Ren, H.},
  year = {2024},
  doi = {10.1145/3649329.3657353},
  url = {https://doi.org/10.1145/3649329.3657353},
  booktitle = {Proceedings of the 61st {ACM},
  journal = {Proceedings of the 61st ACM/IEEE Design …},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, source: ACM},
  abstract = {… produce factual errors, a phenomenon termed hallucination [5]. To mitigate this challenge, the … with RAG, regardless of the quality of the compiler feedback message and LLM (GPT-4). …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{DAC},
  isbn = {979-8-4007-0601-1},
}

@inproceedings{tsutsui_case_2024,
  title = {A {Case},
  author = {Tsutsui, Shojiro and Karino, Michihiro and Kuroki, Kenichi and Fukumoto, Aya and Hamano, Yusuke and Sobata, Kenji and Saito, Temma and Kawamoto, Tatsunori and Odashima, Taku and Kato, Tsuyoshi and Motohashi, Yosuke},
  year = {2024},
  doi = {10.1145/3677052.3698626},
  url = {https://doi.org/10.1145/3677052.3698626},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {108--116},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Inquiry response support system, Large Language Models (LLMs), Non-life insurance company, Retrieval-Augmented Generation (RAG)},
  abstract = {In Japan, non-life insurance companies deliver products through agencies. Major insurance companies provide support through phone calls, emails, etc., at locations nationwide to ensure that their tens of thousands of agents can accurately handle customers, taking into account the characteristics and underwriting rules of a wide variety of insurance products. The documents to be referred to cover a vast amount array of complex rules, and as financial products, precise and courteous responses are always needed in accordance with individual cases are vital. In this study, we developed an inquiry response support system using the retrieval-augmented generation (RAG) architecture of large language models (LLMs) with the aim of improving the inquiry response operations of non-life insurance companies. In addition, we conducted evaluation experiments on the optimal combinations of conditions related to response performance, such as the chunk division units of the target manuals for searching and the number of tokens input into the LLM. Our findings showed that the accuracy improved with an appropriate number of input tokens and item-based division units with meaningful content. In the end, the inquiry response system developed with the proposed architecture is in practical use, with 14,000 users utilizing it in their daily operations.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{geyer_case_2025,
  title = {A {Case},
  author = {Geyer, Werner and He, Jessica and Sarkar, Daita and Brachman, Michelle and Hammond, Chris and Heins, Jennifer and Ashktorab, Zahra and Rosemberg, Carlos and Hill, Charlie},
  year = {2025},
  doi = {10.1145/3729176.3729200},
  url = {https://doi.org/10.1145/3729176.3729200},
  booktitle = {Proceedings of the 4th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Agile Epics Quality, Agile Software Development, Evaluation, Generative AI, Large Language Models, Software Requirements},
  abstract = {The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices.},
  address = {New York, NY, USA},
  series = {{CHIWORK},
  isbn = {979-8-4007-1384-2},
}

@inproceedings{liu_innovative_2024,
  title = {An {Innovative},
  author = {Liu, Jianquan and Adsumilli, Balu and Yanagawa, Yukiko and Dong, Haiwei},
  year = {2024},
  doi = {10.1145/3664647.3689702},
  url = {https://doi.org/10.1145/3664647.3689702},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {11125--11126},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {automotive design, computer vision, generative ai, healthcare, spatial experience, source: ACM},
  abstract = {The ACM Multimedia 2024 industry program offers a unique platform for fostering collaboration between academia and industry. This year's program features a diverse range of industry keynotes, expert talks, seminars, and demonstrations, showcasing the latest advancements in multimedia technology. Renowned experts from industry and academia will share their insights on topics such as generative AI, automotive design, computer vision, spatial experience, healthcare, and more. Attendees will have the opportunity to network with industry leaders, learn about cutting-edge technologies, and explore potential collaborations. The industry program highlights the growing importance of multimedia technology in various domains and demonstrates the innovative ways in which AI and other emerging technologies are transforming industries. By participating in this program, attendees can gain valuable knowledge, expand their professional networks, and contribute to the advancement of the field.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{toslali_agrabot_2024,
  title = {{AgraBOT},
  author = {Toslali, Mert and Snible, Edward and Chen, Jing and Cha, Alan and Singh, Sandeep and Kalantar, Michael and Parthasarathy, Srinivasan},
  year = {2024},
  doi = {10.1145/3663529.3663829},
  url = {https://doi.org/10.1145/3663529.3663829},
  booktitle = {Companion {Proceedings},
  pages = {74--79},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {AI, Document Understanding, LLM, RAG, TPSRM, source: ACM},
  abstract = {In the contemporary business landscape, organizations often rely on third-party services for many functions, including IT services, cloud computing, and business processes. To identify potential security risks, organizations conduct rigorous assessments before engaging with third-party vendors, referred to as Third-Party Security Risk Management (TPSRM). Traditionally, TPSRM assessments are executed manually by human experts and involve scrutinizing various third-party documents such as System and Organization Controls Type 2 (SOC 2) reports and reviewing comprehensive questionnaires along with the security policy and procedures of vendors—a process that is time-intensive and inherently lacks scalability. AgraBOT, a Retrieval Augmented Generation (RAG) framework, can assist TPSRM assessors by expediting TPSRM assessments and reducing the time required from days to mere minutes. AgraBOT utilizes cutting-edge AI techniques, including information retrieval (IR), large language models (LLMs), multi-stage ranking, prompt engineering, and in-context learning to accurately generate relevant answers from third-party documents to conduct assessments. We evaluate AgraBOT on seven real TPSRM assessments, consisting of 373 question-answer pairs, and attain an F1 score of 0.85.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{du_prefillonly_2025,
  title = {{PrefillOnly},
  author = {Du, Kuntai and Wang, Bowen and Zhang, Chen and Cheng, Yiming and Lan, Qing and Sang, Hejian and Cheng, Yihua and Yao, Jiayi and Liu, Xiaoxuan and Qiao, Yifan and Stoica, Ion and Jiang, Junchen},
  year = {2025},
  doi = {10.1145/3731569.3764834},
  url = {https://doi.org/10.1145/3731569.3764834},
  booktitle = {Proceedings of the {ACM},
  pages = {399--414},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Seoul, Republic of Korea},
  keywords = {continuous prefill time estimation, hybrid prefilling, large language models, prefill-only, shortest prefill first},
  abstract = {Besides typical generative applications, like ChatGPT, GitHub Copilot, and Cursor, we observe an emerging trend that LLMs are increasingly used in traditional discriminative tasks, such as recommendation, credit verification, and data labeling. The key characteristic of these emerging use cases is that the LLM generates only a single output token, rather than an arbitrarily long sequence of tokens. We refer to this as a prefill-only workload. However, since existing LLM engines assume arbitrary output lengths, they fail to leverage the unique properties of prefill-only workloads. In this paper, we present PrefillOnly, the first LLM inference engine that improves the inference throughput and latency by fully embracing the properties of prefill-only workloads. First, since it generates only one token, PrefillOnly only needs to store the KV cache of only the last computed layer, rather than of all layers. This drastically reduces the GPU memory footprint of LLM inference and allows handling long inputs without using solutions that reduce throughput, such as cross-GPU KV cache parallelization. Second, because the output length is fixed, rather than arbitrary, PrefillOnly can precisely determine the job completion time (JCT) of each prefill-only request before it starts. This enables efficient JCT-aware scheduling policies such as shortest prefill first. PrefillOnly can process up to 4× larger queries per second without inflating the average and P99 latency.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1870-0},
}

@inproceedings{naik_workshop_2024,
  title = {Workshop {Report},
  author = {Naik, Ravindra and Rajbhoj, Asha and Patwardhan, Manasi and Medicherla, Raveendra Kumar},
  year = {2024},
  doi = {10.1145/3641399.3641437},
  url = {https://doi.org/10.1145/3641399.3641437},
  booktitle = {Proceedings of the 17th {Innovations},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bangalore, India},
  keywords = {source: ACM},
  abstract = {The co-authors have organized and conducting the Generative AI-based Software Engineering workshop, co-located with the 17th Innovations in Software Engineering Conference (ISEC) at Bangalore, India on 22nd Feb. 2024. This report briefly describes the objectives and brief contents of the workshop, and hoping that the execution of the planned contents during the workshop will meet the set objectives.},
  address = {New York, NY, USA},
  series = {{ISEC},
  isbn = {979-8-4007-1767-3},
}

@inproceedings{tao_housing_2025,
  title = {"{Housing},
  author = {Tao, Hongyi and Vyas, Dhaval},
  year = {2025},
  doi = {10.1145/3706598.3713906},
  url = {https://doi.org/10.1145/3706598.3713906},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {co-housing, Generative AI, speculative design, sustainability},
  abstract = {In response to various environmental and societal challenges, co-housing has emerged to support social cohesion, grassroots innovation and ecological regeneration. Co-housing communities typically have smaller personal spaces, closer neighbourly relationships, and engage in more mutually supportive sustainable practices. To understand such communities’ motivations and visions, we developed a speculative design tool that harnesses Generative Artificial Intelligence (GenAI) to facilitate the envisioning of alternative future scenarios that challenge prevailing values, beliefs, lifestyles, and ways of knowing in contemporary society. Within the context of co-housing communities, we conducted a participatory design study with participants in co-creating their future communities. This paper unpacks implications and also reflects on the co-design approach employing GenAI. Our main findings highlight that GenAI, as a catalyst for imagination, empowers individuals to create visualisations that pose questions through a plural and situated speculative discourse.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{cheng_relic_2024,
  title = {{RELIC},
  author = {Cheng, Furui and Zouhar, Vilém and Arora, Simran and Sachan, Mrinmaya and Strobelt, Hendrik and El-Assady, Mennatallah},
  year = {2024},
  doi = {10.1145/3613904.3641904},
  url = {https://doi.org/10.1145/3613904.3641904},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {hallucination detection, human-AI interaction, natural language processing},
  abstract = {Large Language Models (LLMs) are notorious for blending fact with fiction and generating non-factual content, known as hallucinations. To address this challenge, we propose an interactive system that helps users gain insight into the reliability of the generated text. Our approach is based on the idea that the self-consistency of multiple samples generated by the same LLM relates to its confidence in individual claims in the generated texts. Using this idea, we design RELIC, an interactive system that enables users to investigate and verify semantic-level variations in multiple long-form responses. This allows users to recognize potentially inaccurate information in the generated text and make necessary corrections. From a user study with ten participants, we demonstrate that our approach helps users better verify the reliability of the generated text. We further summarize the design implications and lessons learned from this research for future studies of reliable human-LLM interactions.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{yun_generative_2025,
  title = {Generative {AI},
  author = {Yun, Bhada and Feng, Dana and Chen, Ace S. and Nikzad, Afshin and Salehi, Niloufar},
  year = {2025},
  doi = {10.1145/3706598.3713337},
  url = {https://doi.org/10.1145/3706598.3713337},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Human-AI Interaction, Information Visualization, Interaction Design, Knowledge Synthesis, Large Language Models},
  abstract = {Our study of 20 knowledge workers revealed a common challenge: the difficulty of synthesizing unstructured information scattered across multiple platforms to make informed decisions. Drawing on their vision of an ideal knowledge synthesis tool, we developed Yodeai, an AI-enabled system, to explore both the opportunities and limitations of AI in knowledge work. Through a user study with 16 product managers, we identified three key requirements for Generative AI in knowledge work: adaptable user control, transparent collaboration mechanisms, and the ability to integrate background knowledge with external information. However, we also found significant limitations, including overreliance on AI, user isolation, and contextual factors outside the AI’s reach. As AI tools become increasingly prevalent in professional settings, we propose design principles that emphasize adaptability to diverse workflows, accountability in personal and collaborative contexts, and context-aware interoperability to guide the development of human-centered AI systems for product managers and knowledge workers.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{shin_what_2025,
  title = {What {About},
  author = {Shin, Donghoon and Chen, Tze-Yu and Hsieh, Gary and Wang, Lucy Lu},
  year = {2025},
  doi = {10.1145/3715336.3735686},
  url = {https://doi.org/10.1145/3715336.3735686},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1210--1227},
  publisher = {Association for Computing Machinery},
  keywords = {customization, generative AI, rigor-relevance paradox, translational science},
  abstract = {Despite the wealth of knowledge in research papers, practitioners struggle to apply research results to their work due to significant research-practice gaps. This study addresses the rigor-relevance paradox, where academic rigor can undermine the practical relevance of research for designers. Specifically, we explore the potential of large language models (LLMs) to customize translational research artifacts (i.e., design cards) and improve relevance to specific designers’ needs. In our preliminary study (N = 15), designers defined relevance as alignment between the content of the translational artifact and their design context—including target users, modalities/domains, and design stages. Based on these findings, we implemented an LLM-powered pipeline that allows designers to customize research papers into design cards tailored to their contexts. Our evaluation (N = 20) demonstrated that designers perceived customized artifacts as more relevant, actionable, valid, generative, and inspiring than those without customization—even for less topically related papers—indicating LLM-powered customization can be used to support research translation.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-1485-6},
}

@inproceedings{zhao_lookahead_2024,
  title = {Lookahead: {An},
  author = {Zhao, Yao and Xie, Zhitian and Liang, Chen and Zhuang, Chenyi and Gu, Jinjie},
  year = {2024},
  doi = {10.1145/3637528.3671614},
  url = {https://doi.org/10.1145/3637528.3671614},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {6344--6355},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {inference framework, large language model, lossless generation accuracy, multi-branch draft, single-branch draft, trie tree},
  abstract = {As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named lookahead, introduces a multi-branch strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach could be comparable with the performance of the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework has been widely deployed in Alipay since April 2023, and obtained remarkable 2.66x to 6.26x speedup. Our code is available at https://github.com/alipay/PainlessInferenceAcceleration.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{prather_beyond_2025,
  title = {Beyond the {Hype},
  author = {Prather, James and Leinonen, Juho and Kiesler, Natalie and Gorson Benario, Jamie and Lau, Sam and MacNeil, Stephen and Norouzi, Narges and Opel, Simone and Pettit, Vee and Porter, Leo and Reeves, Brent N. and Savelka, Jaromir and Smith, David H., IV and Strickroth, Sven and Zingaro, Daniel},
  year = {2025},
  doi = {10.1145/3689187.3709614},
  url = {https://doi.org/10.1145/3689187.3709614},
  booktitle = {2024 {Working},
  pages = {300--338},
  publisher = {Association for Computing Machinery},
  note = {event-place: Milan, Italy},
  keywords = {artificial intelligence, computing education, genai, generative ai, large language models, pedagogical practices, teaching computing, source: ACM},
  abstract = {Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1208-1},
  series = {{ITiCSE},
}

@inproceedings{jin_chatting_2025,
  title = {Chatting with a {Learning},
  author = {Jin, Yueqiao and Yang, Kaixun and Yan, Lixiang and Echeverria, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela and Fan, Jie Xiang and Li, Xinyu and Gasevic, Dragan and Martinez-Maldonado, Roberto},
  year = {2025},
  doi = {10.1145/3706468.3706545},
  url = {https://doi.org/10.1145/3706468.3706545},
  booktitle = {Proceedings of the 15th {International},
  pages = {579--590},
  publisher = {Association for Computing Machinery},
  keywords = {data visualisation, generative AI chatbots, generative AI literacy, learning analytics dashboard},
  abstract = {Learning analytics dashboards (LADs) simplify complex learner data into accessible visualisations, providing actionable insights for educators and students. However, their educational effectiveness has not always matched the sophistication of the technology behind them. Explanatory and interactive LADs, enhanced by generative AI (GenAI) chatbots, hold promise by enabling dynamic, dialogue-based interactions with data visualisations and offering personalised feedback through text. Yet, the effectiveness of these tools may be limited by learners’ varying levels of GenAI literacy, a factor that remains underexplored in current research. This study investigates the role of GenAI literacy in learner interactions with conventional (reactive) versus scaffolding (proactive) chatbot-assisted LADs. Through a comparative analysis of 81 participants, we examine how GenAI literacy is associated with learners’ ability to interpret complex visualisations and their cognitive processes during interactions with chatbot-assisted LADs. Results show that while both chatbots significantly improved learner comprehension, those with higher GenAI literacy benefited the most, particularly with conventional chatbots, demonstrating diverse prompting strategies. Findings highlight the importance of considering learners’ GenAI literacy when integrating GenAI chatbots in LADs and educational technologies. Incorporating scaffolding techniques within GenAI chatbots can be an effective strategy, offering a more guided experience that reduces reliance on learners’ GenAI literacy.},
  address = {New York, NY, USA},
  series = {{LAK},
  isbn = {979-8-4007-0701-8},
}

@inproceedings{inie_how_2025,
  title = {How {CO2STLY},
  author = {Inie, Nanna and Falk, Jeanette and Selvan, Raghavendra},
  year = {2025},
  doi = {10.1145/3706598.3714227},
  url = {https://doi.org/10.1145/3706598.3714227},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI Hype, carbon footprint, energy consumption, Environmental Sustainability, Generative AI},
  abstract = {The energy cost of developing and deploying Generative AI (GenAI) models has exploded with their mass adoption, as has the ensuing carbon emissions. The climate impact of this is currently unknown. In Human-Computer Interaction, GenAI models are rarely trained but often used. Based on detailed review of 282 papers, we estimate this footprint from energy consumption of the total use of GenAI for CHI 2024 research as between 10,769.63 and 10,925.12 kg CO2e — equal to driving a car for more than 100,000 km. We show that in CHI research, GenAI is most often used for Prototyping, Evaluation \&amp; User studies, and that Data Collection and Fine-tuning models incurs the highest CO2st.1 We find that CHI submissions are unlikely to report GenAI use transparently, which makes precise calculations difficult. By measuring the usage of a subset of the papers on local hardware, we obtain estimations of the energy consumption and carbon footprint. Based on this evidence, we discuss and demonstrate ways to mitigate the issues of GenAI carbon footprint and lack of transparency.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{he_study_2024,
  title = {A {Study},
  author = {He, Yudong and Tang, Yinqiu and Chen, Tianhong},
  year = {2024},
  doi = {10.1145/3696500.3696523},
  url = {https://doi.org/10.1145/3696500.3696523},
  booktitle = {Proceedings of the 2024 {International},
  pages = {136--141},
  publisher = {Association for Computing Machinery},
  note = {event-place: Shanghai, China},
  keywords = {source: ACM},
  abstract = {Construction projects typically involve large-scale operations and are subject to complex external conditions, making it essential to safeguard the interests of contractor enterprises through well-crafted contract clauses. However, the current reliance on expert judgment for identifying contract risks presents several challenges, including lengthy processing times, heavy workloads, and inconsistent results. To address these issues, this study introduces a Large Language Model (LLM)-based approach for automating the identification of risks in construction contracts. The proposed method was rigorously validated on 26 actual contracts, achieving an average accuracy of 76.7\% across four state-of-the-art LLMs. This research advances the application of LLMs in construction contract management, providing practical solutions to existing challenges and setting the stage for further exploration in LLM-driven contract analysis.},
  address = {New York, NY, USA},
  series = {{ICBDDM},
  isbn = {979-8-4007-1027-8},
}

@inproceedings{shen_large_2025,
  title = {A large language model algorithm for green finance innovation for digital technology innovation of heavily polluting enterprises},
  author = {Shen, Yuxin and Lu, Wei},
  year = {2025},
  doi = {10.1145/3759179.3760456},
  url = {https://doi.org/10.1145/3759179.3760456},
  booktitle = {Proceedings of the 10th {International},
  pages = {344--352},
  publisher = {Association for Computing Machinery},
  keywords = {digital technology, Green finance, heavily polluting enterprises, impact, innovation},
  abstract = {Green financial innovation, by providing targeted credit and incentive mechanisms, alleviates the financing constraints of heavily polluting enterprises and intensifies the pressure of environmental regulations, thereby promoting their digital technology innovation to achieve emission reduction, efficiency improvement and green transformation. At present, there are many problems in the digital technology innovation of heavily polluting enterprises. Therefore, this study aims to construct a green finance innovation technology system to enhance the impact of digital technologies on heavily polluting enterprises. This paper utilizes green finance innovation technologies to design and implement a digital technology innovation system based on heavily polluting enterprises. Its performance was verified through experiments. The experimental results show that this method is simple to operate and has a profound impact on research. The results show that the large language model algorithm has significant advantages in the digital technology innovation of heavily polluting enterprises and can effectively solve the technical bottlenecks.},
  address = {New York, NY, USA},
  series = {{ICCSIE},
  isbn = {979-8-4007-1863-2},
}

@inproceedings{borghoff_generative_2025,
  title = {Generative {AI},
  author = {Borghoff, Uwe M. and Minas, Mark and Schopp, Jannis},
  year = {2025},
  doi = {10.1145/3723010.3723012},
  url = {https://doi.org/10.1145/3723010.3723012},
  booktitle = {Proceedings of the 6th {European},
  pages = {161--170},
  publisher = {Association for Computing Machinery},
  keywords = {AI support, AI-based tutoring, experiments, software development project course, software engineering education},
  abstract = {The way software is developed is changing rapidly due to the general availability of generative AI tools. As a result, the software engineering education that is part of every computer science program needs to change. Especially in software engineering courses, such AI tools need to be used and practiced in a meaningful and useful way. The programming project is one such course at our university, and the curriculum will be expanded accordingly in the future. In this paper we describe our approach and a user study among the participants of the last programming project, in which we collected experiences with the use of current AI tools, in particular highlighting their usefulness and limitations. Our study focuses on identifying which aspects of the course students used AI tools for, evaluating successful applications, and uncovering remaining challenges.},
  address = {New York, NY, USA},
  series = {{ECSEE},
  isbn = {979-8-4007-1282-1},
}

@inproceedings{feng_complicated_2025,
  title = {Complicated {Semantic},
  author = {Feng, Yunling and Ling, Gui and Jiang, Yue and Huang, Jianfeng and Ou, Dan and Liu, Qingwen and Lv, Fuyu and Xu, Yajing},
  year = {2025},
  doi = {10.1145/3711896.3737204},
  url = {https://doi.org/10.1145/3711896.3737204},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4435--4446},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {large language models, query reformulation, semantic matching, source: ACM},
  abstract = {In the realm of e-commerce search, semantic matching has consistently been a core issue, as it directly affects user experience and company revenue. However, users' queries often fail to effectively retrieve relevant products due to discrepancies between the user's expression habits and product names written by merchants. Even existing large language model (LLM) based query rewriting methods can bridge the semantic gap for most queries, they are still ineffective for long-tail queries with complicated semantic. In this paper, we propose Complicated Semantic Alignment Query Rewrite(CSA-QR) framework, which mitigates the semantic differences in long-tail queries with complicated semantics. CSA-QR comprises three stages: high-quality supervised fine-tuning (SFT) dataset generation, multi-dimensional alignment dataset generation, and binary feedback Proximal Policy Optimization (PPO) for reinforcement alignment. Initially, we utilize general large language models to generate rewrite candidates, followed by manual annotation to discriminate the candidates, then use the retrieval augmentation generation (RAG) based on existing annotations to produce a higher quality SFT dataset. Subsequently, we decouple the feedback data into user semantic consistency and merchant expression consistency dimensions to collect multi-dimensional alignment data. Finally, we introduce a binary feedback method to train the reward model, enabling it to better guide alignment training within our context. We also identify a set of more appropriate reward model evaluation metrics to guide our iterations. Offline experiments demonstrate the effectiveness of this method in improving retrieval performance. Online A/B tests reveal that our method significantly boosts critical metrics such as product click-through rate (CTR), gross merchandise volume (GMV) and number of transaction (\#Trans) for long-tail complicated queries. CSA-QR has been deployed on Taobao, one of China's most popular online shopping platforms, since September 2024.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{yao_news_2024,
  title = {News {GPT},
  author = {Yao, Shunyu and Ke, Qingqing and Li, Kangtong and Wang, Qiwei and Hu, Jie},
  year = {2024},
  doi = {10.1145/3689299.3689320},
  url = {https://doi.org/10.1145/3689299.3689320},
  booktitle = {Proceedings of the 2024 3rd {International},
  pages = {113--119},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {source: ACM, source: Scopus},
  abstract = {With the continuous development of natural language processing (NLP) technology, large models have become essential tools for handling natural language tasks. In the news domain, large models can be used for automating news generation, thereby enhancing the productivity and quality of news production. As a result, we introduce a new large model—News GPT—which utilizes an external knowledge retrieval module to inject real information, providing authentic and reliable news generation services. By combining Chinese and English news data with general domain data, we have constructed a high-quality, multi-domain news dataset consisting of 1.4B tokens. In the pre-training phase, we expand the Chinese vocabulary for the Llama2-70B model, and in the fine-tuning phase, we design expert prompts to help the model understand downstream tasks better. We have tested News GPT in various aspects, and the experimental results show that News GPT, as an intelligent news assistant, can effectively complete tasks using retrieval tools in different application scenarios. It possesses excellent natural language understanding, knowledge, and logical reasoning abilities, and effectively controls the hallucinations in the generation process. News GPT demonstrates high accuracy and reliability in news generation and can provide substantial support for the news industry.},
  address = {New York, NY, USA},
  series = {{RAIIE},
  isbn = {979-8-4007-1831-1},
  annote = {Cited by: 1},
}

@inproceedings{gao_humanizing_2024,
  title = {Humanizing {Artifacts},
  author = {Gao, Fengsen and Fang, Ke and Chan, Wai Kin (Victor)},
  year = {2024},
  doi = {10.1145/3665463.3678792},
  url = {https://doi.org/10.1145/3665463.3678792},
  booktitle = {Companion {Proceedings},
  pages = {91--96},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tampere, Finland},
  keywords = {artifact, cultural heritage, education, game design, humanized design},
  abstract = {Artifacts are vivid carriers of history and culture. Current applications of cultural heritage (CH) education present knowledge from a third-person perspective, potentially overlooking the emotional connection between users and artifacts. This paper introduces the concept of "Knowledge Actor", which humanizes artifacts, making them natural first-person narrators of knowledge. For instance, in the game, the glazed porcelain artifact is humanized as a boy. Players evoke his memories and advance the game using key information. To gather key information, players talk with ores to solve the creation puzzle or to the map to get location details, simultaneously deducing, acquiring, and memorizing knowledge. The humanized design and generative AI capabilities seamlessly integrate puzzle-solving gameplay with educational objectives. Experimental results indicate this game design fosters emotional connections between users and artifacts, enhances learning outcomes, and improves the game experience. This paper explores new directions of humanized design and generative AI in CH education.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0692-9},
}

@inproceedings{sharma_openroad-assistant_2024,
  title = {{OpenROAD},
  author = {Sharma, Utsav and Wu, Bing-Yue and Kankipati, Sai Rahul Dhanvi and Chhabria, Vidya A. and Rovinski, Austin},
  year = {2024},
  doi = {10.1145/3670474.3685960},
  url = {https://doi.org/10.1145/3670474.3685960},
  booktitle = {Proceedings of the 2024 {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salt Lake City, UT, USA},
  keywords = {source: ACM},
  abstract = {Large language models (LLMs) have shown significant potential in serving as domain-specific chatbots. Recently, these models have emerged as powerful tools for chip design, providing both natural language responses and script generation for domain-specific inquiries. Previous work has demonstrated the effectiveness of LLMs in assisting with physical design automation; however, these approaches often rely on proprietary tools, APIs, technologies, and designs. As a result, access to these models is extremely limited, particularly for new chip designers who could greatly benefit from a design assistant. This paper introduces OpenROAD-Assistant, an open-source chatbot for OpenROAD that relies only on public data and responds to queries in either prose or Python script using the OpenROAD APIs. OpenROAD-Assistant leverages the Llama3-8B foundation model and employs retrieval-aware fine-tuning (RAFT) to respond to physical design-specific questions for OpenROAD. Notably, OpenROAD-Assistant outperforms other foundational models such as ChatGPT3.5, ChatGPT4, Code Llama, Claude3, and other ablation study baselines on the measured metrics (pass@k for scripting and BERTScore/BARTScore for question-answering). OpenROAD-Assistant achieves a 77\% pass@1 score, 80\% pass@3 score for scripting, and it achieves a 98\% BERTScore and 96\% BARTScore on question-answering.},
  address = {New York, NY, USA},
  series = {{MLCAD},
  isbn = {979-8-4007-0699-8},
}

@inproceedings{das_swain_teacher_2024,
  title = {Teacher, {Trainer},
  author = {Das Swain, Vedant and Saha, Koustuv},
  year = {2024},
  doi = {10.1145/3663384.3663401},
  url = {https://doi.org/10.1145/3663384.3663401},
  booktitle = {Proceedings of the 3rd {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Newcastle upon Tyne, United Kingdom},
  keywords = {generative AI, large language models, LLMs, worker performance, worker wellbeing, workplace},
  abstract = {The increasing integration of computing technologies in the workplace has also seen the conceptualization and development of data-driven and algorithmic tools that aim to improve workers’ wellbeing and performance. However, both research and practice have revealed several gaps in the effectiveness and deployment of these tools. Meanwhile, the recent advances in generative AI have highlighted the tremendous capabilities of large language models (LLMs) in processing large volumes of data in producing human-interactive natural language content. This paper explores the opportunities for LLMs in facilitating worker-centered design for Wellbeing Assessment Tools (WATs). In particular, we map features of LLMs against known challenges of WAT. We highlight how the LLMs can bridge or even widen the gaps in worker-centeric WAT. This paper aims to inspire new research directions focused on empowering workers and anticipating harms in integrating LLMs with workplace technologies.},
  address = {New York, NY, USA},
  series = {{CHIWORK},
  isbn = {979-8-4007-1017-9},
}

@inproceedings{yen_search_2025,
  title = {To {Search},
  author = {Yen, Ryan and Xie, Yimeng and Sultanum, Nicole and Zhao, Jian},
  year = {2025},
  doi = {10.1145/3715336.3735752},
  url = {https://doi.org/10.1145/3715336.3735752},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1084--1106},
  publisher = {Association for Computing Machinery},
  keywords = {Code Generation, Human-AI Interaction, Information Seeking, Information-Foraging, source: ACM},
  abstract = {Programmers now use both generative AI (GenAI) and traditional web search for information-seeking, yet how these tools are used individually or in combination remains unclear. To answer this, we conducted a multi-phase investigation, including retrospective interviews to identify foraging behaviours and challenges and an observational study with a technology probe to analyze how contextual information flows across tools. Our findings reveal that effective information-seeking requires adaptable strategies and varying levels of contextual detail. Building on these insights, we propose five design dimensions for developing tools that integrate web search, GenAI, and code editors. We further demonstrated the generative power of these design dimensions with a proof-of-concept prototype, validated through a user study, offering actionable design implications for enhancing integrated information-seeking workflows across web search and GenAI in programming.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1485-6},
  series = {{DIS},
}

@inproceedings{liu_cachegen_2024,
  title = {{CacheGen},
  author = {Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and Jiang, Junchen},
  year = {2024},
  doi = {10.1145/3651890.3672274},
  url = {https://doi.org/10.1145/3651890.3672274},
  booktitle = {Proceedings of the {ACM},
  pages = {38--56},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney, NSW, Australia},
  keywords = {compression, KV cache, large language models, source: ACM},
  abstract = {As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays.CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5–4.3x and the total delay in fetching and processing contexts by 3.2–3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.},
  address = {New York, NY, USA},
  series = {{ACM},
  isbn = {979-8-4007-0614-1},
}

@inproceedings{haroon_twips_2024,
  title = {{TwIPS},
  author = {Haroon, Rukhshan and Dogar, Fahad},
  year = {2024},
  doi = {10.1145/3663548.3675633},
  url = {https://doi.org/10.1145/3663548.3675633},
  booktitle = {Proceedings of the 26th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: St. John's, NL, Canada},
  keywords = {source: ACM},
  abstract = {Autistic individuals often experience difficulties in conveying and interpreting emotional tone and non-literal nuances. Many also mask1 their communication style to avoid being misconstrued by others, spending considerable time and mental effort in the process. To address these challenges in text-based communication, we present TwIPS2, a prototype texting application powered by a large language model (LLM), which can assist users with: a) deciphering tone and meaning of incoming messages, b) ensuring the emotional tone of their message is in line with their intent, and c) coming up with alternate phrasing for messages that could be misconstrued and received negatively by others. We leverage an AI-based simulation and a conversational script to evaluate TwIPS with 8 autistic participants in an in-lab setting. Our findings show TwIPS enables a convenient way for participants to seek clarifications, provides a better alternative to tone indicators, and facilitates constructive reflection on writing technique and style. We also examine how autistic users utilize language for self-expression and interpretation in instant messaging, and gather feedback for enhancing our prototype. We conclude with a discussion around balancing user-autonomy with AI-mediation, establishing appropriate trust levels in AI systems, and autistic users’ customization needs in the context of AI-assisted communication.},
  address = {New York, NY, USA},
  series = {{ASSETS},
  isbn = {979-8-4007-0677-6},
}

@inproceedings{yao_lawyer_2024,
  title = {Lawyer {GPT},
  author = {Yao, Shunyu and Ke, Qingqing and Wang, Qiwei and Li, Kangtong and Hu, Jie},
  year = {2024},
  doi = {10.1145/3689299.3689319},
  url = {https://doi.org/10.1145/3689299.3689319},
  booktitle = {Proceedings of the 2024 3rd {International},
  pages = {108--112},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {source: ACM},
  abstract = {The emergence of large language models has brought about revolutionary changes in the field of natural language processing and has shown extraordinary potential in general tasks and various specific domain tasks, especially in the legal field. However, there are still many factors that constrain the application of large language models in the legal field, with the main problems being the lack of domain knowledge and the ability to apply knowledge to solve problems. Therefore, we propose Lawyer GPT, a legal large model that incorporates domain knowledge by using an external knowledge retrieval module to combine an external knowledge base and possesses legal reasoning capabilities. We have collected a large amount of legal domain data and combined it with general domain data, using GPT-4 Turbo to build a high-quality legal dataset. To make Lawyer GPT's legal reasoning capabilities more reliable, we have performed supervised fine-tuning on this dataset, providing it with a good ability to apply domain knowledge to solve problems and enabling it to independently handle various legal professional issues. In addition, we have constructed a legal knowledge base and used retrieval enhancement techniques to provide Lawyer GPT with tools to retrieve external domain knowledge, thereby improving the factual and rationality of the generated content. Experimental results show that Lawyer GPT has demonstrated good performance in both subjective and objective legal domain tests, showing a strong ability to handle legal issues.},
  address = {New York, NY, USA},
  series = {{RAIIE},
  isbn = {979-8-4007-1831-1},
}

@inproceedings{leong_putting_2024,
  title = {Putting {Things},
  author = {Leong, Joanne and Pataranutaporn, Pat and Danry, Valdemar and Perteneder, Florian and Mao, Yaoli and Maes, Pattie},
  year = {2024},
  doi = {10.1145/3613904.3642393},
  url = {https://doi.org/10.1145/3613904.3642393},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {education, generative artificial intelligence, learning, vocabulary},
  abstract = {Fostering students’ interests in learning is considered to have many positive downstream effects. Large language models have opened up new horizons for generating content tuned to one’s interests, yet it is unclear in what ways and to what extent this customization could have positive effects on learning. To explore this novel dimension, we conducted a between-subjects online study (n=272) featuring different variations of a generative AI vocabulary learning app that enables users to personalize their learning examples. Participants were randomly assigned to control (sentence sourced from pre-existing text) or experimental conditions (generated sentence or short story based on users’ text input). While we did not observe a difference in learning performance between the conditions, the analysis revealed that generative AI-driven context personalization positively affected learning motivation. We discuss how these results relate to previous findings and underscore their significance for the emerging field of using generative AI for personalized\&nbsp;learning.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{lee_altcanvas_2024,
  title = {{AltCanvas},
  author = {Lee, Seonghee and Kohga, Maho and Landau, Steve and O'Modhrain, Sile and Subramonyam, Hari},
  year = {2024},
  doi = {10.1145/3663548.3675600},
  url = {https://doi.org/10.1145/3663548.3675600},
  booktitle = {Proceedings of the 26th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: St. John's, NL, Canada},
  keywords = {source: ACM},
  abstract = {People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which construct images line by line, are suitable for simple tasks like math but not for more expressive artwork. On the other hand, emerging generative AI-based text-to-image tools can produce expressive illustrations from descriptions in natural language, but they lack precise control over image composition and properties. To address this gap, our work integrates generative AI with a constructive approach that provides users with enhanced control and editing capabilities. Our system, AltCanvas, features a tile-based interface enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving speech and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in design and evaluation, we found that participants effectively used the AltCanvas’s workflow to create illustrations.},
  address = {New York, NY, USA},
  series = {{ASSETS},
  isbn = {979-8-4007-0677-6},
}

@inproceedings{drosos_its_2024,
  title = {"{It},
  author = {Drosos, Ian and Sarkar, Advait and Xu, Xiaotong and Negreanu, Carina and Rintel, Sean and Tankelevitch, Lev},
  year = {2024},
  doi = {10.1145/3663384.3663389},
  url = {https://doi.org/10.1145/3663384.3663389},
  booktitle = {Proceedings of the 3rd {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Newcastle upon Tyne, United Kingdom},
  keywords = {source: ACM},
  abstract = {Generative AI tools can help users with many tasks. One such task is data analysis, which is notoriously challenging for non-expert end-users due to its expertise requirements, and where AI holds much potential, such as finding relevant data sources, proposing analysis strategies, and writing analysis code. To understand how data analysis workflows can be assisted or impaired by generative AI, we conducted a study (n=15) using Bing Chat via participatory prompting. Participatory prompting is a recently developed methodology in which users and researchers reflect together on tasks through co-engagement with generative AI. In this paper we demonstrate the value of the participatory prompting method. We found that generative AI benefits the information foraging and sensemaking loops of data analysis in specific ways, but also introduces its own barriers and challenges, arising from the difficulties of query formulation, specifying context, and verifying results.},
  address = {New York, NY, USA},
  series = {{CHIWORK},
  isbn = {979-8-4007-1017-9},
}

@article{esposito_beyond_2024,
  title = {Beyond words: {On},
  author = {Esposito, M. and Palagiano, F. and Lenarduzzi, V. and {...},
  year = {2024},
  doi = {10.1145/3674805.3695401},
  url = {https://doi.org/10.1145/3674805.3695401},
  booktitle = {Proceedings of the 18th {ACM},
  journal = {Proceedings of the 18th …},
  pages = {517--527},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Actionability, Analysis, Explainability, Fine-Tuning, Generative AI, Human Experts, Large Language Model, Management, Retrieval Augmented Generation, Risk, Security, Standards, XAI},
  abstract = {… A large language model can quickly summarize information … of Retrieval-Augmented Generation and fine-tuned LLM in … show that RAG-assisted LLMs have the lowest hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ESEM},
  isbn = {979-8-4007-1047-6},
}

@article{hu_cg-rag_2025,
  title = {Cg-rag: {Research},
  author = {Hu, Y. and Lei, Z. and Dai, Z. and Zhang, A. and Angirekula, A. and {...},
  year = {2025},
  doi = {10.1145/3726302.3729920},
  url = {https://doi.org/10.1145/3726302.3729920},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th …},
  pages = {678--687},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, citation graphs, graph learning, graph retrieval-augmented generation, hybrid retrieval, research question answering},
  abstract = {… LLaMA and closed-source models such as ChatGPT, we first summarize the graph context … Specifically, for each contextual subgraph G𝑖 ∈ 𝑆( G;𝑞), we prompt the LLM to summarize …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{tan_htmlrag_2025,
  title = {Htmlrag: {Html},
  author = {Tan, J. and Dou, Z. and Wang, W. and Wang, M. and Chen, W. and {...},
  year = {2025},
  doi = {10.1145/3696410.3714546},
  url = {https://doi.org/10.1145/3696410.3714546},
  booktitle = {Proceedings of the {ACM},
  journal = {Proceedings of the ACM …},
  pages = {1733--1746},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, HTML, large language model, retrieval-augmented generation},
  abstract = {… Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination … the overall performance of RAG, so we evaluate the LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{sequeda_benchmark_2024,
  title = {A {Benchmark},
  author = {Sequeda, Juan and Allemang, Dean and Jacob, Bryon},
  year = {2024},
  doi = {10.1145/3661304.3661901},
  url = {https://doi.org/10.1145/3661304.3661901},
  booktitle = {Proceedings of the 7th {Joint},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {source: ACM},
  abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16\%. Notably, this accuracy increases to 54\% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0653-0},
  series = {{GRADES},
}

@article{ardimento_rag-based_2024,
  title = {A {RAG},
  author = {Ardimento, P. and Bernardi, M. L. and Cimitile, M. and {...},
  year = {2024},
  doi = {10.1145/3652620.3687784},
  url = {https://doi.org/10.1145/3652620.3687784},
  booktitle = {Proceedings of the {ACM},
  journal = {Proceedings of the ACM …},
  pages = {26--30},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, large language model, learning, retrieval augmented generation, software modeling, tool, UML, source: ACM},
  abstract = {… bear this notice and the full citation on the first page. Copyrights … Retrieval Augmented Generation Large Language Model (RAG-based LLM), also referred to more simply as RAG-LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{MODELS},
  isbn = {979-8-4007-0622-6},
}

@inproceedings{wu_research_2025,
  title = {Research on {Intelligent},
  author = {Wu, Wei and Liu, Zhiruo and Ma, Junfeng and Cao, Jiguang},
  year = {2025},
  doi = {10.1145/3767052.3767087},
  url = {https://doi.org/10.1145/3767052.3767087},
  booktitle = {Proceedings of the 2025 {International},
  pages = {227--233},
  publisher = {Association for Computing Machinery},
  keywords = {Intelligent generation of test questions, Large Language Model, Retrieval-Augmented Generation},
  abstract = {With the acceleration of technological iteration, the intelligent transformation of communication operation and maintenance urgently requires supporting talent assessment tools. Currently, traditional test question generation methods rely on static knowledge bases and manual writing. This article proposes an intelligent test question generation method based on knowledge enhancement to address the problems of knowledge update lag and insufficient scene generalization in traditional test question generation methods. By integrating technologies such as Large Language Model (LLM) and Retrieval-Augmented Generation (RAG), a professional vector knowledge base in the field of communication operation and maintenance is constructed. The RAG process is optimized to suppress the illusion of LLM and achieve intelligent generation of operation and maintenance test questions. This provides an efficient and customizable intelligent solution for evaluating the abilities of communication operation and maintenance talents.},
  address = {New York, NY, USA},
  series = {{BDAIE},
  isbn = {979-8-4007-1601-0},
}

@article{sharma_enhancing_2025,
  title = {Enhancing {Security},
  author = {Sharma, A. N. and Akbar, K. A. and Thuraisingham, B. and {...},
  year = {2025},
  doi = {10.1145/3716815.3729012},
  url = {https://doi.org/10.1145/3716815.3729012},
  booktitle = {Proceedings of the 10th {ACM},
  journal = {Proceedings of the 2025 …},
  pages = {2--12},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, application security, knowledge graphs, multimodal llm interpretability, retrieval-augmented generation, source: ACM},
  abstract = {… LLM hallucinations by incorporation of external knowledge sources ensuring responses are accurate and relevant. RAG … -RAG’s improved contextual information enhances an LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{IWSPA},
  isbn = {979-8-4007-1501-3},
}

@inproceedings{meda_integrating_2025,
  title = {Integrating {Prompt},
  author = {Meda, Kavya Nikhita and Nara, Pavan Subhash Chandrabose and Bozenka, Svoboda and Zormati, Tarek and Turner, Seth and Worley, Wayne and Mitra, Reshmi},
  year = {2025},
  doi = {10.1145/3696673.3723069},
  url = {https://doi.org/10.1145/3696673.3723069},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {180--187},
  publisher = {Association for Computing Machinery},
  note = {event-place: Southeast Missouri State University, Cape Girardeau, MO, USA},
  keywords = {cybersecurity education, embedding models, information retrieval, large language model (LLM), retrieval-augmented generation (RAG), source: ACM},
  abstract = {This paper aims to develop a specialized Large Language Model (LLM) for cybersecurity training, designed to educate users on fundamental cybersecurity concepts. This paper focuses on creating an interactive system where users can ask questions about computer security and receive accurate, informative responses. By addressing cybersecurity as a critical national issue, the LLM empowers individuals and organizations to defend against malicious cyber threats. Our system was developed using Python, utilizing Google Sheets as a database, Gradio for the user interface, and Google Gemini's API for advanced language processing. The implementation followed a test-driven development approach, iterating between coding and testing to ensure functionality and reliability. Key technologies include Mistral's Large 2 model and embedding models for clustering related data. The Retrieval-Augmented Generation (RAG) framework was employed to integrate information retrieval with the LLM, enhancing its accuracy and relevance. Tools such as Google Suite, Colab, and Gradio contributed to creating a robust and user-friendly system. This paper highlights the potential of domain-specific LLMs, offering a practical solution to the growing need for accessible cybersecurity education and fostering awareness to mitigate the risks posed by malicious hackers.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1277-7},
  series = {{ACMSE},
}

@article{gienapp_viability_2025,
  title = {The {Viability},
  author = {Gienapp, L. and Hagen, T. and Fröbe, M. and Hagen, M. and {...},
  year = {2025},
  doi = {10.1145/3726302.3730093},
  url = {https://doi.org/10.1145/3726302.3730093},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th …},
  pages = {159--169},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, crowdsourcing, evaluation, retrieval-augmented generation},
  abstract = {… LLM. In total, we collect 47,320 pairwise human judgments and 10,556 pairwise LLM judgments across 7 RAG-… human and LLM-generated content, as well as the documents they cite. …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{akewar_can_2025,
  title = {Can {LLMs},
  author = {Akewar, Mayur and Quan, Gang and Madireddy, Sandeep and Bhimani, Janki},
  year = {2025},
  doi = {10.1145/3736548.3737835},
  url = {https://doi.org/10.1145/3736548.3737835},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {100--106},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boston, MA, USA},
  keywords = {Large Language Model, Prompt Engineering, Retrieval Augmented Generation, Solid State Drive},
  abstract = {Environmental stressors such as temperature, humidity, vibration, and radiation can severely impact the performance and reliability of SSDs, particularly in edge, automotive, aerospace, and datacenter deployments. Capturing sensor data in the field and conducting accelerated lab experiments are challenging, as they are time-consuming, resource-intensive, and often destructive to hardware. Specialized setups, such as thermal chambers or vibration rigs, are also required, which is why few studies explore this area, and current storage management techniques like RAID, tiering, and deduplication do not consider environmental factors. Models to capture these impacts would open new research opportunities across various fields. However, accurately modeling these effects remains challenging due to, (1) the limited availability of experimental data, (2) the complex, domino-like impact of historical exposure, (3) the interrelated nature of environmental factors, such as temperature and humidity, which exhibit correlation, (4) different response of each type of NAND flash memory TLC, MLC, and SLC to environmental factors, and (5) the difficulty that analytical and simple machine learning models face in generalizing across devices, environments, and unseen combinations of stressors. We believe that LLMs may offer a transformative alternative to this complex problem, with embedded domain knowledge and reasoning capabilities, to facilitate prompt-based natural language interaction. We propose a hybrid framework that combines Chain-of-Thought prompting and Retrieval-Augmented Generation to guide LLMs using physical principles and prior experiments. It enables interpretable "what-if" analysis of SSD behavior under environmental changes. Our results show that the LLM can effectively model the impact of temperature, humidity, and vibration on SSD performance, producing tail latency and bandwidth predictions with minimal error. The code and data are available on GitHub at https://github.com/Damrl-lab/SSD\_LLM.},
  address = {New York, NY, USA},
  series = {{HotStorage},
  isbn = {979-8-4007-1947-9},
}

@article{yu_rag-guided_2024,
  title = {Rag-guided large language models for visual spatial description with adaptive hallucination corrector},
  author = {Yu, J. and Zhang, Y. and Zhang, Z. and Yang, Z. and Zhao, G. and {...},
  year = {2024},
  doi = {10.1145/3664647.3688990},
  url = {https://doi.org/10.1145/3664647.3688990},
  booktitle = {Proceedings of the 32nd {ACM},
  journal = {Proceedings of the …},
  pages = {11407--11413},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, hallucination, retrieval-augmented generation, visual spatial description},
  abstract = {… statements, and an LLM is employed to correct hallucinations in the generated responses. … that combines retrieval-augmented generation and hallucination corrector with large …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{brachman_building_2025,
  title = {Building {Appropriate},
  author = {Brachman, Michelle and Kunde, Siya and Miller, Sarah and Fucs, Ana and Dempsey, Samantha and Jabbour, Jamie and Geyer, Werner},
  year = {2025},
  doi = {10.1145/3708359.3712071},
  url = {https://doi.org/10.1145/3708359.3712071},
  booktitle = {Proceedings of the 30th {International},
  pages = {247--264},
  publisher = {Association for Computing Machinery},
  keywords = {Agentic AI, Conversational UI, Explainable AI, Generative AI, Information Seeking, Mental Models, Reliance, Transparency},
  abstract = {Agentic systems aim to handle complex problems with increasing system autonomy using generative AI. These new agentic systems are becoming more feasible and easier to build. Yet we know little about what end-users need to know to use these systems appropriately. We study one such agentic system, “Gent,” which can break down complex problems into a set of actions, provide a rationale for each action, interact with external information, and cite its sources. Our goals were to understand users’ mental models of the agentic system, the information users leveraged to evaluate the accuracy of the system, and users’ information needs. In our study (N=24), participants interacted with Gent for four information seeking tasks where they could see Gent’s actions, rationale, and sources. Participants’ mental models centered around the search-like qualities of the system, with their confidence impacted by the website sources. Participants’ mental models often lacked insight into the workings of the generative AI model and agentic framework that impact the actions the system takes. Participants used the descriptions of the system’s actions to support their evaluation of the accuracy of the system and wanted to know more about how the system got to its answers. Participants also relied on their own personal knowledge and the style or length of Gent’s responses to evaluate the accuracy. Our results highlight the need for further transparency in agentic AI systems to support end-users in evaluating system outputs and help them build effective mental models.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{chen_towards_2025,
  title = {Towards {Mitigating},
  author = {Chen, Yujia and Chen, Mingyu and Gao, Cuiyun and Jiang, Zhihan and Li, Zhongqi and Ma, Yuchi},
  year = {2025},
  doi = {10.1145/3696630.3728569},
  url = {https://doi.org/10.1145/3696630.3728569},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {468--479},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {code generation, large language model, LLM hallucination, source: ACM, source: Scopus},
  abstract = {Application Programming Interfaces (APIs) are crucial in modern software development. Large Language Models (LLMs) assist in automated code generation but often struggle with API hallucination, including invoking non-existent APIs and misusing existing ones in practical development scenarios. Existing studies resort to Retrieval-Augmented Generation (RAG) methods for mitigating the hallucination issue, but tend to fail since they generally ignore the structural dependencies in practical projects and do not indeed validate whether the generated APIs are available or not. To address these limitations, we propose MARIN, a framework for mitigating API hallucination in code generated by LLMs with hierarchical dependency aware. MARIN consists of two phases: Hierarchical Dependency Mining, which analyzes local and global dependencies of the current function, aiming to supplement comprehensive project context in LLMs' input, and Dependency Constrained Decoding, which utilizes mined dependencies to adaptively constrain the generation process, aiming to ensure the generated APIs align with the project's specifications. To facilitate the evaluation of the degree of API hallucination, we introduce a new benchmark APIHul-Bench and two new metrics including Micro Hallucination Number (MiHN) and Macro Hallucination Rate (MaHR). Experiments on six state-of-the-art LLMs demonstrate that MARIN effectively reduces API hallucinations, achieving an average decrease of 67.52\% in MiHN and 73.56\% in MaHR compared to the RAG approach. Applied to Huawei's internal projects and two proprietary LLMs, MARIN achieves average decreases of 57.33\% in MiHN and 59.41\% in MaHR.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{qi_rag-optimized_2024,
  title = {Rag-optimized tibetan tourism llms: {Enhancing},
  author = {Qi, J. and Yan, S. and Zhang, Y. and Zhang, W. and Jin, R. and Hu, Y. and {...},
  year = {2024},
  doi = {10.1145/3703935.3704112},
  url = {https://doi.org/10.1145/3703935.3704112},
  booktitle = {Proceedings of the 2024 7th {International},
  journal = {Proceedings of the 2024 …},
  pages = {1185--1192},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Hallucination Problem, Large Language Models, Retrieval-Augmented Generation, Vector Databases},
  abstract = {… within the LLM itself. This research demonstrates the potential of RAG technology in en… Therefore, we believe that RAG technology greatly helps optimize the hallucination problem …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{AIPR},
  isbn = {979-8-4007-1717-8},
}

@inproceedings{feng_enabling_2024,
  title = {Enabling {Cost},
  author = {Feng, Sidong and Lu, Haochuan and Jiang, Jianqin and Xiong, Ting and Huang, Likun and Liang, Yinglin and Li, Xiaoqin and Deng, Yuetang and Aleti, Aldeida},
  year = {2024},
  doi = {10.1145/3691620.3695260},
  url = {https://doi.org/10.1145/3691620.3695260},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {1973--1978},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {cost optimization, large language model, retrieval-augmented generation, UI automation test},
  abstract = {UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce CAT to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, CAT employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. CAT then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the CAT's performance and cost-effectiveness, achieving 90\% UI automation with \$0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers' testing process.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{lee_talkin_2024,
  title = {Talkin' '{Bout},
  author = {Lee, Katherine and Cooper, A. Feder and Grimmelmann, James},
  year = {2024},
  doi = {10.1145/3614407.3643696},
  url = {https://doi.org/10.1145/3614407.3643696},
  booktitle = {Proceedings of the 2024 {Symposium},
  pages = {48--63},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boston, MA, USA},
  keywords = {source: ACM},
  abstract = {"Does generative AI infringe copyright?" is an urgent question. It is also a difficult question, for two reasons. First, "generative AI" is not just one product from one company. It is a catch-all name for a massive ecosystem of loosely related technologies. These systems behave differently and raise different legal issues. Second, copyright law is notoriously complicated, and generative-AI systems manage to touch on a great many corners of it. They raise issues of authorship, similarity, direct and indirect liability, and fair use, among much else. These issues cannot be analyzed in isolation, because there are connections everywhere. We aim to bring order to the chaos. To do so, we introduce the generative-AI supply chain: an interconnected set of stages that transform training data into generations. The supply chain reveals all of the places at which companies and users make choices that have copyright consequences. It enables us to trace the effects of upstream technical designs on downstream uses, and to assess who in these complicated sociotechnical systems bears responsibility for infringement when it happens. Because we engage so closely with the technology of generative AI, we are able to shed more light on the copyright questions. We identify the key decisions that courts will need to make as they grapple with these issues, and point out the consequences that would likely flow from different liability regimes. This article is a much-abbreviated version of a forthcoming law review article at The Journal of the Copyright Society.},
  address = {New York, NY, USA},
  series = {{CSLAW},
  isbn = {979-8-4007-0333-1},
}

@inproceedings{yamane_chimera-vdb_2025,
  title = {Chimera-{VDB},
  author = {Yamane, Naoshi and Zielewski, Michael Ryan and Nakamura, Takaki and Suganuma, Takuo},
  year = {2025},
  doi = {10.1145/3725783.3764411},
  url = {https://doi.org/10.1145/3725783.3764411},
  booktitle = {Proceedings of the 16th {ACM},
  pages = {61--67},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Emerald Hall, Seoul, Republic of Korea},
  keywords = {embedding vectors, hierarchical navigable small world, large language models, quantization, retrieval-augmented generation, storage usage, vector database},
  abstract = {In recent years, vector databases have become a core component in Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLM), enabling fast retrieval of documents similar to a given query. However, storing a large number of high-dimensional vectors requires substantial storage capacity. A common solution is to reduce vector precision through quantization, but this often degrades retrieval recall. To address this trade-off, we propose Chimera-VDB, which uses a mixture of high- and low-precision vectors to reduce data size while maintaining retrieval recall. Our approach leverages the graph structure of Hierarchical Navigable Small World (HNSW) networks, selectively preserving only the most search-critical vectors in high-precision, while quantizing the rest to lower precision. Experimental results show that our method reduces storage usage to 24\% compared to storing all vectors in FP32, while maintaining 93\% recall, demonstrating its effectiveness in balancing storage capacity and retrieval performance.},
  address = {New York, NY, USA},
  series = {{APSys},
  isbn = {979-8-4007-1572-3},
}

@inproceedings{zhang_towards_2025,
  title = {Towards {Comprehensive},
  author = {Zhang, Wutong and Zhou, Hefeng and Zhou, Qiang and Li, Yunshen and Liu, Yuxin and Lou, Jiong and Wu, Chentao and Li, Jie},
  year = {2025},
  doi = {10.1145/3731715.3733451},
  url = {https://doi.org/10.1145/3731715.3733451},
  booktitle = {Proceedings of the 2025 {International},
  pages = {1840--1848},
  publisher = {Association for Computing Machinery},
  note = {event-place: Chicago, IL, USA},
  keywords = {contract review, document retrieval, large language model, multi-round retrieval, retrieval-augmented generation, source: Scopus},
  abstract = {Legal document review is a time-consuming and highly specialized task, and the capabilities of intelligent legal review systems are limited and insufficient to complete detailed reviews. Traditional methods struggle with cross-references, dependencies, and context-dependent clauses. Our work introduces a multi-round RAG framework for legal document analysis, which iteratively refines queries and aggregates context to improve recall and understanding. Experiments on diverse contracts show a recall of 78.67\%, outperforming baseline (57.33\%) and single-round RAG (74.67\%). Our analysis shows that iterative refinement effectively filters irrelevant results despite reduced precision. The multi-round approach halves missed cross-clause dependencies but reveals limitations in numerical consistency and obligation scope detection. These insights advance RAG for legal applications and provide a foundation for future work on scalable and accurate contract review.},
  address = {New York, NY, USA},
  series = {{ICMR},
  isbn = {979-8-4007-1877-9},
  annote = {Cited by: 0},
}

@inproceedings{vassos_now_2024,
  title = {Now {I},
  author = {Vassos, Stavros and Goudelis, Stratos and Balaouras, Dimi and Vitalis, Giannis and Nakos, Vasilis and Pigka, Glykeria and Tsagkli, Loukia and Hatzikou, Menia and Tsionas, Zachos and Chasanis, Alexandros and van de Burgt, Stan and Pors, Mark and Papadoudis, Stratos and Loukas, Lefteris},
  year = {2024},
  doi = {10.1145/3688671.3688784},
  url = {https://doi.org/10.1145/3688671.3688784},
  booktitle = {Proceedings of the 13th {Hellenic},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {Deployment, LLMs, NLP, Open-Access Software, Open-Book QA, OpenAI, RAG+C, Retrieval-Augmented Generation, source: Scopus},
  abstract = {Accurate political information is vital for voters to make informed decisions. However, due to the plethora of data and biased sources, accessing concise, factual information still remains a challenge. To tackle this problem, we present an open-access, deployed digital assistant powered by Large Language Models (LLMs), specifically tailored to answer voters’ questions and help them vote for the political party they mostly align with. The user can select up to 3 parties, input their question, and get short, summarized answers from the parties’ published political agendas, which contain hundreds of pages and, thus, are difficult to navigate for the typical citizen. Our NLP system architecture leverages OpenAI’s GPT-4 and incorporates Retrieval-Augmented Generation with Citations (RAG+C) to integrate custom data into LLMs effectively and build user trust. We also describe our database design, underlining the use of an open-source vector database, optimized for high-dimensional semantic search across multiple documents, and a semantic-rich LLM cache, reducing operational expenses and end-user latency time. Our open-access system supports Greek and English and has been deployed live at https://toraksero.gr/for the Greek 2023 Elections, which gathered 30K user sessions and 74\% user satisfaction.},
  address = {New York, NY, USA},
  series = {{SETN},
  isbn = {979-8-4007-0982-1},
  annote = {Cited by: 0},
}

@inproceedings{su_judge_2025,
  title = {{JuDGE},
  author = {Su, Weihang and Yue, Baoqing and Ai, Qingyao and Hu, Yiran and Li, Jiaqi and Wang, Changyue and Zhang, Kaiyuan and Wu, Yueyue and Liu, Yiqun},
  year = {2025},
  doi = {10.1145/3726302.3730295},
  url = {https://doi.org/10.1145/3726302.3730295},
  booktitle = {Proceedings of the 48th {International},
  pages = {3573--3583},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {domain-specific evaluation, judgment document generation, large language model, retrieval augmented generation},
  abstract = {This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: https://github.com/oneal2000/JuDGE},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{signe_substring_2025,
  title = {A {Substring},
  author = {Signé, Quentin and Boughanem, Mohand and Moreno, Jose G. and Belkacem, Thiziri},
  year = {2025},
  doi = {10.1145/3731120.3744624},
  url = {https://doi.org/10.1145/3731120.3744624},
  booktitle = {Proceedings of the 2025 {International},
  pages = {513--521},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {question answering, retrieval augmented generation, technical maintenance},
  abstract = {Hallucination occurs when a language model generates plausible yet nonfactual information. In particular, faithfulness hallucinations (inconsistency with a given context) cannot be tolerated in critical domains such as aircraft maintenance due to the potentially severe consequences. To mitigate this issue, Retrieval Augmented Generation (RAG) methods have been introduced. These approaches are relevant for reducing the risks of hallucination but do not eliminate them, as the generator may still produce content unfaithful to the retrieved context. This paper proposes a novel RAG approach that leverages a substring extraction tool from retrieved documents to minimise hallucinations. Experiments performed on real aircraft maintenance documentation revealed that, despite the lower accuracy of the answers compared to traditional RAG methods, the proposed approach demonstrates an improved control over hallucination risks. This highlights the potential of our method in highly technical use cases where accuracy and reliability are key.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{huang_ket-rag_2025,
  title = {{KET},
  author = {Huang, Yiqian and Zhang, Shiqi and Xiao, Xiaokui},
  year = {2025},
  doi = {10.1145/3711896.3737012},
  url = {https://doi.org/10.1145/3711896.3737012},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {1003--1012},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {graphrag, indexing, retrieval-augmented generation, source: ACM},
  abstract = {Graph-RAG constructs a knowledge graph from text chunks to improve retrieval in Large Language Model (LLM)-based question answering. It is particularly useful in domains such as biomedicine, law, and political science, where retrieval often requires multi-hop reasoning over proprietary documents. Some existing Graph-RAG systems construct KNN graphs based on text chunk relevance, but this coarse-grained approach fails to capture entity relationships within texts, leading to sub-par retrieval and generation quality. To address this, recent solutions leverage LLMs to extract entities and relationships from text chunks, constructing triplet-based knowledge graphs. However, this approach incurs significant indexing costs, especially for large document collections. To ensure a good result accuracy while reducing the indexing cost, we propose KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small set of key text chunks and leverages an LLM to construct a knowledge graph skeleton. It then builds a text-keyword bipartite graph from all text chunks, serving as a lightweight alternative to a full knowledge graph. During retrieval, KET-RAG searches both structures: it follows the local search strategy of existing Graph-RAG systems on the skeleton while mimicking this search on the bipartite graph to improve retrieval quality. We evaluate 13 solutions on three real-world datasets, demonstrating that KET-RAG outperforms all competitors in indexing cost, retrieval effectiveness, and generation quality. Notably, it achieves comparable or superior retrieval quality to Microsoft's Graph-RAG while reducing indexing costs by over an order of magnitude. Additionally, it improves the generation quality by up to 32.4\% while lowering indexing costs by around 20\%.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{ngo_legal_2025,
  title = {Legal {Documents},
  author = {Ngo, Anh Tuan and Doan, Trung Tung and Ngo, Tung Thanh and Nguyen, Viet Hoang},
  year = {2025},
  doi = {10.1145/3731763.3731802},
  url = {https://doi.org/10.1145/3731763.3731802},
  booktitle = {Proceedings of the 2025 10th {International},
  pages = {152--157},
  publisher = {Association for Computing Machinery},
  keywords = {Large Language Models, Legal AI, Legal Document Retrieval, Natural Language Processing, Retrieval-Augmented Generation, Semantic Search, Vietnamese Law},
  abstract = {This paper introduces the development of a Legal Documents Query Application for Vietnamese law, leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques. It outlines the methodologies employed, addresses key challenges, and highlights the significance of the application in improving the accuracy and efficiency of legal document retrieval in specialized domains. By combining cutting-edge AI techniques with domain-specific optimizations, the proposed solution aims to enhance access to legal information for a wide range of users.},
  address = {New York, NY, USA},
  series = {{ICIIT},
  isbn = {979-8-4007-1084-1},
}

@inproceedings{arefeen_irag_2024,
  title = {{iRAG},
  author = {Arefeen, Md Adnan and Debnath, Biplob and Uddin, Md Yusuf Sarwar and Chakradhar, Srimat},
  year = {2024},
  doi = {10.1145/3627673.3680088},
  url = {https://doi.org/10.1145/3627673.3680088},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4341--4348},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {generative ai, large language models (llms), retrieval augmented generation (rag), video analytics, vision language models (vlms)},
  abstract = {Retrieval-augmented generation (RAG) systems combine the strengths of language generation and information retrieval to power many real-world applications like chatbots. Use of RAG for understanding of videos is appealing but there are two critical limitations. One-time, upfront conversion of all content in large corpus of videos into text descriptions entails high processing times. Also, not all information in the rich video data is typically captured in the text descriptions. Since user queries are not known apriori, developing a system for video to text conversion and interactive querying of video data is challenging. To address these limitations, we propose an incremental RAG system called iRAG, which augments RAG with a novel incremental workflow to enable interactive querying of a large corpus of videos. Unlike traditional RAG, iRAG quickly indexes large repositories of videos, and in the incremental workflow, it uses the index to opportunistically extract more details from select portions of the videos to retrieve context relevant to an interactive user query. Such an incremental workflow avoids long video to text conversion times, and overcomes information loss issues due to conversion of video to text, by doing on-demand query-specific extraction of details in video data. This ensures high quality of responses to interactive user queries that are often not known apriori. To the best of our knowledge, iRAG is the first system to augment RAG with an incremental workflow to support efficient interactive querying of a large corpus of videos. Experimental results on real-world datasets demonstrate 23x to 25x faster video to text ingestion, while ensuring that latency and quality of responses to interactive user queries is comparable to responses from a traditional RAG where all video data is converted to text upfront before any user querying.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{yang_boosting_2025,
  title = {Boosting {E},
  author = {Yang, Jiaxi and Jia, Yiling and Yang, Carl and Liang, Yi and Lin, Lu},
  year = {2025},
  doi = {10.1145/3711896.3736864},
  url = {https://doi.org/10.1145/3711896.3736864},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {3495--3506},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {copywriting generation, e-commerce, graph traversal, large language models, retrieval-augmented generation},
  abstract = {In e-commerce, product descriptions and other forms of copywriting play a critical role in shaping consumer purchasing decisions. However, manually crafting such content is both time-consuming and costly, particularly given the vast and diverse item catalogs. Recent advances in large language models (LLMs) have transformed automated text generation, offering immense potential to streamline this process. Despite their capabilities, LLMs continue to face obstacles in e-commerce applications, including a lack of diversity and an inability to fully grasp the nuanced details of specific items. To address these limitations, we propose a novel framework that integrates graph-based knowledge into Retrieval-Augmented Generation (RAG) to enhance content generation. Our approach leverages user reviews to construct an item-feature graph, capturing both explicit and implicit connections between items and features. This structured representation enables the retrieval of diverse, contextually relevant, and factually grounded information, effectively addressing key deficiencies of existing methods. With the constructed graph, we design a graph traversal mechanism that explores a broader range of item-related features, augmenting the generation process with more varied and informative inputs. Extensive experiments demonstrate that our method significantly improves diversity while preserving fidelity, marking a major advancement in automated e-commerce content generation.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{tran_rag_2025,
  title = {A {RAG},
  author = {Tran, Quang-Linh and Pham, Ngo Ngoc Diep and Truong, Quoc Trung and Nguyen, Minh Hung and Le, Hong Cat and Vu, Dang Khoi and Nguyen, Van Minh Thien and Nguyen, Van Kinh and Nguyen, Luu Phuong Ngoc Lam and Le, Tan and Dang, Minh Phuc and Nguyen, Binh and Jones, Gareth J. F. and Gurrin, Cathal},
  year = {2025},
  doi = {10.1145/3731715.3733263},
  url = {https://doi.org/10.1145/3731715.3733263},
  booktitle = {Proceedings of the 2025 {International},
  pages = {1303--1312},
  publisher = {Association for Computing Machinery},
  note = {event-place: Chicago, IL, USA},
  keywords = {large language models, lifelog question answering, multi-modal question answering dataset, retrieval-augmented generation},
  abstract = {Lifelogging is the passive collection, storage and analysis of daily data through wearable sensors. Question Answering (QA) for lifelog data enables natural language interactions with personal daily life records, providing insights into individual routines and behaviours. While this task has great potential for personal analytics and memory augmentation, progress has been limited due to the challenges of lifelog management, since they can comprise of enormous multi-modal data sets spanning a lifetime. We introduce a Retrieval-Augmented Generation (RAG) approach for addressing the lifelog QA task. A RAG approach first includes a retrieval model finding the correct lifelog events containing answers and then a large language model (LLM) generating answers from the questions. In addition, we construct an open-ended lifelog QA benchmark with 14,187 QA pairs to examine the RAG approach to lifelog QA. Using an embedding-based retrieval approach, our lifelog context retriever achieves a performance of 77.67\% Recall@5 and 94.35\% Recall@20 using an embedding-based retrieval approach with the Stella 1.5B model. Combined with the Mistral 7B model, the model achieves scores of 39.54\% ROUGE-L and 3.475 Accuracy on a scale of 5 scored by GPT-4o. This approach potentially provides an effective approach to lifelog QA with high performance that does not require fine-tuning.},
  address = {New York, NY, USA},
  series = {{ICMR},
  isbn = {979-8-4007-1877-9},
}

@inproceedings{chen_question_2025,
  title = {A {Question},
  author = {Chen, Fei and Wen, Zhonghua and Liu, Bo},
  year = {2025},
  doi = {10.1145/3756580.3756654},
  url = {https://doi.org/10.1145/3756580.3756654},
  booktitle = {Proceedings of the 2025 6th {International},
  pages = {446--455},
  publisher = {Association for Computing Machinery},
  keywords = {intelligent question answering, knowledge graph, large language model (LLM), retrieval augmented generation (RAG), the aerospace science and technology},
  abstract = {At present, the data in the aerospace science and technology is massive, heterogeneous, distributed and discrete, and lacks unified management, which seriously affects the quality and efficiency of retrieval. To solve this problem, a construction method of question answering system for aerospace large language models based on knowledge graph and retrieval enhanced generation (RAG) collaboration is proposed. Firstly, the aerospace science and technology information representation model is constructed, and then the aerospace science and technology information resource database is constructed based on the model. On this basis, the large language model in the aerospace field is trained by using the efficient fine-tuning technology; Create knowledge graph and vector database in the neo4j, and use entity attributes and vector indexes to integrate them; Finally, the langchain framework is used to link the knowledge graph, RAG and the large language model of the aerospace field to develop and construct the aerospace intelligent question answering system. The experimental results show that the system covers a wide range of knowledge in the aerospace science and technology field, and can effectively improve the accuracy of the answer and reduce the hallucination rate.},
  address = {New York, NY, USA},
  series = {{ICEKIM},
  isbn = {979-8-4007-1562-4},
}

@inproceedings{xu_muarf_2025,
  title = {{MUARF},
  author = {Xu, Yisen},
  year = {2025},
  doi = {10.1109/ICSE-Companion66252.2025.00071},
  url = {https://doi.org/10.1109/icse-companion66252.2025.00071},
  booktitle = {Proceedings of the {IEEE},
  pages = {226--227},
  publisher = {IEEE Press},
  keywords = {code refactoring, contextual retrieval-augmented generation, large language model, multi-agent communication, prompt engineering},
  abstract = {Refactoring is crucial for maintaining a project, but it requires developers to understand code structure and system design principles well. Recent research on Large Language Models(LLMs) has shown their great capability for handling complex tasks, making them a possible solution for overcoming these challenges. In this paper, we propose MUARF, an LLM-based solution designed to automate method-level code refactoring, aiming to generate correct, high-quality, and human-like refactored code. MUARF leverages Contextual Retrieval-Augmented Generation to search for similar refactoring samples for few-shot learning, uses Multi-Agent Workflow to simulate the human refactoring process, and integrates advanced software engineering tools (e.g., RefactoringMiner, PurityChecker, StyleChecker) to assist refactoring. Evaluation results show that MUARF achieves a compilation pass rate of 86.5\% and a test success rate of 83.8\% for the refactored code it generates. Additionally, metrics such as CodeBLEU score and AST Diff accuracy—which compare human-refactored code with the output of MUARF —highlight the generated code is human-like. The ablation results show that RefactoringMiner and Agentware made the greatest contribution to MUARF.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-3683-1},
}

@inproceedings{jiang_retrieve_2025,
  title = {Retrieve, {Summarize},
  author = {Jiang, Zhouyu and Sun, Mengshu and Liang, Lei and Zhang, Zhiqiang},
  year = {2025},
  doi = {10.1145/3701716.3716889},
  url = {https://doi.org/10.1145/3701716.3716889},
  booktitle = {Companion {Proceedings},
  pages = {1677--1686},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {llms, question answering, retrieval-augmented generation},
  abstract = {Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{song_assessing_2025,
  title = {Assessing and {Post},
  author = {Song, Xiaoshuai and Wang, Zhengyang and He, Keqing and Dong, Guanting and Mou, Yutao and Zhao, Jinxu and Xu, Weiran},
  year = {2025},
  doi = {10.1145/3696410.3714732},
  url = {https://doi.org/10.1145/3696410.3714732},
  booktitle = {Proceedings of the {ACM},
  pages = {1716--1732},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge editing, large language model, retrieval-augmented generation, source: ACM},
  abstract = {The rapid evolution of the Web as a key platform for information dissemination has led to the growing integration of large language models (LLMs) in Web-based applications. However, the swift changes in web content present challenges in maintaining these models' relevance and accuracy. The task of Knowledge Editing (KE) is aimed at efficiently and precisely adjusting the behavior of large language models (LLMs) to update specific knowledge while minimizing any adverse effects on other knowledge. Current research predominantly concentrates on editing white-box LLMs, neglecting a significant scenario: editing black-box LLMs, where access is limited to interfaces and only textual output is provided. In this paper, we initially officially introduce KE on black-box LLMs, followed by presenting a thorough evaluation framework. This framework operates without requiring logits and considers pre- and post-edit consistency, addressing the limitations of current evaluations that are inadequate for black-box LLMs editing and lack comprehensiveness. To address privacy leaks of editing data and style over-editing in existing approaches, we propose a new postEdit framework. postEdit incorporates a retrieval mechanism for editing knowledge and a purpose-trained editing plugin called post-editor, ensuring privacy through downstream processing and maintaining textual style consistency via fine-grained editing. Experiments and analysis conducted on two benchmarks show that postEdit surpasses all baselines and exhibits robust generalization, notably enhancing style retention by an average of +20.82\%. Our code is available on github https://github.com/songxiaoshuai/postEdit.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{jia_structrag_2025,
  title = {{StructRAG},
  author = {Jia, Runsong and Zhang, Bowen and Méndez, Sergio José Rodríguez and Omran, Pouya G.},
  year = {2025},
  doi = {10.1145/3701716.3717819},
  url = {https://doi.org/10.1145/3701716.3717819},
  booktitle = {Companion {Proceedings},
  pages = {2567--2573},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {deep document model, knowledge graph, knowledge graph construction, large language models, retrieval-augmented generation},
  abstract = {Recent advances in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) have shown promise in academic question answering. However, existing approaches often fail to fully utilize document structural information and lack diversity in retrieved contexts. This paper presents StructRAG, a structure-aware RAG framework that leverages scholarly knowledge graphs for enhanced question answering. Our framework features three key innovations: (1) an automated knowledge graph construction pipeline based on Deep Document Model (DDM) that preserves document hierarchical structure, (2) a structure-aware retrieval mechanism that combines semantic relevance with source diversity, and (3) a context-enhanced generation approach that integrates structural metadata for improved answer synthesis. Experimental results on 329 computer science papers demonstrate that StructRAG significantly outperforms vanilla RAG baseline. While maintaining comparable semantic accuracy (91\% vs 90\%), our approach achieves substantially higher diversity in generated answers (Distinct-1: 62\% vs 52\%, Distinct-2: 89\% vs 78\%) and better answer quality across all metrics, with notable improvements in relevance (29\%) and readability (36.5\%). These results demonstrate that StructRAG effectively enhances both the diversity and quality of academic question answering.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@article{shen_pentestagent_2025,
  title = {Pentestagent: {Incorporating},
  author = {Shen, X. and Wang, L. and Li, Z. and Chen, Y. and Zhao, W. and Sun, D. and {...},
  year = {2025},
  doi = {10.1145/3708821.3733882},
  url = {https://doi.org/10.1145/3708821.3733882},
  booktitle = {Proceedings of the 20th {ACM},
  journal = {Proceedings of the 20th …},
  pages = {375--391},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Agent, Large Language Model, Penetration Testing},
  abstract = {… PentestAgent, a novel LLM-based automated penetration … LLM-based techniques like retrieval augmented generation to … LLM Hallucination: Another challenge is LLM hallucination, …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ASIA},
  isbn = {979-8-4007-1410-8},
}

@inproceedings{szabo_applied_2025,
  title = {Applied {Domain},
  author = {Szabó, Barbara Noémi and Gyöngyössy, Natabara Máté and Dalos, Attila},
  year = {2025},
  doi = {10.1145/3759355.3759360},
  url = {https://doi.org/10.1145/3759355.3759360},
  booktitle = {Proceedings of the {Intelligent},
  pages = {29--37},
  publisher = {Association for Computing Machinery},
  keywords = {contrastive learning, embedding model, engineering knowledge management, fine-tuning, retrieval-augmented generation},
  abstract = {Technical documentation related to engineering and manufacturing is often stored in unstructured documents, rendering search and information retrieval processes time-consuming and inefficient. Recently, Retrieval-Augmented Generation (RAG) pipelines and AI agents leveraging dense retrieval capabilities of document embedding language models have emerged as promising solutions to address these challenges. However, these methods typically lack semantic understanding of industry-specific terminology, limiting their retrieval accuracy in specialized domains. In this paper, we identify the fine-tuning of dense retrieval models within RAG systems as an effective and cost-efficient approach for enhancing their industry- and enterprise-specific knowledge. We further investigate whether pretraining based on Masked Language Modeling is necessary for these scenarios. Through empirical evaluation, we show that automatic data preprocessing and synthetic data generation derived from human-labeled retrieval examples provide a cost-effective strategy for preparing fine-tuning data. By training and evaluating an embedding model on 21 281 engineering documents encompassing best practices and lessons learned, totaling 75 million whitespace delimited words, we achieve a significant increase of (23\%) in Recall@10 for domain-specific retrieval performance, while retaining generalization capabilities on general retrieval tasks as measured by the Massive Text Embedding Benchmark (MTEB). Additionally, we present ablation studies on the effects of loss function symmetry, masking ratio in MLM, and sequence packing strategies. Our results demonstrate that domain-specific fine-tuned embeddings consistently outperform both open-source and state-of-the-art proprietary general retrieval models, establishing this fine-tuning strategy as a viable solution even in computationally constrained, single-GPU training environments.},
  address = {New York, NY, USA},
  series = {{IntRob},
  isbn = {979-8-4007-1589-1},
}

@article{nian_w-rag_2025,
  title = {W-rag: {Weakly},
  author = {Nian, J. and Peng, Z. and Wang, Q. and Fang, Y.},
  year = {2025},
  doi = {10.1145/3731120.3744578},
  url = {https://doi.org/10.1145/3731120.3744578},
  booktitle = {Proceedings of the 2025 {International},
  journal = {… of the 2025 International ACM SIGIR …},
  pages = {136--146},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, dense retrieval, open-domain question answering, retrieval augmented generation, weak supervised learning},
  abstract = {… -RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, … This consistency gives us confidence that the W-RAG method generalizes well …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{huang_rd-p_2024,
  title = {{RD},
  author = {Huang, Yubo and Zeng, Guosun},
  year = {2024},
  doi = {10.1145/3627673.3679659},
  url = {https://doi.org/10.1145/3627673.3679659},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {942--952},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {kgqa, large language models, prompter, retrieval-augmented generation, source: Scopus},
  abstract = {Large Language Models (LLMs) face challenges due to hallucination issues. Current solutions use retrieval-augmented generation (RAG), integrating LLMs with external knowledge to enhance answer accuracy. However, the misuse of irrelevant external knowledge can be misleading. In this paper, we propose a novel method called Retrieve-and-Discriminate Prompter (RD-P), which leverages knowledge graphs (KGs) for trustworthy RAG by synchronizing knowledge retrieval and discrimination in a unified model. Specifically, we train a prompter based on a pre-trained language model with shared parameters. It has two key modules: the retriever and the discriminator. The retriever identifies relevant reasoning paths in the KG, while the discriminator evaluates their credibility through "logical coverage calculation" and in turn instructs the retrieval process. Prompts are then constructed to guide LLMs in reasoning and answering questions using both retrieved and implicit knowledge. Experiments on knowledge-intensive question answering (QA) tasks demonstrate that our method significantly improves answer coverage rate while reducing the retrieval scale, achieving superior performance in complex KGQA tasks compared with state-of-the-art RAG methods at a low cost.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
  annote = {Cited by: 3},
}

@inproceedings{salemi_comparing_2025,
  title = {Comparing {Retrieval},
  author = {Salemi, Alireza and Zamani, Hamed},
  year = {2025},
  doi = {10.1145/3731120.3744595},
  url = {https://doi.org/10.1145/3731120.3744595},
  booktitle = {Proceedings of the 2025 {International},
  pages = {286--296},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {parameter-efficient fine-tuning, personalization, retrieval-augmented generation, text classification, text generation, source: ACM},
  abstract = {Despite its substantial impact on various search, recommendation, and question answering tasks, privacy-preserving methods for personalizing large language models (LLMs) have received relatively limited exploration. There is one primary approach in this area through retrieval-augmented generation (RAG), which generates personalized outputs by enriching the input prompt with information retrieved from the user's personal data. This paper studies an orthogonal approach to RAG that involves learning user-dependent LLM parameters through parameter-efficient fine-tuning (PEFT). This paper presents the first systematic study for exploration of PEFT for LLM personalization and provides an extensive comparisons between RAG- and PEFT-based solutions, across a broad set of seven diverse datasets from the LaMP benchmark. Our results demonstrate that, on average, both RAG- and PEFT-based personalization methods yield 14.92\% and 1.07\% improvements over non-personalized LLMs, respectively. When combining RAG with PEFT, we observe a further improvement of 15.98\%, highlighting the effectiveness of their integration in enhancing personalized text generation. Additionally, we identify a positive correlation between the amount of user data available and the effectiveness of PEFT. This finding suggests that RAG is particularly beneficial for cold-start users–users with limited personal data–while PEFT performs better when more user-specific data is available.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{bi_rbdq_2025,
  title = {{RBDQ},
  author = {Bi, Fenglin and Cao, Dongdong and Wang, Zhiyu and Chen, Yang and Zhao, Fangliang and Hu, Tao and Li, Zhi and Zhang, Yanbin and Wang, Wei},
  year = {2025},
  doi = {10.1145/3701716.3715257},
  url = {https://doi.org/10.1145/3701716.3715257},
  booktitle = {Companion {Proceedings},
  pages = {95--103},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {business data queries, large language models, retrieval-augmented generation, text-to-sql, source: ACM, source: Scopus},
  abstract = {Using large language models (LLMs) to convert natural language (NL) into SQL simplifies data access for users by allowing them to use everyday language. However, business departments often distrust LLM-based text-to-SQL systems due to the probabilistic nature of SQL generation, which can result in incorrect but executable SQL queries caused by model hallucinations. This leads to significant concerns regarding the accuracy and reliability of the queried data. In this paper, we present RBDQ, a novel LLM-based text-to-SQL system designed to address the unique challenges of business data queries. RBDQ innovatively introduces the Hierarchical Metrics Query Method and integrates advanced Retrieval-Augmented Generation (RAG) methods along with a self-reflection mechanism to tackle these challenges. RBDQ effectively meets the requirements of business metric queries in real-world scenarios. Currently implemented in the Quality Assurance department at ByteDance, RBDQ has significantly improved operational efficiency and query flexibility. Our experiments demonstrate the system's effectiveness, achieving an Execution Accuracy of 96.20\%.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{wang_topology-aware_2024,
  title = {Topology-aware {Retrieval},
  author = {Wang, Yu and Lipka, Nedim and Zhang, Ruiyi and Siu, Alexa and Zhao, Yuying and Ni, Bo and Wang, Xin and Rossi, Ryan and Derr, Tyler},
  year = {2024},
  doi = {10.1145/3627673.3679746},
  url = {https://doi.org/10.1145/3627673.3679746},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {2442--2452},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {graph structural relations, retrieval-augmented generation, source: Scopus},
  abstract = {Retrieval-augmented Generation has been used to augment Language Models by retrieving texts from external databases. Since real-world texts are often connected in the graph (e.g., papers in citation networks), we use these relations to guide the retrieval process of RAG. Concretely, we investigate proximity and role-based relations, where the former considers topologically close nodes and the latter considers structurally similar nodes. We empirically verify their correlation to text relations, which motivates us to propose the framework of Topology-aware Retrieval-augmented Generation for text generation, which consists of a retrieval module to retrieve texts by their topological relations and an aggregation module to compose retrieved texts into prompts triggering LLMs for text generation. Extensive experiments verify the effectiveness of this framework, signifying the potential of equipping RAG with topological awareness.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
  annote = {Cited by: 1},
}

@inproceedings{chen_generating_2025,
  title = {Generating {Spatially},
  author = {Chen, Bin and Nakamura, Yugo and Fukushima, Shogo and Arakawa, Yutaka},
  year = {2025},
  doi = {10.1145/3732437.3732768},
  url = {https://doi.org/10.1145/3732437.3732768},
  booktitle = {Proceedings of the 2024 {International},
  pages = {74--80},
  publisher = {Association for Computing Machinery},
  keywords = {3D reconstruction, Dense video caption, Human action recognition, Large language model, Retrieval-augmented generation},
  abstract = {Current dense video captioning methods do not effectively capture spatial information about the individual and the surrounding objects in the scene. This limitation can lead to ambiguous captions, making it difficult to detect abnormalities in human activities in the human-centric applications, such as security surveillance and remote indoor caregiving, where detecting abnormalities in human activities is crucial and these activities are closely intertwined with the spatial information of the objects in the scene.We propose a system that enhances dense video captioning of indoor human behavior by integrating two key sources of information: spatial context extracted by Retrieval-Augmented Generation (RAG) from a knowledge base that contains the positions and categories of all objects in the room, and spatial information from RGB-D pairs that includes the positions and categories of people and objects within the Field of View (FOV). By combining these elements, our approach reduces ambiguity and improves the accuracy of behavior descriptions.Our proposed baseline method is evaluated on the custom dataset, achieving scores of 15.7 for METEOR, 20.3 for ROUGE-L, 52.3 for Recall and 52.4 for Precision. These results demonstrates that our system effectively addresses the limitations of PDVC and GVL in collecting spatial information and provides improvements over SG-PDVC and SI-PDVC in this area. We propose a convenient and effective method for generating spatial information-enhanced captions to describe human behavior in untrimmed videos.},
  address = {New York, NY, USA},
  series = {{ICEA},
  isbn = {979-8-4007-1166-4},
}

@inproceedings{yu_mramg-bench_2025,
  title = {{MRAMG},
  author = {Yu, Qinhan and Xiao, Zhiyou and Li, Binghui and Wang, Zhengren and Chen, Chong and Zhang, Wentao},
  year = {2025},
  doi = {10.1145/3726302.3730288},
  url = {https://doi.org/10.1145/3726302.3730288},
  booktitle = {Proceedings of the 48th {International},
  pages = {3616--3626},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {evaluation, large language model, multimodal large language model, multimodal retrieval-augmented multimodal generation},
  abstract = {Recent advances in Retrieval-Augmented Generation (RAG) have significantly improved response accuracy and relevance by incorporating external knowledge into Large Language Models (LLMs). However, existing RAG methods primarily focus on generating text-only answers, even in Multimodal Retrieval-Augmented Generation (MRAG) scenarios, where multimodal elements are retrieved to assist in generating text answers. To address this, we introduce the Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to generate multimodal answers that combine both text and images, fully leveraging the multimodal data within a corpus. Despite growing attention to this challenging task, a notable lack of a comprehensive benchmark persists for effectively evaluating its performance. To bridge this gap, we provide MRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346 documents, 14,190 images, and 4,800 QA pairs, distributed across six distinct datasets and spanning three domains: Web, Academia, and Lifestyle. The datasets incorporate diverse difficulty levels and complex multi-image scenarios, providing a robust foundation for evaluating the MRAMG task. To facilitate rigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both statistical and LLM-based metrics, enabling a thorough analysis of the performance of generative models in the MRAMG task. Additionally, we propose an efficient and flexible multimodal answer generation framework that can leverage LLMs/MLLMs to generate multimodal responses. Our datasets and complete evaluation results for 11 popular generative models are available at https://github.com/MRAMG-Bench/MRAMG.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{li_emails_2025,
  title = {Emails by {LLMs},
  author = {Li, Weijiang and Lai, Yinmeng and Soni, Sandeep and Saha, Koustuv},
  year = {2025},
  doi = {10.1145/3717867.3717872},
  url = {https://doi.org/10.1145/3717867.3717872},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {391--403},
  publisher = {Association for Computing Machinery},
  keywords = {email, generative AI, language, linguistic analysis, LLMs},
  abstract = {The growing excitement around generative AI (and LLMs) is fueling a heightened interest in the development of AI-assisted writing tools. One popular context is AI-assisted email writing, and this paper explores how AI-generated emails compare to human-written emails. We obtained human-written emails from the W3C corpus and generated analogous AI-generated emails using GPT-3.5, GPT-4, Llama-2, and Mistral-7B, and compared AI-generated and human-written emails using a suite of natural language analyses across syntactic, semantic, and psycholinguistic dimensions. AI-generated emails are generally consistent across different LLMs but differ significantly from human-written emails. Specifically, AI-generated emails tend to be more formal, verbose, and complex, whereas human-written emails are often more concise and personalized. While AI-generated emails are slightly more polite, both types exhibit a similar level of empathetic tone in language. Further, we qualitatively examined user perceptions of AI and human-written emails by conducting a small survey of 41 participants and interviewing a subset of them. This study highlights preliminary insights into generative AI’s distinct strengths and weaknesses in assisting email communication, and we discuss the theoretical and practical implications of the evolving landscape of AI-generated content.},
  address = {New York, NY, USA},
  series = {Websci '25},
  isbn = {979-8-4007-1483-2},
}

@inproceedings{zhou_metacognitive_2024,
  title = {Metacognitive {Retrieval},
  author = {Zhou, Yujia and Liu, Zheng and Jin, Jiajie and Nie, Jian-Yun and Dou, Zhicheng},
  year = {2024},
  doi = {10.1145/3589334.3645481},
  url = {https://doi.org/10.1145/3589334.3645481},
  booktitle = {Proceedings of the {ACM},
  pages = {1453--1463},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {llms, metacognition, retrieval-augmented generation},
  abstract = {Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{salemi_learning_2025,
  title = {Learning to {Rank},
  author = {Salemi, Alireza and Zamani, Hamed},
  year = {2025},
  doi = {10.1145/3731120.3744584},
  url = {https://doi.org/10.1145/3731120.3744584},
  booktitle = {Proceedings of the 2025 {International},
  pages = {183--193},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {ranking optimization, retrieval-augmented generation, retrieval-enhanced machine learning, search engine for agents, source: ACM},
  abstract = {This paper investigates the design of a unified search engine to serve multiple retrieval-augmented generation (RAG) agents, each with a distinct task, backbone large language model (LLM), and RAG strategy. We introduce an iterative approach where the search engine generates retrieval results for the RAG agents and gathers feedback on the quality of the retrieved documents during an offline phase. This feedback is then used to iteratively optimize the search engine using an expectation-maximization algorithm, with the goal of maximizing each agent's utility function. Additionally, we adapt this to an online setting, allowing the search engine to refine its behavior based on real-time individual agents feedback to better serve the results for each of them. Experiments on datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our approach significantly on average outperforms baselines across 18 RAG models. We demonstrate that our method effectively ”personalizes” the retrieval for each RAG agent based on the collected feedback. Finally, we provide a comprehensive ablation study to explore various aspects of our method.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@article{peng_eloq_2025,
  title = {Eloq: {Resources},
  author = {Peng, Z. and Nian, J. and Evfimievski, A. and Fang, Y.},
  year = {2025},
  doi = {10.1145/3726302.3730333},
  url = {https://doi.org/10.1145/3726302.3730333},
  booktitle = {Proceedings of the 48th {International},
  journal = {Proceedings of the 48th …},
  pages = {3509--3519},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, large language models, out-of-scope question, question answering, retrieval augmented generation, source: ACM},
  abstract = {… RAG system retrieves a relevant document from a curated knowledge base and presents it to a large language model (LLM… in preventing an LLM from hallucinating or generating …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{lin_scirgen_2025,
  title = {{ScIRGen},
  author = {Lin, J. and Dai, L. and Han, R. and Sui, Y. and Wang, R. and Sun, X. and {...},
  year = {2025},
  doi = {10.1145/3711896.3737432},
  url = {https://doi.org/10.1145/3711896.3737432},
  booktitle = {Proceedings of the 31st {ACM},
  journal = {Proceedings of the 31st …},
  pages = {5619--5630},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, dataset generation, geoscience, information retrieval, large language models (LLMs), question answering, retrieval-augmented generation (RAG), scientific workflows, synthetic dataset, source: ACM},
  abstract = {… of relevant papers even in cases of ambiguous citations. … For each section, we design prompts to enable the LLM to … impact of LLM hallucinations, the second stage prompts the LLM to …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{zhang_way_2025,
  title = {Way to {Specialist},
  author = {Zhang, Yutong and Chen, Lixing and Li, Shenghong and Cao, Nan and Shi, Yang and Ding, Jiaxin and Qu, Zhe and Zhou, Pan and Bai, Yang},
  year = {2025},
  doi = {10.1145/3690624.3709187},
  url = {https://doi.org/10.1145/3690624.3709187},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {1996--2007},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {domain knowledge graph, retrieval-augmented generation, social network., specialized large language models},
  abstract = {Large language models (LLMs) have demonstrated exceptional performance across a wide variety of domains. Nonetheless, generalist LLMs continue to fall short in reasoning tasks necessitating specialized knowledge, e.g., emotional sociology and medicine. Prior investigations into specialized LLMs focused on domain-specific training, which entails substantial efforts in domain data acquisition and model parameter fine-tuning. To address these challenges, this paper proposes the Way-to-Specialist (WTS) framework, which synergizes retrieval-augmented generation with knowledge graphs (KGs) to enhance the specialized capability of LLMs in the absence of specialized training. In distinction to existing paradigms that merely utilize external knowledge from general KGs or static domain KGs to prompt LLM for enhanced domain-specific reasoning, WTS proposes an innovative ”LLM↻KG” paradigm, which achieves bidirectional enhancement between specialized LLM and domain knowledge graph (DKG). The proposed paradigm encompasses two closely coupled components: the DKG-Augmented LLM and the LLM-Assisted DKG Evolution. The former retrieves question-relevant domain knowledge from DKG and uses it to prompt LLM to enhance the reasoning capability for domain-specific tasks; the latter leverages LLM to generate new domain knowledge from processed tasks and use it to evolve DKG. WTS closes the loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling continuous improvement in the domain specialization as it progressively answers and learns from domain-specific questions. We validate the performance of WTS on 7 datasets (e.g., TweetQA, ChatDoctor5k) spanning 6 domains, e.g., emotional sociology, medical, ect. The experimental results show that WTS surpasses the previous SOTA in 5 specialized domains, and achieves a maximum performance improvement of 11.3\%.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1245-6},
}

@inproceedings{chen_multi-agent_2025,
  title = {Multi-{Agent},
  author = {Chen, Xinran and Li, Yuchen and Cai, Hengyi and Ma, Zhuoran and Chen, Xuanang and Xiong, Haoyi and Wang, Shuaiqiang and He, Ben and Sun, Le and Yin, Dawei},
  year = {2025},
  doi = {10.1145/3711896.3737249},
  url = {https://doi.org/10.1145/3711896.3737249},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4341--4352},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {information seeking, large language model, multi-agent system},
  abstract = {The proliferation of complex non-factoid questions in modern information seeking (IS) systems exposes critical limitations in conventional Retrieval-Augmented Generation (RAG) approaches, particularly their static search strategies and the lack of systematic multi-source information integration capabilities. Facing these limitations, we present PASS (Proactive Agent-driven Search System), a novel multi-agent framework that operationalizes human-like proactive search strategies through five specialized agents: Revealer for intent analysis, Navigator for search planning, Seeker/Reader for adaptive retrieval, and Writer for response synthesis, systematically expanding the search space through iterative query refinement and multi-perspective knowledge integration. Crucially, our framework demonstrates remarkable adaptability to mid-sized LLMs, demonstrating its scalability in resource-constrained environments. To comprehensively assess the effectiveness of the proposed framework, we carry out extensive experiments on both mid-sized and proprietary large-scale LLMs, evaluating response quality for complex non-factoid questions using a newly introduced nugget-based assessment. Experimental results from offline nugget-based evaluation and online A/B Tests confirm substantial improvements in answer quality, advancing proactive information seeking methodologies and offering practical pathways for democratizing complex reasoning capabilities to resource-constrained environments.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@article{cuconasu_power_2024,
  title = {The power of noise: {Redefining},
  author = {Cuconasu, F. and Trappolini, G. and Siciliano, F. and Filice, S. and {...},
  year = {2024},
  doi = {10.1145/3626772.3657834},
  url = {https://doi.org/10.1145/3626772.3657834},
  booktitle = {Proceedings of the 47th {International},
  journal = {Proceedings of the 47th …},
  pages = {719--729},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, information retrieval, llm, rag, source: ACM},
  abstract = {… be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it … RAG is primarily designed to improve factual accuracy by providing the model access to …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{zhong_towards_2025,
  title = {Towards {Repository},
  author = {Zhong, Si Cheng and Si, Xujie},
  year = {2025},
  doi = {10.1145/3759425.3763382},
  url = {https://doi.org/10.1145/3759425.3763382},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {27--39},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {Large Language Model, Trustworthy Code Generation},
  abstract = {Recent advancements in large language models (LLMs) suggest great promises in code and proof generations. However, scaling automated formal verification to real-world projects requires resolving cross-module dependencies and global contexts, which are crucial challenges overlooked by existing LLM-based methods with a special focus on targeting isolated, function-level verification tasks. To systematically explore and address the significant challenges of verifying entire software repositories, we introduce RVBench, the first verification benchmark explicitly designed for repository-level evaluation, constructed from four diverse and complex open-source Verus projects. We further introduce RagVerus, an extensible framework that synergizes retrieval-augmented generation with context-aware prompting to automate proof synthesis for multi-module repositories. RagVerus triples proof pass rates on existing benchmarks under constrained model inference budgets, and achieves a 27\% relative improvement on the more challenging RVBench benchmark, demonstrating a scalable and sample-efficient verification solution.},
  address = {New York, NY, USA},
  series = {{LMPL},
  isbn = {979-8-4007-2148-9},
}

@article{yang_knowing_2025,
  title = {Knowing {You},
  author = {Yang, D. and Zeng, L. and Rao, J. and Zhang, Y.},
  year = {2025},
  doi = {10.1145/3726302.3730018},
  url = {https://doi.org/10.1145/3726302.3730018},
  booktitle = {Proceedings of the 48th {International},
  journal = {… of the 48th International ACM SIGIR …},
  pages = {1305--1315},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, inner monologue, large language models, multi-round retrieval, question answering, retrieval augmented generation, source: ACM},
  abstract = {… ) pairs has proven effective for single-step RAG [2, 11], when an LLM can rapidly learn to map a … We suspect that a general-purpose LLM may be over-confident and generate too many …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1592-1},
  series = {{SIGIR},
}

@article{dakshit_faculty_2024,
  title = {Faculty perspectives on the potential of rag in computer science higher education},
  author = {Dakshit, S.},
  year = {2024},
  doi = {10.1145/3686852.3686864},
  url = {https://doi.org/10.1145/3686852.3686864},
  booktitle = {Proceedings of the 25th {Annual},
  journal = {Proceedings of the 25th Annual Conference on …},
  pages = {19--24},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Education, Large Language Models, Learning, Neural Networks, Retrieval Augmented Generation},
  abstract = {… This study is the first to gather faculty feedback on the application of LLM-based RAG in … mented Generation (RAG), which has the added benefit of curbing hallucinations and limiting …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{SIGITE},
  isbn = {979-8-4007-1106-0},
}

@article{doyle_if_2025,
  title = {If {You},
  author = {Doyle, C. and Tucker, A. D.},
  year = {2025},
  doi = {10.1145/3709025.3712220},
  url = {https://doi.org/10.1145/3709025.3712220},
  booktitle = {Proceedings of the 2025 {Symposium},
  journal = {Proceedings of the 2025 Symposium on Computer …},
  pages = {194--205},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Large Language Models, Law, Propositional Logic, Reasoning Models, Retrieval Augmented Generation, source: ACM},
  abstract = {… RAG pipelines are most successful when retrieving factual information that an LLM can … -oriented question of how well RAG improves performance on an LLM task, along with the …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{CSLAW},
  isbn = {979-8-4007-1421-4},
}

@inproceedings{ram_gesturecoach_2025,
  title = {{GestureCoach},
  author = {Ram, Ashwin and Suresh, Varsha and Saberpour Abadian, Artin and Demberg, Vera and Steimle, Jürgen},
  year = {2025},
  doi = {10.1145/3746059.3747705},
  url = {https://doi.org/10.1145/3746059.3747705},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Generative AI, Gesture, LLM, Presentation Talks},
  abstract = {This paper introduces GestureCoach, a system designed to help speakers deliver more engaging talks by guiding them to gesture effectively during rehearsal. GestureCoach combines an LLM-driven gesture recommendation model with a rehearsal interface that proactively cues speakers to gesture appropriately. Trained on experts’ gesturing patterns from TED talks, the model consists of two modules: an emphasis proposal module, which predicts when to gesture by identifying gesture-worthy text segments in the presenter notes, and a gesture identification module, which determines what gesture to use by retrieving semantically appropriate gestures from a curated gesture database. Results of a model performance evaluation and user study (N=30) show that the emphasis proposal module outperforms off-the-shelf LLMs in identifying suitable gesture regions, and that participants rated the majority of these predicted regions and their corresponding gestures as highly appropriate. A subsequent user study (N=10) showed that rehearsing with GestureCoach encouraged speakers to gesture and significantly increased gesture diversity, resulting in more engaging talks. We conclude with design implications for future AI-driven rehearsal systems.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{ross_rarr_2025,
  title = {{RARR},
  author = {Ross, Jonathan J and Khramtsova, Ekaterina and van der Vegt, Anton and Koopman, Bevan and Zuccon, Guido},
  year = {2025},
  doi = {10.1145/3726302.3730337},
  url = {https://doi.org/10.1145/3726302.3730337},
  booktitle = {Proceedings of the 48th {International},
  pages = {3286--3295},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {large language models (llms)., retrieval augmented generation (rag), source: Scopus},
  abstract = {Large Language Models (LLMs) often exhibit hallucinations, which makes detecting and mitigating these errors a critical challenge. The Retrofit Attribution using Research and Revision (RARR) framework addresses this challenge by extracting key aspects of an LLM response, verifying them against retrieved evidence, and resolving errors through re-prompting. In this work, we critically examine RARR and adapt its framework to incorporate publicly available evidence retrieval systems and generative models, thereby operationalizing the approach. We focus on hallucination detection, analyzing how each pipeline component contributes to this task. We also conduct a sentence-level analysis of hallucinations to provide a more granular assessment of RARR's performance. A key finding is that while query generation and retrieval are effective, the agreement module emerges as the weakest link in the RARR pipeline. We offer deeper insights into RARR's strengths, limitations, and potential areas for improvement, thereby broadening our understanding of hallucination detection in LLMs.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{huly_predicting_2025,
  title = {Predicting {RAG},
  author = {Huly, Oz and Carmel, David and Kurland, Oren},
  year = {2025},
  doi = {10.1145/3726302.3730062},
  url = {https://doi.org/10.1145/3726302.3730062},
  booktitle = {Proceedings of the 48th {International},
  pages = {1283--1293},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {llm, performance prediction, rag, text completion},
  abstract = {We address the challenge of predicting the performance of using retrieval augmented generation (RAG) in large language models (LLMs) for the task of text completion; specifically, we predict the perplexity gain attained by applying RAG. We present novel supervised post-retrieval prediction methods that utilize the specific characteristics of the text completion setting. Our predictors substantially outperform a wide variety of prediction methods originally proposed for ad hoc document retrieval. We then show that integrating our post-retrieval predictors with recently proposed post-generation predictors - i.e., those analyzing the next-token distribution - is of much merit: the resultant prediction quality is statistically significantly better than that of using the post-generation predictors alone. Finally, we show that our post-retrieval predictors are as effective as post-generation predictors for selective application of RAG. This finding is of utmost importance in terms of efficiency of selective RAG.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{qian_memorag_2025,
  title = {{MemoRAG},
  author = {Qian, Hongjin and Liu, Zheng and Zhang, Peitian and Mao, Kelong and Lian, Defu and Dou, Zhicheng and Huang, Tiejun},
  year = {2025},
  doi = {10.1145/3696410.3714805},
  url = {https://doi.org/10.1145/3696410.3714805},
  booktitle = {Proceedings of the {ACM},
  pages = {2366--2377},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {long context processing, retrieval-augmented generation, source: ACM},
  abstract = {Processing long contexts presents a significant challenge for large language models (LLMs). While recent advancements allow LLMs to handle much longer contexts than before (e.g., 32K or 128K tokens), it is computationally expensive and can still be insufficient for many applications. Retrieval-Augmented Generation (RAG) is considered a promising strategy to address this problem. However, conventional RAG methods face inherent limitations because of two underlying requirements: 1) explicitly stated queries, and 2) well-structured knowledge. These conditions, however, do not hold in general long-context processing tasks.In this work, we propose MemoRAG, a novel RAG framework empowered by global memory-augmented retrieval. MemoRAG features a dual-system architecture. First, it employs a light but long-range system to create a global memory of the long context. Once a task is presented, it generates draft answers, providing useful clues for the retrieval tools to locate relevant information within the long context. Second, it leverages an expensive but expressive system, which generates the final answer based on the retrieved information. Building upon this fundamental framework, we realize the memory module in the form of KV compression, and reinforce its memorization and cluing capacity from the Generation quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG achieves superior performances across a variety of long-context evaluation tasks, not only complex scenarios where traditional RAG methods struggle, but also simpler ones where RAG is typically applied.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{boumber_llms_2024,
  title = {{LLMs},
  author = {Boumber, Dainis and Tuck, Bryan E. and Verma, Rakesh M. and Qachfar, Fatima Zahra},
  year = {2024},
  doi = {10.1145/3643651.3659898},
  url = {https://doi.org/10.1145/3643651.3659898},
  booktitle = {Proceedings of the 10th {ACM},
  pages = {37--47},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto, Portugal},
  keywords = {business email compromise, explainability, fake news, job scams, language models, opinion spam, phishing, reasoning, retrieval augmented generation, sms spam, social engineering attacks},
  abstract = {This study investigates the effectiveness of Large Language Models (LLMs) in detecting deception using a Retrieval Augmented Generation (RAG) framework for few-shot learning in domain-agnostic settings. Our approach combines the sophisticated reasoning capabilities and extensive knowledge base of LLMs to identify deceptive statements across various contexts, with a focus on the explainability of the detection process. This emphasis on explainability enables a detailed analysis of the model's methodologies in distinguishing between truthful and deceptive statements. Additionally, we examine the impact of different definitions of deception, from overt falsehoods to subtle misrepresentations, on the model's accuracy. Our main contributions include providing initial insights into the adaptability of LLMs for deception detection and highlighting the challenges faced in this endeavor, thereby encouraging further exploration in this area.},
  address = {New York, NY, USA},
  series = {{IWSPA},
  isbn = {979-8-4007-0556-4},
}

@inproceedings{liu_graphcoder_2024,
  title = {{GraphCoder},
  author = {Liu, Wei and Yu, Ailun and Zan, Daoguang and Shen, Bo and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Wang, Qianxiang},
  year = {2024},
  doi = {10.1145/3691620.3695054},
  url = {https://doi.org/10.1145/3691620.3695054},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {570--581},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {code completion, code graphs, large language model, retrieval augmented generation, source: ACM, Accuracy, Codes, Computational modeling, Filtering, Information systems, Mathematical models, Process control, Software, Software algorithms, Software engineering, source: IEEE},
  abstract = {The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target more accurately through code context graph (CCG) that consists of control-flow, data- and control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results demonstrate both the effectiveness and efficiency of GraphCoder: Compared to baseline retrieval-augmented methods, GraphCoder achieves higher exact match (EM) on average, with increases of +6.06 in code match and +6.23 in identifier match, while using less time and space.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
  month = {oct},
}

@inproceedings{dang_authoring_2025,
  title = {Authoring {LLM},
  author = {Dang, Hai and Lafreniere, Ben and Grossman, Tovi and Todi, Kashyap and Li, Michelle},
  year = {2025},
  doi = {10.1145/3708359.3712164},
  url = {https://doi.org/10.1145/3708359.3712164},
  booktitle = {Proceedings of the 30th {International},
  pages = {211--230},
  publisher = {Association for Computing Machinery},
  keywords = {generative AI, large language model, virtual assistants, vision language model, voice-based interaction},
  abstract = {Advances in AI hold the possibility of assisting users with highly varied and individual needs, but the breadth of assistance that these systems could provide creates a challenge for how users specify their goals to the system. To support the authoring of AI assistance for real-world tasks, we propose the concept of Contextually-Driven Prompts (CDPs) that define how an AI assistant should respond to real-world context. We implemented a prototype system for authoring and executing CDPs, which provides suggestions to assist users with finding the right level of assistance for their goal. We also conducted a user study (N=10) to investigate how participants express and refine their goals for real-world tasks. Results revealed a number of strategies for initiating and refining CDPs with suggestions, and implications for the design of future authoring interfaces.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{cui_cirag_2025,
  title = {{CIRAG},
  author = {Cui, Chenxu and Fan, Haihui and Zhang, Jinchao and Shen, Lin and Li, Bo and Wang, Weiping},
  year = {2025},
  doi = {10.1145/3726302.3729921},
  url = {https://doi.org/10.1145/3726302.3729921},
  booktitle = {Proceedings of the 48th {International},
  pages = {1316--1326},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information retrieval, natural language processing, query expansion, reranking, retrieval-augmented generation, source: ACM, source: Scopus},
  abstract = {Retrieval-augmented generation (RAG) paradigms can integrate external knowledge to enhance and validate the output of Large Language Models (LLMs) thereby mitigating generative hallucinations and broadening the model's knowledge scope. Despite advancements, existing RAG methods still suffer from uncertainty of prediction during the multi-round retrieval-generation process, and a lack of the ability to balance the adequacy and redundancy of retrieved information. To address these challenges, we propose CIRAG, an approach that combines the RAG process with collective intelligence. Inspired by the crowd of wisdom, CIRAG simulates individual independent decision-making and information aggregation within a crowd. Specifically, CIRAG first enhances retrieval diversity by expanding queries based on extracted entities, then combines frequency-based and semantic-based reranking to form a multi granularity fusion reranking thereby assessing better relevance, and integrate multiple information sources for accurate content generation. By undertaking these steps in an integrated manner, CIRAG enables the model to acquire comprehensive and non-redundant information for generating responses. We conduct extensive experiments with HotPotQA and 2WikiMultihopQA datasets, popular benchmark for retrieval-based, multi-step question-answering. Experimental results show that our approach surpasses existing advanced RAG framework while providing high portability in query expansion as well as strong comprehensiveness exhibited in the collective intelligence.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{xin_atomr_2025,
  title = {{AtomR},
  author = {Xin, Amy and Liu, Jinxin and Yao, Zijun and Lee, Zhicheng and Cao, Shulin and Hou, Lei and Li, Juanzi},
  year = {2025},
  doi = {10.1145/3711896.3736849},
  url = {https://doi.org/10.1145/3711896.3736849},
  booktitle = {Proceedings of the 31st {ACM},
  volume = {2},
  pages = {3344--3355},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {knowledge-intensive reasoning, large language models, multi-hop qa, retrieval-augmented generation, source: ACM, source: Scopus},
  abstract = {Despite the outstanding capabilities of large language models (LLMs), knowledge-intensive reasoning still remains a challenging task due to LLMs' limitations in compositional reasoning and the hallucination problem. A prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented generation (RAG), which first formulates a reasoning plan by decomposing complex questions into simpler sub-questions, and then applies iterative RAG at each sub-question. However, prior works exhibit two crucial problems: inadequate reasoning planning and poor incorporation of heterogeneous knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how knowledge graph query languages model compositional reasoning through combining predefined operations, we propose three atomic knowledge operators, a unified set of operators for LLMs to retrieve and manipulate knowledge from heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes a complex question into a reasoning tree where each leaf node corresponds to an atomic knowledge operator, achieving question decomposition that is highly fine-grained and orthogonal. Subsequently, in the reasoning execution stage, AtomR executes each atomic knowledge operator, which flexibly selects, retrieves, and operates atomic level knowledge from heterogeneous sources. We also introduce BlendQA, a challenging benchmark specially tailored for heterogeneous knowledge reasoning. Experiments on three single-source and two multi-source datasets show that AtomR outperforms state-of-the-art baselines by a large margin, with absolute F1 score improvements of 9.4\% on 2WikiMultihop and 9.5\% on BlendQA. We release our code and data https://github.com/THU-KEG/AtomR.git.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{proma_personalizing_2025,
  title = {Personalizing {LLM},
  author = {Proma, A. and Pate, N. and Druckman, J. and Ghoshal, G. and {...},
  year = {2025},
  doi = {10.1145/3699682.3728349},
  url = {https://doi.org/10.1145/3699682.3728349},
  booktitle = {Proceedings of the 33rd {ACM},
  journal = {Proceedings of the 33rd …},
  pages = {134--143},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, LLM Agents, Misinformation, Personalization, Persuasion, RAG systems},
  abstract = {… We propose and evaluate a RAG-based LLM pipeline that personalizes information by considering user demographics, personalities, and trust in credible sources. Our evaluation of the …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1313-2},
}

@inproceedings{imasaka_effect_2024,
  title = {Effect of {LLM},
  author = {Imasaka, Yuta and Joho, Hideo},
  year = {2024},
  doi = {10.1145/3673791.3698433},
  url = {https://doi.org/10.1145/3673791.3698433},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {249--258},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {large language model, personality traits, query generation},
  abstract = {Large language models (LLMs) have demonstrated strong performance across various natural language processing tasks and are increasingly integrated into daily life. Just as personality traits are crucial in human communication, they could also play a significant role in the behavior of LLMs, for instance, in the context of Retrieval Augmented Generation. Previous studies have shown that Big Five personality traits could be applied to LLMs, but their specific effects on information retrieval tasks have not been sufficiently explored. This study aims to examine how personality traits assigned to LLM agents affect their query formulation behavior and search performance. We propose a method to accurately assign personality traits to LLM agents based on the Big Five theory and verify its accuracy using the IPIP-NEO-120 scale. We then design a query generation experiment using the NTCIR Ad-Hoc test collections and evaluate the search performance of queries generated by different LLM agents. The results show that our method successfully assigns all five personality traits to LLM agents as intended. Additionally, the query generation experiment suggests that the assigned traits did influence the length and vocabulary choices of generated queries. Finally, the retrieval effectiveness of the traits varied across test collections, showing a relative improvement ranging from -7.7\% to +4.6\%, but these differences were not statistically significant.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0724-7},
}

@inproceedings{hammoud_human_2024,
  title = {Human {Language},
  author = {Hammoud, Ali and Goyal, Chetanya and Pathen, Sakib and Dai, Arlene and Li, Anhang and Kielian, Gregory and Saligane, Mehdi},
  year = {2024},
  doi = {10.1145/3670474.3685971},
  url = {https://doi.org/10.1145/3670474.3685971},
  booktitle = {Proceedings of the 2024 {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salt Lake City, UT, USA},
  keywords = {Analog Layout Automation, GLayout, Large Language Model, Open Source, Parameter Efficient Fine Tuning, Quantized Low Rank Adaptation (QLORA), Retrieval Augmented Generation (RAG)},
  abstract = {Current approaches to Analog Layout Automation apply ML techniques such as Graph Convolutional Neural Networks (GCN) to translate netlist to layout. While these ML approaches have proven to be effective, they lack the powerful reasoning capabilities, an intuitive human interface, and standard evaluation benchmarks that have been improving at a rapid development pace in Large Language Models (LLMs). The GLayout framework introduced in this work translates analog layout into an expressive, technology generic, compact text representation. Then, an LLM is taught to understand analog layout through fine-tuning and in-context learning using Retrieval Augmented Generation (RAG). The LLM is able to successfully layout unseen circuits based on new information provided in-context. We train 3.8, 7, and 22 Billion parameter quantized LLMs on a dataset of less than 50 unique circuits, and text documents providing layout knowledge. The 22B parameter model is tuned in 2 hours on a single NVIDIA A100 GPU. The open-source evaluation set is proposed as an automation benchmark for LLM layout automation tasks, and ranges from 2-transistor circuits to a ΔΣ ADC. The 22B model completes 70\% of the tasks in the evaluation set, and passes DRC and LVS verification on 44\% of evaluations with verified correct blocks up to 4 transistors in size.},
  address = {New York, NY, USA},
  series = {{MLCAD},
  isbn = {979-8-4007-0699-8},
}

@inproceedings{fuchs_practical_2025,
  title = {Practical {Fact},
  author = {Fuchs, Gilad and Zinman, Oded and Ben-Shaul, Ido},
  year = {2025},
  doi = {10.1145/3701716.3717849},
  url = {https://doi.org/10.1145/3701716.3717849},
  booktitle = {Companion {Proceedings},
  pages = {2713--2716},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {fact-checking, hallucinations, large language models, source: Scopus},
  abstract = {The use of Large Language Models (LLM) like ChatGPT in real-world product solutions is significantly limited by the well-known issue of hallucinations. Various methods exist in order to overcome this issue automatically, such as using a different LLM to provide feedback on the accuracy of the generated text or by examining the output consistency given multiple sampled responses. However, these approaches do not guarantee factual accuracy, which is crucial in many specialized domains. In order to enhance the factual correctness of generated text by LLM models, a combination of manual annotations and supportive tools are required. We suggest a practical fact checking system tailored specifically for LLMs which combines a hybrid approach (human and machine) to evaluate the correctness of the generated text. This is particularly vital in fields where hallucinations present significant challenges. We use proprietary LLMs, both directly and through Retrieval-Augmented Generation (RAG), to offer users informed feedback on potential hallucinations via a user-friendly interface. We apply our methodology to the task of generating aspect values for video games in listings from an E-commerce marketplace, demonstrating the utility of our approach.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
  annote = {Cited by: 0},
}

@inproceedings{barat_constructing_2025,
  title = {Constructing {Enterprise},
  author = {Barat, Souvik and Mulpuru, Dushyanthi and Yadav, Abhishek and Korabu, Reshma and Thogaru, Himabindu and Kulkarni, Vinay},
  year = {2025},
  doi = {10.1145/3717383.3717391},
  url = {https://doi.org/10.1145/3717383.3717391},
  booktitle = {Proceedings of the 18th {Innovations},
  publisher = {Association for Computing Machinery},
  keywords = {ChatGPT, Digital Twin, Large Language Model, LLM, Model Driven Engineering},
  abstract = {Over the past year, Large Language Models (LLMs) have proven their value across a diverse range of industrial applications starting from supporting software development to automating customer interactions and enhancing process automation. We harness their potential for constructing Enterprise Digital Twins (EDTs), an emerging decision-making aid for a wide range of business sectors. EDT offers an effective "in silico" business experimentation leading to evidence-based informed decision-making, but its construction requires deep domain expertise spanning multiple aspects of enterprises across multiple stakeholders. Moreover, constructing an effective EDT demands seamless coordination between domain experts and expert modelers. These critical dependencies make the EDT construction challenging. This paper investigates the role of LLMs as domain experts and expert modelers to reduce excessive dependencies on both specializations and their coordination to an extent. Our approach integrates meta-modelling and Model Driven Engineering (MDE) techniques to effectively utilize LLMs with increased precision to alleviate the cognitive burden on domain experts and provide a systematic metamodel guided method for constructing purposive digital twins. We illustrate the approach and demonstrate its efficacy using a real-life EDT use case.},
  address = {New York, NY, USA},
  series = {{ISEC},
  isbn = {979-8-4007-1424-5},
}

@inproceedings{bradland_new_2025,
  title = {A {New},
  author = {Brådland, Henrik and Goodwin, Morten and Andersen, Per-Arne and Nossum, Alexander S. and Gupta, Aditya},
  year = {2025},
  doi = {10.1145/3726302.3729882},
  url = {https://doi.org/10.1145/3726302.3729882},
  booktitle = {Proceedings of the 48th {International},
  pages = {170--179},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {document chunking, natural language processing, passage evaluation, retrieval-augmented generation, text embedding},
  abstract = {Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p \&gt; 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2\% in factual correctness and 21.1\% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhang_litfm_2025,
  title = {{LitFM},
  author = {Zhang, Jiasheng and Maatouk, Ali and Chen, Jialin and Bui, Ngoc and Xie, Qianqian and Tassiulas, Leandros and Xu, Hua and Shao, Jie and Ying, Rex},
  year = {2025},
  doi = {10.1145/3711896.3737028},
  url = {https://doi.org/10.1145/3711896.3737028},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {3728--3739},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {citation graph, foundation model, large language model},
  abstract = {With the advent of large language models (LLMs), managing scientific literature via LLMs has become a promising direction of research. However, existing approaches often overlook the rich structural and semantic relevance among scientific literature, limiting their ability to discern the relationships between pieces of scientific knowledge, and suffer from various types of hallucinations. These methods also focus narrowly on individual downstream tasks, limiting their applicability across use cases. We propose LitFM, the first literature foundation model designed for a wide variety of practical downstream tasks on domain-specific literature, with a focus on citation information. At its core, LitFM contains a novel graph retriever that can provide accurate and diverse recommendations for LLM to integrate graph structure information and relevant literature. LitFM also leverages a knowledge-infused LLM, fine-tuned through a well-developed instruction paradigm. It enables LitFM to extract domain-specific knowledge from literature and reason relationships among them. By integrating citation graphs during both training and inference, LitFM can generalize to unseen papers and accurately assess their relevance within existing literature. Additionally, we introduce new large-scale literature citation benchmark datasets on three academic fields, featuring sentence-level citation information and local context. Extensive experiments validate the superiority of LitFM, achieving 28.1\% improvement on retrieval task in precision, and an average improvement of 7.52\% over state-of-the-art across six downstream literature-related tasks.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{abbasiantaeb_conversational_2025,
  title = {Conversational {Gold},
  author = {Abbasiantaeb, Zahra and Lupart, Simon and Azzopardi, Leif and Dalton, Jeffrey and Aliannejadi, Mohammad},
  year = {2025},
  doi = {10.1145/3726302.3730316},
  url = {https://doi.org/10.1145/3726302.3730316},
  booktitle = {Proceedings of the 48th {International},
  pages = {3455--3465},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {conversational information seeking, evaluation, information nuggets, retrieval-augmented generation, test collection, source: Scopus},
  abstract = {The rise of personalized conversational search systems has been driven by advancements in Large Language Models (LLMs), enabling these systems to retrieve and generate answers for complex information needs. However, the automatic evaluation of responses generated by Retrieval Augmented Generation (RAG) systems remains an understudied challenge. In this paper, we introduce a new resource for assessing the retrieval effectiveness and relevance of responses generated by RAG systems, using a nugget-based evaluation framework. Built upon the foundation of TREC iKAT 2023, our dataset extends to the TREC iKAT 2024 collection, which includes 17 conversations and 20,575 relevance passage assessments, together with 2,279 extracted gold nuggets and 62 manually written gold answers from NIST assessors. While maintaining the core structure of its predecessor, this new collection enables a deeper exploration of generation tasks in conversational settings. Key improvements in iKAT 2024 include: (1) ”gold nuggets” - concise, essential pieces of information extracted from relevant passages of the collection - which serve as a foundation for automatic response evaluation; (2) manually written answers to provide a gold standard for response evaluation; (3) expanded user personas, providing richer contextual grounding; and (4) a transition from Personal Text Knowledge Base (PTKB) ranking to PTKB classification and selection. Built on this resource, we provide a framework for long-form answer generation evaluation, involving nugget extraction and nugget matching, linked to retrieval. This establishes a solid resource for advancing research in personalized conversational search and long-form answer generation. Our resources are publicly available at https://github.com/irlabamsterdam/CONE-RAG.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@inproceedings{grimm_conversational_2024,
  title = {Conversational {Data},
  author = {Grimm, Valentin and Rubart, Jessica and Söhlke, Patrick},
  year = {2024},
  doi = {10.1145/3679058.3688631},
  url = {https://doi.org/10.1145/3679058.3688631},
  booktitle = {Proceedings of the 7th {Workshop},
  publisher = {Association for Computing Machinery},
  note = {event-place: Poznan, Poland},
  keywords = {Conversational Assistant, Conversational Data Storytelling, Data Storytelling, Explainable AI},
  abstract = {Data stories are about revealing and communicating insights from complex data. In this paper, we propose conversational data stories, which support end users in understanding the key findings of the data analysis at hand by natural language conversation. Creating these stories manually means to put a lot of effort into understanding the data and crafting visuals. With increasingly powerful generative large language models (LLMs), natural language processing as well as automating the creation of data stories is a promising field. We present a concept for a conversational data storytelling system that integrates LLMs as well as explainable AI. We present the collected requirements for our system concept and how the requirements are addressed. To show the potential of our approach, we provide a use case scenario and a discussion in this paper. This is supposed to serve as a basis for future research that will aim at investigating the technical reliability and the user experience of such a system.},
  address = {New York, NY, USA},
  series = {{HUMAN},
  isbn = {979-8-4007-1120-6},
}

@inproceedings{wu_xinyu_2024,
  title = {Xinyu: {An},
  author = {Wu, Yiquan and Tang, Bo and Xi, Chenyang and Yu, Yu and Wang, Pengyu and Liu, Yifei and Kuang, Kun and Deng, Haiying and Li, Zhiyu and Xiong, Feiyu and Hu, Jie and Cheng, Peng and Wang, Zhonghao and Wang, Yi and Luo, Yi and Yang, Mingchuan},
  year = {2024},
  doi = {10.1145/3637528.3671537},
  url = {https://doi.org/10.1145/3637528.3671537},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {6003--6014},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {commentary generation, llm-based system, retrieval augmented generation, supervised fine-tuning},
  abstract = {Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, we present an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, we introduce a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. Our experiments confirm the effectiveness of our proposed system. We also observe a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{mao_multi-agent_2025,
  title = {A {Multi},
  author = {Mao, Teng and Yang, Shuangtao and Fu, Bo},
  year = {2025},
  doi = {10.1145/3701716.3716884},
  url = {https://doi.org/10.1145/3701716.3716884},
  booktitle = {Companion {Proceedings},
  pages = {1687--1695},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {intelligent manufacturing, multi-agent systems, multi-source data, multimodal documents, thinking retrieval-augmented generation},
  abstract = {The integration of next-generation information technologies in the manufacturing sector has resulted in the generation of extensive and complex knowledge data by organizations. Effectively managing and utilizing this dispersed knowledge presents a significant challenge. This paper proposes a Multi-Agent-based, Multi-Source, and Multimodal Retrieval-Augmented Generation (RAG) framework specifically designed for the manufacturing industry. By employing advanced methodologies such as multimodal document parsing, multi-source data integration, and intelligent agent-driven querying, the proposed framework aims to address the interconnected challenges of knowledge management and intelligent question answering in complex manufacturing environments. The principal contributions of this research include: (1) the advancement of sophisticated multimodal document parsing techniques, (2) the establishment of robust strategies for multi-source data integration, (3) the development of a comprehensive multi-agent intelligent question-answering system, and (4) the enhancement of the Thinking RAG model. This framework provides a holistic solution to the complexities associated with knowledge management and improves retrieval efficiency in the manufacturing sector.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhai_information_2025,
  title = {Information {Retrieval},
  author = {Zhai, ChengXiang},
  year = {2025},
  doi = {10.1145/3726302.3730349},
  url = {https://doi.org/10.1145/3726302.3730349},
  booktitle = {Proceedings of the 48th {International},
  pages = {3876--3886},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {artificial general intelligence, neurosymbolic architecture, new retrieval tasks, retrieval for agent, retrieval-augmented generation, source: Scopus},
  abstract = {Traditionally, the users of an information retrieval (IR) system have been human users. We present a new perspective on IR research in which the users of an IR system are intelligent agents instead of human users. Extending the current work on retrieval-augmented generation (RAG), we identify five novel IR tasks that an intelligent agent must be able to perform in order to achieve Human-Level Artificial Intelligence, or Artificial General Intelligence (AGI), including 1) External Information Retrieval (EIR) to access new information unseen by the agent, 2) Provenance Information Retrieval (PIR) to trace the provenance of information, 3) Curriculum Information Retrieval (CIR) to actively acquire the most useful new data and information for lifelong learning, 4) Rule Information Retrieval (RIR) to perform reasoning and problem solving, and 5) Scenario Information Retrieval (SIR) to leverage past scenarios for problem solving and decision making. We compare these new IR tasks with the traditional IR tasks performed by an IR system that serves human users and systematically examine the challenges involved in the five new IR tasks, providing a roadmap for new IR research within the broader context of AGI development.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0},
}

@inproceedings{almuntashiri_using_2025,
  title = {Using {LLMs},
  author = {Almuntashiri, Abdullah Hamed and Ibáñez, Luis-Daniel and Chapman, Adriane},
  year = {2025},
  doi = {10.1145/3736229.3736261},
  url = {https://doi.org/10.1145/3736229.3736261},
  booktitle = {Proceedings of the {ProvenanceWeek},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  keywords = {Dataset Search, Information Extraction, LLMs, Provenance},
  abstract = {Having a provenance record facilitates data reuse and experimental reuse. However, provenance capture requires either: specific provenance-enabled systems to be used or human documentation. While there have been many examples of provenance-enabled systems for scientific usage, they are still the exception, not the norm. The one, standard place for provenance information of scientific experiments remains the scientific publication. Unfortunately, provenance buried in text is not immediately useful for computational purposes. Large Language Models (LLMs) have demonstrated exceptional capability across various tasks, particularly in information extraction. In this paper, we explore the potential of LLMs to infer a provenance record for scientific experiments from scientific papers. We develop an extractor, identify the most effective prompt for provenance extraction. Our results emphasise the capability of ChatGPT-4o in accessing and extracting provenance information from biomedical research papers. Additionally, we assess the scalability of the extractor for use in extracting provenance information across a set of biomedical research papers.},
  address = {New York, NY, USA},
  series = {{PW},
  isbn = {979-8-4007-1941-7},
}

@inproceedings{zhang_cream_2024,
  title = {{CREAM},
  author = {Zhang, Jinxu and Yu, Yongqi and Zhang, Yu},
  year = {2024},
  doi = {10.1145/3664647.3680750},
  url = {https://doi.org/10.1145/3664647.3680750},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {925--934},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {document vqa, large language model ranking, multi-page document representation, retrieval augmented generation},
  abstract = {Document Visual Question Answering (DVQA) involves responding to queries based on the contents of document images. Existing works are confined to locating information within a single page and lack support for cross-page question-and-answer interactions. Furthermore, the token length limitation on model inputs can lead to the truncation of answer-relevant segments. In this study, we present CREAM, an innovative methodology that focuses on high-performance retrieval and integrates relevant multimodal document information to effectively address this critical issue. To overcome the limitations of current text embedding similarity methods, we first employ a coarse-to-fine retrieval and ranking approach. The coarse phase calculates the similarity between the query and text chunk embeddings, while the fine phase involves multiple rounds of grouping and ordering with a large language model to identify the text chunks most relevant to the query. Subsequently, integrating an attention pooling mechanism for multi-page document images into the vision encoder allows us to effectively merge the visual information of multi-page documents, enabling the multimodal large language model (MLLM) to simultaneously process both single-page and multi-page documents. Finally, we apply various parameter-efficient tuning methods to enhance document visual question-answering performance. Experiments demonstrate that our approach secures state-of-the-art results across various document datasets.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{feng_forage_2025,
  title = {{FoRAGe},
  author = {Feng, Jiaxu and Gao, Xinyu and Huang, Muqi and Xu, Kanjun and Xiong, Yun and Zhou, Kun and Li, Chuan and Shi, Feng},
  year = {2025},
  doi = {10.1145/3711896.3737223},
  url = {https://doi.org/10.1145/3711896.3737223},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4414--4423},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {click-through rate prediction, diffusion model, food delivery, retrieval-augmented generation},
  abstract = {High Click-Through Rate (CTR) imagery has proven commercial value for food delivery platforms, driving a need for strategies to generate visually compelling images. Our investigations reveal a positive correlation between appropriate food backgrounds and subsequent user engagement. Despite advancements in diffusion models, inpainting new backgrounds does not guarantee high CTR, and fine-tuning diffusion models for this purpose is prohibitively expensive for the fast-paced online food delivery advertising sector. Consequently, there is a lack of cost-effective, transferable generation frameworks tailored to high-CTR food images. In this paper, we propose FoRAGe, a novel high-CTR Food image Retrieval-Augmented Generation pipeline leveraging ControlNet based on Stable Diffusion. Specifically, we construct a comprehensive food image database encompassing a diverse range of background environments. During image generation, FoRAGe retrieves high-quality background exemplars featuring analogous food subjects from the database and employs the retrieved backgrounds as conditions to guide image synthesis via the ControlNet model. Subsequently, a multimodal CTR prediction model is utilized to identify and select optimal images for deployment. Extensive online experiments demonstrate a significant increase in CTR for images generated by our proposed pipeline, and ablation studies further elucidate the impact of different strategies and configurations. Code is available at https://github.com/jiaxu-feng/FoRAGe.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{gu_adaptive_2024,
  title = {Adaptive and {Explainable},
  author = {Gu, Jingyi and Ye, Junyi and Wang, Guiling and Yin, Wenpeng},
  year = {2024},
  doi = {10.1145/3677052.3698681},
  url = {https://doi.org/10.1145/3677052.3698681},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {248--256},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Explainable AI, Large Language Model, Market Trend Forecasting, Portfolio Management, Reinforcement Learning},
  abstract = {Recent strategies for portfolio management often lack flexibility to adjust funds between long and short positions throughout trading periods. This prevents adapting portfolios to the market, which mitigates risks and seizes opportunities. To address these gaps, we propose an adaptive and explainable framework that integrates Large Language Models (LLMs) with Reinforcement Learning (RL) for dynamic long-short position adjustment in response to evolving market conditions. This approach leverages the recent advancements in LLMs for processing unstructured data and their capacity for explainable reasoning. The framework includes two stages: an Explainable Market Forecasting/Reasoning Pipeline, and a Position Reallocation stage. The Market Forecasting/Reasoning Pipeline allows various LLMs to learn market trends from diverse external data sources and determine optimal adjustment ratios with a clear reasoning path. The Portfolio Reallocation stage interacts with the sequential trading process from a pre-trained RL model to enhance decision-making and transparency. Our framework is flexible to accommodate various external data sources from microeconomics to macroeconomics data, diverse data types including time series and news text, along with multiple LLMs. Experiments demonstrate that our framework effectively achieves three times the return and doubles the Sharpe ratio compared to benchmarks. All the data and code are publicly available under NJIT FinTech Lab’s GitHub1.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{chukwu_may_2025,
  title = {May the {Memory},
  author = {Chukwu, Excel and Bindschaedler, Laurent},
  year = {2025},
  doi = {10.1145/3721146.3721951},
  url = {https://doi.org/10.1145/3721146.3721951},
  booktitle = {Proceedings of the 5th {Workshop},
  pages = {200--207},
  publisher = {Association for Computing Machinery},
  note = {event-place: World Trade Center, Rotterdam, Netherlands},
  keywords = {contextual knowledge management, hierarchical state management, large language models, log-structured merge trees, low-rank adaptation, persistent state retention, retrieval-augmented generation, stateful AI systems},
  abstract = {Large language models (LLMs) excel at natural language tasks but lack persistent state management for personalized and adaptive interactions. We propose a framework that endows these models with stateful capabilities by combining retrieval-augmented generation (RAG) and low-rank adaptation (LoRA). Our approach recasts the LLM as an editable component that retains hierarchical knowledge, analogous to deferred merge operations in log-structured merge (LSM) trees. The system integrates short-term context with long-term memory by storing accumulated context in a retrieval system while periodically training and combining lightweight LoRA adapters. Preliminary evaluations demonstrate improved state retention and query performance compared to both standard LLMs and RAG-augmented models, supporting our vision for scalable, stateful AI systems.},
  address = {New York, NY, USA},
  series = {{EuroMLSys},
  isbn = {979-8-4007-1538-9},
}

@inproceedings{deeksha_emu-llm_2025,
  title = {{EMU},
  author = {Deeksha, Deeksha and Krishnan, Ashwin and Nambiar, Manoj},
  year = {2025},
  doi = {10.1145/3680256.3721312},
  url = {https://doi.org/10.1145/3680256.3721312},
  booktitle = {Companion of the 16th {ACM},
  pages = {130--135},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {emulators, large language model, performance testing, performance., source: ACM},
  abstract = {With the advent of Large Language Models (LLMs) in modern web applications, rigorous performance testing has become essential to assess application's speed, stability, and resource usage under diverse workload conditions. While third-party APIs enable rapid integration and scaling in LLM-based applications, scalability testing with these APIs often incurs high costs and adds complexity throughout the development cycle. To address these challenges, we propose EMU-LLM, a framework that automatically selects and integrates emulators to replace third-party APIs while preserving LLM behavior. This emulator-centric approach offers a cost-effective solution for performance evaluation and system testing of LLM-based web applications. Our framework enhances the robustness and efficiency of applications and highlights promising directions for future research. This paper aims to guide researchers, developers, and testers on the significance of emulators in optimizing LLM performance and fostering growth.},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-1130-5},
}

@article{bui_cross-data_2024,
  title = {Cross-data knowledge graph construction for {LLM},
  author = {Bui, T. and Tran, O. and Nguyen, P. and Ho, B. and Nguyen, L. and {...},
  year = {2024},
  doi = {10.1145/3643479.3662055},
  url = {https://doi.org/10.1145/3643479.3662055},
  booktitle = {Proceedings of the 1st {ACM},
  journal = {Proceedings of the 1st …},
  pages = {36--43},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Education, Knowledge Graph, Large language model, Open Intent Discovery, Question-Answering System, source: ACM},
  abstract = {… However, an LLM’s completion might contain "hallucinations" [11] because of limited … of RAG from KG for LLMs in practical scenarios. In this paper, we aim to pioneer a KG-based RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{AIQAM},
  isbn = {979-8-4007-0547-2},
}

@inproceedings{zhao_explainable_2025,
  title = {Explainable {LLM},
  author = {Zhao, Gang and Zhang, Ximing and Lu, Chenji and Zhao, Hui and Wu, Tianshu and Wang, Pengjie and Xu, Jian and Zheng, Bo},
  year = {2025},
  doi = {10.1145/3701716.3715222},
  url = {https://doi.org/10.1145/3701716.3715222},
  booktitle = {Companion {Proceedings},
  pages = {631--640},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {e-commerce, knowledge distillation, large language model, semantic matching, source: ACM},
  abstract = {Effective query-item relevance modeling is pivotal for enhancing user experience and safeguarding user satisfaction in e-commerce search systems. Recently, benefiting from the vast inherent knowledge, Large Language Model (LLM) approach demonstrates strong performance and long-tail generalization ability compared with previous neural-based specialized relevance learning methods. Though promising, current LLM-based method encounters the following inadequacies in practice: Firstly, the relevance modeling process is a black box, making it difficult to clearly understand why LLM can provide the significant improvement or to analyze its relevance judgment errors. This opacity also hinders the reuse of the LLM's rich intrinsic knowledge. Secondly, the massive parameters and computational demands make it challenging to be deployed online. To improve the interpretability of LLM and boost the performance of online relevance models, we propose an Explainable LLM-driven Multi-dimensional Distillation framework for e-commerce relevance learning, which comprises two core components: (1) An Explainable LLM for relevance modeling (ELLM-rele), which decomposes the relevance learning into intermediate steps and models relevance learning as a Chain-of-Thought (CoT) reasoning, thereby enhancing both interpretability and performance of LLM. (2) A Multi-dimensional Knowledge Distillation (MKD) architecture that transfers the knowledge of ELLM-rele to current interaction-based and representation-based student models from both the relevance score distribution and CoT reasoning aspects. Through distilling the probabilistic and CoT reasoning knowledge, MKD improves both the semantic interaction and long-tail generalization abilities of student models. Extensive offline evaluations and online experiments conducted on Taobao search ad scene demonstrate that our proposed ELLM-MD framework significantly enhances e-commerce relevance learning performance and consumer experience.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{wilk_fact-based_2025,
  title = {Fact-based {Counter},
  author = {Wilk, Brian and Shomee, Homaira Huda and Maity, Suman Kalyan and Medya, Sourav},
  year = {2025},
  doi = {10.1145/3696410.3714718},
  url = {https://doi.org/10.1145/3696410.3714718},
  booktitle = {Proceedings of the {ACM},
  pages = {3354--3365},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {counter narrative, fact-based narrative, hate speech, large language model},
  abstract = {Online hatred has become an increasingly pervasive issue, affecting individuals and communities across various digital platforms. To combat hate speech in such platforms, counter narratives (CNs) are regarded as an effective method. In recent years, there has been growing interest in using generative AI tools to construct CNs. However, most of the generative models produce generic responses to hate speech and can hallucinate, reducing their effectiveness. To address the above limitations, we propose a counter narrative generation method that enhances CNs by providing non-aggressive, fact-based narratives with relevant background knowledge from two distinct sources, including a web search module. Furthermore, we conduct a comprehensive evaluation using multiple metrics, including LLM-based measures for persuasion, factuality, and informativeness, along with human and traditional NLP evaluations. Our method significantly outperforms baselines, achieving an average factuality score of 0.915, compared to 0.741, 0.701, and 0.69 for competitive baselines, and performs well in human evaluations.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{he_ai_2024,
  title = {{AI},
  author = {He, Jessica and Houde, Stephanie and Gonzalez, Gabriel E. and Silva Moran, Darío Andrés and Ross, Steven I. and Muller, Michael and Weisz, Justin D.},
  year = {2024},
  doi = {10.1145/3663384.3663398},
  url = {https://doi.org/10.1145/3663384.3663398},
  booktitle = {Proceedings of the 3rd {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Newcastle upon Tyne, United Kingdom},
  keywords = {Brainstorming, Future of work, Generative AI, Group ideation, Mixed initiative, Shared virtual canvas},
  abstract = {The introduction of generative AI into multi-user applications raises novel considerations for the future of collaborative work. How might collaborative work practices change? How might we incorporate generative AI into shared tools with users’ needs at the forefront? We examine these questions in the context of a remote team conducting ideation tasks – an example of collaborative work enabled by a shared digital workspace. We conducted a user study with 17 professionals experienced with virtual group ideation workshops. Our study examined their use of the Collaborative Canvas, a virtual canvas tool with integrated generative AI capabilities that we created as a probe. Participants saw value in using generative AI to assist with group facilitation and to augment perspectives and ideas. However, they worried about losing human perspectives and critical thinking, as well as reputational harms resulting from harmful AI outputs. Participants shared suggestions for appropriate ways to incorporate generative AI capabilities within multi-user applications and identified needs for transparency of content ownership, private digital spaces, and specialized AI capabilities. Based on participants’ insights, we share implications and opportunities for the incorporation of generative AI into collaborative work in ways that place user needs at the forefront.},
  address = {New York, NY, USA},
  series = {{CHIWORK},
  isbn = {979-8-4007-1017-9},
}

@inproceedings{zhou_intermind_2025,
  title = {{InterMind},
  author = {Zhou, Zhiyuan and Liu, Jilong and Wang, Sanwang and Hao, Shijie and Guo, Yanrong and Hong, Richang},
  year = {2025},
  doi = {10.1145/3746027.3754755},
  url = {https://doi.org/10.1145/3746027.3754755},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {5480--5489},
  publisher = {Association for Computing Machinery},
  note = {event-place: Dublin, Ireland},
  keywords = {depression detection, large language model, source: ACM},
  abstract = {Depression poses significant challenges to patients and healthcare organizations, necessitating efficient assessment methods. Existing paradigms typically focus on a patient-doctor way that overlooks multi-role interactions, such as family involvement in the evaluation and caregiving process. Moreover, current automatic depression detection (ADD) methods usually model depression detection as a classification or regression task, lacking interpretability for the decision-making process. To address these issues, we developed InterMind, a doctor-patient-family interactive depression assessment system empowered by large language models (LLMs). Our system enables patients and families to contribute descriptions, generates assistive diagnostic reports for doctors, and provides actionable insights, improving diagnostic precision and efficiency. To enhance LLMs' performance in psychological counseling and diagnostic interpretability, we integrate retrieval-augmented generation (RAG) and chain-of-thoughts (CoT) techniques for data augmentation, which mitigates the hallucination issue of LLMs in specific scenarios after instruction fine-tuning. Quantitative experiments and professional assessments by clinicians validate the effectiveness of our system.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-2035-2},
}

@inproceedings{zhang_characterizing_2025,
  title = {Characterizing and {Optimizing},
  author = {Zhang, Niansong and Zhu, Wenbo and Golden, Courtney and Ilan, Dan and Chen, Hongzheng and Batten, Christopher and Zhang, Zhiru},
  year = {2025},
  doi = {10.1145/3725843.3756132},
  url = {https://doi.org/10.1145/3725843.3756132},
  booktitle = {Proceedings of the 58th {IEEE},
  pages = {1011--1025},
  publisher = {Association for Computing Machinery},
  keywords = {analytical modeling, Compute-in-SRAM, energy efficiency, retrieval-augmented generation (RAG)},
  abstract = {Compute-in-SRAM architectures offer a promising approach to achieving higher performance and energy efficiency across a range of data-intensive applications. However, prior evaluations have largely relied on simulators or small prototypes, limiting the understanding of their real-world potential. In this work, we present a comprehensive performance and energy characterization of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads. We compare the GSI APU against established architectures, including CPUs and GPUs, to quantify its energy efficiency and performance potential. We introduce an analytical framework for general-purpose compute-in-SRAM devices that reveals fundamental optimization principles by modeling performance trade-offs, thereby guiding program optimizations. Exploiting the fine-grained parallelism of tightly integrated memory-compute architectures requires careful data management. We address this by proposing three optimizations: communication-aware reduction mapping, coalesced DMA, and broadcast-friendly data layouts. When applied to retrieval-augmented generation (RAG) over large corpora (10GB–200GB), these optimizations enable our compute-in-SRAM system to accelerate retrieval by 4.8 × –6.6 × over an optimized CPU baseline, improving end-to-end RAG latency by 1.1 × –1.8 ×. The shared off-chip memory bandwidth is modeled using a simulated HBM, while all other components are measured on the real compute-in-SRAM device. Critically, this system matches the performance of an NVIDIA A6000 GPU for RAG while being significantly more energy-efficient (54.4 × -117.9 × reduction). These findings validate the viability of compute-in-SRAM for complex, real-world applications and provide guidance for advancing the technology.},
  address = {New York, NY, USA},
  series = {{MICRO},
  isbn = {979-8-4007-1573-0},
}

@article{sun_application_2024,
  title = {The {Application},
  author = {Sun, Y. and Yang, W. and Liu, Y.},
  year = {2024},
  doi = {10.1145/3686397.3686420},
  url = {https://doi.org/10.1145/3686397.3686420},
  booktitle = {Proceedings of the 2024 8th {International},
  journal = {Proceedings of the 2024 8th International …},
  pages = {142--149},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Knowledge Graph, LLM-RAG, Oral History Archives},
  abstract = {… It uses Large Language Model - Retrieval Augmented Generation (LLM-RAG) for knowledge extraction, and then uses a semantic model for knowledge organization and management. …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{ICISDM},
  isbn = {979-8-4007-1734-5},
}

@inproceedings{chen_react_2025,
  title = {React {Agent},
  author = {Chen, Yulin and Luo, Jing and Tu, Xinhui},
  year = {2025},
  doi = {10.1145/3706890.3707043},
  url = {https://doi.org/10.1145/3706890.3707043},
  booktitle = {Proceedings of the 2024 5th {International},
  pages = {892--897},
  publisher = {Association for Computing Machinery},
  keywords = {Agent, Automated question answering, Large language models, Retrieval-augmented generation},
  abstract = {Recent advancements in large language models have led to significant improvements in various natural language processing tasks, including automated question answering. However, these models still struggle with providing accurate responses to complex biology questions, such as those found in the National College Entrance Examination. To address this issue, researchers have explored retrieval-augmented generation techniques, which incorporate external knowledge sources to enhance answer accuracy. While these methods have shown promise in improving efficiency, they often lack the ability to perform in-depth analysis of complex questions. In response to these limitations, the paper proposes a novel ReAct agent-based question answering method. This method combines biology textbook document libraries with Google search engine capabilities, leveraging the strengths of large language models. By integrating these components, this method can engage in autonomous decision-making and multi-step reasoning, allowing for a more thorough analysis of complex biology questions. Experimental results demonstrate the effectiveness of this new approach. When compared to responses generated directly by ChatGPT and GPT-4o models, the method showed significant improvements in accuracy for biology questions from the National College Entrance Examination. Specifically, the new method achieved accuracy increases of 11.1\% and 4.64\% over ChatGPT and GPT-4o, respectively.},
  address = {New York, NY, USA},
  series = {{ISAIMS},
  isbn = {979-8-4007-1782-6},
}

@inproceedings{datta_raging_2025,
  title = {{RAGing},
  author = {Datta, Priyangshu and Datta, Suchana and Roy, Dwaipayan},
  year = {2025},
  doi = {10.1145/3677389.3702523},
  url = {https://doi.org/10.1145/3677389.3702523},
  booktitle = {Proceedings of the 24th {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, China},
  keywords = {bibliometrics, dataset mention extraction, LLM, RAG, source: Scopus},
  abstract = {Dataset Mention Extraction (DME) is a critical task in the field of scientific information extraction, aiming to identify references to datasets within research papers. In this paper, we explore two advanced methods for DME from research papers, utilizing the capabilities of Large Language Models (LLMs). The first method employs a language model with a prompt-based framework to extract dataset names from text chunks, utilizing patterns of dataset mentions as guidance. The second method integrates the Retrieval-Augmented Generation (RAG) framework, which enhances dataset extraction through a combination of keyword-based filtering, semantic retrieval, and iterative refinement. We observe that both of the proposed methods achieve more than a 25\% improvement in recall compared to the baselines. Further, the RAG-based model achieves an extensive 26\% improvement over the baselines. We also propose exData, a web-based tool for extracting dataset name mentions from a given article.},
  address = {New York, NY, USA},
  series = {{JCDL},
  isbn = {979-8-4007-1093-3},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{li_repomincoder_2024,
  title = {{RepoMinCoder},
  author = {Li, Yifan and Shi, Ensheng and Zheng, Dewu and Duan, Kefeng and Chen, Jiachi and Wang, Yanlin},
  year = {2024},
  doi = {10.1145/3671016.3674819},
  url = {https://doi.org/10.1145/3671016.3674819},
  booktitle = {Proceedings of the 15th {Asia},
  pages = {229--238},
  publisher = {Association for Computing Machinery},
  note = {event-place: Macau, China},
  keywords = {Code Generation, Large Language Model, Screening and Ranking},
  abstract = {Repository-level code generation task involves generating code at a specified location based on unfinished code with repository context. Existing research mainly rely on retrieval-augmented generation methods to complete code. Existing work mainly investigates on improving retrieval results based on the unfinished code, but rarely pays attention to the information loss in the prompt encoding process. In this paper, we propose RepoMinCoder, a novel repository-level code generation framework that adds another round of screening and ranking based on information loss, building upon the canonical retrieval-augmented generation method. Extensive experimental results demonstrate that RepoMinCoder consistently outperforms state-of-the-art methods on public benchmark RepoEval, achieving 3.3\% EM and 2.1\% ES improvement over previous methods. Moreover, we conduct additional experiments to study the effect of various factors in the existing code generation pipeline, including the number of retrieval candidates, the slicing strategy of the retrieval database, and different prompting strategies.},
  address = {New York, NY, USA},
  series = {Internetware '24},
  isbn = {979-8-4007-0705-6},
}

@inproceedings{yan_understanding_2025,
  title = {Understanding and {Detecting},
  author = {Yan, Chuan and Guan, Bowei and Li, Yazhi and Meng, Mark Huasong and Wan, Liuhuo and Bai, Guangdong},
  year = {2025},
  doi = {10.1145/3696410.3714755},
  url = {https://doi.org/10.1145/3696410.3714755},
  booktitle = {Proceedings of the {ACM},
  pages = {3831--3839},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {deployment, large language model, security, testing},
  abstract = {OpenAI has enabled third-party developers to build applications around ChatGPT, known as GPTs, to expand its capability to handle complex and specialized tasks. A key feature of GPTs is Retrieval-Augmented Generation (RAG), which allows developers to upload documents containing domain knowledge or application context, referred to as file knowledge. However, these documents often contain sensitive information, and the security mechanisms governing access control in GPTs remains an underexplored area.In this work, we present the first comprehensive study on file knowledge leakage within GPTs. We develop GPTs-Filtor, leveraging the unique characteristics of GPTs deployment, to perform an in-depth analysis and detection of file knowledge leakage at both user interaction (i.e., prompt) and network transmission levels. Applying GPTs-Filtor to 8,000 popular GPTs across eight different categories, we reveal widespread vulnerabilities in the current GPTs development and deployment model. We detect 618 cases of leakage among 1,331 GPTs that involve uploaded file knowledge, leading to the exfiltration of 3,645 file contents that contain highly-sensitive data such as internal bank audit transaction records. Our work underscores the pressing need for improved security practices in GPTs development and deployment, providing crucial insights for the secure development of this young but rapidly evolving ecosystem.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{bhowmick_raguru_2025,
  title = {{RAGuru},
  author = {Bhowmick, Archisman and S, Rishikesh and Taksande, Ashay and Singh, Kuldeep and Mishra, Mayank and Singhal, Rekha},
  year = {2025},
  doi = {10.1145/3680256.3721326},
  url = {https://doi.org/10.1145/3680256.3721326},
  booktitle = {Companion of the 16th {ACM},
  pages = {109--113},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {deployment, optimization, rag, terraform, source: ACM},
  abstract = {Retrieval Augmented Generation (RAG) architectures have emerged as a powerful solution to enhance the accuracy and relevance of large language models (LLMs) by integrating retrieval mechanisms with generative capabilities. However, the design of an effective RAG pipeline is inherently complex, involving multiple components such as chunking strategies, embedding models, retrieval systems, and choice of LLMs. Each of these components offers numerous configuration options and the selection of the optimal combination is often a daunting task. The challenge is compounded by the need to consider trade-offs between performance, accuracy, and cost, which are not always straightforward and can vary significantly depending on the workload. In this context, we present RAGuru, an innovative tool designed to automate the design and creation of cost and latency-optimized RAG architectures. RAGuru addresses the complexities of RAG design by intelligently selecting and configuring the optimal components based on the user's specific workload requirements and also ensuring higher quality responses from RAG. By using an inhouse dataset of cost and performance metrics, RAGuru ensures that the resulting architecture is cost and latency wise optimal for an use-case, while achieving high accuracy. The architecture design space choices can be fed to terraform[2] as a configuration file for automatic deployment of the cost-performance optimal RAG. We have tested RAGuru in real-world scenarios. In a particular case, the RAG generated by RAGuru, demonstrated comparable performance at approximately half the cost of a conventional RAG system, with only minimal accuracy1 loss.},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-1130-5},
}

@inproceedings{zhang_lsrp_2025,
  title = {{LSRP},
  author = {Zhang, Yingyi and Jia, Pengyue and Li, Xianneng and Xu, Derong and Wang, Maolin and Wang, Yichao and Du, Zhaocheng and Guo, Huifeng and Liu, Yong and Tang, Ruiming and Zhao, Xiangyu},
  year = {2025},
  doi = {10.1145/3711896.3737036},
  url = {https://doi.org/10.1145/3711896.3737036},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {3889--3900},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {cloud-device framework, large language model, privacy-preserving},
  abstract = {Cloud-device collaboration leverages on-cloud Large Language Models (LLMs) for handling public user queries and on-device Small Language Models (SLMs) for processing private user data, collectively forming a powerful and privacy-preserving solution. However, existing approaches often fail to fully leverage the scalable problem-solving capabilities of on-cloud LLMs while underutilizing the advantage of on-device SLMs in accessing and processing personalized data. This leads to two interconnected issues: 1) Limited utilization of the problem-solving capabilities of on-cloud LLMs, which fail to align with personalized user-task needs, and 2) Inadequate integration of user data into on-device SLM responses, resulting in mismatches in contextual user information. In this paper, we propose a Leader-Subordinate Retrieval framework for Privacy-preserving cloud-device collaboration (LSRP), a novel solution that bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM through a dynamic selection of task-specific leader strategies named as user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the data advantages of on-device SLMs through small model feedback Direct Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the on-device SLM. Experiments on two datasets demonstrate that LSRP consistently outperforms state-of-the-art baselines, significantly improving question-answer relevance and personalization, while preserving user privacy through efficient on-device retrieval. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/LSRP.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{wang_intent-driven_2025,
  title = {Intent-{Driven},
  author = {Wang, Zhaodong and Lin, Samuel and Yan, Guanqing and Ghorbani, Soudeh and Yu, Minlan and Zhou, Jiawei and Hu, Nathan and Baruah, Lopa and Peters, Sam and Kamath, Srikanth and Yang, Jerry and Zhang, Ying},
  year = {2025},
  doi = {10.1145/3718958.3750537},
  url = {https://doi.org/10.1145/3718958.3750537},
  booktitle = {Proceedings of the {ACM},
  pages = {347--362},
  publisher = {Association for Computing Machinery},
  note = {event-place: São Francisco Convent, Coimbra, Portugal},
  keywords = {large language models (LLMs), network planning, RAG},
  abstract = {Advancements in Large Language Models (LLMs) are significantly transforming network management practices. In this paper, we present our experience developing Confucius, a multi-agent framework for network management at Meta. We model network management workflows as directed acyclic graphs (DAGs) to aid planning. Our framework integrates LLMs with existing management tools to achieve seamless operational integration, employs retrieval-augmented generation (RAG) to improve long-term memory, and establishes a set of primitives to systematically support human/model interaction. To ensure the accuracy of critical network operations, Confucius closely integrates with existing network validation methods and incorporates its own validation framework to prevent regressions. Remarkably, Confucius is a production-ready LLM development framework that has been operational for two years, with over 60 applications onboarded. To our knowledge, this is the first report on employing multi-agent LLMs for hyper-scale networks.},
  address = {New York, NY, USA},
  series = {{SIGCOMM},
  isbn = {979-8-4007-1524-2},
}

@inproceedings{rejithkumar_probing_2024,
  title = {Probing with {Precision},
  author = {Rejithkumar, Gokul and Anish, Preethu Rose and Shukla, Jyoti and Ghaisas, Smita},
  year = {2024},
  doi = {10.1145/3643666.3648577},
  url = {https://doi.org/10.1145/3643666.3648577},
  booktitle = {Proceedings of the 1st {IEEE},
  pages = {8--14},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {ChatGPT, large language models, mistral, probing questions, prompting, requirements engineering, retrieval augmented generation, source: ACM},
  abstract = {Software Requirements Specifications (SRS) often lack the necessary level of specificity required by software architects to make well-informed architectural decisions. This deficiency compels software architects to probe business analysts to collect more details pertinent to architectural requirements from the clients. In our previous work, we introduced Probing Question-flows (PQ-flows) that can assist business analysts to probe stakeholders and gather architecturally significant information for the creation of a more comprehensive SRS. Key limitations of our previous work were the manually created templatized PQ-flows and the mapping of PQ-flows to the software requirements based on standard Vector Space Model. In this study, we propose a Retrieval Augmented Generation (RAG) prompting framework to address these limitations. We conducted experiments using ChatGPT and Mistral-7B models. We present our findings utilizing human and automated evaluation metrics on a subset of the publicly available PUblic REquirements (PURE) dataset.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0569-4},
  series = {{MO2RE},
}

@inproceedings{li_leveraging_2024,
  title = {Leveraging {Prompt},
  author = {Li, Xiaoyan and Jiang, Cuicui},
  year = {2024},
  doi = {10.1145/3698383.3699622},
  url = {https://doi.org/10.1145/3698383.3699622},
  booktitle = {Proceedings of the {First},
  pages = {6--12},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hangzhou, China},
  keywords = {Attention ability, Cognitive function, Large language model, Logic Reasoning, source: ACM, source: Scopus},
  abstract = {Large language models (LLMs) have demonstrated remarkable problem-solving abilities, but the impact of attention on their logical reasoning capabilities remains underexplored. This study investigates the intersection of cognitive neuroscience and LLMs, focusing on prompt fine-tuning techniques to analyze how humanlike cognitive abilities and disabilities affect the problem-solving performance of these models. Two GPT-4 based models were developed using prompt fine-tuning and retrieval-augmented generation (RAG). The models were evaluated using the Criteria Cognitive Aptitude Test (CCAT) dataset, which assesses cognitive abilities such as problem-solving, critical thinking, and information processing. Results showed that the prompt-tuned GPT-4 model achieved the highest accuracy (81.2\%), while the model lacking attention performed poorly on questions requiring long-term inference. GPT-4's analysis highlighted the importance of attention in solving problems that demand long-term reference and identified the deficiencies in the attention-deficient model. This study sheds light on the mechanisms of problem-solving in the brain and the potential of AI to approximate human-like cognition, paving the way for future research at the intersection of cognitive neuroscience and artificial intelligence.},
  address = {New York, NY, USA},
  series = {{RMEL},
  isbn = {979-8-4007-1295-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{kang_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Kang, Zuheng and He, Yayun and Zhao, Botao and Qu, Xiaoyang and Peng, Junqing and Xiao, Jing and Wang, Jianzong},
  year = {2024},
  doi = {10.1145/3652583.3658086},
  url = {https://doi.org/10.1145/3652583.3658086},
  booktitle = {Proceedings of the 2024 {International},
  pages = {376--384},
  publisher = {Association for Computing Machinery},
  note = {event-place: Phuket, Thailand},
  keywords = {audio deepfake, deepfake detection, llm, retrieval-augmented detection, retrieval-augmented generation, text-to-speech, voice conversion},
  abstract = {With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse. However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues. Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection. We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework. Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets. Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance.},
  address = {New York, NY, USA},
  series = {{ICMR},
  isbn = {979-8-4007-0619-6},
}

@inproceedings{kasundra_adapting_2024,
  title = {Adapting {Open},
  author = {Kasundra, Jaykumar and Dhankhar, Shreyans},
  year = {2024},
  doi = {10.1145/3639856.3639888},
  url = {https://doi.org/10.1145/3639856.3639888},
  booktitle = {Proceedings of the {Third},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bangalore, India},
  keywords = {Automated Evaluation, Generative AI, Large Language Models, Natural Language Generation},
  abstract = {Large-scale language models, such as ChatGPT[3] and GPT-4[32], have demonstrated remarkable capabilities in generating human-like text for various applications. In this paper, we focus on two key aspects: (1) adapting open-source large language models (LLMs) for specific use cases like contract drafting using instruction tuning and parameter-efficient fine-tuning, and (2) analyzing the difference in ChatGPT’s behavior in single-role prompts compared to multi-role prompts for synthetic data generation tasks. We present a method for aligning open-source LLMs to follow instructions for customized contract drafting scenarios using parameter-efficient fine-tuning on synthetic data. Furthermore, we investigate the data quality of the synthetically generated instructions data by ChatGPT with single-role vs. multi-role prompts. Our findings reveal that the model performs better when given single-role prompts, highlighting the importance of strategically designing prompting strategy to generate better quality data using LLMs. By combining the insights from these two aspects, we explore potential implications and opportunities for enhancing generative AI solutions for practical implementations. The Contract Drafting model 1 and data 2 are released.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1649-2},
}

@inproceedings{chen_qilin_2025,
  title = {Qilin: {A},
  author = {Chen, Jia and Dong, Qian and Li, Haitao and He, Xiaohui and Gao, Yan and Cao, Shaosheng and Wu, Yi and Yang, Ping and Xu, Chen and Hu, Yao and Ai, Qingyao and Liu, Yiqun},
  year = {2025},
  doi = {10.1145/3726302.3730279},
  url = {https://doi.org/10.1145/3726302.3730279},
  booktitle = {Proceedings of the 48th {International},
  pages = {3670--3680},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {context-aware ranking, multimodal information retrieval, recommendation, retrieval-augmented generation, search, user behavior analysis, source: ACM},
  abstract = {User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\&amp;R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\&amp;R. To address the growing need for developing better S\&amp;R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\%. In contrast to existing datasets, Qilin offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \&amp; evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\&amp;R systems. We hope that Qilin will significantly contribute to the advancement of multimodal content platforms with S\&amp;R services in the future.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@article{mamalis_can_2023,
  title = {Can large language models revolutionalize open government data portals? a case of using chatgpt in statistics. gov. scot},
  author = {Mamalis, M. E. and Kalampokis, E. and Karamanou, A. and {...},
  year = {2023},
  doi = {10.1145/3635059.3635068},
  url = {https://doi.org/10.1145/3635059.3635068},
  booktitle = {Proceedings of the 27th {Pan},
  journal = {Proceedings of the 27th …},
  pages = {53--59},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, chatgpt, large language model, linked data, natural language processing, open government data},
  abstract = {… only is it possible to augment the large language model’s factuality of responses, but also … Retrieval Augmented Generation, a system could overcome the LLM’s inability to retain factual …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{PCI},
  isbn = {979-8-4007-1626-3},
}

@inproceedings{joshi_reaper_2024,
  title = {{REAPER},
  author = {Joshi, Ashutosh and Sarwar, Sheikh Muhammad and Varshney, Samarth and Nag, Sreyashi and Agrawal, Shrivats and Naik, Juhi},
  year = {2024},
  doi = {10.1145/3627673.3680087},
  url = {https://doi.org/10.1145/3627673.3680087},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4621--4628},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {chain-of-thought, multi-hop reasoning, rag, source: ACM},
  abstract = {Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from heterogeneous data stores that are architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one (or few) retrieval source. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant – in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, which means that a (small) classification model dictates the performance of a large language model. To address this problem, we present REAPER (REAsoning-based PlannER), an LLM-based retrieval planner that we evaluate on a conversational shopping assistant, which shows significant gains in latency over Agent-based systems and scalability to new and unseen use cases when compared to classification-based planning.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{liu_optimizing_2025,
  title = {Optimizing {RAG},
  author = {Liu, Fei and Kang, Zejun and Han, Xing},
  year = {2025},
  doi = {10.1145/3707292.3707358},
  url = {https://doi.org/10.1145/3707292.3707358},
  booktitle = {Proceedings of the 2024 3rd {International},
  pages = {152--159},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {Automotive Industry, Langchain, Ollama, PDF Processing, RAG, self-rag, source: ACM, source: Scopus},
  abstract = {With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing information processing and intelligent production in the automotive industry.},
  address = {New York, NY, USA},
  series = {{AIIIP},
  isbn = {979-8-4007-0730-8},
  annote = {Cited by: 4},
}

@inproceedings{ranade_fabula_2024,
  title = {{FABULA},
  author = {Ranade, Priyanka and Joshi, Anupam},
  year = {2024},
  doi = {10.1145/3625007.3627505},
  url = {https://doi.org/10.1145/3625007.3627505},
  booktitle = {Proceedings of the 2023 {IEEE},
  pages = {603--610},
  publisher = {Association for Computing Machinery},
  note = {event-place: Kusadasi, Turkiye},
  keywords = {knowledge graphs, large language models, narratives, retrieval augmented generation},
  abstract = {Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports.We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.},
  address = {New York, NY, USA},
  series = {{ASONAM},
  isbn = {979-8-4007-0409-3},
}

@inproceedings{bhattacharya_show_2025,
  title = {"{Show},
  author = {Bhattacharya, Aditya and Vanherwegen, Tim and Verbert, Katrien},
  year = {2025},
  doi = {10.1145/3699682.3728321},
  url = {https://doi.org/10.1145/3699682.3728321},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {174--184},
  publisher = {Association for Computing Machinery},
  keywords = {AI Agents, Conversational XAI, Counterfactual Explanation, Explainable AI},
  abstract = {Counterfactual explanations offer actionable insights by illustrating how changes to inputs can lead to different outcomes. However, these explanations often suffer from ambiguity and impracticality, limiting their utility for non-expert users with limited AI knowledge. Augmenting counterfactual explanations with Large Language Models (LLMs) has been proposed as a solution, but little research has examined their benefits and challenges for non-experts. To address this gap, we developed a healthcare-focused system that leverages conversational AI agents to enhance counterfactual explanations, offering clear, actionable recommendations to help patients at high risk of cardiovascular disease (CVD) reduce their risk. Evaluated through a mixed-methods study with 34 participants, our findings highlight the effectiveness of agent-augmented counterfactuals in improving actionable recommendations. Results further indicate that users with prior experience using conversational AI demonstrated greater effectiveness in utilising these explanations compared to novices. Furthermore, this paper introduces a set of generic guidelines for creating augmented counterfactual explanations, incorporating safeguards to mitigate common LLM pitfalls, such as hallucinations, and ensuring the explanations are both actionable and contextually relevant for non-expert users.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1313-2},
}

@inproceedings{wang_social-rag_2025,
  title = {Social-{RAG},
  author = {Wang, Ruotong and Zhou, Xinyi and Qiu, Lin and Chang, Joseph Chee and Bragg, Jonathan and Zhang, Amy X.},
  year = {2025},
  doi = {10.1145/3706598.3713749},
  url = {https://doi.org/10.1145/3706598.3713749},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {AI agent, group communication, large language models, recommender systems, retrieval augmented generation, source: Scopus},
  abstract = {AI agents are increasingly tasked with making proactive suggestions in online spaces where groups collaborate, yet risk being unhelpful or even annoying if they fail to match group preferences or behave in socially inappropriate ways. Fortunately, group spaces have a rich history of prior interactions and affordances for social feedback that can support grounding an agent’s generations to a group’s interests and norms. We present Social-RAG, a workflow for socially grounding agents that retrieves context from prior group interactions, selects relevant social signals, and feeds them into a language model to generate messages in a socially aligned manner. We implement this in PaperPing, a system for posting paper recommendations in group chat, leveraging social signals determined from formative studies with 39 researchers. From a three-month deployment in 18 channels reaching 500+ researchers, we observed PaperPing posted relevant messages in groups without disrupting their existing social practices, fostering group common ground.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@inproceedings{ray_metis_2025,
  title = {{METIS},
  author = {Ray, Siddhant and Pan, Rui and Gu, Zhuohan and Du, Kuntai and Feng, Shaoting and Ananthanarayanan, Ganesh and Netravali, Ravi and Jiang, Junchen},
  year = {2025},
  doi = {10.1145/3731569.3764855},
  url = {https://doi.org/10.1145/3731569.3764855},
  booktitle = {Proceedings of the {ACM},
  pages = {606--622},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Seoul, Republic of Korea},
  keywords = {LLM inference, RAG systems, scheduling},
  abstract = {RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with external knowledge, but using more external knowledge causes higher response delay. Prior work focuses either on reducing the response delay (e.g., better scheduling of RAG queries) or on maximizing quality (e.g., tuning the RAG workflow), but they fall short in systematically balancing the tradeoff between the delay and quality of RAG responses. To balance both quality and response delay, this paper presents METIS, the first RAG system that jointly schedules queries and adapts the key RAG configurations of each query, such as the number of retrieved text chunks and synthesis methods. Using four popular RAG-QA datasets, we show that compared to the state-of-the-art RAG optimization schemes, METIS reduces the generation latency by 1.64 – 2.54× without sacrificing generation quality.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1870-0},
}

@inproceedings{thiede_talking_2024,
  title = {Talking to {Objects},
  author = {Thiede, Christoph and Taeumel, Marcel and Böhme, Lukas and Hirschfeld, Robert},
  year = {2024},
  doi = {10.1145/3689492.3690049},
  url = {https://doi.org/10.1145/3689492.3690049},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {68--84},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pasadena, CA, USA},
  keywords = {ChatGPT, conversational agents, exploratory programming, generative AI, LLMs, natural-language programming, object-oriented programming, semantic tools, Smalltalk},
  abstract = {In exploratory programming, programmers often face a semantic gap between their high-level understanding and the low-level interfaces available for interacting with objects in a system. That is, technical object structure and behavior need to be interpreted as abstract domain concepts, which then increases cognitive load and thus impedes exploration progress. We propose semantic object interfaces that bridge this gap by enabling contextual, natural-language conversations with objects. Our approach leverages an exploratory programming agent powered by a large language model (LLM) to translate natural-language questions into low-level experiments and provide high-level answers. We describe a framework for integrating semantic object interfaces into existing exploratory programming systems, including a prototype implementation in Squeak/Smalltalk using GPT-4o. We showcase the potential of semantic object interfaces through case studies and discuss their feasibility, limitations, and impact on the programming experience. While challenges remain, our approach promises to reduce mental effort and empower programmers to explore and understand systems at a higher level of abstraction for a better programming experience.},
  address = {New York, NY, USA},
  series = {Onward! '24},
  isbn = {979-8-4007-1215-9},
}

@inproceedings{chatterjee_towards_2025,
  title = {Towards {An},
  author = {Chatterjee, Sourish and Verma, Rohit and Kumar, Abhinav and Raghunath, Arun},
  year = {2025},
  doi = {10.1145/3735654.3735945},
  url = {https://doi.org/10.1145/3735654.3735945},
  booktitle = {Proceedings of the {Workshop},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {source: ACM},
  abstract = {Video Retrieval-Augmented Generation (RAG) Workflows augment user query contexts with stored video contexts to guide a Large Language Model (LLM) in providing more context-relevant answers. The challenge is to design a workflow where the video contexts can be generated with practical latency values. It is also important to ensure that the resources available to run the workflows are not under-utilized whenever possible. In this paper, we propose a video RAG workflow that utilizes the Visual Data Management System (VDMS) interfaced with the Kubernetes (K8s) orchestration framework to design an enhanced video RAG workflow (VRAG) for faster video context generation in the pre-processing phase and a faster response generation with no impact on response accuracy. Our experiments showcase that compared to Conventional RAG(CRAG) workflows, VRAG reduces the pre-processing latency by a minimum of 10\%, which decreases manifold with parallelization and the response latency by a minimum of 70\% to as high as 97\% with further parallelization.},
  address = {New York, NY, USA},
  series = {{DEEM},
  isbn = {979-8-4007-1924-0},
}

@inproceedings{shao_are_2025,
  title = {Are {LLMs},
  author = {Shao, Yuchen and Huang, Yuheng and Shen, Jiawei and Ma, Lei and Su, Ting and Wan, Chengcheng},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00204},
  url = {https://doi.org/10.1109/icse55347.2025.00204},
  booktitle = {Proceedings of the {IEEE},
  pages = {1178--1190},
  publisher = {IEEE Press},
  keywords = {defects, empirical software engineering, LLM},
  abstract = {Large language models (LLMs) provide effective solutions in various application scenarios, with the support of retrieval-augmented generation (RAG). However, developers face challenges in integrating LLM and RAG into software systems, due to lacking interface specifications, various requirements from software context, and complicated system management. In this paper, we have conducted a comprehensive study of 100 open-source applications that incorporate LLMs with RAG support, and identified 18 defect patterns. Our study reveals that 77\% of these applications contain more than three types of integration defects that degrade software functionality, efficiency, and security. Guided by our study, we propose systematic guidelines for resolving these defects in software life cycle. We also construct an open-source defect library Hydrangea [1].},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{hu_llm-generated_2025,
  title = {{LLM},
  author = {Hu, Beizhe and Sheng, Qiang and Cao, Juan and Li, Yang and Wang, Danding},
  year = {2025},
  doi = {10.1145/3726302.3730027},
  url = {https://doi.org/10.1145/3726302.3730027},
  booktitle = {Proceedings of the 48th {International},
  pages = {435--445},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {fake news, large language model, recommender system},
  abstract = {Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with 56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{mattis_examples_2024,
  title = {Examples out of {Thin},
  author = {Mattis, Toni and Krebs, Eva and Rinard, Martin C. and Hirschfeld, Robert},
  year = {2024},
  doi = {10.1145/3660829.3660845},
  url = {https://doi.org/10.1145/3660829.3660845},
  booktitle = {Companion {Proceedings},
  pages = {99--107},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lund, Sweden},
  keywords = {example-based programming, generative ai, large language models, live programming, smalltalk},
  abstract = {Programmers often benefit from the availability of concrete run-time data alongside abstract source code. However, programmers need to manually exercise the program to reach an interesting state or write code that reproducibly executes a functionality with concrete inputs to be able to observe concrete data. This work aims to automate this process by leveraging generative AI. We present a framework and a preliminary Smalltalk-based prototype allowing programmers to obtain and run examples for the currently viewed source code section from a large language model. Our approach demonstrates how locally hosted LLMs can be fine-tuned and used for such a task with reasonable computational effort while minimizing common problems like hallucinations and out-of-date knowledge. The framework has direct applications in example-based live programming, where it can suggest new examples, and in learning settings where novices need to know how to use certain functionality.},
  address = {New York, NY, USA},
  series = {Programming '24},
  isbn = {979-8-4007-0634-9},
}

@inproceedings{srivastava_lending_2024,
  title = {Lending an {Ear},
  author = {Srivastava, Varad},
  year = {2024},
  doi = {10.1145/3677052.3698608},
  url = {https://doi.org/10.1145/3677052.3698608},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {301--309},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Claude, Few-Shot, Gemma, GPT, Llama, LLMs, Mistral, NLP, RAG},
  abstract = {Traditional language models in NLP require a considerable amount of labeled examples, which is not always available in data-limited domains like banking. Large Language Models (LLMs) are known to perform effectively with few-shot learning in various domains with just 1-5 examples per class. However, the use of open-source instruction-tuned LLMs, and how they compare to closed-source LLMs and modern few-shot learning approaches like contrastive learning in data-constrained use-cases has been under-explored. Additionally, the understanding of performance-cost trade-offs of these methods, as well as the consideration of infrastructure resource-limited settings through optimal usage of smaller versions of LLMs (7B-9B parameters), a critical concern for budget-limited organizations, has not been studied comprehensively. Our work addresses these gaps by studying the aforementioned approaches over the Banking77 financial intent detection dataset, including the evaluation and comparison of cutting-edge LLMs by Meta, Google, Mistral-AI, OpenAI, and Anthropic in a comprehensive set of few-shot scenarios which include examples selected by a human-expert, as well as with a cost-effective querying method based on retrieval-augmented generation (RAG). We observed that smaller open-source LLMs are able to out-perform larger closed-source ones with effective prompts and RAG. Moreover, they offer a significantly better performance-cost ratio than their larger closed-source counterparts. We also experiment with data-augmentation by using LLMs to generate artificial labeled examples, which is able to improve performance slightly in a data-scarce scenario. Finally, we explored benefits of fine-tuning using three parameter efficient methods and propose BAI-Fintent, an LLM based on fine-tuned Mistral-7B, that out-performs all other approaches at customer banking intent identification.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{huang_penheal_2024,
  title = {{PenHeal},
  author = {Huang, Junjie and Zhu, Quanyan},
  year = {2024},
  doi = {10.1145/3689933.3690831},
  url = {https://doi.org/10.1145/3689933.3690831},
  booktitle = {Proceedings of the {Workshop},
  pages = {11--22},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salt Lake City, UT, USA},
  keywords = {cybersecurity automation, llms, penetration testing, retrieval-augmented generation, vulnerability remediation},
  abstract = {Recent advances in Large Language Models (LLMs) have shown significant potential in enhancing cybersecurity defenses against sophisticated threats. LLM-based penetration testing is an essential step in automating system security evaluations by identifying vulnerabilities. Remediation, the subsequent crucial step, addresses these discovered vulnerabilities. Since details about vulnerabilities, exploitation methods, and software versions offer crucial insights into system weaknesses, integrating penetration testing with vulnerability remediation into a cohesive system has become both intuitive and necessary. This paper introduces PenHeal, a two-stage LLM-based framework designed to autonomously identify and mitigate security vulnerabilities. The framework integrates two LLM-enabled components: the Pentest Module, which detects multiple vulnerabilities within a system, and the Remediation Module, which recommends optimal remediation strategies. The integration is facilitated through Counterfactual Prompting and an Instructor module that guides the LLMs using external knowledge to explore multiple potential attack paths effectively. Our experimental results demonstrate that PenHeal not only automates the identification and remediation of vulnerabilities but also significantly improves vulnerability coverage by 31\%, increases the effectiveness of remediation strategies by 32\%, and reduces the associated costs by 46\% compared to baseline models. These outcomes highlight the transformative potential of LLMs in reshaping cybersecurity practices, offering an innovative solution to defend against cyber threats.},
  address = {New York, NY, USA},
  series = {{AutonomousCyber},
  isbn = {979-8-4007-1229-6},
}

@inproceedings{bhuvaji_retrieval-augmented_2025,
  title = {A {Retrieval},
  author = {Bhuvaji, Sartaj and Chouhan, Prachitee and Irukulla, Madhuroopa and Singhvi, Jay and Bae, Wan D. and Alkobaisi, Shayma},
  year = {2025},
  doi = {10.1145/3672608.3707915},
  url = {https://doi.org/10.1145/3672608.3707915},
  booktitle = {Proceedings of the 40th {ACM},
  pages = {899--906},
  publisher = {Association for Computing Machinery},
  note = {event-place: Catania International Airport, Catania, Italy},
  keywords = {abstractive summarization, BART, LLM, meeting data retrieval, pinecone, speech to text conversion, text summarization},
  abstract = {Meetings are vital for collaboration and decision-making in professional environments, yet recalling key details from past discussions can be challenging and this impacts productivity. In this paper, we address this issue by developing a solution that extracts crucial insights from historical meeting records using Retrieval Augmented Generation (RAG) techniques. Users can easily upload meeting records and query for relevant information. A core feature of our proposed system is grouping meetings based on abstractive summaries, using state-of-the-art clustering algorithms extensively trained for accuracy. Upon user inquiry, the system identifies the most relevant cluster and retrieves related conversations from the Pinecone vector store database. These conversations, paired with custom prompts, are processed through a Large Language Model (LLM) to generate precise responses. Our optimization efforts focus on exploring various encoders and LLMs, with fine-tuning to ensure seamless integration and high performance. This approach tackles challenges in meeting summarization, content discovery, and user-friendly information retrieval.},
  address = {New York, NY, USA},
  series = {{SAC},
  isbn = {979-8-4007-0629-5},
}

@inproceedings{luo_integration_2024,
  title = {Integration of {LLMs},
  author = {Luo, Xiaoyu and Liu, Daping and Dang, Fan and Luo, Hanjiang},
  year = {2024},
  doi = {10.1145/3674399.3674402},
  url = {https://doi.org/10.1145/3674399.3674402},
  booktitle = {Proceedings of the {ACM},
  pages = {1--5},
  publisher = {Association for Computing Machinery},
  note = {event-place: Changsha, China},
  keywords = {Internet of Things, Large Language Model, LLM-based Agent, Smart Home},
  abstract = {The emergence of large language models (LLMs) offers a new opportunity to build LLMs-based applications, such as smart home, as these models have demonstrated general-purpose language understanding by generating coherent and contextually relevant text. However, LLMs are trained on massive amounts of text data to predict tokens, so these models have limitations and it is difficult for them performing physical world tasks directly. To further exploit the potential of LLMs to solve the challenge of integrating them with the physical world, LLMs enhanced and augmented techniques should be addressed, especially reinforcement learning based techniques. In this paper, we study the issue of integrating LLMs with physical world. We first describe the large language models and limitations. Then, we revisit LLMs enhanced and augmented techniques. After that, we present methods of interaction LLMs with physical world, such as integration IoT sensing with LLMs, embodied agent post-training with LLMs, and robot task planning with LLMs. Finally, we provide a case study of smart home powered by LLMs to discuss future research directions of next-generation intelligent smart home, personal health assistant, and LLM-based household robot.},
  address = {New York, NY, USA},
  series = {{ACM},
  isbn = {979-8-4007-1011-7},
}

@inproceedings{kaate_you_2025,
  title = {“{You},
  author = {Kaate, Ilkka and Salminen, Joni and Jung, Soon-Gyo and Xuan, Trang Thi Thu and Häyhänen, Essi and Azem, Jinan Y. and Jansen, Bernard J.},
  year = {2025},
  doi = {10.1145/3708359.3712160},
  url = {https://doi.org/10.1145/3708359.3712160},
  booktitle = {Proceedings of the 30th {International},
  pages = {1624--1638},
  publisher = {Association for Computing Machinery},
  keywords = {AI-generated personas, generative AI, human-computer interaction, misinformation, user experience},
  abstract = {We investigated the presence and acceptance of hallucinations (i.e., accidental misinformation) of an AI-generated persona system that leverages large language models for persona creation from survey data in a 54-user within-subjects experiment. After interacting with the personas, users were given a task to ask the personas a series of questions, including an unanswerable question, meaning the personas lacked the data to answer the question. The AI-generated persona system provided a plausible but incorrect answer half (52\%) of the time, and more than half of the time (57\%), the users accepted the incorrect answer, and the rest of the time, users answered the unanswerable question correctly (no answer). We found that when the AI-generated persona hallucinated, the user was significantly more likely to answer the unanswerable question incorrectly. Also, for genders separately, when the AI-generated persona hallucinated, it was significantly more likely for the female user and the male users to answer the unanswerable question incorrectly. We identified four themes in the AI-generated persona's answers and found that users perceive AI-generated persona's answers as long and unclear for the unanswerable question. Findings imply that personas leveraging LLMs require guardrails to ensure that personas clearly state the possibility of data restrictions and hallucinations when asked unanswerable questions.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{zhu_autopbl_2025,
  title = {{AutoPBL},
  author = {Zhu, Yihao and Ye, Zhoutong and Yuan, Yichen and Tang, Wenxuan and Yu, Chun and Shi, Yuanchun},
  year = {2025},
  doi = {10.1145/3706598.3714261},
  url = {https://doi.org/10.1145/3706598.3714261},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI for education, Large Language Model, Project-based Learning},
  abstract = {Self project-based learning (SPBL) is a popular learning style where learners follow tutorials and build projects by themselves. SPBL combines project-based learning’s benefit of being engaging and effective with the flexibility of self-learning. However, insufficient guidance and support during SPBL may lead to unsatisfactory learning experiences and outcomes. While LLM chatbots (e.g., ChatGPT) could potentially serve as SPBL tutors, we have yet to see an SPBL platform with responsible and systematic LLM integration. To address this gap, we present AutoPBL, an interactive learning platform for SPBL learners. We examined human PBL tutors’ roles through formative interviews to inform our design. AutoPBL features an LLM-guided learning process with checkpoint questions and in-context Q\&amp;A. In a user study where 29 beginners learned machine learning through entry-level projects, we found that AutoPBL effectively improves learning outcomes and elicits better learning behavior and metacognition by clarifying current priorities and providing timely assistance.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{pei_flow--action_2025,
  title = {Flow-of-{Action},
  author = {Pei, Changhua and Wang, Zexin and Liu, Fengrui and Li, Zeyan and Liu, Yang and He, Xiao and Kang, Rong and Zhang, Tieying and Chen, Jianjun and Li, Jianhui and Xie, Gaogang and Pei, Dan},
  year = {2025},
  doi = {10.1145/3701716.3715225},
  url = {https://doi.org/10.1145/3701716.3715225},
  booktitle = {Companion {Proceedings},
  pages = {422--431},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {large language model, multi-agent system, root cause analysis, source: ACM},
  abstract = {In the realm of microservices architecture, the occurrence of frequent incidents necessitates the employment of Root Cause Analysis (RCA) for swift issue resolution. It is common that a serious incident can take several domain experts hours to identify the root cause. Consequently, a contemporary trend involves harnessing Large Language Models (LLMs) as automated agents for RCA. Though the recent ReAct framework aligns well with the Site Reliability Engineers (SREs) for its thought-action-observation paradigm, its hallucinations often lead to irrelevant actions and directly affect subsequent results. Additionally, the complex and variable clues of the incident can overwhelm the model one step further. To confront these challenges, we propose Flow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced LLM-based multi-agent system. By explicitly summarizing the diagnosis steps of SREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA process towards the correct trajectory. To facilitate the rational and effective utilization of SOPs, we design an SOP-centric framework called SOP flow. SOP flow contains a series of tools, including one for finding relevant SOPs for incidents, another for automatically generating SOPs for incidents without relevant ones, and a tool for converting SOPs into code. This significantly alleviates the hallucination issues of ReAct in RCA tasks. We also design multiple auxiliary agents to assist the main agent by removing useless noise, narrowing the search space, and informing the main agent whether the RCA procedure can stop. Compared to the ReAct method's 35.50\% accuracy, our Flow-of-Action method achieves 64.01\%, meeting the accuracy requirements for RCA in real-world systems.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{xu_generative_2025,
  title = {Generative {Artificial},
  author = {Xu, Yao and Jian, Xiao},
  year = {2025},
  doi = {10.1145/3722237.3722377},
  url = {https://doi.org/10.1145/3722237.3722377},
  booktitle = {Proceedings of the 2024 3rd {International},
  pages = {802--806},
  publisher = {Association for Computing Machinery},
  keywords = {Digital Textbooks, Retrieval-Augmented Generation, Vocational Education, source: ACM},
  abstract = {This research focuses on the application of AIGC in vocational education digital textbooks and how RAG technology can be used to further improve the interactivity and educational effectiveness of digital textbooks. The paper first analyzes the development of AIGC technology and its great potential in the field of education, especially the role of LLM in content generation. It then discusses the concept, risks, and construction strategies of vocational education digital textbooks, emphasizing the importance of digital textbooks in teaching and their differences compared with paper textbooks. The study also designs a knowledge retrieval question-answering system based on RAG, which combines school-based textbooks and cooperative enterprise manuals as a knowledge base to generate responses to student questions. While ensuring accuracy and completeness, the quality of the responses was improved by the merge of students' queries with relevant texts using RAG technology. Regarding implementation, RAG technology consists of knowledge base construction and knowledge retrieval. It includes segmenting the document, recognizing and vectorizing it in order to build a vector database. During user retrieval, queries are sorted through the vector database interface, and the matched knowledge fragments and query statements are merged to generate answers including specific domain knowledge. This framework of the RAG question-answering system can provide accurate answers to satisfy students of all levels in personalized teaching while maintaining explainability and timeliness of the content.},
  address = {New York, NY, USA},
  series = {{ICAIE},
  isbn = {979-8-4007-1269-2},
}

@inproceedings{pallucchini_self-explanatory_2025,
  title = {Self-explanatory and {Retrieval},
  author = {Pallucchini, Filippo and Zhang, Xulang and Mao, Rui and Cambria, Erik},
  year = {2025},
  doi = {10.1145/3672608.3707894},
  url = {https://doi.org/10.1145/3672608.3707894},
  booktitle = {Proceedings of the 40th {ACM},
  pages = {131--137},
  publisher = {Association for Computing Machinery},
  note = {event-place: Catania International Airport, Catania, Italy},
  keywords = {financial sentiment analysis, interpretability, LLM, RAG, semantics, source: Scopus},
  abstract = {Enriching sentences with qualitative knowledge is crucial for enhancing sentiment prediction and making the most of the available labelled data for training models. This is particularly important in domains like the financial one, where texts are usually brief and contain much-implied information. In this article, we introduce FLEX (Financial Language Enhancement with Guided LLM Execution), an automated system capable of retrieving information from a Large Language Model (LLM) to enrich financial sentences, making them more knowledge-dense and explicit. FLEX generates multiple potentially enhanced sentences and uses a new logic to determine the most suitable one. To mitigate hallucinations in LLMs, we developed a new algorithm to select the most appropriate sentences. This approach ensures the original meaning is preserved, reduces excessive syntactic similarity between versions, and maintains the lowest possible perplexity. These enhanced sentences are more interpretable and directly useful for downstream tasks like financial sentiment analysis (FSA). Compared to state-of-the-art methods, FLEX shows improvements in the accuracy of processing FSA tasks.},
  address = {New York, NY, USA},
  series = {{SAC},
  isbn = {979-8-4007-0629-5},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{saha_ir_explain_2025,
  title = {ir\_explain: {A},
  author = {Saha, Sourav and Agarwal, Harsh and V, Venktesh and Anand, Avishek and Mohanty, Swastik and Majumdar, Debapriyo and Mitra, Mandar},
  year = {2025},
  doi = {10.1145/3726302.3730343},
  url = {https://doi.org/10.1145/3726302.3730343},
  booktitle = {Proceedings of the 48th {International},
  pages = {3563--3572},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {axiomatic ranking, explainable information retrieval, interpretable by design, post-hoc interpretability, probing},
  abstract = {While recent advancements in Neural Ranking Models have resulted in significant improvements over traditional statistical retrieval models, it is generally acknowledged that the use of large neural architectures and the application of complex language models in Information Retrieval (IR) have reduced the transparency of retrieval methods. Consequently, Explainability and Interpretability have emerged as important research topics in IR. Several axiomatic and post-hoc explanation methods, as well as approaches that attempt to be interpretable-by-design, have been proposed. We present ir\_explain, an open-source Python library that implements a variety of well-known techniques for Explainable IR (ExIR) within a common, extensible framework. It supports the three standard categories of post-hoc explanations, namely pointwise, pairwise, and listwise explanations. The library is designed to make it easy to reproduce state-of-the-art ExIR baselines on standard test collections, as well as to explore new approaches to explaining IR models and methods. To facilitate adoption, ir\_explain is well-integrated with widely-used toolkits such as Pyserini, PyTerrier (work in progress) and ir\_datasets. Downstream applications of ir\_explain include explaining the Retrieval-Augmented Generation (RAG) pipeline. The development version of the library is available on GitHub. We release the library as a pip package (https://pypi.org/project/ir-explain/); source code is available from https://github.com/souravsaha/ir\_explain.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{xiong_enhancing_2025,
  title = {Enhancing the {Patent},
  author = {Xiong, Qiushi and Xu, Zhipeng and Liu, Zhenghao and Wang, Mengjia and Chen, Zulong and Sun, Yue and Gu, Yu and Li, Xiaohua and Yu, Ge},
  year = {2025},
  doi = {10.1145/3726302.3729970},
  url = {https://doi.org/10.1145/3726302.3729970},
  booktitle = {Proceedings of the 48th {International},
  pages = {337--347},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {large language models, memory graph, patent matching, retrieval-augmented generation},
  abstract = {Intellectual Property (IP) management involves strategically protecting and utilizing intellectual assets to enhance organizational innovation, competitiveness, and value creation. Patent matching is a crucial task in intellectual property management, which facilitates the organization and utilization of patents. Existing models often rely on the emergent capabilities of Large Language Models (LLMs) and leverage them to identify related patents directly. However, these methods usually depend on matching keywords and overlook the hierarchical classification and categorical relationships of patents. In this paper, we propose MemGraph, a method that augments the patent matching capabilities of LLMs by incorporating a memory graph derived from their parametric memory. Specifically, MemGraph prompts LLMs to traverse their memory to identify relevant entities within patents, followed by attributing these entities to corresponding ontologies. After traversing the memory graph, we utilize extracted entities and ontologies to improve the capability of LLM in comprehending the semantics of patents. Experimental results on the PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a 17.68\% performance improvement over baseline LLMs. The further analysis highlights the generalization ability of MemGraph across various LLMs, both in-domain and out-of-domain, and its capacity to enhance the internal reasoning processes of LLMs during patent matching. All data and codes are available at https://github.com/NEUIR/MemGraph.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{xu_aligning_2025,
  title = {Aligning the {Objective},
  author = {Xu, Junjielong and Fu, Ying and Tan, Shin Hwei and He, Pinjia},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00169},
  url = {https://doi.org/10.1109/icse55347.2025.00169},
  booktitle = {Proceedings of the {IEEE},
  pages = {2548--2560},
  publisher = {IEEE Press},
  keywords = {automated program repair, large language model, objective alignment},
  abstract = {Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10\% and reduces the patch sampling number by 90\%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{salemi_towards_2024,
  title = {Towards a {Search},
  author = {Salemi, Alireza and Zamani, Hamed},
  year = {2024},
  doi = {10.1145/3626772.3657733},
  url = {https://doi.org/10.1145/3626772.3657733},
  booktitle = {Proceedings of the 47th {International},
  pages = {741--751},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {large language model, neural ranking model, retrieval augmentation, retrieval-enhanced machine learning, text generation},
  abstract = {This paper introduces uRAG-a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{song_how_2024,
  title = {How {Much},
  author = {Song, Seok Hwan and Tavanapong, Wallapak},
  year = {2024},
  doi = {10.1145/3627673.3679840},
  url = {https://doi.org/10.1145/3627673.3679840},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {2128--2137},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {large language model, math word problem, prompt engineering, quantitative reasoning, trustworthy information extraction, source: ACM},
  abstract = {Real-world quantitative reasoning problems are complex, often including extra information irrelevant to the question (or "IR noise" for short). State-of-the-art (SOTA) prompting methods have increased the Large Language Model's ability for quantitative reasoning on grade-school Math Word Problems (MWPs). To assess how well these SOTA methods handle IR noise, we constructed four new datasets with IR noise, each consisting of 300 problems from each of the four public datasets: MAWPS, ASDiv, SVAMP, and GSM8K, with added IR noise. We called the collection of these new datasets "MPN"–Math Word Problems with IR Noise. We evaluated SOTA prompting methods using MPN. We propose Noise Reduction Prompting (NRP) and its variant (NRP+) to reduce the impact of IR noise. Findings: Our IR noise significantly degrades the performance of Chain-of-Thought (CoT) Prompting on three different backend models: ChatGPT (gpt-3.5-turbo-0613), PaLM2, and Llama3-8B-instruct. Among them, ChatGPT offers the best accuracy on MPN with and without IR noise. With IR noise, performances of CoT, Least-To-Most Prompting, Progressive-Hint Prompting, and Program-aided Language Models with ChatGPT were significantly impacted, each with an average accuracy drop of above 12\%. NRP is least impacted by the noise, with a drop in average accuracy to only around 1.9\%. Our NRP+ and NRP perform comparably in the presence of IR noise.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{kanakaris_network-informed_2025,
  title = {Network-informed {Prompt},
  author = {Kanakaris, Nikos and Ping, Heng and Xiao, Xiongye and Ahmed, Nesreen K. and Luceri, Luca and Ferrara, Emilio and Bogdan, Paul},
  year = {2025},
  doi = {10.1145/3701716.3717539},
  url = {https://doi.org/10.1145/3701716.3717539},
  booktitle = {Companion {Proceedings},
  pages = {2651--2660},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {class imbalance, disinformation spread, fake news detection, graph classification, graph-aware prompt engineering, large language models, organized disinformation campaign detection, prompt engineering, retrieval-augmented generation},
  abstract = {Detecting organized political campaigns, commonly known as astroturf campaigns, is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Their ultimate goal is to analyze the relationships and interactions (e.g. re-posting) among users and the textual similarities of their posts. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on 𝕏 (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2×-3× improvements in terms of precision, recall and F1 scores.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhu_study_2024,
  title = {A {Study},
  author = {Zhu, Guibin and Zhao, Bo and Tang, Jianbo},
  year = {2024},
  doi = {10.1145/3700297.3700326},
  url = {https://doi.org/10.1145/3700297.3700326},
  booktitle = {Proceedings of the 2024 {International},
  pages = {166--170},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, BOPPPS, Large Language Model, Personalized Teaching, Smart Teaching},
  abstract = {Smart teaching refers to the in-depth use of modern information technology to promote the process of education, which is characterized by using digital, network, intelligent and multimedia technologies. BOPPPS teaching model is a new type of student-centered teaching model. This teaching model is widely used around the world. The traditional BOPPPS teaching model makes it difficult to implement personalized teaching in the classroom. The new development of artificial intelligence technology provides new method for smart teaching, especially the AIGC Large Language Model represented by ChatGPT. This paper introduces AIGC technology into personalized teaching, and study the application of AIGC in various aspects of BOPPPS teaching model, by taking the design of smart teaching course of video surveillance as an example. In particular, AIGC is utilized for Pre-assessment. It is proposed to use an agent as a mediator between students and the AIGC large language model to test the students individually, and to design personalized test contents for personalized feedback. Comparatively better results were obtained in the actual teaching process, which can provide a reference for other smart teaching researchers.},
  address = {New York, NY, USA},
  series = {{ISAIE},
  isbn = {979-8-4007-0710-0},
}

@inproceedings{jacobs_thats_2025,
  title = {That's {Not},
  author = {Jacobs, Sven and Kempf, Maurice and Kiesler, Natalie},
  year = {2025},
  doi = {10.1145/3754508.3754512},
  url = {https://doi.org/10.1145/3754508.3754512},
  booktitle = {Proceedings of the 2025 {Conference},
  publisher = {Association for Computing Machinery},
  keywords = {Feedback, GenAI, Generative AI, Large Language Models, Programming Education, source: ACM},
  abstract = {The potential of Generative AI (GenAI) for generating feedback in computing education has been the subject of numerous studies. However, there is still limited research on how computing students engage with this feedback and to what extent it supports their problem-solving. For this reason, we built a custom web application providing students with Python programming tasks, a code editor, GenAI feedback, and compiler feedback. Via a think-aloud protocol including eye-tracking and a post-interview with 11 undergraduate students, we investigate (1) how much attention the generated feedback received from learners and (2) to what extent the generated feedback is helpful (or not). In addition, students’ attention to GenAI feedback is compared with that towards the compiler feedback. We further investigate differences between students with and without prior programming experience. The findings indicate that GenAI feedback generally receives a lot of visual attention, with inexperienced students spending twice as much fixation time. More experienced students requested GenAI less frequently, and could utilize it better to solve the given problem. It was more challenging for inexperienced students to do so, as they could not always comprehend the GenAI feedback. They often relied solely on the GenAI feedback, while compiler feedback was not read. Understanding students’ attention and perception toward GenAI feedback is crucial for developing educational tools that support student learning.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-2078-9},
  series = {{UKICER},
}

@inproceedings{wang_llm-powered_2025,
  title = {{LLM},
  author = {Wang, Tianfu and Zhan, Yi and Lian, Jianxun and Hu, Zhengyu and Yuan, Nicholas Jing and Zhang, Qi and Xie, Xing and Xiong, Hui},
  year = {2025},
  doi = {10.1145/3701716.3715244},
  url = {https://doi.org/10.1145/3701716.3715244},
  booktitle = {Companion {Proceedings},
  journal = {arXiv preprint arXiv:2503.07937},
  pages = {510--519},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {intelligent tutoring system, large language model, multi-agent, source: ACM, source: Google Scholar},
  abstract = {Intelligent Tutoring Systems (ITSs) have revolutionized education by offering personalized learning experiences. However, as goal-oriented learning, which emphasizes efficiently achieving specific objectives, becomes increasingly important in professional contexts, existing ITSs often struggle to deliver this type of targeted learning experience. In this paper, we propose GenMentor, an LLM-powered multi-agent framework designed to deliver goal-oriented, personalized learning within ITS. GenMentor begins by accurately mapping learners' goals to required skills using a fine-tuned LLM trained on a custom goal-to-skill dataset. After identifying the skill gap, it schedules an efficient learning path using an evolving optimization approach, driven by a comprehensive and dynamic profile of learners' multifaceted status. Additionally, GenMentor tailors learning content with an exploration-drafting-integration mechanism to align with individual learner needs. Extensive automated and human evaluations demonstrate GenMentor's effectiveness in learning guidance and content quality. Furthermore, we have deployed it in practice and also implemented it as an application. Practical human study with professional learners further highlights its effectiveness in goal alignment and resource targeting, leading to enhanced personalization. Supplementary resources are available at ttps://github.com/GeminiLight/gen-mentor.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{jin_i_2025,
  title = {"{I},
  author = {Jin, Seungwan and Kim, Bogoan and Han, Kyungsik},
  year = {2025},
  doi = {10.1145/3706598.3713732},
  url = {https://doi.org/10.1145/3706598.3713732},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {large language model, mental health apps, scoping review, user engagement},
  abstract = {Over the past decade, mobile apps have been widely adopted as a digital intervention method for mental health support, offering scalable and accessible solutions to address the growing global mental health challenges. However, sustaining user engagement in real-world settings remains a major challenge in the development of these applications. This study systematically examines factors that hinder user engagement in existing mobile mental health support systems through a scoping review of the literature. After an initial identification of 1,267 papers, we conducted a final analysis of 111 empirical studies using mobile app-based mental health support systems. The study investigates the main factors that negatively affect user engagement from user and system perspectives. Based on these findings, we propose guidelines for sustaining and enhancing user engagement and for structuring personalized emotional interaction design along three dimensions: adaptive, continuous, and multimodal interactions. Furthermore, we discuss the potential for integration with advanced AI methods (e.g., LLM-based generative AI agents) as a way to achieve these design implications and suggestions. Our results provide critical insights for enhancing long-term user engagement in the development of future mental health support systems.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{lubos_llm-generated_2024,
  title = {{LLM},
  author = {Lubos, Sebastian and Tran, Thi Ngoc Trang and Felfernig, Alexander and Polat Erdeniz, Seda and Le, Viet-Man},
  year = {2024},
  doi = {10.1145/3631700.3665185},
  url = {https://doi.org/10.1145/3631700.3665185},
  booktitle = {Adjunct {Proceedings},
  pages = {276--285},
  publisher = {Association for Computing Machinery},
  note = {event-place: Cagliari, Italy},
  keywords = {decision-making, explanation generation, explanations, feature-based explanations, item-based explanations, knowledge-based explanations, large language model, recommender systems},
  abstract = {Users are often confronted with situations where they have to decide in favor or against an offered item, like a book, movie, or recipe. Those suggested items are commonly determined by a recommender system, which considers personal preferences to identify relevant items. However, those systems often lack transparency and comprehensibility in revealing why a specific item is recommended. For this purpose, explanations have been added as a powerful tool to help users with their final decisions. In this paper, we present and evaluate the capabilities of a Large Language Model (LLM) to come up with high-quality explanations to further improve the support of users for three different recommendation approaches, including feature-based recommendation, collaborative filtering, and knowledge-based recommendation. We explain how an LLM can be applied to generate personalized explanations and evaluate the explanation goals in an online user study. Our findings highlight that LLM-generated explanations are highly appreciated by users as they help in the evaluation of recommended items. Furthermore, we discuss which characteristics of the LLM-based explanations were perceived positively and how those findings can be used for future research.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-0466-6},
}

@inproceedings{harvey_framework_2025,
  title = {A {Framework},
  author = {Harvey, Emma and Kizilcec, Rene F. and Koenecke, Allison},
  year = {2025},
  doi = {10.1145/3715275.3732137},
  url = {https://doi.org/10.1145/3715275.3732137},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {2025--2039},
  publisher = {Association for Computing Machinery},
  keywords = {audit, chatbot, dialect bias, large language model, quality-of-service harm},
  abstract = {Increasingly, individuals who engage in online activities are expected to interact with large language model (LLM)-based chatbots. Prior work has shown that LLMs can display dialect bias, which occurs when they produce harmful responses when prompted with text written in minoritized dialects. However, whether and how this bias propagates to systems built on top of LLMs, such as chatbots, is still unclear. We conduct a review of existing approaches for auditing LLMs for dialect bias and show that they cannot be straightforwardly adapted to audit LLM-based chatbots due to issues of substantive and ecological validity. To address this, we present a framework for auditing LLM-based chatbots for dialect bias by measuring the extent to which they produce quality-of-service harms, which occur when systems do not work equally well for different people. Our framework has three key characteristics that make it useful in practice. First, by leveraging dynamically generated instead of pre-existing text, our framework enables testing over any dialect, facilitates multi-turn conversations, and represents how users are likely to interact with chatbots in the real world. Second, by measuring quality-of-service harms, our framework aligns audit results with the real-world outcomes of chatbot use. Third, our framework requires only query access to an LLM-based chatbot, meaning that it can be leveraged equally effectively by internal auditors, external auditors, and even individual users in order to promote accountability. To demonstrate the efficacy of our framework, we conduct a case study audit of Amazon Rufus, a widely-used LLM-based chatbot in the customer service domain. Our results reveal that Rufus produces lower-quality responses to prompts written in minoritized English dialects, and that these quality-of-service harms are exacerbated by the presence of typos in prompts.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{lee_mvprompt_2025,
  title = {{MVPrompt},
  author = {Lee, ChungHa and Lee, DaeHo and Hong, Jin-Hyuk},
  year = {2025},
  doi = {10.1145/3706598.3713876},
  url = {https://doi.org/10.1145/3706598.3713876},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI Artists, Collaboration, Creativity Support Tools, Generative AI, Mise-en-scène, Music Video, Prompt Support, Text-to-Video},
  abstract = {Music videos have traditionally been the domain of experts, but with text-to-video generative AI models, AI artists can now create them more easily. However, accurately reflecting the desired music-visual mise-en-scène remains challenging without specialized knowledge, highlighting the need for supportive tools. To address this, we conducted a design workshop with seven music video experts, identified design goals, and developed MVPrompt—a tool for generating music-visual mise-en-scène prompts. In a user study with 24 AI artists, MVPrompt outperformed the Baseline, effectively supporting the collaborative creative process. Specifically, the Visual Theme stage facilitated the exploration of tone and manner, while the Visual Scene \&amp; Grammar stage refined prompts with detailed mise-en-scène elements. By enabling AI artists to specify mise-en-scène creatively, MVPrompt enhances the experience of making music video scenes with text-to-video generative AI.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{leitner_characterizing_2025,
  title = {Characterizing {Network},
  author = {Leitner, Maxyn Rose and Dorn, Rebecca and Morstatter, Fred and Lerman, Kristina},
  year = {2025},
  doi = {10.1145/3717867.3717926},
  url = {https://doi.org/10.1145/3717867.3717926},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {472--483},
  publisher = {Association for Computing Machinery},
  keywords = {Classification, Community Dynamics, Computational Social Science, Large language models (LLMs), Network analysis, Online Harassment, Retrieval-Augmented Generation (RAG)},
  abstract = {Content Warning: Trans-antagonistic Rhetoric and Terminology The recent proliferation of short form video social media sites such as TikTok has been effectively utilized for increased visibility, communication, and community connection amongst trans/nonbinary creators online. However, these same platforms have also been exploited by right-wing actors targeting trans/nonbinary people, enabling such anti-trans actors to efficiently spread hate speech and propaganda. Given these divergent groups, what are the differences in network structure between anti-trans and pro-trans communities on TikTok, and to what extent do they amplify the effects of anti-trans content? In this paper, we collect a sample of TikTok videos containing pro and anti-trans content, and develop a taxonomy of trans related sentiment to enable the classification of content on TikTok, and ultimately analyze the reply network structures of pro-trans and anti-trans communities. In order to accomplish this, we worked with hired expert data annotators from the trans/nonbinary community in order to generate a sample of highly accurately labeled data. From this subset, we utilized a novel classification pipeline leveraging Retrieval-Augmented Generation (RAG) with annotated examples and taxonomy definitions to classify content into pro-trans, anti-trans, or neutral categories. We find that incorporating our taxonomy and its logics into our classification engine results in improved ability to differentiate trans related content, and that Results from network analysis indicate many interactions between posters of pro-trans and anti-trans content exist, further demonstrating targeting of trans individuals, and demonstrating the need for better content moderation tools.},
  address = {New York, NY, USA},
  series = {Websci '25},
  isbn = {979-8-4007-1483-2},
}

@inproceedings{fang_llm_2024,
  title = {On {LLM},
  author = {Fang, Jingchao and Arechiga, Nikos and Namikoshi, Keiichi and Bravo, Nayeli and Hogan, Candice and Shamma, David A.},
  year = {2024},
  doi = {10.1145/3652988.3673967},
  url = {https://doi.org/10.1145/3652988.3673967},
  booktitle = {Proceedings of the 24th {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: GLASGOW, United Kingdom},
  keywords = {large language model, LLM, methods, persuasive conversation, synthetic data, Wizard of Oz, WoZ, source: ACM},
  abstract = {The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard “role-plays” a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs’ role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs’ role-playing ability in WoZ experiments and reveals LLMs’ behavior patterns at scale.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0625-7},
  series = {{IVA},
}

@inproceedings{pradeep_great_2025,
  title = {The {Great},
  author = {Pradeep, Ronak and Thakur, Nandan and Upadhyay, Shivani and Campos, Daniel and Craswell, Nick and Soboroff, Ian and Dang, Hoa Trang and Lin, Jimmy},
  year = {2025},
  doi = {10.1145/3726302.3730090},
  url = {https://doi.org/10.1145/3726302.3730090},
  booktitle = {Proceedings of the 48th {International},
  pages = {180--190},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {atomic facts, automatic evaluation, nugget evaluation},
  abstract = {Large Language Models (LLMs) have significantly enhanced the capabilities of information access systems, especially with retrieval-augmented generation (RAG). Nevertheless, the evaluation of RAG systems remains a barrier to continued progress, a challenge we tackle in this work by proposing an automatic evaluation framework that is validated against human annotations. We believe that the nugget evaluation methodology provides a solid foundation for evaluating RAG systems. This approach, originally developed for the TREC Question Answering (QA) Track in 2003, evaluates systems based on atomic facts that should be present in good answers. Our efforts focus on ”refactoring” this methodology, where we describe the AutoNuggetizer framework that specifically applies LLMs to both automatically create nuggets and automatically assign nuggets to system answers. In the context of the TREC 2024 RAG Track, we calibrate a fully automatic approach against strategies where nuggets are created manually or semi-manually by human assessors and then assigned manually to system answers. Based on results from a community-wide evaluation, we observe strong agreement at the run level between scores derived from fully automatic nugget evaluation and human-based variants. The agreement is stronger when individual framework components such as nugget assignment are automated independently. This suggests that our evaluation framework provides tradeoffs between effort and quality that can be used to guide the development of future RAG systems. However, further research is necessary to refine our approach, particularly in establishing robust per-topic agreement to diagnose system failures effectively.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{liu_towards_2024,
  title = {Towards {Automatic},
  author = {Liu, Lei and Yang, Xiaoyan and Li, Fangzhou and Chi, Chenfei and Shen, Yue and Lyu, Shiwei and Zhang, Ming and Ma, Xiaowei and Lv, Xiangguo and Ma, Liya and Zhang, Zhiqiang and Xue, Wei and Huang, Yiran and Gu, Jinjie},
  year = {2024},
  doi = {10.1145/3637528.3671575},
  url = {https://doi.org/10.1145/3637528.3671575},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {5466--5475},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {source: ACM},
  abstract = {Large language models (LLMs) are gaining increasing interests to improve clinical efficiency, owing to their unprecedented performance in modelling natural language. Ensuring the reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0490-1},
  series = {{KDD},
}

@inproceedings{barone_combining_2025,
  title = {Combining {Evidence},
  author = {Barone, Mariano and Romano, Antonio and Riccio, Giuseppe and Postiglione, Marco and Moscato, Vincenzo},
  year = {2025},
  doi = {10.1145/3726302.3729931},
  url = {https://doi.org/10.1145/3726302.3729931},
  booktitle = {Proceedings of the 48th {International},
  pages = {1087--1097},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {fact-checking, generative ai, healthcare, large language models},
  abstract = {Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{yen_memolet_2024,
  title = {Memolet: {Reifying},
  author = {Yen, Ryan and Zhao, Jian},
  year = {2024},
  doi = {10.1145/3654777.3676388},
  url = {https://doi.org/10.1145/3654777.3676388},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Human-AI, Memory Reuse, Retrieval Augmented Generation},
  abstract = {As users engage more frequently with AI conversational agents, conversations may exceed their “memory” capacity, leading to failures in correctly leveraging certain memories for tailored responses. However, in finding past memories that can be reused or referenced, users need to retrieve relevant information in various conversations and articulate to the AI their intention to reuse these memories. To support this process, we introduce Memolet, an interactive object that reifies memory reuse. Users can directly manipulate Memolet to specify which memories to reuse and how to use them. We developed a system demonstrating Memolet’s interaction across various memory reuse stages, including memory extraction, organization, prompt articulation, and generation refinement. We examine the system’s usefulness with an N=12 within-subject study and provide design implications for future systems that support user-AI conversational memory reusing.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{bose_prompts_2025,
  title = {From {Prompts},
  author = {Bose, Dibyendu Brinto},
  year = {2025},
  doi = {10.1145/3696630.3728702},
  url = {https://doi.org/10.1145/3696630.3728702},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1660--1665},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {code generation, large language model(LLM), property-based-testing},
  abstract = {Large Language Models (LLMs) have shown promise in automated code generation, but ensuring correctness remains a significant challenge. Traditional unit testing evaluates functional correctness but often fails to capture deeper logical constraints. We apply Property-Based Testing (PBT) as an alternative evaluation strategy to StarCoder and CodeLlama on MBPP and HumanEval. Our results reveal that while pass@k evaluation shows moderate success, PBT exposes additional correctness gaps. A significant portion of generated solutions only partially adhere to correctness properties (30–32\%), while 18–23\% fail outright. Property extraction is also imperfect, with 9–13\% of constraints missing. These findings highlight that unit test-based evaluations may overestimate solution correctness by not capturing fundamental logical errors. Our study demonstrates that combining unit testing with PBT can offer a more comprehensive assessment of generated code correctness, revealing limitations that traditional verification approaches miss.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{esmaelizadeh_integrating_2024,
  title = {On {Integrating},
  author = {Esmaelizadeh, Armin and Rorseth, Joel and Yu, Andy and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw and Taghva, Kazem},
  year = {2024},
  doi = {10.1145/3665601.3669849},
  url = {https://doi.org/10.1145/3665601.3669849},
  booktitle = {Proceedings of the {Conference},
  pages = {50--53},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {Data Science, Explainable AI, Machine Learning Model Diagnostics},
  abstract = {Herein, we advocate for the integration of the pipelines for data science (e.g., extraction, cleaning, and exploration) and machine learning (e.g., training data collection, feature selection, model selection, and parameter tuning), toward responsible and trustworthy artificial intelligence. We argue that the metadata generated by the machine-learning pipeline, which includes model outputs and model accuracy scores, is best managed and analyzed using data-science tools, thereby obtaining actionable insights into model performance, interpretability, and bias. We illustrate via two examples from our recent work as proof of concept: data summarization for model performance diagnostics; and input and output exploration to understand retrieval-augmented language models.},
  address = {New York, NY, USA},
  series = {{GUIDE},
  isbn = {979-8-4007-0694-3},
}

@inproceedings{tayal_dynamic_2024,
  title = {Dynamic {Contexts},
  author = {Tayal, Anuja and Tyagi, Aman},
  year = {2024},
  doi = {10.1145/3589335.3651905},
  url = {https://doi.org/10.1145/3589335.3651905},
  booktitle = {Companion {Proceedings},
  pages = {1338--1341},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {conversational systems, few-shot, prompting, question generation, rag, source: ACM},
  abstract = {When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0172-6},
}

@inproceedings{xiang_civil_2025,
  title = {Civil {Aviation},
  author = {Xiang, Ziyu and Luo, Yinhui and Fu, Qiang and Xu, Wenhao},
  year = {2025},
  doi = {10.1145/3716895.3716958},
  url = {https://doi.org/10.1145/3716895.3716958},
  booktitle = {Proceedings of the 5th {International},
  pages = {351--357},
  publisher = {Association for Computing Machinery},
  keywords = {LLM, LLM valuation, RAG, text chunking, vector search},
  abstract = {Efficient research on civil aviation laws using a large language model (LMM) is a hot issue in current research In this paper, we build a RAG-based model of civil aviation laws and regulations by analyzing civil aviation law-related documents. We obtain high-quality civil aviation data on civil aviation laws by combining manual and the LLM approaches, and carefully categorize the samples according to the frequency of changes in the answers to the assessment indicators, in order to observe the capability of the LLM more accurately We also evaluate and analyze mainstream and advanced Chinese LLM on our dataset. Extensive experiments and valuable insights show that the use of RAG for retrieval of civil aviation LLM is challenging and deserves further research!},
  address = {New York, NY, USA},
  series = {{ICAICE},
  isbn = {979-8-4007-1800-7},
}

@inproceedings{wang_towards_2025,
  title = {Towards {LLM},
  author = {Wang, Chenxu and Zhang, Xumiao and Lu, Runwei and Lin, Xianshang and Zeng, Xuan and Zhang, Xinlei and An, Zhe and Wu, Gongwei and Gao, Jiaqi and Tian, Chen and Chen, Guihai and Liu, Guyue and Liao, Yuhong and Lin, Tao and Cai, Dennis and Zhai, Ennan},
  year = {2025},
  doi = {10.1145/3718958.3750505},
  url = {https://doi.org/10.1145/3718958.3750505},
  booktitle = {Proceedings of the {ACM},
  pages = {496--511},
  publisher = {Association for Computing Machinery},
  note = {event-place: São Francisco Convent, Coimbra, Portugal},
  keywords = {AIOps, data center networks, incident management, large language model, network troubleshooting, root cause analysis},
  abstract = {Root causing and failure localization are critical to maintain reliability in cloud network operations. When an incident is reported, network operators must review massive volumes of monitoring data and identify the root cause (i.e., error device) as fast as possible, making it extremely challenging even for experienced operators. Large language models (LLMs) have shown great potential in text understanding and reasoning. In this paper, we present BiAn, an LLM-based framework designed to assist operators in efficient incident investigation. BiAn processes monitoring data and generates error device rankings with detailed explanations. To date, BiAn has been deployed in our network infrastructure for 10 months and it has successfully assisted operators in identifying error devices more quickly, reducing time to root causing by 20.5\% (55.2\% for high-risk incidents). Extensive performance evaluations based on 17 months of real cases further demonstrate that BiAn achieves accurate and fast failure localization. It improves accuracy by 9.2\% compared to the baseline approach.},
  address = {New York, NY, USA},
  series = {{SIGCOMM},
  isbn = {979-8-4007-1524-2},
}

@article{gubanov_cancerkg_2024,
  title = {Cancerkg. org-a web-scale, interactive, verifiable knowledge graph-llm hybrid for assisting with optimal cancer treatment and care},
  author = {Gubanov, M. and Pyayt, A. and Karolak, A.},
  year = {2024},
  doi = {10.1145/3627673.3680094},
  url = {https://doi.org/10.1145/3627673.3680094},
  booktitle = {Proceedings of the 33rd {ACM},
  journal = {Proceedings of the 33rd ACM …},
  pages = {4497--4505},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, artificial intelligence (AI), cancer, data management, LLM},
  abstract = {… the user needs better than just an LLM, KG or a search-engine in … RAG-based hybrid scales to thousands of data sources, “understands” multi-modal knowledge, does not hallucinate, …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{hofstatter_fid-light_2023,
  title = {{FiD},
  author = {Hofstätter, Sebastian and Chen, Jiecao and Raman, Karthik and Zamani, Hamed},
  year = {2023},
  doi = {10.1145/3539618.3591687},
  url = {https://doi.org/10.1145/3539618.3591687},
  booktitle = {Proceedings of the 46th {International},
  pages = {1437--1447},
  publisher = {Association for Computing Machinery},
  note = {event-place: Taipei, Taiwan},
  keywords = {fusion-in-decoder, kilt, retrieval augmented generation},
  abstract = {Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {978-1-4503-9408-6},
}

@inproceedings{chong_llm-net_2025,
  title = {{LLM},
  author = {Chong, Zan-Kai and Ohsaki, Hiroyuki and Ng, Bryan},
  year = {2025},
  doi = {10.1145/3731806.3731827},
  url = {https://doi.org/10.1145/3731806.3731827},
  booktitle = {Proceedings of the 2025 14th {International},
  pages = {313--320},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {The centralization of Large Language Models (LLMs) development has created significant barriers to AI advancement, limiting the democratization of these powerful technologies. This centralization, coupled with the scarcity of high-quality training data and mounting complexity in maintaining comprehensive expertise across rapidly expanding knowledge domains, poses critical challenges to the continued growth of LLMs. While solutions like Retrieval-Augmented Generation (RAG) offer potential remedies, maintaining up-to-date expert knowledge across diverse domains remains a significant challenge, particularly given the exponential growth of specialized information. This paper introduces LLM Networks (LLM-Net), a blockchain-based framework that democratizes LLMs-as-a-Service through a decentralized network of specialized LLM providers. By leveraging collective computational resources and distributed domain expertise, LLM-Net incorporates fine-tuned expert models for various specific domains, ensuring sustained knowledge growth while maintaining service quality through collaborative prompting mechanisms. The framework’s robust design includes blockchain technology for transparent transaction and performance validation, establishing an immutable record of service delivery. Our simulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet, Llama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the reputation-based mechanism in maintaining service quality by selecting high-performing respondents (LLM providers). Thereby, the results demonstrate the potential of LLM-Net to sustain AI advancement through the integration of decentralized expertise and blockchain-based accountability.},
  address = {New York, NY, USA},
  series = {{ICSCA},
  isbn = {979-8-4007-1012-4},
}

@inproceedings{fang_cross-cultural_2025,
  title = {A {Cross},
  author = {Fang, Yu and Huang, Shihong and Ogan, Amy},
  year = {2025},
  doi = {10.1145/3706468.3706528},
  url = {https://doi.org/10.1145/3706468.3706528},
  booktitle = {Proceedings of the 15th {International},
  pages = {473--483},
  publisher = {Association for Computing Machinery},
  keywords = {Affective computing, Confusion, Cross-cultural models, Retrieval-augmented generation},
  abstract = {In traditional lecture delivery setting, it is very challenging to identify which part of the lecture material that students are struggling with. One approach to identify difficult concepts is to capture students’ confusion during class time. However, most existing confusion detectors focus on an individual student rather than a classroom, and only on a single ethnicity group which could propagate bias when developing pedagogical technologies. In this paper, we leverage two existing ‘Confused’ facial expression datasets (DAiSEE and DevEmo) with an East Asian ‘Confused’ facial expression dataset that we collected. Through model performance and explainableAI, we address potential cultural biases in detecting emotions, particularly in confusion, and identified culturally-specific features that align with prior research. As a proof-of-concept, we deployed this cross-cultural confusion machine learning model in a live semester-long class. This work to integrate cross-cultural facial features highlights the importance of fostering inclusivity in educational technologies.},
  address = {New York, NY, USA},
  series = {{LAK},
  isbn = {979-8-4007-0701-8},
}

@inproceedings{shailya_lext_2025,
  title = {{LExT},
  author = {Shailya, Krithi and Rajpal, Shreya and Krishnan, Gokul S and Ravindran, Balaraman},
  year = {2025},
  doi = {10.1145/3715275.3732104},
  url = {https://doi.org/10.1145/3715275.3732104},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1565--1587},
  publisher = {Association for Computing Machinery},
  keywords = {Evaluation, Explanations, Faithfulness, Healthcare, Language Models, Plausibility, Trustworthiness},
  abstract = {As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{li_corpuslm_2024,
  title = {{CorpusLM},
  author = {Li, Xiaoxi and Dou, Zhicheng and Zhou, Yujia and Liu, Fangchao},
  year = {2024},
  doi = {10.1145/3626772.3657778},
  url = {https://doi.org/10.1145/3626772.3657778},
  booktitle = {Proceedings of the 47th {International},
  pages = {26--37},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {generative retrieval, knowledge-intensive language tasks, rag, source: Scopus},
  abstract = {Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose CorpusLM, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
  annote = {Cited by: 9},
}

@inproceedings{guo_bkrag_2024,
  title = {{BKRAG},
  author = {Guo, Jun and Chen, Bojian and Zhao, Zhichao and He, Jindong and Chen, Shichun and Hu, Donglan and Pan, Hao},
  year = {2024},
  doi = {10.1145/3689218.3689224},
  url = {https://doi.org/10.1145/3689218.3689224},
  booktitle = {Proceedings of the 2024 6th {International},
  pages = {14--20},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, Hong Kong},
  keywords = {Power project, RAG, requirements similarity analysis, Rerank model},
  abstract = {This paper proposes an innovative method called BKRAG (A BGE Reranker Retrieval Augmented Generation for similarity analysis of power project requirements), which integrates information retrieval techniques and NLP to achieve automated analysis and similarity evaluation of power project requirements. The core of the BKRAG method lies in the utilization of a Rerank model to re-rank the initially retrieved candidate documents, improving their semantic matching degree with user queries, thereby optimizing the results of requirements similarity analysis. In this paper, we elaborate on the construction principles and workflow of the BKRAG method and verify its effectiveness through a series of experiments. Results demonstrate that the BKRAG can significantly improve the retrieval accuracy of power project requirement documents and the performance of requirements similarity analysis. The research findings of this paper not only provide a new solution for the field of power project requirements analysis, but also offer new insights into the cross-application of information retrieval and natural language processing technologies.},
  address = {New York, NY, USA},
  series = {{PRIS},
  isbn = {979-8-4007-1825-0},
}

@inproceedings{strubberg_ux_2025,
  title = {From {UX},
  author = {Strubberg, Brandon C. and Blackburn, Hayley},
  year = {2025},
  doi = {10.1145/3711670.3764640},
  url = {https://doi.org/10.1145/3711670.3764640},
  booktitle = {Proceedings of the 43rd {ACM},
  pages = {189--194},
  publisher = {Association for Computing Machinery},
  keywords = {digital literacies, Generative artificial intelligence, retrieval augmented generation, technical communication, user experience, source: ACM},
  abstract = {This experience report reflects on a collaborative effort to examine how generative artificial intelligence chatbots (gen AI chatbots) can support technical communication (TC) students in developing gen AI and user experience (UX) literacies aligned with emerging professional practices. Upper-division students in a UX research course investigated the usability and design of a custom-built chatbot that had been integrated into a general education TC course as a learning assistant. Students engaged with the chatbot both as users and evaluators, learning from the developer and analyzing its underlying architecture, training corpus, and decision-making processes. This report reviews both the chatbot's design and development, as well as the details of the UX course, and posits that giving students active roles in the design, testing, and critique of gen AI tools fosters durable skills that bridge the academic and professional spheres.},
  address = {New York, NY, USA},
  series = {{SIGDOC},
  isbn = {979-8-4007-1444-3},
}

@inproceedings{shi_retrieval-enhanced_2024,
  title = {Retrieval-enhanced {Knowledge},
  author = {Shi, Yucheng and Tan, Qiaoyu and Wu, Xuansheng and Zhong, Shaochen and Zhou, Kaixiong and Liu, Ninghao},
  year = {2024},
  doi = {10.1145/3627673.3679722},
  url = {https://doi.org/10.1145/3627673.3679722},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {2056--2066},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {model editing, question answering, retrieval-augmented generation, source: ACM, source: Scopus},
  abstract = {Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions, since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that traditional similarity-based searches might miss. In addition, our framework includes a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge. Our code is available at: https://github.com/sycny/RAE.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
  annote = {Cited by: 4; All Open Access; Gold Open Access},
}

@inproceedings{wang_how_2025,
  title = {How {LLM},
  author = {Wang, Yibo and Hou, Yunan and Lai, Zeqi and Li, Hewu and Wu, Qian and Liu, Jun and Li, Yuanjie and Xie, Xin and Han, Zhifeng},
  year = {2025},
  doi = {10.1145/3748749.3749084},
  url = {https://doi.org/10.1145/3748749.3749084},
  booktitle = {Proceedings of the 2025 3rd {Workshop},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  note = {event-place: Coimbra, Portugal},
  keywords = {Large Language Model (LLM), LEO Satellite Network (LSN), Network Experiment Reproduction, Network Simulation},
  abstract = {Reproducing network experiments is critical to advancing the research in computer networks. However, in reality many researchers often struggle with experiment reproduction: not only because reading, understanding, and debugging prior work is time-consuming and labor-intensive, but also because not all papers publicly release their code, thereby forcing subsequent researchers to re-implement experiments from scratch.In this paper, we explore an intriguing question: can recent large language models (LLMs) assist in understanding research papers and generating code, thereby accelerating the reproduction of network experiments? Focusing on the rapidly evolving area of low-Earth-orbit (LEO) satellite networks (LSN), we present LASER1, a semi-automated, LLM-assisted tool designed to facilitate the reproduction of LSN experiments. LASER judiciously integrates the capabilities of LLM with LSN simulation to ease the burden of LSN experimentation. Our case studies provide preliminary evidence that LASER can efficiently reproduce experimental results consistent with those reported in the original papers, while substantially reducing the manual effort required by LSN researchers},
  address = {New York, NY, USA},
  series = {{LEO},
  isbn = {979-8-4007-2090-1},
}

@inproceedings{birillo_one_2024,
  title = {One {Step},
  author = {Birillo, Anastasiia and Artser, Elizaveta and Potriasaeva, Anna and Vlasov, Ilya and Dzialets, Katsiaryna and Golubev, Yaroslav and Gerasimov, Igor and Keuning, Hieke and Bryksin, Timofey},
  year = {2024},
  doi = {10.1145/3699538.3699556},
  url = {https://doi.org/10.1145/3699538.3699556},
  booktitle = {Proceedings of the 24th {Koli},
  publisher = {Association for Computing Machinery},
  keywords = {Generative AI, in-IDE learning, LLMs, Next-Step Hints, Programming Education},
  abstract = {Students often struggle with solving programming problems when learning to code, especially when they have to do it online, with one of the most common disadvantages of working online being the lack of personalized help. This help can be provided as next-step hint generation, i.e., showing a student what specific small step they need to do next to get to the correct solution. There are many ways to generate such hints, with large language models (LLMs) being among the most actively studied right now. While LLMs constitute a promising technology for providing personalized help, combining them with other techniques, such as static analysis, can significantly improve the output quality. In this work, we utilize this idea and propose a novel system to provide both textual and code hints for programming tasks. The pipeline of the proposed approach uses a chain-of-thought prompting technique and consists of three distinct steps: (1) generating subgoals — a list of actions to proceed with the task from the current student’s solution, (2) generating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. During the second step, we apply static analysis to the generated code to control its size and quality. The tool is implemented as a modification to the open-source JetBrains Academy plugin, supporting students in their in-IDE courses. To evaluate our approach, we propose a list of criteria for all steps in our pipeline and conduct two rounds of expert validation. Finally, we evaluate the next-step hints in a classroom with 14 students from two universities. Our results show that both forms of the hints — textual and code — were helpful for the students, and the proposed system helped them to proceed with the coding tasks.},
  address = {New York, NY, USA},
  series = {Koli {Calling},
  isbn = {979-8-4007-1038-4},
}

@inproceedings{liu_tigervector_2025,
  title = {{TigerVector},
  author = {Liu, Shige and Zeng, Zhifang and Chen, Li and Ainihaer, Adil and Ramasami, Arun and Chen, Songting and Xu, Yu and Wu, Mingxi and Wang, Jianguo},
  year = {2025},
  doi = {10.1145/3722212.3724456},
  url = {https://doi.org/10.1145/3722212.3724456},
  booktitle = {Companion of the 2025 {International},
  pages = {553--565},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {graph database, retrieval-augmented generation, vector database, source: ACM},
  abstract = {In this paper, we introduce TigerVector, a system that integrates vector search and graph query within TigerGraph, a Massively Parallel Processing (MPP) native graph database. We extend the vertex attribute type with the embedding type. To support fast vector search, we devise an MPP index framework that interoperates efficiently with the graph engine. The graph query language GSQL is enhanced to support vector type expressions and enable query compositions between vector search results and graph query blocks. These advancements elevate the expressive power and analytical capabilities of graph databases, enabling seamless fusion of unstructured and structured data in ways previously unattainable. Through extensive experiments, we demonstrate TigerVector's hybrid search capability, scalability, and superior performance compared to other graph databases (including Neo4j and Amazon Neptune) and a highly optimized specialized vector database (Milvus). TigerVector was integrated into TigerGraph v4.2, the latest release of TigerGraph, in December 2024.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{zhang_eclass_2025,
  title = {℡{EClass},
  author = {Zhang, Yunyi and Yang, Ruozhen and Xu, Xueqiang and Li, Rui and Xiao, Jinfeng and Shen, Jiaming and Han, Jiawei},
  year = {2025},
  doi = {10.1145/3696410.3714940},
  url = {https://doi.org/10.1145/3696410.3714940},
  booktitle = {Proceedings of the {ACM},
  pages = {2032--2042},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {hierarchical text classification, large language model, taxonomy enrichment, weakly-supervised text classification},
  abstract = {Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental web text mining task with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose ℡EClass, \&lt;u\&gt;T\&lt;/u\&gt;axonomy \&lt;u\&gt;E\&lt;/u\&gt;nrichment and \&lt;u\&gt;L\&lt;/u\&gt;LM-\&lt;u\&gt;E\&lt;/u\&gt;nhanced weakly-supervised hierarchical text \&lt;u\&gt;Class\&lt;/u\&gt;ification, which combines the general knowledge of LLMs and task-specific features mined from an unlabeled corpus. ℡EClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that ℡EClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{haag_last_2025,
  title = {The {Last},
  author = {Haag, David and Kumar, Devender and Gruber, Sebastian and Hofer, Dominik P., MSc and Sareban, Mahdi and Treff, Gunnar and Niebauer, Josef and Bull, Christopher N and Schmidt, Albrecht and Smeddinck, Jan David},
  year = {2025},
  doi = {10.1145/3706598.3713307},
  url = {https://doi.org/10.1145/3706598.3713307},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {adaptive interventions, context-aware computing, digital health, generative AI, healthcare AI, human-AI interaction, JITAIs, just-in-time adaptive interventions, large language models, LLMs, source: ACM},
  abstract = {We evaluated the viability of using Large Language Models (LLMs) to trigger and personalize content in Just-in-Time Adaptive Interventions (JITAIs) in digital health. As an interaction pattern representative of context-aware computing, JITAIs are being explored for their potential to support sustainable behavior change, adapting interventions to an individual's current context and needs. Challenging traditional JITAI implementation models, which face severe scalability and flexibility limitations, we tested GPT-4 for suggesting JITAIs in the use case of heart-healthy activity in cardiac rehabilitation. Using three personas representing patients affected by CVD with varying severeness and five context sets per persona, we generated 450 JITAI decisions and messages. These were systematically evaluated against those created by 10 laypersons (LayPs) and 10 healthcare professionals (HCPs). GPT-4-generated JITAIs surpassed human-generated intervention suggestions, outperforming both LayPs and HCPs across all metrics (i.e., appropriateness, engagement, effectiveness, and professionalism). These results highlight the potential of LLMs to enhance JITAI implementations in personalized health interventions, demonstrating how generative AI could revolutionize context-aware computing.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@article{abedu_llm-based_2024,
  title = {Llm-based chatbots for mining software repositories: {Challenges},
  author = {Abedu, S. and Abdellatif, A. and Shihab, E.},
  year = {2024},
  doi = {10.1145/3661167.3661218},
  url = {https://doi.org/10.1145/3661167.3661218},
  booktitle = {Proceedings of the 28th {International},
  journal = {… of the 28th International Conference on …},
  pages = {201--210},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Conversational Development Assistant, Large Language Model, Software Chatbots},
  abstract = {… To prevent the LLM from hallucinating, we instructed it to respond with “I don’t … LLM in answering questions related to software repositories. Therefore, we build a chatbot using the RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{EASE},
  isbn = {979-8-4007-1701-7},
}

@inproceedings{lajewska_explainability_2024,
  title = {Explainability for {Transparent},
  author = {Łajewska, Weronika and Spina, Damiano and Trippas, Johanne and Balog, Krisztian},
  year = {2024},
  doi = {10.1145/3626772.3657768},
  url = {https://doi.org/10.1145/3626772.3657768},
  booktitle = {Proceedings of the 47th {International},
  pages = {1040--1050},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {conversational information-seeking, explainable ai, source: ACM},
  abstract = {The increasing reliance on digital information necessitates advancements in conversational search systems, particularly in terms of information transparency. While prior research in conversational information-seeking has concentrated on improving retrieval techniques, the challenge remains in generating responses useful from a user perspective. This study explores different methods of explaining the responses, hypothesizing that transparency about the source of the information, system confidence, and limitations can enhance users' ability to objectively assess the response. By exploring transparency across explanation type, quality, and presentation mode, this research aims to bridge the gap between system-generated responses and responses verifiable by the user. We design a user study to answer questions concerning the impact of (1) the quality of explanations enhancing the response on its usefulness and (2) ways of presenting explanations to users. The analysis of the collected data reveals lower user ratings for noisy explanations, although these scores seem insensitive to the quality of the response. Inconclusive results on the explanations presentation format suggest that it may not be a critical factor in this setting.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{jin_intentiongpt_2025,
  title = {{IntentionGPT},
  author = {Jin, Yilun and Wu, Chunqi and Zhang, Ying and Shi, Chenlong and Li, Zhao and Yin, Wei and Pan, Xuming},
  year = {2025},
  doi = {10.1145/3701716.3718370},
  url = {https://doi.org/10.1145/3701716.3718370},
  booktitle = {Companion {Proceedings},
  pages = {3053--3057},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {intent classification, retrieval-augmented generation, vision language model, source: ACM},
  abstract = {In recent years, Vision Language Models (VLMs) have significantly advanced multi-modal reasoning and generation tasks, offering promising solutions for e-commerce applications like multi-modal user intent recognition. However, VLMs trained on open-domain datasets often struggle with understanding fine-grained e-commerce semantics. This limitation stems primarily from two key factors: the scarcity of labeled multi-modal data for training and the lack of domain-specific knowledge to effectively reason. To overcome the above problems, we introduce IntentionGPT, a novel framework for multi-modal user intent recognition in e-commerce with limited labeled data. Specifically, we first propose a self-data augmentation method to generate diverse synthetic samples from minimal seed labeled samples to tackle data scarcity challenge. Meanwhile, a new collaborative filtering mechanism is proposed to filter out noise samples to enhance data quality. Second, a structure-aware retrieval method is proposed for retrieving domain knowledge to enhance the model's multi-modal reasoning ability. Third, the model soups method are utilized to fuse models from different training stages, combining their strengths and mitigating potential biases for robust performance. This work systematically addresses both data scarcity and domain adaptation challenges, and achieves first place in the WWW2025 multi-modal dialogue system intent recognition challenge.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{gao_fast_2025,
  title = {Fast {State},
  author = {Gao, Shiwei and Chen, Youmin and Shu, Jiwu},
  year = {2025},
  doi = {10.1145/3689031.3696072},
  url = {https://doi.org/10.1145/3689031.3696072},
  booktitle = {Proceedings of the {Twentieth},
  pages = {128--143},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {LLM, machine learning system, state management},
  abstract = {The growing complexity of LLM usage today, e.g., multi-round conversation and retrieval-augmented generation (RAG), makes contextual states (i.e., KV cache) reusable across user requests. Given the capacity constraints of GPU memory, only a limited number of contexts can be cached on GPU for reusing. Existing inference systems typically evict part of the KV cache and restore it by recomputing it from the original tokens or offloading it to host storage for later retrieval, both of which introduce substantial computational or I/O overheads.We propose HCache, a novel LLM state restoration method. Its key idea is to restore LLM states from intermediate activations and thus utilize computational and I/O resources with low overhead. We enhance HCache with two techniques, including i) a bubble-free restoration scheduler that integrates resource-complementary methods to optimize the balance between computation and IO tasks; and ii) a chunk-based storage manager to address the layout mismatch issue (i.e., layer-before-token saving versus token-before-layer restoration). Our evaluations, conducted using real-world tasks, show that HCache reduces the TTFT by up to 1.93× compared to KV offload while consuming 1.92-2.40× less storage space; compared to token recomputation, HCache achieves up to 5.73× reduction in TTFT.},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{tian_towards_2025,
  title = {Towards {Explainable},
  author = {Tian, Xianyang and Xu, Xiang and Wang, Chao and Ruan, Tong and Wu, Baohua and Que, Maofei and Ni, Shenghua and Zhuang, Zhuoran and Liu, Jingping},
  year = {2025},
  doi = {10.1145/3701716.3715264},
  url = {https://doi.org/10.1145/3701716.3715264},
  booktitle = {Companion {Proceedings},
  pages = {476--484},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {explainable search, large language model, relevance analysis},
  abstract = {Search result explanations are essential in E-commerce, helping users understand the relevance of the returned results. Existing methods primarily focus on explaining relevance based on either product content or behavioral data. However, we argue that combining both content and behavior data can provide more comprehensive and accurate explanations. In this paper, we propose a novel approach to generate relevance explanations. First, we utilize the content data to train a domain-specific large language model (LLM) that generates relevance labels and reasoning processes for queries and items. Then, we introduce the BehaviorRAG framework to retrieve behavioral data related to queries and items, allowing the model to generate explainable reasons for their relevance. Finally, the LLM integrates outputs from both the content- and behavior-based modules to produce a final explanation. To evaluate the effectiveness of our methods, we conduct extensive experiments on both our built dataset and publicly available datasets. The results demonstrate that our method outperforms current state-of-the-art baselines in predicting relevance and generating explainable reasons. Furthermore, online A/B testing on the Fliggy app demonstrates that incorporating explanations generated by our approach into the search results leads to a 2.30\% increase in the Unique Visitors List to Order. The codes are publicly available at https://github.com/tonnyaudio/explainable.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{wang_rcagent_2024,
  title = {{RCAgent},
  author = {Wang, Zefan and Liu, Zichuan and Zhang, Yingying and Zhong, Aoxiao and Wang, Jihong and Yin, Fengbin and Fan, Lunting and Wu, Lingfei and Wen, Qingsong},
  year = {2024},
  doi = {10.1145/3627673.3680016},
  url = {https://doi.org/10.1145/3627673.3680016},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4966--4974},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {cloud systems, large language model, root cause analysis},
  abstract = {Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA–predicting root causes, solutions, evidence, and responsibilities–and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{liu_improving_2025,
  title = {Improving {AI},
  author = {Liu, Rongxin and Zhao, Julianna and Xu, Benjamin and Perez, Christopher and Zhukovets, Yuliia and Malan, David J.},
  year = {2025},
  doi = {10.1145/3641554.3701945},
  url = {https://doi.org/10.1145/3641554.3701945},
  booktitle = {Proceedings of the 56th {ACM},
  pages = {715--721},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {ai, artificial intelligence, generative ai, large language models, llms, source: ACM},
  abstract = {In 2023, we developed and deployed AI-based tools in CS50 at Harvard University to provide students with 24/7 interactive assistance, approximating a 1:1 teacher-to-student ratio. These tools offer code explanations, style suggestions, and responses to course-related inquiries, emulating human educators to foster critical thinking. However, maintaining alignment with instructional goals is challenging, especially with frequent updates to the underlying large language models (LLMs). We thus propose a continuous improvement process for LLM-based systems using a collaborative human-in-the-loop approach. We introduce a systematic evaluation framework for assessing and refining the performance of AI-based tutors, combining human-graded and model-graded evaluations. Using few-shot prompting and fine-tuning, we aim to ensure our AI tools adopt pedagogically sound teaching styles. Fine-tuning with a small, high-quality dataset has shown significant improvements in aligning with teaching goals, as confirmed through multi-turn conversation evaluations. Additionally, our framework includes a model-evaluation backend that teaching assistants periodically review, ensuring the AI system remains effective and aligned with instructional objectives. This paper offers insights into our methods and the impact of these AI tools on CS50 and contributes to the discourse on AI in education, showcasing scalable, personalized learning enhancements.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0531-1},
  series = {{SIGCSETS},
}

@inproceedings{liang_kag_2025,
  title = {{KAG},
  author = {Liang, Lei and Bo, Zhongpu and Gui, Zhengke and Zhu, Zhongshu and Zhong, Ling and Zhao, Peilong and Sun, Mengshu and Zhang, Zhiqiang and Zhou, Jun and Chen, Wenguang and Zhang, Wen and Chen, Huajun},
  year = {2025},
  doi = {10.1145/3701716.3715240},
  url = {https://doi.org/10.1145/3701716.3715240},
  booktitle = {Companion {Proceedings},
  pages = {334--343},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {information retrieval, kbqa, knowledge graph, knowledge reasoning, rag, source: Scopus},
  abstract = {The recently developed Retrieval-Augmented Generation (RAG) technology has enabled the efficient construction of domain-specific applications. The key technologies of RAG are retrieval based on similarity and reasoning based on next-token prediction. However, this approach differs significantly from how humans solve problems. Humans typically follow certain analytical logic, reasoning while retrieving relevant information, and then connecting the clues to serve as references, ultimately generating an answer. In this process, the focus is on the semantic type and clear relationships between the keywords rather than similarity and co-occurrence. This difference in methodology results in the answers generated by RAG technology being insufficiently accurate or valuable.In this work, we concentrate on establishing semantic relationships between keywords to enable a more precise expression of knowledge and propose the Knowledge Augmented Generation(KAG) framework. KAG performs semantic parsing and reasoning on both documents and questions, involving three specific strategies: In the indexing phase, we complete the semantic information of keywords and the semantic relationships between them through information extraction and semantic reasoning; in the reasoning phase of question answering, we leverage semantic parsing to transform questions into Logical Forms with clear semantic types and relationships; in the retrieval phase, we predict the semantic relationships between Logical Form elements and structured index, thereby obtaining the required references.We compared KAG with existing RAG methods in three multi-hop QA datasets and the results show that KAG significantly outperforms existing methods, achieving a new state-of-the-art. We also applied KAG to real E-Government Q\&amp;A business scenario, and achieving significant improvements in professionalism compared to traditional RAG methods. Meanwhile, to help developers easily build accurate and efficient domain knowledge QA services, our KAG natively supports the open-source KG engine OpenSPG.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
  annote = {Cited by: 6; All Open Access; Gold Open Access},
}

@inproceedings{liu_dual_2025,
  title = {The {Dual},
  author = {Liu, Haiyun and Xue, Jiahao and Zhao, Shangqing and Liu, Yao and Lu, Zhuo},
  year = {2025},
  doi = {10.1145/3733965.3733972},
  url = {https://doi.org/10.1145/3733965.3733972},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {20--25},
  publisher = {Association for Computing Machinery},
  note = {event-place: USA},
  keywords = {adversarial attacks, large language model (llm), network security},
  abstract = {Large language models (LLMs) have profoundly shaped various domains, including several types of network systems. With their powerful capabilities, LLMs have recently been proposed to enhance network security. However, the development of LLMs can introduce new risks due to their potential vulnerabilities and misuse. In this paper, we are motivated to review the dual role of LLMs in network security. Our goal is to explore how LLMs impact network security and ultimately shed light on how to evaluate LLMs from a network security perspective. We further discuss several future research directions regarding how to scientifically enable LLMs to assist with network security.},
  address = {New York, NY, USA},
  series = {{WiseML},
  isbn = {979-8-4007-1531-0},
}

@inproceedings{li_accurate_2025,
  title = {Accurate {Insights},
  author = {Li, Haoran and Cheng, Xusen and Zhang, Xiaoping},
  year = {2025},
  doi = {10.1145/3706598.3713526},
  url = {https://doi.org/10.1145/3706598.3713526},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Clinical Decision Support Systems, Graph-based Retrieval-Augmented Generation (Graph RAG), Human-Computer Interaction, Large Medical Language Models, Multi-Agent Systems},
  abstract = {Healthcare question-answering (QA) systems can assist physicians in making medical decisions. However, traditional medical QA systems struggle with multi-agents interaction and domain-specific knowledge processing, thereby reducing the accuracy and credibility of clinical decision-making. We thus develop a multi-agent decision-making system by combining a fine-tuned medical model, biomedical knowledge graphs, and PubMed data. By summarizing the symptoms described by users, our system can automatically convene clinical experts from various fields, retrieve domain knowledge, and provide clinical decision support for users. We have validated the system performance using both technical and user-centric approaches in terms of information accuracy, user satisfaction, user trust, ect. We thus provide an effective tool for healthcare professionals to make accurate and timely decisions. Furthermore, this study also reveals new design and research opportunities, including (1) optimizing multi-agent collaboration mechanisms for more complex medical decision-making, (2) improving interaction design to enhance system transparency and explainability, and (3) expanding the system to support a broader range of medical issues and multimodal data.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{wei_garag_2024,
  title = {{GARAG},
  author = {Wei, Zizhong and Huang, Dengrong and Zhang, Jichen and Song, Chen and Zhang, Sijia and Zhang, Jianing and Li, Zhaochuan and Jiang, Kai and Li, Rui and Duan, Qiang},
  year = {2024},
  doi = {10.1145/3695080.3695156},
  url = {https://doi.org/10.1145/3695080.3695156},
  booktitle = {Proceedings of the 2024 {International},
  pages = {442--447},
  publisher = {Association for Computing Machinery},
  note = {event-place: Dali, China},
  keywords = {source: ACM, source: Scopus},
  abstract = {Large language models (LLMs) possess exceptional capabilities, but their effectiveness is limited by their knowledge base. Retrieval-Augmented Generation (RAG) techniques integrate external knowledge to enhance performance in question-answering tasks. However, RAG techniques may encounter difficulties in dealing with irrelevant content, complex queries, as well as accurately evaluating the importance and relevance of information. To address these challenges, we developed an optimization question-answering system General Adaptive RAG (GARAG), specifically tailored for question-and-answer tasks. Our system utilizes a retrieval agent to identify relevant content and a generation agent to construct answers based on the question and retrieved information. Additionally, we incorporated modules for question complexity measurement, question correction, query transformation, query routing, and adaptive retrieval to improve intent recognition and document retrieval. We implemented an answer corrector and discriminator, as well as a self-reflection mechanism, to enhance output quality and factuality. Empirical experiments conducted on real-world questions validate the effectiveness of our question-and-answer system.},
  address = {New York, NY, USA},
  series = {{ICCBD},
  isbn = {979-8-4007-1022-3},
  annote = {Cited by: 2},
}

@inproceedings{zheng_evaluating_2025,
  title = {Evaluating {Non},
  author = {Zheng, Qingxiao and Chen, Minrui and Park, Hyanghee and Xu, Zhongwei and Huang, Yun},
  year = {2025},
  doi = {10.1145/3706598.3714219},
  url = {https://doi.org/10.1145/3706598.3714219},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {End-User AI Creation Tool, Generative AI, Large language models, Public Service, User Experience, source: ACM},
  abstract = {Public libraries in the U.S. are increasingly facing labor shortages, tight budgets, and overworked staff, creating a pressing need for conversational agents to assist patrons. The democratization of generative AI has empowered public service professionals to develop AI agents by leveraging large language models. To understand the needs of non-AI library professionals in creating their own conversational agents, we conducted semi-structured interviews with library professionals (n=11) across the U.S. Insights from these interviews informed the design of AgentBuilder, a prototype tool that enables non-AI experts to create conversational agents without coding skills. We then conducted think-aloud sessions and follow-up interviews to evaluate the prototype experience and identify the key evaluation criteria emphasized by library professionals (n=12) when developing conversational agents. Our findings highlight how these professionals perceive the prototype experience and reveal five essential evaluation criteria: interpreting user intent, faithful paraphrasing, proper alignment with authoritative sources, tailoring the tone of voice, and handling unknown answers effectively. These insights provide valuable guidance for designing AI-supported "end-user creation tools" in public service domains beyond libraries.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1394-1},
  series = {{CHI},
}

@inproceedings{liu_teaching_2024,
  title = {Teaching {CS50},
  author = {Liu, Rongxin and Zenke, Carter and Liu, Charlie and Holmes, Andrew and Thornton, Patrick and Malan, David J.},
  year = {2024},
  doi = {10.1145/3626252.3630938},
  url = {https://doi.org/10.1145/3626252.3630938},
  booktitle = {Proceedings of the 55th {ACM},
  pages = {750--756},
  publisher = {Association for Computing Machinery},
  note = {event-place: Portland, OR, USA},
  keywords = {ai, artificial intelligence, generative ai, large language models, llms, source: ACM},
  abstract = {In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had "a personal tutor.” Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0423-9},
  series = {{SIGCSE},
}

@inproceedings{ju_toward_2025,
  title = {Toward {Affective},
  author = {Ju, Hyojin and Lee, Jungeun and Yang, Seungwon and Ok, Jungseul and Hwang, Inseok},
  year = {2025},
  doi = {10.1145/3706598.3714122},
  url = {https://doi.org/10.1145/3706598.3714122},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Empathy, Large Language Model, Microaggression, Personalized Analogy Generation},
  abstract = {The importance of empathy cannot be overstated in modern societies where people of diverse backgrounds increasingly interact together. The HCI community has strived to foster affective empathy through immersive technologies. Many previous techniques are built upon a premise that presenting the same experience as-is may help evoke the same emotion, which however faces limitations in matters where the emotional responses largely differ across individuals. In this paper, we present a novel concept of generating a personalized experience based on a large language model (LLM) to facilitate affective empathy between individuals despite their differences. As a case study to showcase its effectiveness, we developed EmoSync, an LLM-based agent that generates personalized analogical microaggression situations, facilitating users to personally resonate with a specific microaggression situation of another person. EmoSync is designed and evaluated along a 3-phased user study with 100+ participants. We comprehensively discuss implications, limitations, and possible applications.Disclaimer: Readers may find content of a discriminative or stereotypical nature, which is inevitable given this work’s theme.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{zheng_revolutionizing_2025,
  title = {Revolutionizing {Database},
  author = {Zheng, Yihang and Li, Bo and Lin, Zhenghao and Luo, Yi and Zhou, Xuanhe and Lin, Chen and Li, Guoliang and Su, Jinsong},
  year = {2025},
  doi = {10.1145/3711896.3737405},
  url = {https://doi.org/10.1145/3711896.3737405},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5960--5971},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {ai4db, benchmark, database qa, llm, source: ACM},
  abstract = {The development of Large Language Models (LLMs) has revolutionized QA across various industries, including the database domain. However, there lacks a thorough evaluation regarding the capabilities of different LLMs in database QA. To this end, we introduce DQABench, the first comprehensive database QA benchmark for LLMs. DQABench features an innovative LLM-based method to automate the generation, cleaning, and rewriting of evaluation dataset, resulting in over 200,000 QA pairs in English and Chinese. These QA pairs cover a wide range of database-specific knowledge extracted from manuals, online communities, and DB instances, allowing for assessment of LLMs' Retrieval-Augmented Generation (RAG) and Tool Invocation Generation (TIG) capabilities in the database QA task. Furthermore, we propose a highly modular and scalable testbed DQATestbed, with basic and advanced components such as Fine-tuning, Question Classification Routing (QCR), RAG, TIG, and Prompt Template Engineering (PTE). Finally, we provide an evaluation pipeline that computes various metrics throughout a standardized evaluation process to ensure the accuracy and fairness. Our evaluation reveals the strengths and limitations of nine open-source and commercial LLMs, and the impact of various service components (e.g., fine-tuning, QCR, RAG, TIG). The proposed benchmark dataset is available at https://github.com/XMUDM/DQABench.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{sun_multimodal_2025,
  title = {A {Multimodal},
  author = {Sun, Yongqian and Zheng, Tinghua and Wen, Xidao and Kuang, Weihua and Liu, Heng and Zhang, Shenglin and Shen, Chao and Wu, Bo and Pei, Dan},
  year = {2025},
  doi = {10.1145/3696630.3728561},
  url = {https://doi.org/10.1145/3696630.3728561},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {378--388},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {anomaly detection, failure triage, LLMs, RAG, root cause analysis, software change},
  abstract = {Frequent changes in large-scale online service systems often lead to failures, threatening system reliability. To overcome the limitations of existing techniques in erroneous change detection, failure triage, and root cause change analysis, this paper presents a multimodal intelligent change assessment framework based on large language models. Our framework integrates retrieval-augmented generation techniques and leverages unified representation of multimodal data, enhanced knowledge access, and domain-specific LLMs to automate the entire change management lifecycle. Experiments on two microservice system datasets show that our method outperforms state-of-the-arts in accuracy, efficiency, and minimizing manual intervention. Furthermore, SCELM has been operational for over 11 months in real world, reducing response and resolution times for erroneous changes by 90\%, significantly improving incident handling efficiency. This work provides a robust solution for change management and valuable insights into improving system stability and optimizing operational workflows.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{wang_diagnosing_2025,
  title = {Diagnosing and {Resolving},
  author = {Wang, Yifan and Birman, Kenneth P.},
  year = {2025},
  doi = {10.1145/3721146.3721958},
  url = {https://doi.org/10.1145/3721146.3721958},
  booktitle = {Proceedings of the 5th {Workshop},
  pages = {139--147},
  publisher = {Association for Computing Machinery},
  note = {event-place: World Trade Center, Rotterdam, Netherlands},
  keywords = {AI-Ops, RAG LLM, root cause analysis, source: ACM},
  abstract = {Today's cloud-hosted applications and services are complex systems, and a performance or functional instability can have dozens or hundreds of potential root causes. Our hypothesis is that by combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface, problem identification and resolution can be simplified. ARCA is a new multi-modal RAG LLM system that targets this domain. Step-wise evaluations show that ARCA outperforms state-of-the-art alternatives.},
  address = {New York, NY, USA},
  series = {{EuroMLSys},
  isbn = {979-8-4007-1538-9},
}

@inproceedings{deldjoo_toward_2025,
  title = {Toward {Holistic},
  author = {Deldjoo, Yashar and Mehta, Nikhil and Sathiamoorthy, Maheswaran and Zhang, Shuai and Castells, Pablo and McAuley, Julian},
  year = {2025},
  doi = {10.1145/3726302.3730354},
  url = {https://doi.org/10.1145/3726302.3730354},
  booktitle = {Proceedings of the 48th {International},
  pages = {3932--3942},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {bias amplification, factuality, fairness, generative recommender systems, hallucination, holistic evaluation framework, llm},
  abstract = {Recommender systems powered by generative models (Gen-RecSys) extend beyond classical item-ranking by producing open-ended content, which simultaneously unlocks richer user experiences and introduces new risks. On one hand, these systems can enhance personalization and appeal through dynamic explanations and multi-turn dialogues. On the other hand, they might venture into unknown territory-hallucinating nonexistent items, amplifying bias, or leaking private information. Traditional accuracy metrics cannot fully capture these challenges, as they fail to measure factual correctness, content safety, or alignment with user intent.This paper makes two main contributions. First, we categorize the evaluation challenges of Gen-RecSys into two groups: (i) existing concerns that are exacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new risks (e.g., item hallucinations, contradictory explanations). Second, we propose a holistic evaluation approach that includes scenario-based assessments and multi-metric checks-incorporating relevance, factual grounding, bias detection, and policy compliance. Our goal is to provide a guiding framework so that researchers and practitioners can thoroughly assess Gen-RecSys, ensuring both effective personalization and responsible deployment.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{huang_generating_2024,
  title = {Generating {Educational},
  author = {Huang, Chieh-Yang and Wei, Jing and Huang, Ting-Hao Kenneth},
  year = {2024},
  doi = {10.1145/3690712.3690718},
  url = {https://doi.org/10.1145/3690712.3690718},
  booktitle = {Proceedings of the {Third},
  pages = {16--22},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {Educational Material Generation, Large Language Model, Text Generation, Text Readability},
  abstract = {This study introduces the leveled-text generation task, aiming to rewrite educational materials to specific readability levels while preserving meaning. We assess the capability of GPT-3.5, LLaMA-2 70B, and Mixtral 8x7B, to generate content at various readability levels through zero-shot and few-shot prompting. Evaluating 100 processed educational materials reveals that few-shot prompting significantly improves performance in readability manipulation and information preservation. LLaMA-2 70B performs better in achieving the desired difficulty range, while GPT-3.5 maintains original meaning. However, manual inspection highlights concerns such as misinformation introduction and inconsistent edit distribution. These findings emphasize the need for further research to ensure the quality of generated educational content.},
  address = {New York, NY, USA},
  series = {{In2Writing},
  isbn = {979-8-4007-1031-5},
}

@inproceedings{engelmann_reanimator_2025,
  title = {{REANIMATOR},
  author = {Engelmann, Björn and Haak, Fabian and Schaer, Philipp and Erfanian Abdoust, Mani and Netze, Linus and Bittkowski, Meik},
  year = {2025},
  doi = {10.1145/3726302.3730342},
  url = {https://doi.org/10.1145/3726302.3730342},
  booktitle = {Proceedings of the 48th {International},
  pages = {3691--3701},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information retrieval, large language models, rag, scientific literature, table information extraction, table retrieval, test collection},
  abstract = {Retrieval test collections are essential for evaluating information retrieval systems, yet they often lack generalizability across tasks. To overcome this limitation, we introduce REANIMATOR, a versatile framework designed to enable the repurposing of existing test collections by enriching them with extracted and synthetic resources. REANIMATOR enhances test collections from PDF files by parsing full texts and machine-readable tables, as well as related contextual information. It then employs state-of-the-art large language models to produce synthetic relevance labels. Including an optional human-in-the-loop step can help validate the resources that have been extracted and generated. We demonstrate its potential with a revitalized version of the TREC-COVID test collection, showcasing the development of a retrieval-augmented generation system and evaluating the impact of tables on retrieval-augmented generation. REANIMATOR enables the reuse of test collections for new applications, lowering costs and broadening the utility of legacy resources.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{yan_challenges_2025,
  title = {Challenges in {C},
  author = {Yan, Yanyan and Feng, Yang and He, Qi and Zeng, Jun and Xu, Baowen},
  year = {2025},
  doi = {10.1145/3759425.3763380},
  url = {https://doi.org/10.1145/3759425.3763380},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {21--26},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {Large Language Model, Program Analysis, Program Generation, Program Translation, source: ACM},
  abstract = {C++ programming language is one of the mainstream choices for developing various systems due to its efficiency and widespread application, particularly in fields with high-performance requirements. However, C++ programs may have many memory management and security issues, such as dangling pointers and memory leaks, which pose increasing challenges in modern software development. As a modern programming language designed to address memory safety issues, Rust has gained widespread attention for its ownership system and memory safety features, driving research and practice in migrating C++ code to Rust. However, the differences in syntax and features between C++ and Rust, as well as C++'s complex and object-oriented features, make it extremely difficult to directly convert C++ code into Rust code. With the development of large language models (LLMs), significant progress has been made in code translation and understanding. This paper aims to investigate the use of LLMs to convert C++ code into Rust code by decomposing the C++ code into independent features, such as classes, templates, and functions etc., and extracting the dependent global symbol definitions through program analysis. We selected GPT-4-turbo and Deepseek-v3 for experimentation, analyzed their performance of translation results, and investigated the root causes made by GPT-4-turbo and Deepseek-v3. By manually classifying errors, we identified the root causes of translation issues and provided findings and suggestions for future research on translating C++ code into Rust code.},
  address = {New York, NY, USA},
  series = {{LMPL},
  isbn = {979-8-4007-2148-9},
}

@inproceedings{poudel_scalable_2025,
  title = {A {Scalable},
  author = {Poudel, Pratik and Guan, Boyuan and Sanchez, Nicole and Bahreini, Kiavash and Cui, Wencong and Lopez, Andres and Najafi, Hamed and Fu, Zhaohui and Bobadilla, Leonardo and Liu, Jason},
  year = {2025},
  doi = {10.1145/3708035.3736017},
  url = {https://doi.org/10.1145/3708035.3736017},
  booktitle = {Practice and {Experience},
  publisher = {Association for Computing Machinery},
  keywords = {Ceph-based Storage System, Environmental Data Management, Generative AI (GenAI), Geospatial Visualization, Smart Data Pipeline},
  abstract = {Environmental data originates from diverse sources, posing challenges in management, processing, and visualization. This paper introduces a scalable, AI-driven data pipeline framework for environmental data management and discovery. The framework integrates workflow orchestration, automated data ingestion and processing, federated storage, and seamless geospatial visualization. It employs a Ceph-based storage system to handle large, heterogeneous datasets, leveraging its fault-tolerant, distributed architecture for high-performance storage across object, block, and file interfaces. To enhance data discoverability and interoperability, the framework incorporates Generative AI (GenAI) for automated metadata generation, reducing manual annotation overhead while improving real-time processing and cross-platform integration. Additionally, the system enables interdisciplinary collaboration through standardized metadata structures and scalable data federation. A case study using buoy data validates the framework’s capabilities, including data processing, cleaning, and visualization. By addressing critical data integration and accessibility challenges, the system fosters a scalable, efficient, and intelligent research data-sharing ecosystem for environmental science studies.},
  address = {New York, NY, USA},
  series = {{PEARC},
  isbn = {979-8-4007-1398-9},
}

@inproceedings{dou_construction_2025,
  title = {Construction and {Innovative},
  author = {Dou, Juhua and Zheng, Bo},
  year = {2025},
  doi = {10.1145/3746709.3746933},
  url = {https://doi.org/10.1145/3746709.3746933},
  booktitle = {Proceedings of the 2025 6th {International},
  pages = {1324--1329},
  publisher = {Association for Computing Machinery},
  keywords = {AI Agent, College EFL Teaching, Dify platform, Large Language Model},
  abstract = {Artificial intelligence has brought unlimited possibilities to education. Based on the Deepseek large language model and the Dify agent platform, this paper uses RAG, TTS, NLP and other technologies to build multiple different types of college EFL teaching agents. While greatly reducing the teaching workload of teachers, it can also establish a one-to-one personal learning assistant for learners to help them learn flexibly and efficiently. Teaching practice shows that the college EFL teaching agent can better adapt to the teaching methods of the intelligent era. It is an innovative exploration of the digital transformation of college EFL course in higher educational institutions.},
  address = {New York, NY, USA},
  series = {{CIBDA},
  isbn = {979-8-4007-1316-3},
}

@inproceedings{salemi_optimization_2024,
  title = {Optimization {Methods},
  author = {Salemi, Alireza and Kallumadi, Surya and Zamani, Hamed},
  year = {2024},
  doi = {10.1145/3626772.3657783},
  url = {https://doi.org/10.1145/3626772.3657783},
  booktitle = {Proceedings of the 47th {International},
  pages = {752--762},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {personalization, ranking optimization, retrieval-augmented generation, text generation},
  abstract = {This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization–one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model. This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input. Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{qian_tackling_2025,
  title = {Tackling the {Length},
  author = {Qian, Hongjin and Liu, Zheng and Zhang, Peitian and Mao, Kelong and Zhou, Yujia and Chen, Xu and Dou, Zhicheng},
  year = {2025},
  doi = {10.1145/3690624.3709240},
  url = {https://doi.org/10.1145/3690624.3709240},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {1150--1160},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {context length, knowledge-intensive task, large language model},
  abstract = {Knowledge-intensive tasks often require complex reasoning and contextual understanding over long contexts. However, the learning and deployment of long-LLMs remains a challenging problem despite recent progresses. In this work, we propose that the short LLMs have great potentiality for solving knowledge-intensive tasks that have long context, i.e. they can be solved by purely working with oracle short-contexts within the input long-context. On top of this argument, we propose a framework called DCISO DynamiC knowledge-Intensive task S\&gt;Olver), which enables a short-LLM to address the knowledge-intensive tasks with long context via dynamic context browsing. In our framework, the short-LLM prompts itself to reason for two critical decisions: 1) how to access to the appropriate part of context within the input, 2) how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, DCISO can serve as a general framework to handle diversified knowledge-intensive long-context problems. We comprehensively evaluate different types of tasks from popular long-context benchmarks, where DCISO is able to achieve a substantially improved performance. Our codes will be released at this repository.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1245-6},
}

@inproceedings{alvarado_garcia_emerging_2025,
  title = {Emerging {Data},
  author = {Alvarado Garcia, Adriana and Candello, Heloisa and Badillo-Urquiola, Karla and Wong-Villacres, Marisol},
  year = {2025},
  doi = {10.1145/3706598.3714069},
  url = {https://doi.org/10.1145/3706598.3714069},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI, AI practitioners, data governance, data practices, data work, GenAI, generative AI, LLMs, synthetic data},
  abstract = {Data is one of the foundational aspects of making Artificial Intelligence (AI) work as intended. As large language models (LLMs) become the epicenter of AI, it is crucial to understand better how the datasets that maintain such models are created. The emergent nature of LLMs makes it critical to understand the challenges practitioners developing Gen AI technologies face to design alternatives for better responding to Gen AI’s ethical issues. In this paper, we provide such understanding by reporting on 25 interviews with practitioners who handle data in three distinct development stages of different LLMs. Our contributions are (1) empirical evidence of how uncertainty, data practices, and reliance mechanisms change across LLMs’ development cycle; (2) how the unique qualities of LLMs impact data practices and their implications for the future of Gen AI technologies; and (3) provide three opportunities for HCI researchers interested in supporting practitioners developing Gen AI technologies.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{liu_detecting_2025,
  title = {Detecting {AI},
  author = {Liu, Zifeng and Jiao, Xinyue and Xing, Wanli and Zhu, Wangda},
  year = {2025},
  doi = {10.1145/3641554.3701942},
  url = {https://doi.org/10.1145/3641554.3701942},
  booktitle = {Proceedings of the 56th {ACM},
  pages = {701--707},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {ai-generated content, explainable ai, gpt model, online programming education, plagiarism detection, pseudocode, source: ACM},
  abstract = {Despite extensive research on code plagiarism detection in higher education and for programming languages like Java and Python, limited work has focused on K-12 settings, particularly for pseudocode. This study aims to address this gap by building explainable machine learning models for pseudocode plagiarism detection in online programming education. To achieve this, we construct a comprehensive dataset comprising 7,838 pseudocode submissions from 2,578 high school students enrolled in an online programming foundations course, along with 6,300 pseudocode samples generated by three versions of generative pre-trained transformer (GPT) models. Utilizing this dataset, we develop an explainable model to detect AI-generated pseudocode across various assessments. The model not only identifies AI-generated content but also provides insights into its predictions at both the student and problem levels, thus enhancing our understanding of AI-generated pseudocode in K-12 education. Furthermore, we analyzed SHAP values and key features of the model to pinpoint student submissions that closely resemble AI-generated pseudocode. This research offers implications for developing robust educational technologies and methodologies to uphold academic integrity in online programming courses.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0531-1},
  series = {{SIGCSETS},
}

@inproceedings{pasquini_neural_2024,
  title = {Neural {Exec},
  author = {Pasquini, Dario and Strohmeier, Martin and Troncoso, Carmela},
  year = {2024},
  doi = {10.1145/3689932.3694764},
  url = {https://doi.org/10.1145/3689932.3694764},
  booktitle = {Proceedings of the 2024 {Workshop},
  pages = {89--100},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salt Lake City, UT, USA},
  keywords = {adversarial inputs, ai readteam, llms, prompt injection, rag},
  abstract = {We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., "Ignore previous instructions and..."), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches. Code available at https://github.com/pasquini-dario/LLM\_NeuralExec},
  address = {New York, NY, USA},
  series = {{AISec},
  isbn = {979-8-4007-1228-9},
}

@inproceedings{albrecht-crane_thinking_2025,
  title = {Thinking {Smarter},
  author = {Albrecht-Crane, Christa},
  year = {2025},
  doi = {10.1145/3711670.3764628},
  url = {https://doi.org/10.1145/3711670.3764628},
  booktitle = {Proceedings of the 43rd {ACM},
  pages = {121--127},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {This paper examines Google's NotebookLM as a case study of how consumer-facing generative AI technologies misalign with educational values and user needs. While marketed as an "AI-powered research assistant," NotebookLM exemplifies the disconnect between AI industry promises and actual capabilities. Through technical analysis of large language model mechanisms, this paper reveals how NotebookLM's statistical compression methods fundamentally differ from human cognitive processes of reading and analysis. The paper argues that despite claims of source-grounding, NotebookLM produces outputs that compress rather than comprehend texts, often missing crucial arguments and generating confabulated content. Drawing on examinations of "smart" technology rhetoric and extreme usability design, the analysis demonstrates how the tool's frictionless interface obscures computational limitations while potentially undermining cognitive development. The paper concludes by advocating for critical AI literacy in writing studies and technical communication, proposing pedagogical approaches that demystify AI hype and preserve the essential friction necessary for meaningful learning.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1444-3},
  series = {{SIGDOC},
}

@inproceedings{zha_mentigo_2025,
  title = {Mentigo: {An},
  author = {Zha, Siyu and Liu, Yujia and Zheng, Chengbo and Xu, Jiaqi and Yu, Fuze and Gong, Jiangtao and Xu, Yingqing},
  year = {2025},
  doi = {10.1145/3706598.3713952},
  url = {https://doi.org/10.1145/3706598.3713952},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Agent, creative problem solving, Generative AI, mentor},
  abstract = {Creative Problem-Solving (CPS) promotes creative and critical thinking while enhancing real-world problem-solving skills, making it essential for middle school education. However, providing personalized mentorship in CPS projects at scale is challenging due to resource constraints and diverse student needs. To address this, we developed Mentigo, an AI-driven mentor agent designed to guide middle school students through the CPS process. Using a dataset of real classroom interactions, we encoded CPS task stages, adaptive guidance strategies, and personalized feedback mechanisms to inform Mentigo‘s dynamic mentoring framework powered by large language models (LLMs). A comparative experiment with 12 students and evaluations from five expert educators demonstrated improved student engagement, creativity, and task performance. Our findings highlight design implications for using LLM-based AI mentors to enhance CPS learning in educational environments.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{yang_evaluation_2024,
  title = {On the {Evaluation},
  author = {Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie},
  year = {2024},
  doi = {10.1145/3691620.3695529},
  url = {https://doi.org/10.1145/3691620.3695529},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {1607--1619},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {empirical study, large language model, unit test generation, source: ACM},
  abstract = {Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{ma_towards_2025,
  title = {Towards {Human},
  author = {Ma, Shuai and Chen, Qiaoyi and Wang, Xinru and Zheng, Chengbo and Peng, Zhenhui and Yin, Ming and Ma, Xiaojuan},
  year = {2025},
  doi = {10.1145/3706598.3713423},
  url = {https://doi.org/10.1145/3706598.3713423},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI-Assisted Decision-making, Appropriate Reliance, Deliberation, Human-AI Collaboration, Large Language Models},
  abstract = {Traditional AI-assisted decision-making systems often provide fixed recommendations that users must either accept or reject entirely, limiting meaningful interaction—especially in cases of disagreement. To address this, we introduce Human-AI Deliberation, an approach inspired by human deliberation theories that enables dimension-level opinion elicitation, iterative decision updates, and structured discussions between humans and AI. At the core of this approach is Deliberative AI, an assistant powered by large language models (LLMs) that facilitates flexible, conversational interactions and precise information exchange with domain-specific models. Through a mixed-methods user study, we found that Deliberative AI outperforms traditional explainable AI (XAI) systems by fostering appropriate human reliance and improving task performance. By analyzing participant perceptions, user experience, and open-ended feedback, we highlight key findings, discuss potential concerns, and explore the broader applicability of this approach for future AI-assisted decision-making systems.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{peng_tilse_2024,
  title = {“{TILSE},
  author = {Peng, Beibei and Wang, Xindi and Xu, Lei},
  year = {2024},
  doi = {10.1145/3700297.3700365},
  url = {https://doi.org/10.1145/3700297.3700365},
  booktitle = {Proceedings of the 2024 {International},
  pages = {398--403},
  publisher = {Association for Computing Machinery},
  keywords = {Personalized Feedback, Prompt Generation, RAG Technology, TILSE Framework, source: ACM},
  abstract = {This paper introduces the TILSE grading prompt framework, integrating RAG (Retrieval-Augmented Generation) technology to address the challenge of generating accurate and personalized feedback in educational settings. The TILSE framework's modular design, comprising Task, Input, Logic, Style, and Example modules, allows for flexible and contextually relevant prompt generation. By dynamically retrieving pertinent knowledge, RAG technology enhances the precision and adaptability of feedback, making it more tailored to individual student needs. Experiments with ChatGPT 4.0 demonstrate that the TILSE framework significantly outperforms traditional methods, particularly in complex educational scenarios, by providing more accurate and personalized feedback. This research offers a novel solution to the limitations of existing feedback systems and contributes to the advancement of AI-driven educational tools.},
  address = {New York, NY, USA},
  series = {{ISAIE},
  isbn = {979-8-4007-0710-0},
}

@inproceedings{joho_instruction-response_2025,
  title = {An {Instruction},
  author = {Joho, Hideo and Jose, Joemon M},
  year = {2025},
  doi = {10.1145/3726302.3730346},
  url = {https://doi.org/10.1145/3726302.3730346},
  booktitle = {Proceedings of the 48th {International},
  pages = {3843--3852},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {explainable ai, information retrieval tasks, instruction-response study, large language models},
  abstract = {The increasing use of retrieval-augmented applications, where large language models (LLMs) are instructed to generate queries, assess relevance, and synthesise responses, has introduced new challenges in Information Retrieval (IR). The lack of transparency in LLMs means that even subtle variations in instructions can significantly impact the quality, consistency, and reliability of their responses. To address this issue, we propose Instruction-Response Study, an experimental framework for systematically analysing how task instructions influence LLM-generated responses in IR tasks. This paper presents the core components of the framework and demonstrates its utility through four case studies, examining 1) the effect of IR tasks on query formulation, 2) the impact of topic information size on retrieval effectiveness, 3) the reproducibility of LLM-generated queries, and 4) the role of meta-instructions in diversifying instruction design. The findings highlight how the proposed framework enables controlled experimentation on instruction design and its effects, offering a foundation for optimising prompt engineering and enhancing retrieval-augmented applications.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{sharifymoghaddam_rankllm_2025,
  title = {{RankLLM},
  author = {Sharifymoghaddam, Sahel and Pradeep, Ronak and Slavescu, Andre and Nguyen, Ryan and Xu, Andrew and Chen, Zijian and Zhang, Yilin and Chen, Yidi and Xian, Jasper and Lin, Jimmy},
  year = {2025},
  doi = {10.1145/3726302.3730331},
  url = {https://doi.org/10.1145/3726302.3730331},
  booktitle = {Proceedings of the 48th {International},
  pages = {3681--3690},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information retrieval, large language models, python, reranking},
  abstract = {The adoption of large language models (LLMs) as rerankers in multi-stage retrieval systems has gained significant traction in academia and industry. These models refine a candidate list of retrieved documents, often through carefully designed prompts, and are typically used in applications built on retrieval-augmented generation (RAG). This paper introduces RankLLM, an open-source Python package for reranking that is modular, highly configurable, and supports both proprietary and open-source LLMs in customized reranking workflows. To improve usability, RankLLM features optional integration with Pyserini for retrieval and provides integrated evaluation for multi-stage pipelines. Additionally, RankLLM includes a module for detailed analysis of input prompts and LLM responses, addressing reliability concerns with LLM APIs and non-deterministic behavior in Mixture-of-Experts (MoE) models. This paper presents the architecture of RankLLM, along with a detailed step-by-step guide and sample code. We reproduce results from RankGPT, LRL, RankVicuna, RankZephyr, and other recent models. RankLLM integrates with common inference frameworks and a wide range of LLMs. This compatibility allows for quick reproduction of reported results, helping to speed up both research and real-world applications. The complete repository is available at rankllm.ai, and the package can be installed via PyPI.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{liu_gram_2024,
  title = {{GRAM},
  author = {Liu, Xuanqing and Wang, Runhui and Song, Yang and Kong, Luyang},
  year = {2024},
  doi = {10.1145/3637528.3671602},
  url = {https://doi.org/10.1145/3637528.3671602},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {5476--5486},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {generative modeling, retrieval augmented generation, schema matching},
  abstract = {Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy. The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{wang_jupybara_2025,
  title = {Jupybara: {Operationalizing},
  author = {Wang, Huichen Will and Birnbaum, Larry and Setlur, Vidya},
  year = {2025},
  doi = {10.1145/3706598.3713913},
  url = {https://doi.org/10.1145/3706598.3713913},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Actionable Insights, Data Science, Data Storytelling, Exploratory Data Analysis, Human-AI Collaboration, Large Language Model, Multi-Agent System, Pragmatics., Rhetoric, Semantics},
  abstract = {Mining and conveying actionable insights from complex data is a key challenge of exploratory data analysis (EDA) and storytelling. To address this challenge, we present a design space for actionable EDA and storytelling. Synthesizing theory and expert interviews, we highlight how semantic precision, rhetorical persuasion, and pragmatic relevance underpin effective EDA and storytelling. We also show how this design space subsumes common challenges in actionable EDA and storytelling, such as identifying appropriate analytical strategies and leveraging relevant domain knowledge. Building on the potential of LLMs to generate coherent narratives with commonsense reasoning, we contribute Jupybara, an AI-enabled assistant for actionable EDA and storytelling implemented as a Jupyter Notebook extension. Jupybara employs two strategies—design-space-aware prompting and multi-agent architectures—to operationalize our design space. An expert evaluation confirms Jupybara’s usability, steerability, explainability, and reparability, as well as the effectiveness of our strategies in operationalizing the design space framework with LLMs.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{wen_6g-xsec_2024,
  title = {{6G},
  author = {Wen, Haohuang and Sharma, Prakhar and Yegneswaran, Vinod and Porras, Phillip and Gehani, Ashish and Lin, Zhiqiang},
  year = {2024},
  doi = {10.1145/3696348.3696881},
  url = {https://doi.org/10.1145/3696348.3696881},
  booktitle = {Proceedings of the 23rd {ACM},
  pages = {77--85},
  publisher = {Association for Computing Machinery},
  note = {event-place: Irvine, CA, USA},
  keywords = {6G, Anomaly Detection, Large Language Model, OpenRAN},
  abstract = {The evolution from 5G to 6G cellular networks signifies a crucial advancement towards enhanced robustness and automation driven by the promise of ubiquitous Artificial Intelligence (AI) to overhaul network operations, commonly referred to as AIOps. However, 6G network operators also need to deal with evolving threats at the edge to ensure data integrity and availability. We introduce 6G-XSEC, the first framework that seeks to automatically monitor, analyze, and explain anomalies and threats at the cellular network edge. Our framework enhances the emerging Open Radio Access Network (O-RAN) control plane with run-time analytic capabilities and explainability. A distinguishing aspect of our framework is the use of expert referencing, a coupling of lightweight unsupervised deep learning-based anomaly detection with large language models (LLMs) to first detect, analyze, and subsequently explain complicated real-world cellular threats and anomalies at run-time, based on enhanced security telemetry from the O-RAN data plane. We build a prototype 6G-XSEC framework and evaluate it against 5 end-to-end cellular attacks from the literature, achieving 100\% detection rate with our best model. We also propose effective LLM prompt templates for attack analysis and present qualitative results from 5 popular LLMs.},
  address = {New York, NY, USA},
  series = {{HotNets},
  isbn = {979-8-4007-1272-2},
}

@inproceedings{chen_open_2025,
  title = {Open {Local},
  author = {Chen, Haoting and Rodríguez Méndez, Sergio José and Omran, Pouya Ghiasnezhad},
  year = {2025},
  doi = {10.1145/3701716.3717820},
  url = {https://doi.org/10.1145/3701716.3717820},
  booktitle = {Companion {Proceedings},
  pages = {2551--2559},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge graph construction, large language model, natural language processing},
  abstract = {This manuscript introduces paper2lkg, a novel Local Knowledge Graph Construction (KGC) pipeline designed to transform individual academic papers into their structured local Knowledge Graph (KG) representations. The pipeline harnesses Large Language Models (LLMs), particularly generative LLMs, to automate key Natural Language Processing (NLP) tasks in KGC. The constructed local KGs can potentially be used to enrich an existing academic KG that lacks detailed local representations of individual papers or further integrated into new academic KGs through Knowledge Graph Alignment (KGA).},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{prakash_integrating_2024,
  title = {Integrating {LLMs},
  author = {Prakash, Kishore and Rao, Shashwat and Hamza, Rayan and Lukich, Jack and Chaudhari, Vatsal and Nandi, Arnab},
  year = {2024},
  doi = {10.1145/3663649.3664371},
  url = {https://doi.org/10.1145/3663649.3664371},
  booktitle = {Proceedings of the 3rd {International},
  pages = {33--39},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {ChatGPT, database systems education, foundation models, intro to db, large language models, llm, undergrad databases},
  abstract = {Large Language Models (LLMs) have sparked a drastic improvement in the ways computers can understand, process, and generate language. As LLM-based offerings become mainstream, we explore the incorporation of such LLMs into introductory or undergraduate database systems education. Students and instructors are both faced with the calculator dilemma: while the use of LLM-based tools may “solve” tasks such as assignments and exams, do they impede or accelerate the learning itself? We review deficiencies of using existing off-the-shelf tools for learning, and further articulate the differentiated needs of database systems students as opposed to trained data practitioners. Building on our exploration, we outline a vision that integrates LLMs into database education in a principled manner, keeping pedagogical best practices in mind. If implemented correctly, we posit that LLMs can drastically amplify the impact of existing instruction, minimizing costs and barriers towards learning database systems fundamentals.},
  address = {New York, NY, USA},
  series = {{DataEd},
  isbn = {979-8-4007-0678-3},
}

@inproceedings{lin_fraudulent_2025,
  title = {A {Fraudulent},
  author = {Lin, Hongyu and Zhong, Shuxin and Fang, Yan and Hong, Zhiqing and Lyu, Wenjun and Xie, Qipeng and Wang, Haotian and Wang, Lu and Wu, Kaishun},
  year = {2025},
  doi = {10.1145/3711896.3737184},
  url = {https://doi.org/10.1145/3711896.3737184},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4590--4598},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {blind shipment detection, large language model, multi-modal data aggregation},
  abstract = {An emerging type of fraud involves malicious senders exploiting the blind shipment and cash-on-delivery (COD) mechanisms by dispatching large volumes of unsolicited, low-cost parcels. If unsuspecting receivers accept these parcels, they pay for both shipping and goods; otherwise, logistics providers bear the round-trip shipping costs. Existing detection techniques, which rely on extensive labeled cases, struggle with this emerging fraud because receivers' unawareness and low transaction values discourage complaints, resulting in few confirmed cases. Therefore, we propose leveraging receivers' complaints, though not initially collected for fraud detection, to uncover subtle indicators of fraud patterns, while addressing three challenges: (C1) noise-rich dialogues(C2) data privacy concerns, and (C3) ever-evolving fraud patterns. To address them, we design BLOFF, a Blind shipment detection Framework for LO gistics Fraud powered by large language models (LLMs). Specifically, BLOFF includes three components: i) Sensitivity Anonymization to protect sensitive user information; ii) Dialogue Profile Distillation to transform informal dialogues into structured representation, addressing C1, and distill knowledge from a teacher LLM (GPT-4o) to a lightweight student LLM (ChatGLM4-9B), addressing C2; ii) Multi-faceted Context Augmentation to enhance the interpretation of fraud signatures and adaptation of evolving patterns, addressing C3. We evaluate BLOFF on about 56,000 complaints records collected from JD Logistics between January and November 2024. Results show that BLOFF outperforms state-of-the-art methods, achieving a 10.19\% improvement in precision. Furthermore, during its real-world deployment in December 2024, BLOFF identified over 90 fraudulent parcels with a 91.4\% precision.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{shi_you_2025,
  title = {You {Are},
  author = {Shi, Yimin and Fei, Yang and Zhang, Shiqi and Wang, Haixun and Xiao, Xiaokui},
  year = {2025},
  doi = {10.1145/3726302.3730118},
  url = {https://doi.org/10.1145/3726302.3730118},
  booktitle = {Proceedings of the 48th {International},
  pages = {1810--1819},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {large language model, persona, random walk, recommendation, source: ACM},
  abstract = {In e-commerce, user representations are essential for various applications. Existing methods often use deep learning techniques to convert customer behaviors into implicit embeddings. However, these embeddings are difficult to understand and integrate with external knowledge, limiting the effectiveness of applications such as customer segmentation, search navigation, and product recommendations. To address this, our paper introduces the concept of the customer persona. Condensed from a customer's numerous purchasing histories, a customer persona provides a multi-faceted and human-readable characterization of specific purchase behaviors and preferences, such as Busy Parents or Bargain Hunters.This work then focuses on representing each customer by multiple personas from a predefined set, achieving readable and informative explicit user representations. To this end, we propose an effective and efficient solution GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs and few-shot learning to infer personas for customers. To reduce overhead, GPLR applies LLM-based labeling to only a fraction of users and utilizes a random walk technique to predict personas for the remaining customers. To further enhance efficiency, we propose an approximate solution called RevAff for this random walk-based computation. RevAff provides an absolute error ε guarantee while improving the time complexity of the exact solution by a factor of at least O( ε ⋅{\textbar},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{mei-seung_navigating_2024,
  title = {Navigating the “{Cooked},
  author = {Mei-seung, Cheng},
  year = {2024},
  doi = {10.1145/3678610.3678630},
  url = {https://doi.org/10.1145/3678610.3678630},
  booktitle = {Proceedings of the 2024 10th {International},
  pages = {76--81},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {This article explores the integration of Generative AI (GenAI) technologies, such as ChatGPT, Bard, and LaMDA, in academic writing classrooms, examining both their potential to transform learning and the challenges they present. Building on Activity Theory, the study assesses the transformation of students' roles, the writing assistant tool, and the rules and division of labor within the academic community after technology integration. We argue that GenAI, while offering powerful potential for personalized feedback and learning, disrupts traditional educational dynamics. This raises critical questions about student roles, data integrity, and the evolving responsibilities of teachers. We propose eleven research questions to guide future investigations. These questions emphasize the need for a nuanced understanding of how GenAI impacts the learning experience and its implications for academic integrity. We also highlight the ethical considerations surrounding its use. This work aims to contribute to the ongoing conversation surrounding AI in education, promoting a more comprehensive understanding of the opportunities and challenges presented by this transformative technology.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1679-9},
  series = {{ICSLT},
}

@inproceedings{ma_ambigchat_2025,
  title = {{AmbigChat},
  author = {Ma, Jiaju and Shi, Lei and Robertsen, Kenneth Aleksander and Chi, Peggy},
  year = {2025},
  doi = {10.1145/3746059.3747686},
  url = {https://doi.org/10.1145/3746059.3747686},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {conversational interfaces, factual question answering, Language ambiguity, large language model},
  abstract = {When conversing with large language models, it is common for users to ask an ambiguous open-domain question that could lead to multiple answers, especially when exploring new topics. For example, “Who won the US Open?” can result in different athletes according to the referenced events and years. We propose AmbigChat, an automatic approach that hierarchically disambiguates a factual question and guides users to navigate answers via UI widgets in a multi-turn conversational interface. Using the ambiguity taxonomy we generated from an analysis of 5,000 queries, AmbigChat identifies ambiguous facets of a question and constructs a disambiguation tree, where each level corresponds to a facet. Users can traverse the tree to explore answers via interactive disambiguation widgets and expand the conversation by referencing tree nodes through drag and drop. We iterated our interaction design with six design professionals and tested the effectiveness of the disambiguation tree generation algorithm on a variety of factual queries. Our evaluation with 16 participants shows that AmbigChat not only helps the participants find answers more easily and efficiently, but also facilitates structured explorations of the topic space.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{liu_enhancing_2025,
  title = {Enhancing {Automotive},
  author = {Liu, Fei and Ren, Huanhuan and Guan, Yu and Li, Na},
  year = {2025},
  doi = {10.1145/3726010.3726012},
  url = {https://doi.org/10.1145/3726010.3726012},
  booktitle = {Proceedings of the 2024 {International},
  pages = {6--13},
  publisher = {Association for Computing Machinery},
  keywords = {Automotive Industry, Graph RAG, Langchain, Ollama, PDF Processing},
  abstract = {This research explores state-of-the-art retrieval augmented generation (RAG) methodologies utilizing a locally hosted Ollama model to address the rising demand for streamlined offline PDF chatbots in the automotive industry. In this paper we present a Langchain-based optimization of the a forementioned framework for modeling all input text from automotive documents. For example, using JoinChpy which use specialized finetuning to a base model where it is fine-tuned adding specialized modifications, the first on PDF and retrieval algorithms as well as context compression strictly for automotive literature We define embedding pipeline classes and graph RAG agents.To instantiate our methodology, we developed our own automotive industry document dataset and compared the enhanced graph RAG and self-RAG agents with the original RAG baseline on our automotive dataset as well as QuAC and CANARD. The results demonstrate a consistent increase along the dimensions, in correctness, swiftness, relevance, and fidelity, specifically for automotive domain specific content. Hence offering a guideline on how to implement a local RAG in automotive setting, thus serving as a significant contribution to the research in an industrial context to process information and make a step towards smart manufacturing.},
  address = {New York, NY, USA},
  series = {{ICADI},
  isbn = {979-8-4007-1284-5},
}

@inproceedings{weber_perses_2025,
  title = {Perses: {Unlocking},
  author = {Weber, Dominik M. and Tzachristas, Ioannis and Sui, Aifen},
  year = {2025},
  doi = {10.1145/3708821.3736189},
  url = {https://doi.org/10.1145/3708821.3736189},
  booktitle = {Proceedings of the 20th {ACM},
  pages = {344--357},
  publisher = {Association for Computing Machinery},
  keywords = {Automatic Algorithm Configuration, Cybersecurity Automation, LLMs, Penetration Testing, Vulnerability Detection},
  abstract = {Misconfiguration poses a ubiquitous security vulnerability for even modern hardened systems. Recent advances in Large Language Model (LLM)-based penetration testing have proven its efficacy in vulnerability detection. Prior research has predominantly focused on large, non-local models, evaluating them exclusively on Linux and Windows systems. We developed Perses, an extensible multi-LLM framework based on heterogeneity in model selection and task decomposition, designed to enable small local models to autonomously detect and exploit misconfiguration vulnerabilities. Perses is evaluated on a FreeBSD port of an existing privilege escalation benchmark, analyzing the effects of heterogeneity, the viability of automatic LLM assignment, the criticality of introduced LLM-roles, and the detriment incurred through evaluation on a system the models have little inherent knowledge on. Experimental results reveal that Perses, in its default configuration, enables Small Models to consistently exploit 87.5\% of the evaluated vulnerabilities, significantly improving upon the 12.5\% reported in prior work. We further find that SMAC-based automatic configuration is feasible, each role is essential for Perses’ performance, and that the models contain sufficient knowledge on FreeBSD. These results showcase the power of Small Models in privilege escalation, presenting a viable alternative to Large Models.},
  address = {New York, NY, USA},
  series = {{ASIA},
  isbn = {979-8-4007-1410-8},
}

@inproceedings{ehl_supporting_2025,
  title = {Supporting {Software},
  author = {Ehl, Marco and Ahmadian, Amir Shayan and Großer, Katharina and Elsofi, Duaa Adel Ali and Herrmann, Marc and Specht, Alexander and Schneider, Kurt and Jürjens, Jan},
  year = {2025},
  doi = {10.1145/3672608.3707798},
  url = {https://doi.org/10.1145/3672608.3707798},
  booktitle = {Proceedings of the 40th {ACM},
  pages = {1647--1656},
  publisher = {Association for Computing Machinery},
  note = {event-place: Catania International Airport, Catania, Italy},
  keywords = {knowledge discovery, large language model, privacy, quality model, security},
  abstract = {Security and privacy are increasingly essential concepts in software engineering. New threats and corresponding countermeasures are continuously discovered. Concurrently, projects are becoming more complex and are exposed to a greater number of threats. This presents a significant challenge for software engineers. As a result, security and privacy are often neglected due to a lack of knowledge, limited time, and financial constraints. While systematic literature reviews exist to address the increasing volume of publications, software engineers still require up-to-date knowledge of current threats and measures. This paper presents an automated, time-efficient, and cost-effective method for discovering knowledge from state-of-the-art literature and project artifacts, such as design documents. The presented method utilizes Large Language Models (LLMs) for data extraction and is demonstrated through a prototypical implementation and evaluation. This evaluation involves security and privacy in open-access scientific publications and project documentation from European Union research and development projects. The extracted knowledge is used to populate a quality model that is specifically designed to provide software engineers with information that helps them apply the findings. This quality model offers software engineers valuable, up-to-date insights into security and privacy, bridging the gap between scientific research and practical applications.},
  address = {New York, NY, USA},
  series = {{SAC},
  isbn = {979-8-4007-0629-5},
}

@inproceedings{tu_r-eval_2024,
  title = {R-{Eval},
  author = {Tu, Shangqing and Wang, Yuanchun and Yu, Jifan and Xie, Yuyang and Shi, Yaran and Wang, Xiaozhi and Zhang, Jing and Hou, Lei and Li, Juanzi},
  year = {2024},
  doi = {10.1145/3637528.3671564},
  url = {https://doi.org/10.1145/3637528.3671564},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {5813--5824},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {domain knowledge, evaluation, large language model},
  abstract = {Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains. Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval to facilitate both the industry and the researchers.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{mi_empower_2025,
  title = {Empower {Vision},
  author = {Mi, Liang and Wang, Weijun and Tu, Wenming and He, Qingfeng and Kong, Rui and Fang, Xinyu and Dong, Yazhu and Zhang, Yikang and Li, Yuanchun and Li, Meng and Dai, Haipeng and Chen, Guihai and Liu, Yunxin},
  year = {2025},
  doi = {10.1145/3689031.3717472},
  url = {https://doi.org/10.1145/3689031.3717472},
  booktitle = {Proceedings of the {Twentieth},
  pages = {261--277},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {Large language model, Machine learning system},
  abstract = {Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, VaLoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype VaLoRA on five popular vision tasks on three LMMs. Experiment results reveal that VaLoRA improves 24-62\% of the accuracy compared to the original LMMs and reduces 20-89\% of the latency compared to the state-of-the-art LoRA model serving systems.},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{lo_noel_2025,
  title = {Noel: {A},
  author = {Lo, Priscilla Y. and Veldhuis, Annemiek and Antle, Alissa N. and DiPaola, Steve},
  year = {2025},
  doi = {10.1145/3706598.3713836},
  url = {https://doi.org/10.1145/3706598.3713836},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {artificial intelligence, chatbot, children, design thinking, education, empathy, large language model, persona, visual impairment},
  abstract = {Designing for others encourages children to empathize with and consider different perspectives and needs. A chatbot persona could allow children to design for stakeholder groups that are challenging to involve directly in educational activities, such as people with disabilities. In this paper, we explore how an artificial intelligence chatbot persona leveraging the GPT-4 large language model can support children’s design empathy while designing for others. We report the design, development process, and implementation of a chatbot persona representing a 12-year-old child with low vision named Noel. The exploratory case study consisted of three 90- to 120-minute workshop sessions with nineteen students (ages 11 to 13) in a grade 6/7 classroom. Results illustrate ways that Noel supported students throughout the design process, their expressions of design empathy, and their experiences. We present implications for developers and educators along with future directions for research.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{ding_diagram_2025,
  title = {"{The},
  author = {Ding, Zijian and Brachman, Michelle and Chan, Joel and Geyer, Werner},
  year = {2025},
  doi = {10.1145/3698061.3726935},
  url = {https://doi.org/10.1145/3698061.3726935},
  booktitle = {Proceedings of the 2025 {Conference},
  pages = {606--625},
  publisher = {Association for Computing Machinery},
  keywords = {Generative AI, Hypothesis Exploration, Node-link Diagram, Shared Representation},
  abstract = {Data analysis encompasses a spectrum of tasks, from high-level conceptual reasoning to lower-level execution. While AI-powered tools increasingly support execution tasks, there remains a need for intelligent assistance in conceptual tasks. This paper investigates the design of interactive tree diagrams as effective shared representations for AI-assisted hypothesis exploration. We developed a system with ordered node-link diagram augmented with AI-generated information hints and visualizations. Through a design probe (n=22), participants generated diagrams averaging 21.82 hypotheses. Our findings showed that the node-link diagram acts as “guardrails" for hypothesis exploration, facilitating structured workflows, providing overviews, and enabling backtracking. The AI-generated information hints, particularly visualizations, aided users in transforming abstract ideas into data-backed concepts while reducing cognitive load. We further discuss how node-link diagrams can support both parallel exploration and iterative refinement in hypothesis formulation, potentially enhancing the breadth and depth of human-AI collaborative data analysis.},
  address = {New York, NY, USA},
  series = {C\&amp;{C},
  isbn = {979-8-4007-1289-0},
}

@inproceedings{zhang_human-imperceptible_2024,
  title = {Human-{Imperceptible},
  author = {Zhang, Quan and Zeng, Binqi and Zhou, Chijin and Go, Gwihwan and Shi, Heyuan and Jiang, Yu},
  year = {2024},
  doi = {10.1145/3663529.3663786},
  url = {https://doi.org/10.1145/3663529.3663786},
  booktitle = {Companion {Proceedings},
  pages = {502--506},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {Large Language Models, Retrieval Poisoning Attack, source: ACM},
  abstract = {Presently, with the assistance of advanced LLM application development frameworks, more and more LLM-powered applications can effortlessly augment the LLMs' knowledge with external content using the retrieval augmented generation (RAG) technique. However, these frameworks' designs do not have sufficient consideration of the risk of external content, thereby allowing attackers to undermine the applications developed with these frameworks. In this paper, we reveal a new threat to LLM-powered applications, termed retrieval poisoning, where attackers can guide the application to yield malicious responses during the RAG process. Specifically, through the analysis of LLM application frameworks, attackers can craft documents visually indistinguishable from benign ones. Despite the documents providing correct information, once they are used as reference sources for RAG, the application is misled into generating incorrect responses. Our preliminary experiments indicate that attackers can mislead LLMs with an 88.33\% success rate, and achieve a 66.67\% success rate in the real-world application, demonstrating the potential impact of retrieval poisoning.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{loukas_making_2023,
  title = {Making {LLMs},
  author = {Loukas, Lefteris and Stogiannidis, Ilias and Diamantopoulos, Odysseas and Malakasiotis, Prodromos and Vassos, Stavros},
  year = {2023},
  doi = {10.1145/3604237.3626891},
  url = {https://doi.org/10.1145/3604237.3626891},
  booktitle = {Proceedings of the {Fourth},
  pages = {392--400},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Anthropic, Claude, Cohere, Few-shot, GPT, LLMs, NLP, OpenAI},
  abstract = {Standard Full-Data classifiers in NLP demand thousands of labeled examples, which is impractical in data-limited domains. Few-shot methods offer an alternative, utilizing contrastive learning techniques that can be effective with as little as 20 examples per class. Similarly, Large Language Models (LLMs) like GPT-4 can perform effectively with just 1-5 examples per class. However, the performance-cost trade-offs of these methods remain underexplored, a critical concern for budget-limited organizations. Our work addresses this gap by studying the aforementioned approaches over the Banking77 financial intent detection dataset, including the evaluation of cutting-edge LLMs by OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We complete the picture with two additional methods: first, a cost-effective querying method for LLMs based on retrieval-augmented generation (RAG), able to reduce operational costs multiple times compared to classic few-shot approaches, and second, a data augmentation method using GPT-4, able to improve performance in data-limited scenarios. Finally, to inspire future research, we provide a human expert’s curated subset of Banking77, along with extensive error analysis.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-0240-2},
}

@inproceedings{huang_accessibility_2025,
  title = {Accessibility {Scout},
  author = {Huang, William and Su, Xia and Froehlich, Jon E. and Zhang, Yang},
  year = {2025},
  doi = {10.1145/3746059.3747624},
  url = {https://doi.org/10.1145/3746059.3747624},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Accessibility, Accessibility Assessment, Computer Vision, Large Language Model, Personalization},
  abstract = {Assessing the accessibility of unfamiliar built environments is critical for people with disabilities. However, manual assessments, performed by users or their personal health professionals, are laborious and unscalable, while automatic machine learning methods often neglect an individual user’s unique needs. Recent advances in Large Language Models (LLMs) enable novel approaches to this problem, balancing personalization with scalability to enable more adaptive and context-aware assessments of accessibility. We present Accessibility Scout, an LLM-based accessibility scanning system that identifies accessibility concerns from photos of built environments. With use, Accessibility Scout becomes an increasingly capable "accessibility scout", tailoring accessibility scans to an individual’s mobility level, preferences, and specific environmental interests through collaborative Human-AI assessments. We present findings from three studies: a formative study with six participants to inform the design of Accessibility Scout, a technical evaluation of 500 images of built environments, and a user study with 10 participants of varying mobility. Results from our technical evaluation and user study show that Accessibility Scout can generate personalized accessibility scans that extend beyond traditional ADA considerations. Finally, we conclude with a discussion on the implications of our work and future steps for building more scalable and personalized accessibility assessments of the physical world.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{xuan_assessing_2024,
  title = {Assessing the {Potential},
  author = {Xuan, Li and Haoxiang, Zhang and Baozheng, Jiang and You, Li},
  year = {2024},
  doi = {10.1145/3698385.3699878},
  url = {https://doi.org/10.1145/3698385.3699878},
  booktitle = {Proceedings of the {First},
  pages = {57--82},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hangzhou, China},
  keywords = {artificial intelligence, chemical engineering, large language model, process design, source: ACM},
  abstract = {This study investigates the emerging potential of large language models in chemical engineering by evaluating GPT-4's performance on questions from general chemistry, physical chemistry, and reactor and process design. GPT-4 correctly answered 45\% of the problems without error, with overall accuracy increasing to 62\% when partially correct answers were included. These results highlight both the capabilities and limitations of LLMs in this field. We discuss the challenges and future prospects of integrating LLMs into chemical engineering, aiming to provide insights and direction for future interdisciplinary research. Additionally, a problem set is provided for further exploration.},
  address = {New York, NY, USA},
  series = {{IOTMMIM},
  isbn = {979-8-4007-1297-5},
}

@inproceedings{aruleba_scoping_2025,
  title = {A {Scoping},
  author = {Aruleba, Kehinde and Oyelere, Solomon Sunday and Obaido, George Rabeshi and Sanusi, Ismaila Temitayo},
  year = {2025},
  doi = {10.1145/3754508.3754517},
  url = {https://doi.org/10.1145/3754508.3754517},
  booktitle = {Proceedings of the 2025 {Conference},
  publisher = {Association for Computing Machinery},
  keywords = {Generative AI, Introductory Computer Science (CS1), Large Language Models (LLMs), Programming Education, Student Engagement},
  abstract = {As Large Language Models (LLMs) like ChatGPT and GitHub Copilot gain traction in computing education, understanding their role in introductory programming (CS1) is essential. This scoping review synthesises 38 empirical studies published between 2022 and 2024, focusing on student and educator engagement with LLMs in CS1 contexts. Following Arksey and O’Malley’s five-stage framework and PRISMA-ScR guidelines, we identify four thematic areas: (1) varied student prompting behaviours, from surface-level code copying to iterative refinement; (2) evolving educator practices, from passive allowance to guided integration; (3) assessment-related tensions, notably the “assistance dilemma”; and (4) ethical concerns around bias, integrity, and access. While LLMs support debugging and code comprehension, their value depends on pedagogical framing and learner agency. Gaps remain in longitudinal research, diverse learner representation, and alignment with curriculum frameworks. We offer practical recommendations for scaffolded GenAI integration, prompt engineering strategies, and ethical classroom use. This review supports the development of CS1 curricula that foster critical AI literacy, inclusive participation, and thoughtful engagement with human–AI collaboration in programming education.},
  address = {New York, NY, USA},
  series = {{UKICER},
  isbn = {979-8-4007-2078-9},
}

@inproceedings{paduraru_enhancing_2025,
  title = {Enhancing {Game},
  author = {Paduraru, Ciprian and Paduraru, Miruna and Stefanescu, Alin},
  year = {2025},
  doi = {10.1145/3696630.3728553},
  url = {https://doi.org/10.1145/3696630.3728553},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {286--296},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {agentic AI, behavior trees, game AI, generative AI, large language models, non-player characters (NPCs)},
  abstract = {Integrating advanced AI behaviors is central to creating immersive and dynamic video game experiences. This paper presents a novel approach to improving AI behaviors in games using Large Language Models (LLMs) and agent-based AI. By orchestrating various interconnected parts, we propose a framework that facilitates the creation of complex behavior trees (BTs) for non-player characters (NPCs). Our method bridges the gap between source code and visual tools in game engines and enables both technical and non-technical stakeholders to effectively contribute to the development process. We also aim to increase the diversity of observable behaviors and testability of games through the same methods. The proposed architecture is designed to be adaptable to different game engines to ensure scalability and flexibility. In a collaboration between industry and academia, we validate our approach and demonstrate its potential to improve game AI development and make it more accessible and efficient. To promote the adoption of the methods, we consider small-sized models that run on typical developer platforms without the need for external solutions or expensive computing resources.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{monteiro_imago_2025,
  title = {Imago {Obscura},
  author = {Monteiro, Kyzyl and Wu, Yuchen and Das, Sauvik},
  year = {2025},
  doi = {10.1145/3746059.3747633},
  url = {https://doi.org/10.1145/3746059.3747633},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {generative AI, human-AI teaming, intent-aware tool, usable privacy},
  abstract = {Users often struggle to navigate the privacy / publicity boundary in sharing images online: they may lack awareness of image privacy risks or the ability to apply effective mitigation strategies. To address this challenge, we introduce and evaluate Imago Obscura, an intent-aware AI-powered image-editing copilot that enables users to identify and mitigate privacy risks in images they intend to share. Driven by design requirements from a formative user study with 7 image-editing experts, Imago Obscura enables users to articulate their image-sharing intent and privacy concerns. The system uses these inputs to surface contextually pertinent privacy risks, and then recommends and facilitates application of a suite of obfuscation techniques found to be effective in prior literature — e.g., inpainting, blurring, and generative content replacement. We evaluated Imago Obscura with 15 end-users in a lab study and found that it improved users’ awareness of image privacy risks and their ability to address them, enabling more informed sharing decisions.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{jennings_whats_2024,
  title = {What's the {Game},
  author = {Jennings, Nicholas and Wang, Han and Li, Isabel and Smith, James and Hartmann, Bjoern},
  year = {2024},
  doi = {10.1145/3654777.3676358},
  url = {https://doi.org/10.1145/3654777.3676358},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Generative AI, Human-AI interaction, Procedural Content Generation, source: ACM},
  abstract = {Procedural content generation (PCG), the process of algorithmically creating game components instead of manually, has been a common tool of game development for decades. Recent advances in large language models (LLMs) enable the generation of game behaviors based on player input at runtime. Such code generation brings with it the possibility of entirely new gameplay interactions that may be difficult to integrate with typical game development workflows. We explore these implications through GROMIT, a novel LLM-based runtime behavior generation system for Unity. When triggered by a player action, GROMIT generates a relevant behavior which is compiled without developer intervention and incorporated into the game. We create three demonstration scenarios with GROMIT to investigate how such a technology might be used in game development. In a system evaluation we find that our implementation is able to produce behaviors that result in significant downstream impacts to gameplay. We then conduct an interview study with n=13 game developers using GROMIT as a probe to elicit their current opinion on runtime behavior generation tools, and enumerate the specific themes curtailing the wider use of such tools. We find that the main themes of concern are quality considerations, community expectations, and fit with developer workflows, and that several of the subthemes are unique to runtime behavior generation specifically. We outline a future work agenda to address these concerns, including the need for additional guardrail systems for behavior generation.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0628-8},
  series = {{UIST},
}

@inproceedings{sharma_generative_2024,
  title = {Generative {Echo},
  author = {Sharma, Nikhil and Liao, Q. Vera and Xiao, Ziang},
  year = {2024},
  doi = {10.1145/3613904.3642459},
  url = {https://doi.org/10.1145/3613904.3642459},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {Confirmation Bias, Conversational Search, Echo Chamber Effect, Generative AI, Information Diversity, Information Seeking, Large Language Models},
  abstract = {Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers—limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user’s view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{tong_missteps_2025,
  title = {From {Missteps},
  author = {Tong, Zhenyu and Qin, Chuan and Fang, Chuyu and Yao, Kaichun and Chen, Xi and Zhang, Jingshuai and Zhu, Chen and Zhu, Hengshu},
  year = {2025},
  doi = {10.1145/3690624.3709225},
  url = {https://doi.org/10.1145/3690624.3709225},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {1373--1384},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {dense retrieval, large language model, query generation, source: ACM},
  abstract = {Document retrieval, designed to recall query-relevant documents from expansive collections, is essential for information-seeking tasks, such as web search and open-domain question-answering. Advances in representation learning and pretrained language models (PLMs) have driven a paradigm shift from traditional sparse retrieval methods to more effective dense retrieval approaches, forging enhanced semantic connections between queries and documents and establishing new performance benchmarks. However, reliance on extensive annotated document-query pairs limits their competitiveness in low-resource scenarios. Recent research efforts employing the few-shot capabilities of large language models (LLMs) and prompt engineering for synthetic data generation have emerged as a promising solution. Nonetheless, these approaches are hindered by the generation of lower-quality data within the conventional dense retrieval training process. To this end, in this paper, we introduce iGFT, a framework aimed at enhancing low-resource dense retrieval by integrating a three-phase process — Generation, Filtering, and Tuning — coupled with an iterative optimization strategy. Specifically, we first employ supervised fine-tuning on limited ground truth data, enabling an LLM to function as the generator capable of producing potential queries from given documents. Subsequently, we present a multi-stage filtering module to minimize noise in the generated data while retaining samples poised to significantly improve the dense retrieval model's performance in the follow-up fine-tuning process. Furthermore, we design a novel iterative optimization strategy that dynamically optimizes the query generator for producing more informative queries, thereby enhancing the efficacy of the entire framework. Finally, extensive experiments conducted on a series of publicly available retrieval benchmark datasets have demonstrated the effectiveness of the proposed iGFT.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1245-6},
}

@inproceedings{wang_lekube_2024,
  title = {{LeKUBE},
  author = {Wang, Changyue and Su, Weihang and Hu, Yiran and Ai, Qingyao and Wu, Yueyue and Luo, Cheng and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
  year = {2024},
  doi = {10.1145/3673791.3698407},
  url = {https://doi.org/10.1145/3673791.3698407},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {175--185},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {domain-specific evaluation, knowledge update, large language model, source: ACM},
  abstract = {Recent advances in Large Language Models (LLMs) have significantly shaped the applications of AI in multiple fields, including the studies of legal intelligence. Trained on extensive legal texts, including statutes and legal documents, the legal LLMs can capture important legal knowledge/concepts effectively and provide important support for downstream legal applications such as legal consultancy. Yet, the dynamic nature of legal statutes and interpretations also poses new challenges to the use of LLMs in legal applications. Particularly, how to update the legal knowledge of LLMs effectively and efficiently has become an important research problem in practice. Existing benchmarks for evaluating knowledge update methods are mostly designed for the open domain and cannot address the specific challenges of the legal domain, such as the nuanced application of new legal knowledge, the complexity and lengthiness of legal regulations, and the intricate nature of legal reasoning.To address this gap, we introduce the Legal Knowledge Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for legal LLMs across five dimensions. Specifically, we categorize the needs of knowledge updates in the legal domain with the help of legal professionals, and then hire annotators from law schools to create synthetic updates to the Chinese Criminal and Civil Code as well as sets of questions of which the answers would change after the updates. Through a comprehensive evaluation of state-of-the-art knowledge update methods, we reveal a notable gap between existing knowledge update methods and the unique needs of the legal domain, emphasizing the need for further research and development of knowledge update mechanisms tailored for legal LLMs.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{singh_bias-aware_2025,
  title = {Bias-{Aware},
  author = {Singh, Karanbir and Ngu, William},
  year = {2025},
  doi = {10.1145/3701716.3716885},
  url = {https://doi.org/10.1145/3701716.3716885},
  booktitle = {Companion {Proceedings},
  pages = {1705--1712},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {agents, bias, fairness, information retrieval, large language models, retrieval augmented generation, source: ACM},
  abstract = {Advancements in retrieving accessible information have evolved faster in the last few years compared to the decades since the internet's creation. Search engines, like Google, have been the \#1 way to find relevant data. They have always relied on the user's abilities to find the best information in its billions of links and sources at everybody's fingertips. The advent of large language models (LLMs) has completely transformed the field of information retrieval. LLMs excel not only at retrieving relevant knowledge, but also in summarizing it effectively, making information more accessible and consumable for users. On top of it, the rise of AI Agents has introduced another aspect to information retrieval, i.e. dynamic information retrieval which enables the integration of real-time data such as weather forecasts and financial data with the knowledge base to curate context-aware knowledge. However, despite these advancements, agents remain susceptible to issues of bias and fairness deeply rooted in the knowledge base and training of LLMs. This study introduces a novel approach to bias-aware knowledge retrieval by leveraging agentic framework and the innovative use of bias detectors as tools to identify and highlight inherent biases in the retrieved content. By empowering users with transparency and awareness, this approach aims to foster more equitable information systems and promote the development of responsible AI.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{cima_contextualized_2025,
  title = {Contextualized {Counterspeech},
  author = {Cima, Lorenzo and Miaschi, Alessio and Trujillo, Amaury and Avvenuti, Marco and Dell'Orletta, Felice and Cresci, Stefano},
  year = {2025},
  doi = {10.1145/3696410.3714507},
  url = {https://doi.org/10.1145/3696410.3714507},
  booktitle = {Proceedings of the {ACM},
  pages = {5022--5033},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {content moderation, counterspeech, generative ai, online toxicity, personalization, source: ACM},
  abstract = {AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct a LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{yu_preliminary_2025,
  title = {A {Preliminary},
  author = {Yu, Junji and Shu, Honglin and Fu, Michael and Wang, Dong and Tantithamthavorn, Chakkrit and Kamei, Yasutaka and Chen, Junjie},
  year = {2025},
  doi = {10.1145/3713081.3731746},
  url = {https://doi.org/10.1145/3713081.3731746},
  booktitle = {Proceedings of the 34th {ACM},
  pages = {161--168},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {large language model, multilingual vulnerability, vulnerability detection},
  abstract = {Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.},
  address = {New York, NY, USA},
  series = {{ISSTA},
  isbn = {979-8-4007-1474-0},
}

@inproceedings{ma_integrating_2024,
  title = {Integrating {AI},
  author = {Ma, Iris and Krone-Martins, Alberto and Videira Lopes, Cristina},
  year = {2024},
  doi = {10.1145/3649165.3690094},
  url = {https://doi.org/10.1145/3649165.3690094},
  booktitle = {Proceedings of the 2024 on {ACM},
  pages = {130--136},
  publisher = {Association for Computing Machinery},
  note = {event-place: Virtual Event, NC, USA},
  keywords = {education, large language models, llms, software engineering, source: ACM},
  abstract = {RAGMan is an LLM-powered tutoring system that can support a variety of course-specific and homework-specific AI tutors. RAGMan leverages Retrieval Augmented Generation (RAG), as well as strict instructions, to ensure the alignment of the AI tutors' responses. By using RAGMan's AI tutors, students receive assistance with their specific homework assignments without directly obtaining solutions, while also having the ability to ask general programming-related questions. RAGMan was deployed as an optional resource in an introductory programming course with an enrollment of 455 students. It was configured as a set of five homework-specific AI tutors. This paper describes the interactions the students had with the AI tutors, the students' feedback, and a comparative grade analysis. Overall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions. When students posed questions within the intended scope, the AI tutors delivered accurate responses 98\% of the time. Among the students who used AI tutors, 78\% reported that the tutors helped their learning. Beyond AI tutors' ability to provide valuable suggestions, students reported appreciating them for fostering a safe learning environment free from judgment.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0598-4},
  series = {{SIGCSE},
}

@inproceedings{ho_enhancing_2025,
  title = {Enhancing {Visitor},
  author = {Ho, Hoang Phuoc and Ramesh, Vani and Zaloudek, Ivo and Rikhtehgar, Delaram Javdani and Wang, Shenghui},
  year = {2025},
  doi = {10.1145/3708359.3712145},
  url = {https://doi.org/10.1145/3708359.3712145},
  booktitle = {Proceedings of the 30th {International},
  pages = {660--671},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational Agents, Cultural Heritage, Human-Computer Interaction, Large Language Models, Multimodal Interaction, User Engagement, Voice User Interfaces},
  abstract = {Conversational agents in art exhibitions can enhance user engagement and understanding of artworks by providing contextual information, especially through voice interactions. However, creating a deeper personal connection with art — which often requires direct aesthetic and visual experiences — remains a challenge. This paper examines how integrating visual perception into conversational agents can enhance alignment with visitors’ artistic interpretations, thereby fostering deeper engagement with interactive art exhibitions. We introduce a voice-based conversational agent enhanced with visual capabilities via a multimodal large language model (MLLM), allowing the agent to perceive, interpret and discuss artworks in real-time with visitors. The system utilizes a simplified Retrieval-Augmented Generation (RAG) architecture, which collects voice inputs, retrieves relevant information from a domain knowledge graph, and uses the LLM to generate conversational responses, which are then converted into voice outputs. A user study with 36 participants, divided into two groups, was conducted to compare the enhanced system with a baseline system that lacked visual input. Our results show that the visually enhanced system significantly improved visitor engagement and perception. Content analysis of the conversational transcripts further revealed a wider range of conversational topics, deeper visitor perceptions, and the agent’s ability to provide more nuanced, visually-related discussions.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{hu_hedrarag_2025,
  title = {{HedraRAG},
  author = {Hu, Zhengding and Murthy, Vibha and Pan, Zaifeng and Li, Wanlu and Fang, Xiaoyi and Ding, Yufei and Wang, Yuke},
  year = {2025},
  doi = {10.1145/3731569.3764806},
  url = {https://doi.org/10.1145/3731569.3764806},
  booktitle = {Proceedings of the {ACM},
  journal = {arXiv preprint arXiv …},
  pages = {623--638},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Seoul, Republic of Korea},
  keywords = {heterogeneous RAG, LLM, vector search, source: Google Scholar},
  abstract = {In this paper, we identify and tackle emerging system-level challenges in serving heterogeneous RAG workflows, characterized by complex stages and diverse request patterns. We present HedraRAG, a new system built on RAGraph, a graph-based abstraction that exposes optimization opportunities across stage-level parallelism, intra-request similarity, and inter-request skewness. These opportunities are expressed through graph transformations, including node splitting, reordering, edge addition and rewiring. Transformations are dynamically applied to wavefronts of subgraphs across concurrent requests and scheduled onto the CPU-GPU pipeline. Experiments across a wide range of workflows demonstrate that HedraRAG achieves more that 1.5× and up to 5× speedup over existing frameworks, offering a comprehensive solution for heterogeneous RAG workload serving.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1870-0},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{azzopardi_simiir_2024,
  title = {{SimIIR},
  author = {Azzopardi, Leif and Breuer, Timo and Engelmann, Björn and Kreutz, Christin and MacAvaney, Sean and Maxwell, David and Parry, Andrew and Roegiest, Adam and Wang, Xi and Zerhoudi, Saber},
  year = {2024},
  doi = {10.1145/3673791.3698427},
  url = {https://doi.org/10.1145/3673791.3698427},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {197--202},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {conversational ir, interaction, interactive ir, open source framework, simulation, source: ACM},
  abstract = {Evaluating the interactions between users and systems presents many challenges. Simulation offers a reliable, re-usable, and repeatable methodology to explore how different users, user behaviours and/or retrieval systems impact performance. With Large Language Models and Generative AI now widely available and accessible, new affordances are possible. These allow researchers to create more ”realistic” simulated users that can generate queries and judge items like humans, and to develop new retrieval systems where responses and interactions are conversational and based on retrieval augmented generation. This resource paper presents a community-led initiative to update the Simulation of Interactive Information Retrieval (SimIIR) Framework to enable the simulation of conversational search using LLMs. The largest update provides a conversational search workflow which involves a number of new possible interactions with a search system or agent – enabling a host of new development and evaluation opportunities. Other developments include the Markovian Users, Cognitive States, LLM-based components for assessing snippets/documents/responses, generating queries, deciding on when to stop/continue, and PyTerrier integration. This paper aims to mark the release of SimIIR 3.0 and invites the community to build, extend, and use the resource.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{narayanan_venkit_search_2025,
  title = {Search {Engines},
  author = {Narayanan Venkit, Pranav and Laban, Philippe and Zhou, Yilun and Mao, Yixin and Wu, Chien-Sheng},
  year = {2025},
  doi = {10.1145/3715275.3732089},
  url = {https://doi.org/10.1145/3715275.3732089},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1325--1340},
  publisher = {Association for Computing Machinery},
  keywords = {Answer Engines, Ethical Audit, Fairness and Ethics, Generative Search Engine, RAG systems},
  abstract = {As Large Language Model (LLM) applications transition from research to widespread sociotechnical products, they are fundamentally changing how people access and process information. Answer engines - LLM-powered search tools that generate source-cited summaries rather than just returning relevant links - represent a particularly significant shift from traditional search engines. Through a qualitative study of 21 expert users comparing answer engines to traditional search, we identified 16 ethical and societal limitations in current answer engine implementations. We showcase how current LLM-based search engine still lack the neccessary features to be treated as a safe sociotechnical system for public consumption. Based on these findings, we propose a framework of 16 corresponding design recommendations to guide the development of more trustworthy answer engine systems. We showcase of this application is still nascent and how we need to be more sensitive to their social ramifications, to create a better and safer experience for individuals.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{cao_javabench_2024,
  title = {{JavaBench},
  author = {Cao, Jialun and Chen, Zhiyong and Wu, Jiarong and Cheung, Shing-Chi and Xu, Chang},
  year = {2024},
  doi = {10.1145/3691620.3695470},
  url = {https://doi.org/10.1145/3691620.3695470},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {870--882},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {large language model, object-oriented programming, program synthesis, source: ACM},
  abstract = {Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8\% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs' capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3\% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92\%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24\% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench. We also release a leaderboard and invite model developers to participate and test their models against JavaBench at https://java-bench.github.io/leaderboard.html.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{li_bridging_2025,
  title = {Bridging the {Gap},
  author = {Li, Hao and Ren, Yubing and Cao, Yanan and Li, Yingjie and Fang, Fang and Lin, Zheng and Wang, Shi},
  year = {2025},
  doi = {10.1145/3696410.3714571},
  url = {https://doi.org/10.1145/3696410.3714571},
  booktitle = {Proceedings of the {ACM},
  pages = {1811--1821},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {few-shot learning, information extraction, large language model, structure generation, source: ACM},
  abstract = {Large language models (LLMs) achieve superior performance in generative tasks. However, due to the natural gap between language model generation and structured information extraction in three dimensions: task type, output format, and modeling granularity, they often fall short in structured information extraction, a crucial capability for effective data utilization on the web. In this paper, we define the generation process of the language model as the controllable state transition, aligning the generation and extraction processes to ensure the integrity of the output structure and adapt to the goals of the information extraction task. Furthermore, we propose the Structure2Text decider to help the language model understand the fine-grained extraction information, which converts the structured output into natural language and makes state decisions, thereby focusing on the task-specific information kernels, and alleviating language model hallucinations and incorrect content generation. We conduct extensive experiments and detailed analyses on myriad information extraction tasks, including named entity recognition, relation extraction, and event argument extraction. Our method not only achieves significant performance improvements but also considerably enhances the model's capability to generate precise and relevant content, making the extracted content easy to parse.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{guo_optimizing_2025,
  title = {Optimizing {Case},
  author = {Guo, Siyuan and Liu, Huiwu and Chen, Xiaolong and Xie, Yuming and Zhang, Liang and Han, Tao and Chen, Hechang and Chang, Yi and Wang, Jun},
  year = {2025},
  doi = {10.1145/3711896.3737254},
  url = {https://doi.org/10.1145/3711896.3737254},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4487--4498},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {case-based reasoning, functional testing, large language model, reinforcement learning, test script generation},
  abstract = {In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{dao_llm-powered_2024,
  title = {{LLM},
  author = {Dao, Dung and Teo, Jun Yi Claire and Wang, Wenru and Nguyen, Hoang D.},
  year = {2024},
  doi = {10.1145/3643479.3662049},
  url = {https://doi.org/10.1145/3643479.3662049},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  note = {event-place: Phuket, Thailand},
  keywords = {Conversational, Design, Diabetes, Diabetes Prevention, Dialogue, Fine-tuning, GPT-3.5, Multimodal},
  abstract = {The global prevalence of diabetes remains high despite rising life expectancy with improved quality and access to healthcare services. The significant burden that diabetes imposes warrants efforts to improve existing interventions in diabetes care. Present research on diabetes management has shown that artificial intelligence (AI) and Large Language Models (LLM) play an important role in various aspects of the diabetes continuum but a distinct lack of studies in diabetes prevention is observed. Our research introduces a comprehensive digital solution, leveraging the capabilities of GPT-3.5 models maintained by OpenAI, focused specifically on the active prevention of diabetes. The system encompasses a user-friendly interface accessible via mobile and web applications, an AI-powered chatbot for instant Q\&amp;A and advice, personalized reminder systems, a data analysis module for tailored guidance, resource aggregators for health-related information, and an emotional support module to ensure a holistic approach to prevention. Furthermore, our experiments involved testing the quality of responses generated by a fine-tuned GPT-3.5 model, utilizing the Assistants API or a retrieval-augmented generation (RAG) system powered by FAISS for enhanced context awareness and personalized advice. The testing focused on a structured dataset of questions and answers related to diabetes prevention, with results highlighting the superiority of the GPT-3.5 model combined with the Assistants API in providing relevant, detailed, and personalized responses, thus demonstrating its potential as an invaluable tool in the proactive prevention of diabetes.},
  address = {New York, NY, USA},
  series = {{AIQAM},
  isbn = {979-8-4007-0547-2},
}

@inproceedings{wang_metmap_2024,
  title = {{MeTMaP},
  author = {Wang, Guanyu and Li, Yuekang and Liu, Yi and Deng, Gelei and Li, Tianlin and Xu, Guosheng and Liu, Yang and Wang, Haoyu and Wang, Kailong},
  year = {2024},
  doi = {10.1145/3650105.3652297},
  url = {https://doi.org/10.1145/3650105.3652297},
  booktitle = {Proceedings of the 2024 {IEEE},
  pages = {12--23},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {augmented generation, metamorphic testing, vector matching, source: ACM},
  abstract = {Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation.This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51\% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications.},
  address = {New York, NY, USA},
  series = {{FORGE},
  isbn = {979-8-4007-0609-7},
}

@inproceedings{shi_flowxpert_2025,
  title = {{FlowXpert},
  author = {Shi, Binpeng and Luo, Yu and Wang, Jingya and Zhao, Yongxin and Zhang, Shenglin and Hao, Bowen and Zhao, Chenyu and Sun, Yongqian and Zhang, Zhi and Sun, Ronghua and Li, Haihua and Song, Wei and Chen, Xiaolong and Miao, Jingbo and Pei, Dan},
  year = {2025},
  doi = {10.1145/3711896.3737221},
  url = {https://doi.org/10.1145/3711896.3737221},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4839--4850},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {incident management, large language model, troubleshooting, workflow orchestration},
  abstract = {Incident management remains a critical yet challenging task for large-scale cloud services. Most cloud service providers abstract troubleshooting into predefined workflows for different incidents, offering step-by-step guidance. However, manually crafting workflows is resource-consuming and knowledge-intensive, hindering large-scale deployment. Most automated techniques for workflow orchestration rely on large language models (LLMs) to handle complex tasks but overlook key aspects of troubleshooting, including complex expertise, domain requirements, and the reliability of AI feedback. These limitations undermine workflow quality. Therefore, we propose FlowXpert, a novel framework for troubleshooting workflow orchestration. Leveraging LLMs, it first builds a knowledge base centered on incident-aware nodes to precisely depict expertise. Then, fed into AI feedback and synthetic preference data, reinforcement learning is applied to refine the workflow generator and evaluator. To assess troubleshooting workflows, we introduce OpsFlowBench based on Huawei Cloud's datacenter switch operation documents. Benchmark tests under the tailored STEPScore metric validate its effectiveness. Furthermore, during a 10-week deployment in Huawei Cloud's datacenter network, FlowXpert provided valuable support to both on-call engineers and AI executors, as evidenced by empirical data and case study.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{yang_design_2025,
  title = {Design {Activity},
  author = {Yang, Boyin and Dudley, John J and Kristensson, Per Ola},
  year = {2025},
  doi = {10.1145/3719160.3736609},
  url = {https://doi.org/10.1145/3719160.3736609},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {End-user interaction with LLMs and Multimodal models, Generative AI},
  abstract = {Large Language Models (LLMs) can enhance structured design thinking, yet existing copilot approaches integrate them into human workflows rather than exploring their autonomous potential. This paper investigates how LLM-based communicative AI agents can independently tackle open-ended design problems and how their strengths and limitations inform human-AI collaboration. We iteratively design a system where AI agents play different roles and simulate human design activity through conversational turns. The agents investigate user needs, identify design constraints, and explore the design space, with useful insights emerging from their interactions. To assess reasoning quality, we conducted a human jury evaluation with five HCI researchers and explored potential applications through a contextual inquiry with seven professionals. Our findings demonstrate that integrating human design thinking techniques enhances AI reasoning. AI agents effectively tackle design problems, generating low-novelty yet well-grounded and practical solutions that meet key design requirements.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{battaglini-fischer_fails_2025,
  title = {{FAILS},
  author = {Battaglini-Fischer, Sándor and Srinivasan, Nishanthi and Szarvas, Bálint László and Chu, Xiaoyu and Iosup, Alexandru},
  year = {2025},
  doi = {10.1145/3680256.3721320},
  url = {https://doi.org/10.1145/3680256.3721320},
  booktitle = {Companion of the 16th {ACM},
  pages = {187--194},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {failure characterization, failure recovery, incident report, llm, operational data analytics, reliability, system design},
  abstract = {Large Language Model (LLM) services such as ChatGPT, DALL·E, and Cursor have quickly become essential for society, businesses, and individuals, empowering applications such as chatbots, image generation, and code assistance. The complexity of LLM systems makes them prone to failures and affects their reliability and availability, yet their failure patterns are not fully understood, making it an emerging problem. However, there are limited datasets and studies in this area, particularly lacking an open-access tool for analyzing LLM service failures based on incident reports. Addressing these problems, in this work we propose FAILS, the first open-sourced framework for incident reports collection and analysis on different LLM services and providers. FAILS provides comprehensive data collection, analysis, and visualization capabilities, including: (1) It can automatically collect, clean, and update incident data through its data scraper and processing components;(2) It provides 17 types of failure analysis, allowing users to explore temporal trends of incidents, analyze service reliability metrics, such as Mean Time to Recovery (MTTR) and Mean Time Between Failures (MTBF);(3) It leverages advanced LLM tools to assist in data analysis and interpretation, enabling users to gain observations and insights efficiently. All functions are integrated in the backend, allowing users to easily access them through a web-based frontend interface. FAILS supports researchers, engineers, and general users to understand failure patterns and further mitigate operational incidents and outages in LLM services. The framework is publicly available on https://github.com/atlarge-research/FAILS.},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-1130-5},
}

@inproceedings{yuan_scx_2025,
  title = {{SCX},
  author = {Yuan, Mu and Zhang, Lan and Zeng, Liekang and Jiang, Siyang and Yang, Bufang and Duan, Di and Xing, Guoliang},
  year = {2025},
  doi = {10.1145/3718958.3750509},
  url = {https://doi.org/10.1145/3718958.3750509},
  booktitle = {Proceedings of the {ACM},
  pages = {39--54},
  publisher = {Association for Computing Machinery},
  note = {event-place: São Francisco Convent, Coimbra, Portugal},
  keywords = {confidential computing, device-cloud collaboration, KV-cache, large language model, model inference},
  abstract = {Transformer models have revolutionized fields like natural language processing and computer vision but face privacy concerns in sensitive applications such as medical diagnostics. Existing confidential serving methods, including cryptography-based, memory isolation-based, and access control-based, offer trade-offs between privacy and efficiency but often struggle with high latency or hardware dependencies. This work proposes stateless KV-cache encoding (SCX), a novel framework that encodes the intermediate key-value cache during Transformer inference using user-controlled keys. SCX ensures that the cloud can neither recover the input nor independently complete the next token prediction, effectively preserving privacy. By introducing efficient encoding and decoding schemes, SCX addresses communication complexity and attack vulnerabilities while ensuring zero loss of inference quality. Experiments on large Transformer models demonstrate that SCX achieves lower latency (e.g., 36ms for LLaMA-7B), outperforming state-of-the-art cryptography and memory isolation methods by orders of magnitude. Moreover, SCX can complementarily work with advanced KV-cache management techniques to further enhance KV-cache communication efficiency by 85\%, marking a significant step toward practical, privacy-preserving large Transformer serving.},
  address = {New York, NY, USA},
  series = {{SIGCOMM},
  isbn = {979-8-4007-1524-2},
}

@inproceedings{tang_test_2024,
  title = {Test {Large},
  author = {Tang, Zuoyin and He, Jianhua and Pe, Dashuai and Liu, Kezhong and Gao, Tao and Zheng, Jiawei},
  year = {2024},
  doi = {10.1145/3691555.3696825},
  url = {https://doi.org/10.1145/3691555.3696825},
  booktitle = {Proceedings of the 19th {Workshop},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington D.C., DC, USA},
  keywords = {Connected autonomous vehicles, driving theory test, large language model, mobile cloud computing, mobile edge computing, remote driving},
  abstract = {Handling long tail corner cases is a major challenge faced by autonomous vehicles (AVs). While large language models (LLMs) hold great potentials to handle the corner cases with excellent generalization and explanation capabilities and received increasing research interest on application to autonomous driving, there are still technical barriers to be tackled, such as strict model performance and huge computing resource requirements of LLMs, which are difficult to be met locally at AVs. In this paper, we investigate a new approach of applying remote or edge LLMs to support autonomous driving. With this approach connected autonomous vehicles (CAVs) send driving assistance requests to the LLMs. LLMs deployed at the edge of the networks or remote clouds process the requests and generate driving assistance instructions for the CAVs. A key issue for such LLM assisted driving system is the assessment of LLMs on their understanding of driving theory and skills, ensuring they are qualified to undertake safety critical driving assistance tasks for CAVs. As there is no published work on assessing LLM of driving theory and skills, we design and run driving theory tests for several proprietary LLM models (OpenAI GPT models, Baidu Ernie and Ali QWen) and open-source LLM models (Tsinghua MiniCPM-2B and MiniCPM-Llama3-V2.5) with more than 500 multiple-choices theory test questions. These questions are close to the official UK driving theory test ones. Model accuracy, cost and processing latency are measured from the experiments. Experiment results show that while model GPT-4 passes the test with improved domain knowledge and Ernie has an accuracy of 85\% (just below the 86\% passing threshold), other LLM models including GPT-3.5 fail the test. For the test questions with images, the multimodal model GPT4-o has an excellent accuracy result of 96\%, and the MiniCPM-Llama3-V2.5 achieves an accuracy of 76\%. While GPT-4 holds stronger potential for CAV driving assistance applications, the cost of using model GPT4 is much higher, almost 50 times of that of using GPT3.5. The results can help make decision on the use of the existing LLMs for CAV applications and balancing on the model performance and cost.},
  address = {New York, NY, USA},
  series = {{MobiArch},
  isbn = {979-8-4007-1247-0},
}

@inproceedings{zhang_enhancing_2023,
  title = {Enhancing {Financial},
  author = {Zhang, Boyu and Yang, Hongyang and Zhou, Tianyu and Ali Babar, Muhammad and Liu, Xiao-Yang},
  year = {2023},
  doi = {10.1145/3604237.3626866},
  url = {https://doi.org/10.1145/3604237.3626866},
  booktitle = {Proceedings of the {Fourth},
  pages = {349--356},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Instruction Tuning, Large Language Models, Retrieval Augmented Generation, Sentiment Analysis},
  abstract = {Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs’ sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15\% to 48\% performance gain in accuracy and F1 score.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-0240-2},
}

@inproceedings{aodeng_inreactable_2025,
  title = {{InReAcTable},
  author = {Aodeng, Gerile and Li, Guozheng and Feng, Yunshan and Chen, Qiyang and Zhang, Yu and Liu, Chi Harold},
  year = {2025},
  doi = {10.1145/3746059.3747719},
  url = {https://doi.org/10.1145/3746059.3747719},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {exploratory data analysis, large language models., Tabular data, visual data story},
  abstract = {Insights in tabular data capture valuable patterns that help analysts understand critical information. Organizing related insights into visual data stories is crucial for in-depth analysis. However, constructing such stories is challenging because of the complexity of the inherent relations between extracted insights. Users face difficulty sifting through a vast number of discrete insights to integrate specific ones into a unified narrative that meets their analytical goals. Existing methods either heavily rely on user expertise, making the process inefficient, or employ automated approaches that cannot fully capture their evolving goals. In this paper, we introduce InReAcTable, a framework that enhances visual data story construction by establishing both structural and semantic connections between data insights. Each user interaction triggers the Acting module, which utilizes an insight graph for structural filtering to narrow the search space, followed by the Reasoning module using the retrieval-augmented generation method based on large language models for semantic filtering, ultimately providing insight recommendations aligned with the user’s analytical intent. Based on the InReAcTable framework, we develop an interactive prototype system that guides users to construct visual data stories aligned with their analytical requirements. We conducted a case study and a user experiment to demonstrate the utility and effectiveness of the InReAcTable framework and the prototype system for interactively building visual data stories.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{valtolina_teacher-driven_2025,
  title = {A {Teacher},
  author = {Valtolina, Stefano and Matamoros Aragon, Ricardo Anibal and Epifania, Francesco},
  year = {2025},
  doi = {10.1145/3750069.3750121},
  url = {https://doi.org/10.1145/3750069.3750121},
  booktitle = {Proceedings of the 16th {Biannual},
  publisher = {Association for Computing Machinery},
  keywords = {Acceptability and Usability, Conversational interface, End User Development objects, Machine learning for education},
  abstract = {This paper presents a software framework that enables teachers to design reliable, personalised conversational agents tailored to their pedagogical goals and student learning preferences. The system combines a Retrieval-Augmented Generation (RAG) architecture with a visual configuration environment, allowing educators to upload, validate, and organise domain-specific teaching materials into a teacher-curated content corpus. Educators can configure adaptive tutoring strategies based on the VARK model (Visual, Auditory, Reading/Writing, Kinesthetic), allowing the conversational agents to address diverse learning preferences and educational contexts. Unlike fully autonomous or black-box educational AI systems, this approach foregrounds teacher agency and pedagogical alignment, enabling intuitive control over content and interaction style. A preliminary evaluation with university educators assessed usability (SUS), perceived utility (UTAUT), cognitive load (NASA-TLX), and creative-technical capacity (CTS), revealing promising results and informing future design directions. The system supports the development of human-centred AI tutors that are transparent, configurable, and grounded in teacher expertise.},
  address = {New York, NY, USA},
  series = {{CHItaly},
  isbn = {979-8-4007-2102-1},
}

@inproceedings{yang_optimization_2025,
  title = {Optimization of {RAG},
  author = {Yang, Junwen and Zhou, Yanci and Li, Yijin and Zhu, Guohua},
  year = {2025},
  doi = {10.1145/3728199.3728221},
  url = {https://doi.org/10.1145/3728199.3728221},
  booktitle = {Proceedings of the 2025 3rd {International},
  pages = {142--148},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {The Large Language Model (LLM) has some limitations in dealing with illusion problems, acquiring the latest knowledge, and dealing with complex tasks. Retrieval Enhanced Generation (RAG) combines retrieval based and generation based models, utilizing massive external data to assist the LLM in generating more informative, accurate, and contextually relevant responses. Query rewriting solves the above problem by generating new retrieval queries from the user's original queries to obtain external knowledge. The RAG framework MQRF-RAG, based on multiple query rewrites, constructs four different styles of query problems, enabling it to obtain more comprehensive and accurate retrieval documents. Based on Markov decision process optimization rewriter and lightweight prompt adaptive rewriting strategy selection, it better handles complex tasks. The test results show that MQRF-RAG outperforms existing rewriting methods in document retrieval, and the retrieved documents provide accurate external knowledge for the response model, significantly improving its response performance.},
  address = {New York, NY, USA},
  series = {{CNML},
  isbn = {979-8-4007-1323-1},
}

@inproceedings{dubey_auctions_2024,
  title = {Auctions with {LLM},
  author = {Dubey, Avinava and Feng, Zhe and Kidambi, Rahul and Mehta, Aranyak and Wang, Di},
  year = {2024},
  doi = {10.1145/3637528.3672022},
  url = {https://doi.org/10.1145/3637528.3672022},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {713--722},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {auction design, computational advertising},
  abstract = {We study an auction setting in which bidders bid for placement of their content within a summary generated by a large language model (LLM), e.g., an ad auction in which the display is a summary paragraph of multiple ads. This generalizes the classic ad settings such as position auctions to an LLM generated setting, which allows us to handle general display formats. We propose a novel factorized framework in which an auction module and an LLM module work together via a prediction model to provide welfare maximizing summary outputs in an incentive compatible manner. We provide a theoretical analysis of this framework and synthetic experiments to demonstrate the feasibility and validity of the system together with welfare comparisons.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{chen_knowledge_2025,
  title = {Knowledge {Reasoning},
  author = {Chen, Yun and Shen, Hao and Xie, Bangpeng and Wang, Jiayu and Zhao, Wenkai and Pan, Zhijun and Fu, Chaoran and Wang, Xiaohui and Liang, Xiao and Zhang, Zhonghao and Cai, Changyu},
  year = {2025},
  doi = {10.1145/3747912.3747923},
  url = {https://doi.org/10.1145/3747912.3747923},
  booktitle = {Proceedings of the 2025 {International},
  pages = {71--76},
  publisher = {Association for Computing Machinery},
  keywords = {Direct Preference Optimization, Large Language Model, Power Industry},
  abstract = {Although the current large language models (LLM) have very strong general capabilities, they lack professional knowledge of power systems and also do not possess the reasoning ability required to solve complex problems in the field of power systems. Therefore, we introduced a two-stage training method. Specifically, we first constructed and cleaned the Q\&amp;A dataset of power professional knowledge. Based on these data, we fine-tuned the open-source large model to achieve the injection of power professional knowledge. Secondly, we constructed a power preference dataset based on the current most advanced closed-source LLM and expert experience, and used DPO to enable the open-source LLM to learn the expert thought chain in the field of power systems, improving the reasoning ability of the open-source LLM in the field of power systems.},
  address = {New York, NY, USA},
  series = {{SECA},
  isbn = {979-8-4007-1513-6},
}

@inproceedings{frazier_customizing_2024,
  title = {Customizing {ChatGPT},
  author = {Frazier, Matthew and Damevski, Kostadin and Pollock, Lori},
  year = {2024},
  doi = {10.1145/3649217.3653570},
  url = {https://doi.org/10.1145/3649217.3653570},
  booktitle = {Proceedings of the 2024 on {Innovation},
  pages = {633--639},
  publisher = {Association for Computing Machinery},
  note = {event-place: Milan, Italy},
  keywords = {chatgpt, computer science principles, conversational agent, exploratory search, source: ACM},
  abstract = {This paper explores leveraging conversational agents, specifically ChatGPT, to enhance the introduction of computing, focused on the Advanced Placement Computer Science Principles (CSP) course in secondary schools. Despite the potential benefits for diverse student audiences, little research has investigated their effectiveness and engagement in this context. We examine the customization of ChatGPT for secondary school CSP students, assessing its impact on exploratory searches for learning CSP concepts. Results from 20 high school students in grades 10-12 (ages 15-18) in a CSP course indicate that students preferred a customized ChatGPT, with its terminology more suitable to secondary school level, examples more understandable, and better connections to personal experiences compared to standard ChatGPT.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0600-4},
  series = {{ITiCSE},
}

@inproceedings{most_lost_2025,
  title = {Lost in {OCR},
  author = {Most, Alexander and Winjum, Joseph and Bhattarai, Manish and Jones, Shawn and Ranasinghe, Nishath Rajiv and Biswas, Ayan and O'Malley, Dan},
  year = {2025},
  doi = {10.1145/3704268.3742698},
  url = {https://doi.org/10.1145/3704268.3742698},
  booktitle = {Proceedings of the 2025 {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Nottingham, United Kingdom},
  keywords = {source: ACM, source: Scopus},
  abstract = {Retrieval-Augmented Generation (RAG) has become a popular technique for enhancing the reliability and utility of Large Language Models (LLMs) by grounding responses in external documents. Traditional RAG systems rely on Optical Character Recognition (OCR) to first process scanned documents into text. However, even state-of-the-art OCRs can introduce errors, especially in degraded or complex documents. Recent vision-language approaches, such as ColPali, propose direct visual embedding of documents, eliminating the need for OCR. This study presents a systematic comparison between a vision-based RAG system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2 (90B) and Nougat OCR across varying document qualities. Beyond conventional retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark to assess end-to-end question-answering performance. Our findings indicate that while vision-based RAG performs well on documents it has been fine-tuned on, OCR-based RAG is better able to generalize to unseen documents of varying quality. We highlight the key trade-offs between computational efficiency and semantic accuracy, offering practical guidance for RAG practitioners in selecting between OCR-dependent and vision-based document retrieval systems in production environments.},
  address = {New York, NY, USA},
  series = {{DocEng},
  isbn = {979-8-4007-1351-4},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{moayeri_worldbench_2024,
  title = {Worldbench: {Quantifying},
  author = {Moayeri, M. and Tabassi, E. and Feizi, S.},
  year = {2024},
  doi = {10.1145/3630106.3658967},
  url = {https://doi.org/10.1145/3630106.3658967},
  booktitle = {Proceedings of the 2024 {ACM},
  journal = {… of the 2024 ACM Conference on …},
  pages = {1211--1228},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Bias, Factuality, Fairness, Geographic Disparity, Large Language Models},
  abstract = {… we highlight is the impact of retrieval augmented generation (RAG), which is … , hallucinated citations pose a serious challenge in LLM reliability. On one hand, producing false citations …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-0450-5},
}

@inproceedings{roy_exploring_2024,
  title = {Exploring {LLM},
  author = {Roy, Devjeet and Zhang, Xuchao and Bhave, Rashi and Bansal, Chetan and Las-Casas, Pedro and Fonseca, Rodrigo and Rajmohan, Saravan},
  year = {2024},
  doi = {10.1145/3663529.3663841},
  url = {https://doi.org/10.1145/3663529.3663841},
  booktitle = {Companion {Proceedings},
  pages = {208--219},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {AIOps, Cloud Computing, Incident Management, Root Cause Analysis, source: ACM},
  abstract = {The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team’s specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at a large IT corporation. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{cheng_oak_2025,
  title = {Oak {Story},
  author = {Cheng, Alan Y. and Zou, Carolyn Q. and Xie, Anthony and Hsu, Matthew and Yan, Felicia and Huang, Felicity and Zhang, David K. and Sharma, Arjun and Poole, Rashon and Wan Rosli, Daniel and Cuadra, Andrea and Pea, Roy and Landay, James A.},
  year = {2025},
  doi = {10.1145/3746059.3747698},
  url = {https://doi.org/10.1145/3746059.3747698},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Narrative-based education engages children in learning, but traditional approaches offer limited adaptability to individual preferences. Although large language models (LLMs) offer promising opportunities for interactive narratives, balancing their unpredictability with structured learning objectives remains challenging. To answer this challenge, we designed and built Oak Story, an educational mobile application for 4th–6th graders centered on local oak woodland ecosystems. Oak Story employs a learning-goal-directed LLM architecture that adapts the narrative, as well as multimodal real-world activities, to each individual student while ensuring that learning goals are met. In a between-participants study (N = 47), we find that Oak Story produces statistically significant increases in learning gains, engagement, and perceived agency compared to a control with static sequencing within and between scenes. These findings demonstrate an effective architectural approach for LLM-based educational systems that successfully balances learner agency with pedagogical structure.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{kassaie_exploiting_2025,
  title = {Exploiting {Query},
  author = {Kassaie, Besat and Kane, Andrew and Tompa, Frank Wm.},
  year = {2025},
  doi = {10.1145/3704268.3742687},
  url = {https://doi.org/10.1145/3704268.3742687},
  booktitle = {Proceedings of the 2025 {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Nottingham, United Kingdom},
  keywords = {ARQMath Lab, large language model (LLM), math-aware search engine, mathematical community question answering (Math CQA), Mathematical information retrieval (MIR), query reformulation, source: ACM},
  abstract = {Mathematical formulas introduce complications to the standard approaches used in information retrieval. By studying how traditional (sparse) search systems perform in matching queries to documents, we hope to gain insights into which features in the formulas and in the accompanying natural language text signal likely relevance.In this paper, we focus on query rewriting for the ARQMath benchmarks recently developed as part of CLEF, the Conference and Labs of the Evaluation Forum. In particular, we improve mathematical community question answering applications by using responses from a large language model (LLM) to reformulate queries. Beyond simply replacing the query by the LLM response or concatenating the response to the query, we explore whether improvements accrue from the LLM selecting a subset of the query terms, augmenting the query with additional terms, or re-weighting the query terms. We also examine whether such query reformulation is equally advantageous for math features extracted from formulas and for keyword terms. As a final step, we use reciprocal rank fusion (RRF) to combine several component approaches in order to improve ranking results. In two experiments involving real-world mathematical questions, we show that combining four strategies for term selection, term augmentation, and term re-weighting improves nDCG'@1000 by 5\%, MAP'@1000 by 7\%, and P'@10 by more than 9\% over using the question as given.},
  address = {New York, NY, USA},
  series = {{DocEng},
  isbn = {979-8-4007-1351-4},
}

@inproceedings{hegde_self_2025,
  title = {Self {Supervised},
  author = {Hegde, Raveendra R and Sharma, Saurabh},
  year = {2025},
  doi = {10.1145/3703412.3703421},
  url = {https://doi.org/10.1145/3703412.3703421},
  booktitle = {Proceedings of the 4th {International},
  publisher = {Association for Computing Machinery},
  keywords = {Language generation, LLM, Self Supervised},
  abstract = {While we can customize large language models (LLMs) on specific domains by finetuning using the domain specific labeled data, performance of the customized models is highly dependent on the quality of the labeled data. Obtaining high-quality labeled data for custom domains often requires considerable human effort and associated costs. However, in many cases, unlabeled data is readily available at little or no cost. Existing methods either rely on continued pre-training or use general purpose models trained for synthesis. But, continued pre-training necessitates vast amounts of data and adversely affects instruction tuned models. On the other hand, general purpose synthesis models might not capture the nuances of custom data. We present a framework (SSLC) for customizing LLMs using unlabeled text to enhance contextual question answering on custom data. Our approach employs few-shot synthesis using an instruction-tuned model, curates the synthesized data using a LLM response scorer and finetunes the model on this synthesized data. We demonstrate that the approach significantly improves contextual question answering performance compared to the baselines. It outperforms baselines in 75\% (9/12) of the experiments as evidenced by both quantitative and qualitative metrics. On an average, it outperforms un-customized models by 19.3 percentage points and state-of-the-art approach by 4.4 percentage points in human evaluation(proxy) accuracy.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1161-9},
}

@inproceedings{di_enhancing_2025,
  title = {Enhancing {Code},
  author = {Di, Yifeng and Zhang, Tianyi},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00165},
  url = {https://doi.org/10.1109/icse55347.2025.00165},
  booktitle = {Proceedings of the {IEEE},
  pages = {1359--1371},
  publisher = {IEEE Press},
  keywords = {code generation, code refinement, LLM},
  abstract = {Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1\% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7\% faster and with 10.5\% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{liu_symagent_2025,
  title = {{SymAgent},
  author = {Liu, Ben and Zhang, Jihai and Lin, Fangquan and Yang, Cheng and Peng, Min and Yin, Wotao},
  year = {2025},
  doi = {10.1145/3696410.3714768},
  url = {https://doi.org/10.1145/3696410.3714768},
  booktitle = {Proceedings of the {ACM},
  pages = {98--108},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {knowledge graph, large language model agent, self-learning, source: ACM},
  abstract = {Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM's inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{hu_aptness_2024,
  title = {{APTNESS},
  author = {Hu, Yuxuan and Tan, Minghuan and Zhang, Chenwei and Li, Zixuan and Liang, Xiaodan and Yang, Min and Li, Chengming and Hu, Xiping},
  year = {2024},
  doi = {10.1145/3627673.3679687},
  url = {https://doi.org/10.1145/3627673.3679687},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {900--909},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {appraisal theory, emotional support strategy, empathetic response generation, empathy, retrieval augmented generation, source: ACM},
  abstract = {Empathetic response generation is designed to comprehend the emotions of others and select the most appropriate strategies to assist them in resolving emotional challenges. Empathy can be categorized into cognitive empathy and affective empathy. The former pertains to the ability to understand and discern the emotional issues and situations of others, while the latter involves the capacity to provide comfort. To enhance one's empathetic abilities, it is essential to develop both these aspects. Therefore, we develop an innovative framework that combines retrieval augmentation and emotional support strategy integration. Our framework starts with the introduction of a comprehensive emotional palette for empathy. We then apply appraisal theory to decompose this palette and create a database of empathetic responses. This database serves as an external resource and enhances the LLM's empathy by integrating semantic retrieval mechanisms. Moreover, our framework places a strong emphasis on the proper articulation of response strategies. By incorporating emotional support strategies, we aim to enrich the model's capabilities in both cognitive and affective empathy, leading to a more nuanced and comprehensive empathetic response. Finally, we extract datasets ED and ET from the empathetic dialogue dataset EmpatheticDialogues and ExTES based on dialogue length. Experiments demonstrate that our framework can enhance the empathy ability of LLMs from both cognitive and affective empathy perspectives. Our code is released at https://github.com/CAS-SIAT-XinHai/APTNESS.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{singh_rags_2025,
  title = {{RAGs},
  author = {Singh, Ravi Kumar and Singh, Kuldeep and Kunde, Shruti and Mishra, Mayank and Singhal, Rekha and Nambiar, Manoj},
  year = {2025},
  doi = {10.1145/3680256.3721327},
  url = {https://doi.org/10.1145/3680256.3721327},
  booktitle = {Companion of the 16th {ACM},
  pages = {114--120},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {complex query, cost-efficient rag, query orchestration},
  abstract = {Large Language Models (LLMs) have become integral to modern business operations, especially for tasks involving reasoning over large datasets. One prominent application of LLMs is in chatbot systems, where customers provide natural language queries, often complex in nature, requiring decomposition to retrieve relevant information from various data sources. These queries may span structured databases, unstructured data, or public information from the internet, making efficient data retrieval and reasoning vital for real-time, accurate responses. In this paper, we propose two cost-efficient ”Query Orchestration” approaches (Context Latent and Context Acute) to address these challenges. By leveraging graph-based retrieval-augmented generation (RAG) techniques for vector search, we optimize data retrieval while minimizing reliance on LLMs for reasoning to reduce costs. Our approach is validated through experiments on a banking use case, where we demonstrate its effectiveness in providing high-quality},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-1130-5},
}

@inproceedings{xu_large_2025,
  title = {Large {Language},
  author = {Xu, Yulu and Chen, Shishuo and Tang, Lisirui and Yun, Jiya and Li, Gangmin and Wang, Chengyu},
  year = {2025},
  doi = {10.1145/3722150.3722167},
  url = {https://doi.org/10.1145/3722150.3722167},
  booktitle = {Proceedings of the 2025 9th {International},
  pages = {35--39},
  publisher = {Association for Computing Machinery},
  keywords = {Finetune, HeXie Management Theory, Large Language Models, RAG},
  abstract = {HeXie Management Theory (HXMT) is a modern management theory for organizations. It provides macro-level considerations for both the internal mechanisms of an organization and its overall operational patterns. It has been utilized in organizations such as healthcare, rural construction, university management, and large-scale engineering projects and has proved useful. There are press demands for its wider adoption. Large Language Models (LLMs) have been widely used in natural language processing and content generation. Re-training LLMs will help them possess HeXie management theory, which can be useful. However, there are two popular methods to achieve this goal: fine-tuning and RAG; each approach has pros and cons. This paper reports our efforts in a comparative study of the two approaches. Our research employs datasets from HXMT and chooses the open-source platforms LlaMA-2, LlaMA-3, and ERNIE-Speed for fine-tuning based on four metrics and manual evaluations, with RAG we used ERNIE models with five dimensions. Our results show that the RAG-trained ERNIE-speed-App performs better than the fine-tuning training ERNIE-speed-8k model under the same training data volume. this may shed some light on similar applications where new theory needs to be integrated into an LLM to make it specialized for particular applications. Our work is available at https://alex17swim.com/hxjun2.},
  address = {New York, NY, USA},
  series = {{CCEAI},
  isbn = {979-8-4007-1164-0},
}

@inproceedings{maslych_mitigating_2025,
  title = {Mitigating {Response},
  author = {Maslych, Mykola and Katebi, Mohammadreza and Lee, Christopher and Hmaiti, Yahya and Ghasemaghaei, Amirpouya and Pumarada, Christian and Palmer, Janneese and Segarra Martinez, Esteban and Emporio, Marco and Snipes, Warren and McMahan, Ryan P. and LaViola Jr., Joseph J.},
  year = {2025},
  doi = {10.1145/3719160.3736636},
  url = {https://doi.org/10.1145/3719160.3736636},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational interfaces, LLM, response latency., user study, VR},
  abstract = {We investigated the challenges of mitigating response delays in free-form conversations with virtual agents powered by Large Language Models (LLMs) within Virtual Reality (VR). For this, we used conversational fillers, such as gestures and verbal cues, to bridge delays between user input and system responses and evaluate their effectiveness across various latency levels and interaction scenarios. We found that latency above 4 seconds degrades quality of experience, while natural conversational fillers improve perceived response time, especially in high-delay conditions. Our findings provide insights for practitioners and researchers to optimize user engagement whenever conversational systems’ responses are delayed by network limitations or slow hardware. We also contribute an open-source pipeline that streamlines deploying conversational agents in virtual environments.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@article{khurana_why_2024,
  title = {Why and when llm-based assistants can go wrong: {Investigating},
  author = {Khurana, A. and Subramonyam, H. and Chilana, P. K.},
  year = {2024},
  doi = {10.1145/3640543.3645200},
  url = {https://doi.org/10.1145/3640543.3645200},
  booktitle = {Proceedings of the 29th {International},
  journal = {Proceedings of the 29th …},
  pages = {288--303},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, feature-rich software, help-seeking, large language models, prompt-based interactions},
  abstract = {… is known as Retrieval Augmented Generation (RAG) Vector search … For the search approach used in RAG, we tried different … that the LLM outputs were credible: “... it [LLM] gave me what …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-0508-3},
}

@inproceedings{kazemitabaar_codeaid_2024,
  title = {{CodeAid},
  author = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley, Austin Zachary and Denny, Paul and Craig, Michelle and Grossman, Tovi},
  year = {2024},
  doi = {10.1145/3613904.3642773},
  url = {https://doi.org/10.1145/3613904.3642773},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {AI assistants, AI tutoring, class deployment, design guidelines, educational technology, generative AI, intelligent tutoring systems, large language models, programming education},
  abstract = {Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{wang_rap-gen_2023,
  title = {{RAP},
  author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
  year = {2023},
  doi = {10.1145/3611643.3616256},
  url = {https://doi.org/10.1145/3611643.3616256},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {146--158},
  publisher = {Association for Computing Machinery},
  note = {event-place: San Francisco, CA, USA},
  keywords = {Automated program repair, Neural networks, Pretrained language models, Retrieval-augmented generation, source: ACM},
  abstract = {Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70\% to 54.15\% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0327-0},
  series = {{ESEC},
}

@inproceedings{li_mm-forecast_2024,
  title = {{MM},
  author = {Li, Haoxuan and Yang, Zhengmao and Ma, Yunshan and Bin, Yi and Yang, Yang and Chua, Tat-Seng},
  year = {2024},
  doi = {10.1145/3664647.3681593},
  url = {https://doi.org/10.1145/3664647.3681593},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {2776--2785},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {multimodal event forecasting, multimodal large language model, temporal event forecasting},
  abstract = {We study an emerging and intriguing problem of multimodal temporal event forecasting with large language models. Compared to using text or graph modalities, the investigation of utilizing images for temporal event forecasting has not been fully explored, especially in the era of large language models (LLMs). To bridge this gap, we are particularly interested in two key questions of: 1) why images will help in temporal event forecasting, and 2) how to integrate images into the LLM-based forecasting framework. To answer these research questions, we propose to identify two essential functions that images play in the scenario of temporal event forecasting, i.e., highlighting and complementary. Then, we develop a novel framework, named MM-Forecast. It employs an Image Function Identification module to recognize these functions as verbal descriptions using multimodal large language models (MLLMs), and subsequently incorporates these function descriptions into LLM-based forecasting models. To evaluate our approach, we construct a new multimodal dataset, MidEast-TE-mm, by extending an existing event dataset MidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast can correctly identify the image functions, and further more, incorporating these verbal function descriptions significantly improves the forecasting performance. The dataset, code, and prompts are available at https://github.com/LuminosityX/MM-Forecast.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{zhou_instructpipe_2025,
  title = {{InstructPipe},
  author = {Zhou, Zhongyi and Jin, Jing and Phadnis, Vrushank and Yuan, Xiuxiu and Jiang, Jun and Qian, Xun and Wright, Kristen and Sherwood, Mark and Mayes, Jason and Zhou, Jingtao and Huang, Yiyi and Xu, Zheng and Zhang, Yinda and Lee, Johnny and Olwal, Alex and Kim, David and Iyengar, Ram and Li, Na and Du, Ruofei},
  year = {2025},
  doi = {10.1145/3706598.3713905},
  url = {https://doi.org/10.1145/3706598.3713905},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Deep Learning, Deep Neural Networks, Graph Compiler, Large Language Models, Low-code Development, Node-graph Editor, Visual Analytics, Visual Programming, Visual Prototyping},
  abstract = {Visual programming has the potential of providing novice programmers with a low-code experience to build customized processing pipelines. Existing systems typically require users to build pipelines from scratch, implying that novice users are expected to set up and link appropriate nodes from a blank workspace. In this paper, we introduce InstructPipe, an AI assistant for prototyping machine learning (ML) pipelines with text instructions. We contribute two large language model (LLM) modules and a code interpreter as part of our framework. The LLM modules generate pseudocode for a target pipeline, and the interpreter renders the pipeline in the node-graph editor for further human-AI collaboration. Both technical and user evaluation (N=16) shows that InstructPipe empowers users to streamline their ML pipeline workflow, reduce their learning curve, and leverage open-ended commands to spark innovative ideas.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{qian_shape-it_2024,
  title = {{SHAPE},
  author = {Qian, Wanli and Gao, Chenfeng and Sathya, Anup and Suzuki, Ryo and Nakagaki, Ken},
  year = {2024},
  doi = {10.1145/3654777.3676348},
  url = {https://doi.org/10.1145/3654777.3676348},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Code-Generation, LLMs, Shape Display, Text-based Authoring},
  abstract = {This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands. By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming. We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes. Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user’s textual command into executable code and allows for quick exploration through a web-based control interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10). The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI. However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{djeffal_reflexive_2025,
  title = {Reflexive {Prompt},
  author = {Djeffal, Christian},
  year = {2025},
  doi = {10.1145/3715275.3732118},
  url = {https://doi.org/10.1145/3715275.3732118},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1757--1768},
  publisher = {Association for Computing Machinery},
  keywords = {Accountability, AI alignment, AI Ethics, AI Governance, Human-AI Interaction, Prompt Engineering, Responsible AI},
  abstract = {Responsible prompt engineering has emerged as a critical pracitce for ensuring that generative artificial intelligence (AI) systems are aligned with ethical, legal, and social principles. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. It is, therefore, necessary to examine how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes “Reflexive Prompt Engineering”, a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of emerging practices, this article illustrates how responsible prompt engineering serves as a crucial connection between AI development and deployment, enabling organizations to align AI outputs without modifying underlying model architectures. This approach links with broader “Responsibility by Design” principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering as an essential component of AI literacy.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{malaborbor_enhancing_2024,
  title = {Enhancing {Data},
  author = {Malaborbor, Rose Ann Caparas and Rivera, Vincent Sulit and Diloy, Marlon A. and De Luna, Leandro R. and Dela Cruz, Aira Leigh Y. and Velasco, Abigail T.},
  year = {2024},
  doi = {10.1145/3696230.3696247},
  url = {https://doi.org/10.1145/3696230.3696247},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {271--276},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {A System, BLEU, LangChain technology, llama cpp Large Language Models (LLMs), METEOR, Q\&amp, Retrieval-Augmented Generation (RAG) framework, ROUGE, source: ACM, source: Scopus},
  abstract = {With the advent of the digital age, data privacy has become increasingly relevant, provoking widespread concern and attention. As technology facilitates increased communication and automation in various aspects of life, companies and organizations must protect personal information. The National Privacy Commission (NPC) enforces the Data Privacy Act of 2012, which protects individuals' privacy rights. Despite legislative efforts, public awareness of data privacy remains low, leaving individuals vulnerable to cybercrime. To address this gap, this study proposes the development of a Question Answering (Q\&amp;A) system leveraging LangChain technology and llama cpp Large Language Models (LLMs) to educate individuals about the Data Privacy Act of 2012. It proposes a methodology that utilizes document embeddings, vector storage, and semantic search techniques to facilitate efficient Q\&amp;A processing. As part of the performance evaluation, BLEU, METEOR, and ROUGE metrics are used. These metrics demonstrate varying degrees of alignment between candidate responses and reference texts. Results show varying levels of alignment between candidate responses and reference texts. This includes notable strengths in addressing certain queries, but also areas for improvement. Overall, the study underscores the importance of leveraging advanced technologies to educate individuals about data privacy rights and responsibilities.},
  address = {New York, NY, USA},
  series = {{ICDTE},
  isbn = {979-8-4007-1757-4},
  annote = {Cited by: 0},
}

@inproceedings{futamura_itochat2024_2025,
  title = {{iTOChat2024},
  author = {Futamura, Sho and Nakao, Isshin and Kita, Shusaku and Fujimoto, Ryusei and Mine, Tsunenori},
  year = {2025},
  doi = {10.1145/3732437.3732766},
  url = {https://doi.org/10.1145/3732437.3732766},
  booktitle = {Proceedings of the 2024 {International},
  pages = {50--56},
  publisher = {Association for Computing Machinery},
  keywords = {chatbot, large language model (LLM), mobile recommendation systems},
  abstract = {Many high school students struggle to determine which university department suits their interests based on limited information from websites or open campuses. In particular, in the case of the School of Engineering at Kyushu University, 12 different departments are divided into five groups, with one of the departments joining two groups, and a special group called Group VI, where all students who belong to this group have the opportunity to join any department when they become sophomores. This complex system makes it difficult for high school students to determine which department best suits their aptitudes. To alleviate this problem, we present iTOChat2024, a mobile department recommendation system designed for high school students attending the open campus of the School of Engineering, Kyushu University, which uses ChatGPT for its recommendations. It prompts high school students to enter information according to specific questions, and based on their responses, ChatGPT provides a ranked list of recommended departments along with the reasons for the recommendations and keywords representing the students’ interests. This paper describes the features of iTOChat2024 and discusses the evaluation results of student satisfaction and an analysis of the actual recommendations generated during the open campus event.},
  address = {New York, NY, USA},
  series = {{ICEA},
  isbn = {979-8-4007-1166-4},
}

@article{chen_chineseecomqa_2025,
  title = {Chineseecomqa: {A},
  author = {Chen, H. and Lv, K. and Hu, C. and Li, Y. and Yuan, Y. and He, Y. and {...},
  year = {2025},
  doi = {10.1145/3711896.3737374},
  url = {https://doi.org/10.1145/3711896.3737374},
  booktitle = {Proceedings of the 31st {ACM},
  journal = {Proceedings of the 31st …},
  pages = {5311--5321},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, benchmark, e-commerce, large language models},
  abstract = {… of LLM validation, Retrieval-Augmented Generation (RAG) … [23] A perfectly calibrated model should exhibit confidence … , we prompt the LLM to generate confidence scores (ranging 0 …},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{duvvuru_llm-agents_2025,
  title = {{LLM},
  author = {Duvvuru, Venkata Sai Aswath and Zhang, Bohan and Vierhauser, Michael and Agrawal, Ankit},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00223},
  url = {https://doi.org/10.1109/icse55347.2025.00223},
  booktitle = {Proceedings of the {IEEE},
  pages = {385--397},
  publisher = {IEEE Press},
  keywords = {AI for SE, simulation testing, sUAS},
  abstract = {Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{tuck_llms_2025,
  title = {{LLMs},
  author = {Tuck, Bryan E.},
  year = {2025},
  doi = {10.1145/3716815.3729018},
  url = {https://doi.org/10.1145/3716815.3729018},
  booktitle = {Proceedings of the 10th {ACM},
  pages = {34--35},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {adversarial machine learning, content detection, large language models, nlp security, prompt engineering, tutorial, source: ACM},
  abstract = {With Large Language Models (LLMs) powering critical applications, adversarial threats present urgent challenges to their safety and reliability. This tutorial explores adversarial threats against LLMs by covering foundational concepts, identifying key security implications, examining specific attack vectors (such as data poisoning, evasion techniques, and prompt-engineering vulnerabilities), and highlighting LLMs' dual roles as both targets and enablers of malicious activity. We critically assess current defensive approaches, discuss recent criticisms regarding detection reliability and ethical considerations, and outline key open research challenges. Attendees will gain practical insights into anticipating and mitigating adversarial threats to secure the deployment and application of LLM systems.},
  address = {New York, NY, USA},
  series = {{IWSPA},
  isbn = {979-8-4007-1501-3},
}

@inproceedings{yu_llm-guided_2025,
  title = {{LLM},
  author = {Yu, YiMing and Zutty, Jason},
  year = {2025},
  doi = {10.1145/3712255.3734340},
  url = {https://doi.org/10.1145/3712255.3734340},
  booktitle = {Proceedings of the {Genetic},
  pages = {2363--2370},
  publisher = {Association for Computing Machinery},
  note = {event-place: NH Malaga Hotel, Malaga, Spain},
  keywords = {automated machine learning, computer aided/automated design, large language models, neuroevolution},
  abstract = {In machine learning, Neural Architecture Search (NAS) requires domain knowledge of model design and a large amount of trial-and-error to achieve promising performance. Meanwhile, evolutionary algorithms have traditionally relied on fixed rules and pre-defined building blocks. The Large Language Model (LLM)-Guided Evolution (GE) framework transformed this approach by incorporating LLMs to directly modify model source code for image classification algorithms on CIFAR data and intelligently guide mutations and crossovers. A key element of LLM-GE is the "Evolution of Thought" (EoT) technique, which establishes feedback loops, allowing LLMs to refine their decisions iteratively based on how previous operations performed. In this study, we perform NAS for object detection by improving LLM-GE to modify the architecture of You Only Look Once (YOLO) models to enhance performance on the KITTI dataset. Our approach intelligently adjusts the design and settings of YOLO to find the optimal algorithms against objective such as detection accuracy and speed. We show that LLM-GE produced variants with significant performance improvements, such as an increase in Mean Average Precision from 92.5\% to 94.5\%. This result highlights the flexibility and effectiveness of LLM-GE on real-world challenges, offering a novel paradigm for automated machine learning that combines LLM-driven reasoning with evolutionary strategies.},
  address = {New York, NY, USA},
  series = {{GECCO},
  isbn = {979-8-4007-1464-1},
}

@inproceedings{tang_this_2025,
  title = {"{This},
  author = {Tang, Yilin and Fang, Yuyang and Wang, Tianle and Sun, Lingyun and Chen, Liuqing},
  year = {2025},
  doi = {10.1145/3746059.3747597},
  url = {https://doi.org/10.1145/3746059.3747597},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial intelligence (AI), Blind and low vision, Hallucination, Human-centered AI, Large visual language models (LVLMs)},
  abstract = {Visual question-answering (VQA) tools powered by large visual language models (LVLMs) are used to assist blind and low-vision (BLV) individuals in overcoming visual challenges, raising concerns about hallucinations and associated risks. Existing literature overlooks the variations of hallucinations across distinct usage scenarios and types in the context of VQA for BLV people, resulting in limited understanding of their perceptions and insufficient guidance for targeted mitigation strategies. By analyzing 3,467 real-world VQA cases from BLV users, we developed a manifestation-scenario-based dual-dimensional hallucination typology, uncovering eight scenarios and five types of hallucinations. Through interviews with 16 BLV users, we examined their awareness levels, detection strategies, mental models of hallucinations, and their tolerance of associated risks, identifying key gaps between their perceptions and real situations. By designing with 12 BLV users, we uncovered their expectations for hallucination-mitigating solutions, including enhanced information provision, transparency in processing, verification strategies, and feedback mechanisms.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{sagtani_improving_2025,
  title = {Improving {FIM},
  author = {Sagtani, Hitesh and Mehrotra, Rishabh and Liu, Beyang},
  year = {2025},
  doi = {10.1145/3701551.3703563},
  url = {https://doi.org/10.1145/3701551.3703563},
  booktitle = {Proceedings of the {Eighteenth},
  pages = {801--810},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hannover, Germany},
  keywords = {a/b-testing, code completions, large language model},
  abstract = {Fill-in-the-Middle (FIM) models play a vital role in code completion tasks, leveraging both prefix and suffix context to provide more accurate and contextually relevant suggestions. This paper presents approaches to improve FIM code completion while addressing the challenge of maintaining low latency for real-time coding assistance. We enhance FIM code completion by incorporating context and curriculum examples in the training process. We identify patterns where completion suggestions fail more frequently, revealing complexities that smaller language models struggle with. To address these challenges, we develop a curriculum dataset by extracting hard-to-complete patterns from code repositories and generate context examples using semantic and static analysis tools (e.g. TSC compiler). We fine-tune various sized models, including StarCoder and DeepSeek, on this enhanced dataset. Our evaluation encompasses three key dimensions: the Santa Coder FIM task, the Amazon CCEval benchmark, and a new Multi-Line Infilling evaluation benchmark derived from SWE-bench. Comprehensive ablation studies across multiple model sizes reveal that while all fine-tuned models show improvements, the performance gains are more pronounced for smaller parameter models and that incorporating difficult-to-complete examples as part of curriculum learning improves completion performance. This finding is particularly sig- nificant given the latency constraints of code completion tasks. While larger models like GPT and Claude perform well in multi- line completions but are prohibitively challenging to use given high latency, and our fine-tuned models achieve a balance between per- formance and latency. Finally, we validate our approach through online A/B testing, demonstrating tangible improvements in Completion Acceptance Rate (CAR) and Completion Persistence Rate (CPR), with zero latency impact.},
  address = {New York, NY, USA},
  series = {{WSDM},
  isbn = {979-8-4007-1329-3},
}

@inproceedings{treanor_slice_2025,
  title = {Slice of {Life},
  author = {Treanor, Mike and Samuel, Ben and Nelson, Mark J.},
  year = {2025},
  doi = {10.1145/3723498.3723806},
  url = {https://doi.org/10.1145/3723498.3723806},
  booktitle = {Proceedings of the 20th {International},
  publisher = {Association for Computing Machinery},
  keywords = {Large Language Models, Playable Experiences, Social Simulation},
  abstract = {This paper describes the social physics game Slice of Life. In Slice of Life, the player strives to achieve various social goals by choosing social interactions for characters to engage in. These interactions are governed by a social simulation system called Ensemble with Social Practices (ESP). The ways to achieve the player’s social goals are numerous and any given playthrough of the game will result in drastically different social worlds. Slice of Life also makes use of the underlying social simulation system’s detailed state to generate symbolically grounded prompts for a large language model (LLM) that generates context-appropriate character dialogue. Rather than using LLMs for novelty or for economic reasons, the underlying social simulation technology, we argue, necessitates this approach in order to make it feasible to have nuanced dialogue that reflects the many ways characters could have gotten themselves into particular social situations. The purpose of this paper is to provide a detailed account of Slice of Life’s design, how its social physics simulation enables interactive conversations based on social practices, and to illustrate how the generative possibilities of LLMs can be uniquely useful when applied as its natural language generation (NLG) system, without giving up authorial control of the gameplay or story.},
  address = {New York, NY, USA},
  series = {{FDG},
  isbn = {979-8-4007-1856-4},
}

@inproceedings{guo_multimodal_2025,
  title = {Multimodal {Behavioral},
  author = {Guo, Dongyang and Abdrabou, Yasmeen and Thaqi, Enkeleda and Kasneci, Enkelejda},
  year = {2025},
  doi = {10.1145/3716553.3750787},
  url = {https://doi.org/10.1145/3716553.3750787},
  booktitle = {Proceedings of the 27th {International},
  pages = {475--484},
  publisher = {Association for Computing Machinery},
  keywords = {Behavioral Patterns, Educational Technology, Eye Tracking, Human-AI Interaction, LLM, Multimodal Analysis},
  abstract = {Eye-tracking data reveals valuable insights into users’ cognitive states but is difficult to analyze due to its structured, non-linguistic nature. While large language models (LLMs) excel at reasoning over text, they struggle with temporal and numerical data. This paper presents a multimodal human–AI collaborative framework designed to enhance cognitive pattern extraction from eye-tracking signals. The framework includes: (1) a multi-stage pipeline using horizontal and vertical segmentation alongside LLM reasoning to uncover latent gaze patterns; (2) an Expert–Model Co-Scoring Module that integrates expert judgment with LLM output to generate trust scores for behavioral interpretations; and (3) a hybrid anomaly detection module combining LSTM-based temporal modeling with LLM-driven semantic analysis. Our results across several LLMs and prompt strategies show improvements in consistency, interpretability, and performance, with up to 50\% accuracy in difficulty prediction tasks. This approach offers a scalable, interpretable solution for cognitive modeling and has broad potential in adaptive learning, human–computer interaction, and educational analytics.},
  address = {New York, NY, USA},
  series = {{ICMI},
  isbn = {979-8-4007-1499-3},
}

@inproceedings{schneider_engineering_2025,
  title = {Engineering {Prompts},
  author = {Schneider, Nicole R. and Ramachandran, Nandini and O'Sullivan, Kent and Samet, Hanan},
  year = {2025},
  doi = {10.1145/3701716.3717807},
  url = {https://doi.org/10.1145/3701716.3717807},
  booktitle = {Companion {Proceedings},
  pages = {1633--1634},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {source: ACM},
  abstract = {Large Language Models (LLMs) are often used for tasks that involve reasoning about the physical world, like recommending travel itineraries. However, success at these tasks requires the LLM to have been exposed to the relevant places, which is not true for lesser-known or alternatively named places, like Indigenous place names. Our prompting technique handles this issue using Retrieval Augmented Generation, encoding a spatial graph of common places and a mapping to their Indigenous alternatives. Our method improves LLM performance on spatial tasks involving lesser-known place names, thus advancing AI fairness.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{yan_assertllm_2025,
  title = {{AssertLLM},
  author = {Yan, Zhiyuan and Fang, Wenji and Li, Mengming and Li, Min and Liu, Shang and Xie, Zhiyao and Zhang, Hongce},
  year = {2025},
  doi = {10.1145/3658617.3697756},
  url = {https://doi.org/10.1145/3658617.3697756},
  booktitle = {Proceedings of the 30th {Asia},
  pages = {614--621},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {source: ACM},
  abstract = {Assertion-based verification (ABV) is a critical method to ensure logic designs comply with their architectural specifications. ABV requires assertions, which are generally converted from specifications through human interpretation by verification engineers. Existing methods for generating assertions from specification documents are limited to sentences extracted by engineers, discouraging their practical applications. In this work, we present AssertLLM, an automatic assertion generation framework that processes complete specification documents. AssertLLM can generate assertions from both natural language and waveform diagrams in specification files. It first converts unstructured specification sentences and waveforms into structured descriptions using natural language templates. Then, a customized Large Language Model (LLM) generates the final assertions based on these descriptions. Our evaluation demonstrates that AssertLLM can generate more accurate and higher-quality assertions compared to GPT-4o and GPT-3.5.},
  address = {New York, NY, USA},
  series = {{ASPDAC},
  isbn = {979-8-4007-0635-6},
}

@inproceedings{rao_riskrag_2025,
  title = {{RiskRAG},
  author = {Rao, Pooja S. B. and Šćepanović, Sanja and Zhou, Ke and Bogucka, Edyta Paulina and Quercia, Daniele},
  year = {2025},
  doi = {10.1145/3706598.3713979},
  url = {https://doi.org/10.1145/3706598.3713979},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI model, AI risk, harm, incident, model cards, responsible AI, risk report},
  abstract = {Risk reporting is essential for documenting AI models, yet only 14\% of model cards mention risks, out of which 96\% copying content from a small set of cards, leading to a lack of actionable insights. Existing proposals for improving model cards do not resolve these issues. To address this, we introduce RiskRAG, a Retrieval Augmented Generation based risk reporting solution guided by five design requirements we identified from literature, and co-design with 16 developers: identifying diverse model-specific risks, clearly presenting and prioritizing them, contextualizing for real-world uses, and offering actionable mitigation strategies. Drawing from 450K model cards and 600 real-world incidents, RiskRAG pre-populates contextualized risk reports. A preliminary study with 50 developers showed that they preferred RiskRAG over standard model cards, as it better met all the design requirements. A final study with 38 developers, 40 designers, and 37 media professionals showed that RiskRAG improved their way of selecting the AI model for a specific application, encouraging a more careful and deliberative decision-making. The RiskRAG project page is accessible at: https://social-dynamics.net/ai-risks/card.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{niu_iceage_2025,
  title = {{ICEAGE},
  author = {Niu, Chenxu and Zhang, Wei and Side, Mert and Chen, Yong},
  year = {2025},
  doi = {10.1145/3733723.3733731},
  url = {https://doi.org/10.1145/3733723.3733731},
  booktitle = {Proceedings of the 37th {International},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {More and more scientific applications store datasets in scientific data formats such as HDF5 and netCDF. However, existing search methods for scientific data formats generally require researchers to be familiar with the formats and metadata structure, resulting in a steep learning curve. Therefore, researchers need a natural language query method to query scientific data. In this paper, we propose ICEAGE, a novel Intelligent Contextual Exploration and Answer Generation Engine that bridges the gap between natural language querying and scientific data and metadata retrieval. Based on a retrieval-augmented generation framework, ICEAGE generates reliable and human-readable responses without requiring extensive domain-specific fine-tuning by applying unique indexing method for scientific datasets. Our experimental results demonstrate that ICEAGE significantly outperforms existing methods in terms of accuracy, throughput in both CPU-GPU and CPU-only environments.},
  address = {New York, NY, USA},
  series = {{SSDBM},
  isbn = {979-8-4007-1462-7},
}

@inproceedings{lee_can_2025,
  title = {Can an {LLM},
  author = {Lee, Cho-Ting and Neeser, Andrew and Xu, Shengzhe and Katyan, Jay and Cross, Patrick and Pathakota, Sharanya and Norman, Marigold and Simeone, John and Chandrasekaran, Jaganmohan and Ramakrishnan, Naren},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00101},
  url = {https://doi.org/10.1109/icse55347.2025.00101},
  booktitle = {Proceedings of the {IEEE},
  pages = {294--306},
  publisher = {IEEE Press},
  note = {ISSN: 1558-1225},
  keywords = {code generation, data cleaning, end-user programming, LLMs, source: ACM, source: IEEE},
  abstract = {Spreadsheets are routinely used in business and scientific contexts, and one of the most vexing challenges is performing data cleaning prior to analysis and evaluation. The ad-hoc and arbitrary nature of data cleaning problems, such as typos, inconsistent formatting, missing values, and a lack of standardization, often creates the need for highly specialized pipelines. We ask whether an LLM can find its way around a spreadsheet and how to support end-users in taking their free-form data processing requests to fruition. Just like RAG retrieves context to answer users' queries, we demonstrate how we can retrieve elements from a code library to compose data preprocessing pipelines. Through comprehensive experiments, we demonstrate the quality of our system and how it is able to continuously augment its vocabulary by saving new codes and pipelines back to the code library for future retrieval.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
  month = {apr},
}

@inproceedings{nangia_-context_2025,
  title = {In-{Context},
  author = {Nangia, Aditya and Ayachitula, Sriya and Kundu, Chinmay},
  year = {2025},
  doi = {10.1145/3734436.3734459},
  url = {https://doi.org/10.1145/3734436.3734459},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {169--174},
  publisher = {Association for Computing Machinery},
  note = {event-place: USA},
  keywords = {code generation, llm, vulnerability propagation},
  abstract = {The widespread adoption of Large Language Models (LLMs) in software development has accelerated coding workflows, with tools like GitHub Copilot and Google Gemini reducing development time by up to 55.8\%. However, these systems suffer from security and trustworthiness challenges. In this paper, we demonstrate a novel attack - ”Vulnerability propagation attack” in the conext of the code generated by LLMs. We present a formal framework for evaluating vulnerability propagation in LLM-assisted development, focusing on two key metrics: Vulnerability Carry Rate (VCR), which quantifies the intra-session propagation of vulnerabilities, and Context Retention Factor (CRF), which measures cross-session persistence. Using three leading LLMs – GPT-4, Sonar, and Gemini – we evaluate 10 critical Common Weakness Enumeration (CWE) categories across three tiers of prompts (direct injection, implicit vulnerabilities, and adversarial few-shot prompting). Our findings reveal that adversarial few-shot prompting exacerbates vulnerability propagation, with VCR values reaching 100\% under high-difficulty conditions and CRF values demonstrating significant retention across sessions. These results underscore the need for session-aware security measures and robust vulnerability detection frameworks in AI-assisted development environments.},
  address = {New York, NY, USA},
  series = {{SACMAT},
  isbn = {979-8-4007-1503-7},
}

@inproceedings{tian_text--sql_2025,
  title = {Text-to-{SQL},
  author = {Tian, Yuan and Lee, Daniel and Wu, Fei and Mai, Tung and Qian, Kun and Sahai, Siddhartha and Zhang, Tianyi and Li, Yunyao},
  year = {2025},
  doi = {10.1145/3708359.3712083},
  url = {https://doi.org/10.1145/3708359.3712083},
  booktitle = {Proceedings of the 30th {International},
  pages = {1398--1425},
  publisher = {Association for Computing Machinery},
  keywords = {Databases, Domain Adaptation, Interactive Data Annotation, LLMs, Natural Language Interface, PCFG, Text-to-SQL},
  abstract = {Text-to-SQL models, which parse natural language (NL) questions to executable SQL queries, are increasingly adopted in real-world applications. However, deploying such models in the real world often requires adapting them to the highly specialized database schemas used in specific applications. We find that existing text-to-SQL models experience significant performance drops when applied to new schemas, primarily due to the lack of domain-specific data for fine-tuning. This data scarcity also limits the ability to effectively evaluate model performance in new domains. Continuously obtaining high-quality text-to-SQL data for evolving schemas is prohibitively expensive in real-world scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop text-to-SQL data annotation system. SQLsynth streamlines the creation of high-quality text-to-SQL datasets through human-LLM collaboration in a structured workflow. A within-subjects user study comparing SQLsynth with manual annotation and ChatGPT shows that SQLsynth significantly accelerates text-to-SQL data annotation, reduces cognitive load, and produces datasets that are more accurate, natural, and diverse. Our code is available at https://github.com/adobe/nl\_sql\_analyzer.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{yan_efficient_2024,
  title = {Efficient {Mixture},
  author = {Yan, Mengyi and Wang, Yaoshu and Pang, Kehan and Xie, Min and Li, Jianxin},
  year = {2024},
  doi = {10.1145/3637528.3671873},
  url = {https://doi.org/10.1145/3637528.3671873},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {3690--3701},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {data preprocessing, LLMs, low-resource, mixture of expert},
  abstract = {Data preprocessing (DP) that transforms erroneous and raw data to a clean version is a cornerstone of the data mining pipeline. Due to the diverse requirements of downstream tasks, data scientists and domain experts have to handcraft domain-specific rules or train ML models with annotated examples, which is costly/time-consuming. In this paper, we present MELD (\&lt;u\&gt;M\&lt;/u\&gt;ixture of \&lt;u\&gt;E\&lt;/u\&gt;xperts on \&lt;u\&gt;L\&lt;/u\&gt;arge Language Models for \&lt;u\&gt;D\&lt;/u\&gt;ata Preprocessing), a universal solver for low-resource DP. MELD adopts a Mixture-of-Experts (MoE) architecture that enables the amalgamation and enhancement of domain-specific experts trained on limited annotated examples. To fine-tune MELD, we develop a suite of expert-tuning and MoE-tuning techniques, including a retrieval augmented generation (RAG) system, meta-path search for data augmentation, expert refinement and router network training based on information bottleneck. To further verify the effectiveness of MELD, we theoretically prove that MoE in MELD is superior than a single expert and the router network is able to dispatch data to the right experts. Finally, we conducted extensive experiments on 19 datasets over 10 DP tasks to show that MELD outperforms the state-of-the-art methods in both effectiveness and efficiency. More importantly, MELD is able to be fine-tuned in a low-resource environment, e.g. a local, single and low-priced 3090 GPU.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{wu_orchid_2025,
  title = {Orchid: {A},
  author = {Wu, Zhen and Kumyol, Serkan and Wong, Shing Yin and Hu, Xiaozhu and Tong, Xin and Braud, Tristan},
  year = {2025},
  doi = {10.1145/3698061.3726906},
  url = {https://doi.org/10.1145/3698061.3726906},
  booktitle = {Proceedings of the 2025 {Conference},
  pages = {774--791},
  publisher = {Association for Computing Machinery},
  keywords = {authorial agency and machine contingency, emergent narrative, Interactive Digital Narrative, LLMs, technology probe},
  abstract = {Integrating Large Language Models (LLMs) into Interactive Digital Narratives (IDNs) enables dynamic storytelling where user interactions shape the narrative in real time, challenging traditional authoring methods. This paper presents the design study of Orchid, a creative approach for authoring LLM-driven IDNs. Orchid allows users to structure the hierarchy of narrative stages and define the rules governing LLM narrative generation and transitions between stages. The development of Orchid consists of three phases. 1) Formulating Orchid through desk research on existing IDN practices. 2) Implementation of a technology probe based on Orchid. 3) Evaluating how IDN authors use Orchid to design IDNs, verify Orchid’s hypotheses, and explore user needs for future authoring tools. This study confirms that authors are open to LLM-driven IDNs but desire strong authorial agency in narrative structures, highlighted in accuracy in branching transitions and story details. Future design implications for Orchid include introducing deterministic variable handling, support for trans-media applications, and narrative consistency across branches.},
  address = {New York, NY, USA},
  series = {C\&amp;{C},
  isbn = {979-8-4007-1289-0},
}

@inproceedings{meng_deconstructing_2025,
  title = {Deconstructing {Depression},
  author = {Meng, Han and Zhang, Renwen and Wang, Ganyi and Yang, Yitian and Qin, Peinuan and Lee, Jungup and Lee, Yi-Chieh},
  year = {2025},
  doi = {10.1145/3706598.3714255},
  url = {https://doi.org/10.1145/3706598.3714255},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI-assisted Coding, Causal Knowledge Graph, Chatbot, Depression, Large Language Model, Social Stigma},
  abstract = {Mental-illness stigma is a persistent social problem, hampering both treatment-seeking and recovery. Accordingly, there is a pressing need to understand it more clearly, but analyzing the relevant data is highly labor-intensive. Therefore, we designed a chatbot to engage participants in conversations; coded those conversations qualitatively with AI assistance; and, based on those coding results, built causal knowledge graphs to decode stigma. The results we obtained from 1,002 participants demonstrate that conversation with our chatbot can elicit rich information about people’s attitudes toward depression, while our AI-assisted coding was strongly consistent with human-expert coding. Our novel approach combining large language models (LLMs) and causal knowledge graphs uncovered patterns in individual responses and illustrated the interrelationships of psychological constructs in the dataset as a whole. The paper also discusses these findings’ implications for HCI researchers in developing digital interventions, decomposing human psychological constructs, and fostering inclusive attitudes.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{do_paige_2025,
  title = {{PAIGE},
  author = {Do, Tiffany D. and Shafqat, Usama Bin and Ling, Elsie and Sarda, Nikhil},
  year = {2025},
  doi = {10.1145/3706598.3713460},
  url = {https://doi.org/10.1145/3706598.3713460},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {artificial intelligence in education, content transformation, large language models, personalized learning},
  abstract = {Generative AI is revolutionizing content creation and has the potential to enable real-time, personalized educational experiences. We investigated the effectiveness of converting textbook chapters into AI-generated podcasts and explored the impact of personalizing these podcasts for individual learner profiles. We conducted a 3x3 user study with 180 college students in the United States, comparing traditional textbook reading with both generalized and personalized AI-generated podcasts across three textbook subjects. The personalized podcasts were tailored to students’ majors, interests, and self-described instructional preferences. Our findings show that students found the AI-generated podcast format to be more enjoyable than textbooks and that personalized podcasts led to significantly improved learning outcomes, although this was subject-specific. These results highlight that AI-generated podcasts can offer an engaging and effective modality transformation of textbook material, with personalization enhancing content relevance. We conclude with design recommendations for leveraging AI in education, informed by student feedback.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{tan_contextualized_2025,
  title = {Contextualized {Visual},
  author = {Tan, Hui Li and Gu, Ying and Li, Liyuan and Leong, Mei Chee and Chen, Nancy F.},
  year = {2025},
  doi = {10.1145/3747327.3764895},
  url = {https://doi.org/10.1145/3747327.3764895},
  booktitle = {Companion {Proceedings},
  pages = {185--189},
  publisher = {Association for Computing Machinery},
  keywords = {AI for education, dense image captioning, human computer interaction, multi-cultural, vision-language models},
  abstract = {Interactive visual storytelling through conversational agents offers a means to enhance early childhood language learning. We developed a picture-guided conversational chatbot, driven by dense image captioning, for early childhood mother tongue language learning. However, state-of-the-art image captioning systems fall short in meeting the educational needs of young learners. They often lack cultural contextualization, use vocabulary that exceeds children’s developmental level, and fail to align with curriculum-relevant learning goals. We investigated a contextualized dense image captioning framework, which augments dense image captioning with cultural and curriculum-aligned keyword retrieval through a Retrieval-Augmented Generation (RAG) module. This enables the generation of culturally appropriate, age-level suitable, and educationally anchored captions that enhance learner engagement and pedagogical relevance. We demonstrate that our approach outperforms existing captioning models in terms of linguistic appropriateness, and curriculum and cultural alignment. The contextualized dense image captioning framework supports the development of culturally grounded, education-oriented conversational agents for young learners.},
  address = {New York, NY, USA},
  series = {{ICMI},
  isbn = {979-8-4007-2076-5},
}

@inproceedings{ghassel_hierarchical_2025,
  title = {Hierarchical {Lexical},
  author = {Ghassel, Abdellah and Robinson, Ian and Tanase, Gabriel and Cooper, Hal and Thompson, Bryan and Han, Zhen and Ioannidis, Vassilis and Adeshina, Soji and Rangwala, Huzefa},
  year = {2025},
  doi = {10.1145/3711896.3737233},
  url = {https://doi.org/10.1145/3711896.3737233},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4457--4466},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {data generation, graph structures, question answering},
  abstract = {Retrieval-Augmented Generation (RAG) grounds large language models in external evidence, yet it still falters when answers must be pieced together across semantically distant documents. We close this gap with the Hierarchical Lexical Graph (HLG), a three-tier index that (i) traces every atomic proposition to its source(ii) clusters propositions into latent topics, and (iii) links entities and relations to expose cross-document paths. On top of HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG, which performs fine-grained entity-aware beam search over propositions for high-precision factoid questions, and TopicGraphRAG, which selects coarse topics before expanding along entity links to supply broad yet relevant context for exploratory queries. Additionally, existing benchmarks lack the complexity required to rigorously evaluate multi-hop summarization systems, often focusing on single-document queries or limited datasets. To address this, we introduce a synthetic dataset generation pipeline that curates realistic, multi-document question-answer pairs, enabling robust evaluation of multi-hop retrieval systems. Extensive experiments across five datasets demonstrate that our methods outperform naive chunk-based RAG, achieving an average relative improvement of 23.1\% in retrieval recall and correctness. Open-source Python library is available at https://github.com/awslabs/graphrag-toolkit.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{qiao_oversight_2025,
  title = {Oversight in {Action},
  author = {Qiao, Shuying and Denny, Paul and Giacaman, Nasser},
  year = {2025},
  doi = {10.1145/3716640.3716651},
  url = {https://doi.org/10.1145/3716640.3716651},
  booktitle = {Proceedings of the 27th {Australasian},
  pages = {95--104},
  publisher = {Association for Computing Machinery},
  keywords = {chatbots, computing education, discussion forums, instructor-in-the-loop, Large language models, LLMs, software engineering education},
  abstract = {The integration of large language models (LLMs) into computing education offers many potential benefits to student learning, and several novel pedagogical approaches have been reported in the literature. However LLMs also present challenges, one of the most commonly cited being that of student over-reliance. This challenge is compounded by the fact that LLMs are always available to provide instant help and solutions to students, which can undermine their ability to independently solve problems and diagnose and resolve errors. Providing instructor oversight of LLM-generated content can mitigate this problem, however it is often not practical in real-time learning contexts. Online class discussion forums, which are widely used in computing education, present an opportunity for exploring instructor oversight because they operate asynchronously. Unlike real-time interactions, the discussion forum format aligns with the expectation that responses may take time, making oversight not only feasible but also pedagogically appropriate. In this practitioner paper, we present the design, deployment, and evaluation of a ‘bot’ module that is controlled by the instructor, and integrated into an online discussion forum. The bot assists the instructor by generating draft responses to student questions, which are reviewed, modified, and approved before release. Key features include the ability to leverage course materials, access archived discussions, and publish responses anonymously to encourage open participation. We report our experiences using this tool in a 12-week second-year software engineering course on object-oriented programming. Instructor feedback confirmed the tool successfully alleviated workload but highlighted a need for improvement in handling complex, context-dependent queries. We report the features that were viewed as most beneficial, and suggest avenues for future exploration.},
  address = {New York, NY, USA},
  series = {{ACE},
  isbn = {979-8-4007-1425-2},
}

@inproceedings{yang_research_2024,
  title = {Research and {Practice},
  author = {Yang, Da and Liu, Shutian and Fu, Haoyang and Shen, Jiayi},
  year = {2024},
  doi = {10.1145/3700297.3700331},
  url = {https://doi.org/10.1145/3700297.3700331},
  booktitle = {Proceedings of the 2024 {International},
  pages = {193--198},
  publisher = {Association for Computing Machinery},
  keywords = {Course Ideological and Political Education, Educational Innovation, Knowledge Graph, Large Language Model (LLM)},
  abstract = {Knowledge graphs and large language models (LLMs) have become important tools for educational innovation. This paper explores the application of these two technologies in the construction of ideological and political education in university courses. The paper begins by analyzing the importance of course-based ideological and political education and the challenges currently faced. It then introduces the role of knowledge graphs in integrating educational resources and constructing knowledge systems, as well as the potential and current status of LLMs in natural language processing and providing personalized educational content. This study presents a method that integrates the use of knowledge graphs and LLMs to construct resources and application systems for course-based ideological and political education. The results of practical case studies demonstrate that the proposed method improves the efficiency of constructing ideological and political education content, enhances the effectiveness of moral education within courses, and contributes to the innovative development of ideological and political education.},
  address = {New York, NY, USA},
  series = {{ISAIE},
  isbn = {979-8-4007-0710-0},
}

@inproceedings{wu_netllm_2024,
  title = {{NetLLM},
  author = {Wu, Duo and Wang, Xianda and Qiao, Yaqi and Wang, Zhi and Jiang, Junchen and Cui, Shuguang and Wang, Fangxin},
  year = {2024},
  doi = {10.1145/3651890.3672268},
  url = {https://doi.org/10.1145/3651890.3672268},
  booktitle = {Proceedings of the {ACM},
  pages = {661--678},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney, NSW, Australia},
  keywords = {deep learning, job scheduling, large language model adaptation, network optimization, video streaming},
  abstract = {Many networking tasks now employ deep learning (DL) to solve complex prediction and optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.Motivated by the recent success of large language models (LLMs), this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the powerful pre-trained knowledge, the LLM is promising to serve as the foundation model to achieve "one model for all tasks" with even better performance and stronger generalization. In pursuit of this vision, we present NetLLM, the first framework that provides a coherent design to harness the powerful capabilities of LLMs with low efforts to solve networking problems. Specifically, NetLLM empowers the LLM to effectively process multimodal data in networking and efficiently generate task-specific answers. Besides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire domain knowledge for networking. Across three networking-related use cases - viewport prediction, adaptive bitrate streaming and cluster job scheduling, we showcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art algorithms.},
  address = {New York, NY, USA},
  series = {{ACM},
  isbn = {979-8-4007-0614-1},
}

@inproceedings{zhang_differential-perceptive_2024,
  title = {Differential-{Perceptive},
  author = {Zhang, Xian and Wen, Haokun and Wu, Jianlong and Qin, Pengda and Xue', Hui and Nie, Liqiang},
  year = {2024},
  doi = {10.1145/3664647.3681453},
  url = {https://doi.org/10.1145/3664647.3681453},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {4148--4157},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {source: ACM},
  abstract = {Change captioning involves describing the subtle changes between a pair of similar images. Although existing efforts have achieved compelling success, they overlook the potential of multimodal large language models (MLLMs) in tackling this challenging task. In this work, we aim to empower MLLMs with the capability to perceive subtle differences between paired images and enhance their performance in generating change captions. Specifically, we present a diFferentIal-perceptive aNd rEtRieval-augmented MLLM (FINER-MLLM) tailored for this task. In particular, FINER-MLLM leverages LoRA fine-tuned MLLM's image encoder to extract image patch features, enabling the capture of detailed image information. Subsequently, within MLLM's feature extraction, typically Q-Former, FINER-MLLM incorporates dual constraints: the intra-image feature independence constraint and the inter-image feature alignment constraint. These constraints ensure that the features can comprehensively extract subtle visual information within each image and that corresponding features across images align effectively. Last, we introduced the retrieval augmentation to first retrieve the relevant corpus to facilitate the MLLM's decoder i.e., LLM, in generating accurate change captions. Extensive experiments on three benchmark datasets, i.e., CLEVR-Change, Spot-the-Diff, and Image-Editing-Request, demonstrate the superiority of our proposed method.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{pang_understanding_2025,
  title = {Understanding the {LLM},
  author = {Pang, Rock Yuren and Schroeder, Hope and Smith, Kynnedy Simone and Barocas, Solon and Xiao, Ziang and Tseng, Emily and Bragg, Danielle},
  year = {2025},
  doi = {10.1145/3706598.3713726},
  url = {https://doi.org/10.1145/3706598.3713726},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {HCI theory, human-AI interaction, Large language models, systematic literature review},
  abstract = {Large language models (LLMs) have been positioned to revolutionize HCI, by reshaping not only the interfaces, design patterns, and sociotechnical systems that we study, but also the research practices we use. To-date, however, there has been little understanding of LLMs’ uptake in HCI. We address this gap via a systematic literature review of 153 CHI papers from 2020-24 that engage with LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in HCI projects; (3) contribution types; and (4) acknowledged limitations and risks. We find LLM work in 10 diverse domains, primarily via empirical and artifact contributions. Authors use LLMs in five distinct roles, including as research tools or simulated users. Still, authors often raise validity and reproducibility concerns, and overwhelmingly study closed models. We outline opportunities to improve HCI research with and on LLMs, and provide guiding questions for researchers to consider the validity and appropriateness of LLM-related work.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{espinal_extended_2025,
  title = {An {eXtended},
  author = {Espinal, Wendy Yunuen Arevalo and Jimenez, Jaime and Corneo, Lorenzo},
  year = {2025},
  doi = {10.1145/3703790.3703792},
  url = {https://doi.org/10.1145/3703790.3703792},
  booktitle = {Proceedings of the 14th {International},
  pages = {10--18},
  publisher = {Association for Computing Machinery},
  keywords = {Data Transformation, Device and Data Integration, Extended Reality, Generative AI, Internet of Things},
  abstract = {The multidisciplinary nature of XR applications makes device and data integration a resource-intensive and time-consuming task, especially in the context of the Internet of Things (IoT).This paper presents Visualize Interactive Objects, VIO for short, a data transformation framework aimed at simplifying visualization and interaction of IoT devices and their data into XR applications. VIO comprises a software runtime\&nbsp;(VRT) running on XR headsets, and a JSON-based syntax for defining VIO Descriptions (VDs). The VRT interprets VDs to facilitate visualization and interaction within the application. By raising the level of abstraction, VIO enhances interoperability among XR experiences and enables developers to integrate IoT data with minimal coding effort.A comprehensive evaluation demonstrated that VIO is lightweight, incurring in negligible overhead compared to native implementations. Ten Large Language Models (LLM) were used to generate VDs and native source-code from user intents. The results showed that LLMs have superior syntactical and semantical accuracy in generating VDs compared to native XR application development code, thus indicating that the task of creating VDs can be effectively automated using LLMs. Additionally, a user study with 12 participants found that VIO is developer-friendly and easily extensible.},
  address = {New York, NY, USA},
  series = {{IoT},
  isbn = {979-8-4007-1285-2},
}

@inproceedings{aneja_beyond_2025,
  title = {Beyond {Semantics},
  author = {Aneja, Urvashi and Gupta, Aarushi and Vashistha, Aditya},
  year = {2025},
  doi = {10.1145/3715275.3732180},
  url = {https://doi.org/10.1145/3715275.3732180},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {2784--2795},
  publisher = {Association for Computing Machinery},
  keywords = {ChatGPT, Contextual Integrity, Gender bias, Krutrim, Large Language Models, Majority World, Social Sector},
  abstract = {Like many other countries, India has witnessed a surge in LLM-based applications and initiatives, with multiple players directing their efforts towards building and customising LLMs more suited to the country’s socio-demographic characteristics. However, concomitant to the rise in the popularity of LLMs, numerous studies and anecdotes highlighting various harms such as misinformation, discrimination, and bias have emerged. Gender bias in LLMs – defined as the tendency of these models to reflect and perpetuate stereotypes, inequalities, or prejudices based on gender – has received significant scholarly attention in the last few years. However, only a handful of studies have analysed this issue against the backdrop of India’s sociocultural setting, and almost none (to the best of our knowledge) have looked at it in relation to critical social sectors. We therefore undertake an exploratory study to understand the different sources and manifestations of gender biases in LLMs customised for Indian languages and deployed in resource-constrained settings. Through detailed key informant interviews with LLM application developers, field visits to deployment and testing sites, prompting exercises, and expert workshops, we unpack gender-related concerns that emerge at each stage of the LLM lifecycle. In this, we shift away from a narrow construct that views gender bias in LLMs solely as prejudiced semantics fixable through improved engineering. Instead, we recognise it as a reflection of broader, structural inequities that demand a more grounded, interdisciplinary effort to both understand and address.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{staudinger_reproducibility_2024,
  title = {A {Reproducibility},
  author = {Staudinger, Moritz and Kusa, Wojciech and Piroi, Florina and Lipani, Aldo and Hanbury, Allan},
  year = {2024},
  doi = {10.1145/3673791.3698432},
  url = {https://doi.org/10.1145/3673791.3698432},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {186--196},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {boolean query, llms, query generation, systematic reviews, source: ACM},
  abstract = {Systematic literature reviews (SLRs) are a cornerstone of academic research, yet they are often labour-intensive and time-consuming due to the detailed literature curation process. The advent of generative AI and large language models (LLMs) promises to revolutionize this process by assisting researchers in several tedious tasks, one of them being the generation of effective Boolean queries that will select the publications to consider including in a review. This paper presents an extensive study of Boolean query generation using LLMs for systematic reviews, reproducing and extending the work of Wang et al. and Alaniz et al. Our study investigates the replicability and reliability of results achieved using ChatGPT and compares its performance with open-source alternatives like Mistral and Zephyr to provide a more comprehensive analysis of LLMs for query generation.Therefore, we implemented a pipeline, which automatically creates a Boolean query for a given review topic by using a previously defined LLM, retrieves all documents for this query from the PubMed database and then evaluates the results. With this pipeline we first assess whether the results obtained using ChatGPT for query generation are reproducible and consistent. We then generalize our results by analyzing and evaluating open-source models and evaluating their efficacy in generating Boolean queries.Finally, we conduct a failure analysis to identify and discuss the limitations and shortcomings of using LLMs for Boolean query generation. This examination helps to understand the gaps and potential areas for improvement in the application of LLMs to information retrieval tasks. Our findings highlight the strengths, limitations, and potential of LLMs in the domain of information retrieval and literature review automation. Our code is available online.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{cai_aiget_2025,
  title = {{AiGet},
  author = {Cai, Runze and Janaka, Nuwan and Kim, Hyeongcheol and Chen, Yang and Zhao, Shengdong and Huang, Yun and Hsu, David},
  year = {2025},
  doi = {10.1145/3706598.3713953},
  url = {https://doi.org/10.1145/3706598.3713953},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI, HMD, human-ai interaction, incidental learning, informal learning, knowledge discovery, large language model, multimodal information, smart glasses, wearable-AI assistance},
  abstract = {Unlike the free exploration of childhood, the demands of daily life reduce our motivation to explore our surroundings, leading to missed opportunities for informal learning. Traditional tools for knowledge acquisition are reactive, relying on user initiative and limiting their ability to uncover hidden interests. Through formative studies, we introduce AiGet, a proactive AI assistant integrated with AR smart glasses, designed to seamlessly embed informal learning into low-demand daily activities (e.g., casual walking and shopping). AiGet analyzes real-time user gaze patterns, environmental context, and user profiles, leveraging large language models to deliver personalized, context-aware knowledge with low disruption to primary tasks. In-lab evaluations and real-world testing, including continued use over multiple days, demonstrate AiGet’s effectiveness in uncovering overlooked yet surprising interests, enhancing primary task enjoyment, reviving curiosity, and deepening connections with the environment. We further propose design guidelines for AI-assisted informal learning, focused on transforming everyday moments into enriching learning experiences.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{yang_evaluating_2025,
  title = {Evaluating {Large},
  author = {Yang, Longxing and Luo, Yixing and Gao, Hao and Fan, Yingshuang and Zhang, Jingru and Li, Xiaofeng and Dong, Xiaogang and Gu, Bin and Jin, Zhi and Yang, Mengfei},
  year = {2025},
  doi = {10.1145/3696630.3728560},
  url = {https://doi.org/10.1145/3696630.3728560},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {366--377},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {aerospace software, evaluation, large language models, requirements question answering, source: ACM, source: Scopus},
  abstract = {Aerospace software presents significant challenges to requirements engineering due to its design complexity and stringent safety standards. When manually drafting requirement documents, engineers need strong domain knowledge while also navigating heterogeneous data, which leads to errors and inefficiencies. This paper evaluates the capabilities of large language models (LLMs) in understanding aerospace software requirements and their potential to assist in requirements question answering (QA). We develop an aerospace requirements QA benchmark based on industrial software assets, books, and research materials, creating a total of 6, 696 QA pairs across ten tasks and three heterogeneous data formats: text, tables, and formulas. We then evaluate the domain-specific performance of five mainstream open-source LLMs using zero-shot learning, few-shot learning, and retrieval-augmented generation (RAG) techniques. We further categorize hallucinations from LLMs and quantitatively analyze error distributions. Moreover, we conduct a user study to assess the LLM's practical usefulness when applying to requirements QA. The evaluation results show that (1) LLMs demonstrate limited performance in the aerospace software domain, (2) RAG techniques significantly enhance the capabilities of LLMs for text-based tasks, while few-shot learning improves the performance of most LLMs, (3) four distinct types of QA hallucinations are identified, and (4) LLM QA is particularly beneficial for junior engineers. This research provides valuable perspectives for the future application of LLMs in aerospace software.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
  annote = {Cited by: 0},
}

@inproceedings{kim_legisflow_2025,
  title = {{LegisFlow},
  author = {Kim, Junghwan and Jeon, Hyeonseok and Heo, Dongseok and Lee, Jung and Suh, Bongwon},
  year = {2025},
  doi = {10.1145/3746059.3747752},
  url = {https://doi.org/10.1145/3746059.3747752},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {AI in Law, Interactive Legal Research Tools, Legal Information Retrieval, Legislative Amendment Tracking, Natural Language Search in Law, Statutory Law Research, Temporal Analysis of Legislation, User-Centered Legal Systems},
  abstract = {In South Korea’s statutory law system, legal research faces challenges like tracking frequent amendments and understanding complex statute relationships. LegisFlow, an innovative AI-powered system, tackles these issues with features such as interactive amendment timelines and advanced inter-statute relationship analysis. Developed based on insights from Korean legal experts, it provides intuitive visualizations and context-aware search capabilities. A user study with 10 legal professionals demonstrated that LegisFlow significantly enhances efficiency, reducing task completion times by up to 36\% (e.g., 440s vs. 690s in inter-statute comparison, p = 0.022) and lowering cognitive load, with workflows streamlined by 70\% fewer manual steps. LegisFlow transforms statutory law research by setting a new standard for AI-assisted tools, providing a scalable, user-centered solution for professionals in Korea and beyond.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{koziolek_llm-based_2024,
  title = {{LLM},
  author = {Koziolek, Heiko and Grüner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
  year = {2024},
  doi = {10.1145/3643795.3648384},
  url = {https://doi.org/10.1145/3643795.3648384},
  booktitle = {Proceedings of the 1st {International},
  pages = {22--29},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {ChatGPT, code generation, DCS, GPT-4, IEC 61131-3, industrial automation, large language models, PLC},
  abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
  address = {New York, NY, USA},
  series = {{LLM4Code},
  isbn = {979-8-4007-0579-3},
}

@inproceedings{zhao_noteit_2025,
  title = {{NoteIt},
  author = {Zhao, Running and Jiang, Zhihan and Zhang, Xinchen and Chang, Chirui and Chen, Handi and Deng, Weipeng and Jin, Luyao and Qi, Xiaojuan and Qian, Xun and Ngai, Edith C.H.},
  year = {2025},
  doi = {10.1145/3746059.3747626},
  url = {https://doi.org/10.1145/3746059.3747626},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {multimodal large language model, multimodal learning, note generation, video understanding},
  abstract = {Users often take notes for instructional videos to access key knowledge later without revisiting long videos. Automated note generation tools enable users to obtain informative notes efficiently. However, notes generated by existing research or off-the-shelf tools fail to preserve the information conveyed in the original videos comprehensively, nor can they satisfy users’ expectations for diverse presentation formats and interactive features when using notes digitally. In this work, we present NoteIt, a system, which automatically converts instructional videos to interactable notes using a novel pipeline that faithfully extracts hierarchical structure and multimodal key information from videos. With NoteIt’s interface, users can interact with the system to further customize the content and presentation formats of the notes according to their preferences. We conducted both a technical evaluation and a comparison user study (N=36). The solid performance in objective metrics and the positive user feedback demonstrated the effectiveness of the pipeline and the overall usability of NoteIt.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{tian_customized_2024,
  title = {Customized {FinGPT},
  author = {Tian, Felix and Byadgi, Ajay and Kim, Daniel S and Zha, Daochen and White, Matt and Xiao, Kairong and Liu, Xiao-Yang},
  year = {2024},
  doi = {10.1145/3677052.3698637},
  url = {https://doi.org/10.1145/3677052.3698637},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {469--477},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {source: ACM},
  abstract = {Current large language models (LLMs) have proven useful for analyzing financial data, but most existing models, such as BloombergGPT and FinGPT, lack customization for specific user needs. In this paper, we address this gap by developing FinGPT Search Agents tailored for two types of users: individuals and institutions. For individuals, we leverage Retrieval-Augmented Generation (RAG) to search local documents and user-specified data sources. For institutions, we employ dynamic vector databases and fine-tune models on proprietary data. There are several key issues to address, including data privacy, the time-sensitive nature of financial information, and the need for fast responses. Experiments show that FinGPT Search Agent outperform existing models in accuracy, relevance, and response time, making them promising for real-world financial applications.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{cordioli_exploring_2025,
  title = {Exploring {LLM},
  author = {Cordioli, Luca and Piro, Ludovica and Valoriani, Matteo and Matera, Maristella},
  year = {2025},
  doi = {10.1145/3750069.3750160},
  url = {https://doi.org/10.1145/3750069.3750160},
  booktitle = {Proceedings of the 16th {Biannual},
  publisher = {Association for Computing Machinery},
  keywords = {Extended Reality, Human-AI Interaction, Interaction Design, Large Language Models, User Study},
  abstract = {This paper illustrates a user study comparing three interaction modalities in XR knowledge-retrieval tasks: (i) menu-based interfaces (MB), (ii) spatially anchored infographics (IG), and (iii) a multimodal LLM-powered agent (LM) combining voice, gesture, and deictic input. Sixteen participants explored an XR environment using the three modalities. Results indicate that the LM modality was perceived as significantly more usable than IG, but did not yield statistically significant improvements in efficiency or cognitive load compared to MB. While LLM-driven agents offer promising interaction patterns, our findings suggest that their objective benefits over traditional modalities remain inconclusive in complex XR tasks. Our findings also highlight the need to identify trade-offs among modalities and inform design guidelines for hybrid XR interfaces combining conversational AI with visual scaffolding.},
  address = {New York, NY, USA},
  series = {{CHItaly},
  isbn = {979-8-4007-2102-1},
}

@inproceedings{dietz_principles_2025,
  title = {Principles and {Guidelines},
  author = {Dietz, Laura and Zendel, Oleg and Bailey, Peter and Clarke, Charles L. A. and Cotterill, Ellese and Dalton, Jeff and Hasibi, Faegheh and Sanderson, Mark and Craswell, Nick},
  year = {2025},
  doi = {10.1145/3731120.3744588},
  url = {https://doi.org/10.1145/3731120.3744588},
  booktitle = {Proceedings of the 2025 {International},
  pages = {218--229},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {llm tropes, llm-based evaluation, validity of experimentation},
  abstract = {Relevance judgments for information retrieval (IR) evaluation, once the domain of human assessors, are now often produced by Large Language Models (LLMs). While some studies report alignment between LLM and human judgments, claims that LLMs can replace human judges raise concerns about reliability, validity, and long-term impact. As IR systems increasingly rely on LLM-generated signals, evaluation risks becoming self-reinforcing, leading to potentially misleading conclusions. This paper examines scenarios where LLM evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation. We highlight key risks, including bias reinforcement, reproducibility challenges, and inconsistencies in assessment methodologies. To address these concerns, we propose tests to quantify adverse effects, guardrails, and a collaborative framework for constructing reusable test collections that integrate LLM judgments responsibly. By providing perspectives from academia and industry, this work aims to establish best practices for the principled use of LLMs in IR evaluation.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{li_omniquery_2025,
  title = {{OmniQuery},
  author = {Li, Jiahao Nick and Zhang, Zhuohao (Jerry) and Ma, Jiaju},
  year = {2025},
  doi = {10.1145/3706598.3713448},
  url = {https://doi.org/10.1145/3706598.3713448},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  note = {Type: Conference paper},
  keywords = {contextual augmentation, diary study, multimodal question answering, personal memory, RAG, source: Scopus},
  abstract = {People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they only support retrieving individual pieces of information like certain objects in photos, and struggle with answering more complex queries that involve interpreting interconnected memories like sequential events. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments individual captured memories through integrating scattered contextual information from multiple interconnected memories. Given a question, OmniQuery retrieves relevant augmented memories and uses a large language model (LLM) to generate answers with references. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5\%, outperforming a conventional RAG system by winning or tying for 74.5\% of the time.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{jorke_gptcoach_2025,
  title = {{GPTCoach},
  author = {Jörke, Matthew and Sapkota, Shardul and Warkenthien, Lyndsea and Vainio, Niklas and Schmiedmayer, Paul and Brunskill, Emma and Landay, James A.},
  year = {2025},
  doi = {10.1145/3706598.3713819},
  url = {https://doi.org/10.1145/3706598.3713819},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {conversational agents, health coaching, large language models (LLMs), personal informatics, Physical activity},
  abstract = {Mobile health applications show promise for scalable physical activity promotion but are often insufficiently personalized. In contrast, health coaching offers highly personalized support but can be prohibitively expensive and inaccessible. This study draws inspiration from health coaching to explore how large language models (LLMs) might address personalization challenges in mobile health. We conduct formative interviews with 12 health professionals and 10 potential coaching recipients to develop design principles for an LLM-based health coach. We then built GPTCoach, a chatbot that implements the onboarding conversation from an evidence-based coaching program, uses conversational strategies from motivational interviewing, and incorporates wearable data to create personalized physical activity plans. In a lab study with 16 participants using three months of historical data, we find promising evidence that GPTCoach gathers rich qualitative information to offer personalized support, with users feeling comfortable sharing concerns. We conclude with implications for future research on LLM-based physical activity support.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{vaithilingam_semantic_2025,
  title = {Semantic {Commit},
  author = {Vaithilingam, Priyan and Kim, Munyeong and Acosta-Parenteau, Frida-Cecilia and Lee, Daniel and Mhedhbi, Amine and Glassman, Elena L. and Arawjo, Ian},
  year = {2025},
  doi = {10.1145/3746059.3747778},
  url = {https://doi.org/10.1145/3746059.3747778},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {AI agents, human-AI grounding, impact analysis, intent specification, large language models, memory management},
  abstract = {As AI agents increasingly rely on memory systems to align with user intent, updating these memories presents challenges of semantic conflict and ambiguity. Inspired by impact analysis in software engineering, we introduce SemanticCommit, a mixed-initiative interface to help users integrate new intent into intent specifications—natural language documents like AI memory lists, Cursor Rules, and game design documents—while maintaining consistency. SemanticCommit detects potential semantic conflicts using a knowledge graph-based retrieval-augmented generation pipeline, and assists users in resolving them with LLM support. Through a within-subjects study with 12 participants comparing SemanticCommit to a chat-with-document baseline (OpenAI Canvas), we find differences in workflow: half of our participants adopted a workflow of impact analysis when using SemanticCommit, where they would first flag conflicts without AI revisions then resolve conflicts locally, despite having access to a global revision feature. Additionally, users felt SemanticCommit offered a greater sense of control without increasing workload. Our findings indicate that AI agent interfaces should help users validate AI retrieval independently from generation, suggesting that the benefits from improved control can offset the costs of manual review. Our work speaks to the need for AI system designers to think about updating memory as a process that involves human feedback and decision-making.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{zhang_mmr_2025,
  title = {{MMR},
  author = {Zhang, Tao and Zhao, Likun},
  year = {2025},
  doi = {10.1145/3706890.3706951},
  url = {https://doi.org/10.1145/3706890.3706951},
  booktitle = {Proceedings of the 2024 5th {International},
  pages = {348--351},
  publisher = {Association for Computing Machinery},
  keywords = {Large Language Models, Medical Dialogue Generation, Multi-step Reasoning},
  abstract = {With the advancements of large language models in text and visual tasks, researchers are increasingly exploring their applications in medical scenarios. While existing studies have successfully applied these models to medical dialogue systems, challenges remain in accurately calculating medication dosages and handling multi-turn reasoning due to the low tolerance for error in medical contexts. To address this, we propose Math Multi-step Reasoning in Medical Dialogue Generation (MMR), which enhances reasoning by iteratively breaking down complex problems into simpler questions using a “Least to Most Prompting” (LMP)strategy. MMR integrates Chain of Thought, React mechanisms, and Retrieval-Augmented Generation (RAG) with a domain-specific knowledge base to improve reasoning accuracy. Supported by the MedDGQA dataset, MMR outperforms state-of-the-art methods in both objective and subjective evaluations.},
  address = {New York, NY, USA},
  series = {{ISAIMS},
  isbn = {979-8-4007-1782-6},
}

@inproceedings{shi_learnable_2024,
  title = {A {Learnable},
  author = {Shi, Yunxiao and Xu, Min and Zhang, Haimin and Zi, Xing and Wu, Qiang},
  year = {2024},
  doi = {10.1145/3689091.3690087},
  url = {https://doi.org/10.1145/3689091.3690087},
  booktitle = {Proceedings of the 2nd {International},
  pages = {12--20},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {information retrieval and generation, multi-agent system, multimodal, personalized search, source: ACM},
  abstract = {Large language models (LLMs) and retrieval-augmented generation (RAG) techniques have revolutionized traditional information access, enabling AI agent to search and summarize information on behalf of users during dynamic dialogues. Despite their potential, current AI search engines exhibit considerable room for improvement in several critical areas. These areas include the support for multimodal information, the delivery of personalized responses, the capability to logically answer complex questions, and the facilitation of more flexible interactions. This paper proposes a novel AI Search Engine framework called the Agent Collaboration Network (ACN). The ACN framework consists of multiple specialized agents working collaboratively, each with distinct roles such as Account Manager, Solution Strategist, Information Manager, and Content Creator. This framework integrates mechanisms for picture content understanding, user profile tracking, and online evolution, enhancing the AI search engine's response quality, personalization, and interactivity. A highlight of the ACN is the introduction of a Reflective Forward Optimization method (RFO), which supports the online synergistic adjustment among agents. This feature endows the ACN with online learning capabilities, ensuring that the system has strong interactive flexibility and can promptly adapt to user feedback. This learning method may also serve as an optimization approach for agent-based systems, potentially influencing other domains of agent applications.},
  address = {New York, NY, USA},
  series = {{MMGR},
  isbn = {979-8-4007-1202-9},
}

@inproceedings{hoseini_end--end_2025,
  title = {End-{To},
  author = {Hoseini, Sayed and Herrmann, Vincent and Quix, Christoph},
  year = {2025},
  doi = {10.1145/3735654.3735942},
  url = {https://doi.org/10.1145/3735654.3735942},
  booktitle = {Proceedings of the {Workshop},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {AutoML, Data Wrangling, LLMs, Semantic Data Management},
  abstract = {Machine Learning (ML) in industrial chemistry is often hindered by the complexity of preprocessing heterogeneous datasets. In this proof-of-concept study, we explore the use of semantic data management to support LLM-driven automation of end-to-end ML pipelines in a real-world Chemistry 4.0 setting. A semantic model is used to capture domain knowledge and metadata in a machine-readable form, guiding LLMs through natural language prompts to generate complete data wrangling and ML modeling code. We evaluate several state-of-the-art LLMs on their ability to autonomously produce functionally correct Python code for preprocessing and Gaussian Process modeling. Our results show that, when guided by structured semantic context, larger LLMs can reliably generate accurate pipelines, significantly reducing the need for manual intervention. These findings provide an encouraging starting point for further exploration toward leveraging the semantic model to improve the robustness of code generation by systematically integrating relevant information into the generation process, rather than relying solely on the raw intelligence of the LLM.},
  address = {New York, NY, USA},
  series = {{DEEM},
  isbn = {979-8-4007-1924-0},
}

@inproceedings{ruan_specrover_2025,
  title = {{SpecRover},
  author = {Ruan, Haifeng and Zhang, Yuntong and Roychoudhury, Abhik},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00080},
  url = {https://doi.org/10.1109/icse55347.2025.00080},
  booktitle = {Proceedings of the {IEEE},
  pages = {963--974},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50\% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost (\$0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better "signal" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{correia_unveiling_2024,
  title = {Unveiling the {Potential},
  author = {Correia, João and Nicholson, Morgan C. and Coutinho, Daniel and Barbosa, Caio and Castelluccio, Marco and Gerosa, Marco and Garcia, Alessandro and Steinmacher, Igor},
  year = {2024},
  doi = {10.1145/3664646.3664758},
  url = {https://doi.org/10.1145/3664646.3664758},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {10--18},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {source: ACM},
  abstract = {Large language models and other foundation models (FMs) boost productivity by automating code generation, supporting bug fixes, and generating documentation. We propose that FMs can further support Open Source Software (OSS) projects by assisting developers and guiding the community. Currently, core developers and maintainers answer queries about processes, architecture, and source code, but their time is limited, often leading to delays. To address this, we introduce DevMentorAI, a tool that enhances developer-project interactions by leveraging source code and technical documentation. DevMentorAI uses the Retrieval Augmented Generation (RAG) architecture to identify and retrieve relevant content for queries. We evaluated DevMentorAI with a case study on PDF.js project, using real questions from a development chat room and comparing the answers provided by DevMentorAI to those from humans. A Mozilla expert rated the answers, finding DevMentorAI's responses more satisfactory in 8/14 of cases, equally satisfactory in 3/14, and less satisfactory in 3/14. These results demonstrate the potential of using foundation models and the RAG approach to support developers and reduce the burden on core developers.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0685-1},
  series = {{AIware},
}

@inproceedings{sattabun_enhancing_2025,
  title = {Enhancing {Reading},
  author = {Sattabun, Thunpitcha and Siriborvornratanakul, Thitirat},
  year = {2025},
  doi = {10.1145/3726101.3726109},
  url = {https://doi.org/10.1145/3726101.3726109},
  booktitle = {Proceedings of the 2025 7th {Asia},
  pages = {40--51},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {The development of critical thinking skills in childhood is a vital aspect of cognitive growth, and one effective yet often underutilized approach is encouraging reflective thinking through questions posed at the end of fairy tales. This study explores how Open-source Large Language Models (LLMs) can be used to generate expert-like questions, focusing on identifying the most effective strategy for this task. Using a dataset of 10,580 question sets derived from 278 children's fairy tales from the Project Gutenberg repository, we evaluated two established approaches—prompting and fine-tuning—across three LLMs: T5-base, Llama 2 7B, and Mistral 7B. The results revealed that while simple prompting methods are desirable for their ease of use, fine-tuning outperformed prompting in generating high-quality questions. Among the models tested, the fine-tuned Mistral 7B model achieved the best results, with a ROUGE-L score of 0.58 and a BLEU-4 score of 0.39. These findings highlight the practical trade-off between simplicity and performance, underscoring the importance of advanced fine-tuning skills for achieving optimal results in question generation tasks, particularly in educational contexts.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0728-5},
  series = {{APIT},
}

@inproceedings{pratelli_evaluation_2025,
  title = {Evaluation of {Reliability},
  author = {Pratelli, Manuel and Bianchi, John and Pinelli, Fabio and Petrocchi, Marinella},
  year = {2025},
  doi = {10.1145/3717867.3717924},
  url = {https://doi.org/10.1145/3717867.3717924},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {179--188},
  publisher = {Association for Computing Machinery},
  keywords = {Generative Question Answering, Good Editorial Practices, Inter-annotator agreement, LLMs, Reliability Evaluation},
  abstract = {In this study, we investigate the use of a large language model to assist in the evaluation of the reliability of the vast number of existing online news publishers, addressing the impracticality of relying solely on human expert annotators for this task. In the context of the Italian news media market, we first task the model with evaluating expert-designed reliability criteria using a representative sample of news articles. We then compare the model’s answers with those of human experts. The dataset consists of 352 news articles annotated by three human experts and the LLM. Examining 6,081 annotations over six criteria, we observe good agreement between LLM and human annotators in three evaluated criteria, including the critical ability to detect instances where a text negatively targets an entity or individual. For two additional criteria, such as the detection of sensational language and the recognition of bias in news content, LLMs generate fair annotations, albeit with certain trade-offs. Furthermore, we show that the LLM is able to help resolve disagreements among human experts, especially in tasks such as identifying cases of negative targeting.},
  address = {New York, NY, USA},
  series = {Websci '25},
  isbn = {979-8-4007-1483-2},
}

@inproceedings{fok_qlarify_2024,
  title = {Qlarify: {Recursively},
  author = {Fok, Raymond and Chang, Joseph Chee and August, Tal and Zhang, Amy X. and Weld, Daniel S.},
  year = {2024},
  doi = {10.1145/3654777.3676397},
  url = {https://doi.org/10.1145/3654777.3676397},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Information Retrieval, Interactive Documents, Large Language Models, Mixed-Initiative User Interfaces, Scientific Papers},
  abstract = {Navigating the vast scientific literature often starts with browsing a paper’s abstract. However, when a reader seeks additional information, not present in the abstract, they face a costly cognitive chasm during their dive into the full text. To bridge this gap, we introduce recursively expandable abstracts, a novel interaction paradigm that dynamically expands abstracts by progressively incorporating additional information from the papers’ full text. This lightweight interaction allows scholars to specify their information needs by quickly brushing over the abstract or selecting AI-suggested expandable entities. Relevant information is synthesized using a retrieval-augmented generation approach, presented as a fluid, threaded expansion of the abstract, and made efficiently verifiable via attribution to relevant source-passages in the paper. Through a series of user studies, we demonstrate the utility of recursively expandable abstracts and identify future opportunities to support low-effort and just-in-time exploration of long-form information contexts through LLM-powered interactions.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{farzi_does_2025,
  title = {Does {UMBRELA},
  author = {Farzi, Naghmeh and Dietz, Laura},
  year = {2025},
  doi = {10.1145/3726302.3730317},
  url = {https://doi.org/10.1145/3726302.3730317},
  booktitle = {Proceedings of the 48th {International},
  pages = {3214--3222},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {large language models, llm evaluation, relevance criteria},
  abstract = {We reproduce the UMBRELA LLM Judge evaluation framework across a range of large language models (LLMs) to assess its generalizability beyond the original study. Our investigation evaluates how LLM choice affects relevance assessment accuracy, focusing on leaderboard rank correlation and per-label agreement metrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very comparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we obtain slightly lower performance, which further degrades with smaller LLMs.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{adhikari_exploring_2025,
  title = {Exploring {LLMs},
  author = {Adhikari, Divya Mani and Hartland, Alexander and Weber, Ingmar and Cannanure, Vikram Kamath},
  year = {2025},
  doi = {10.1145/3719160.3736606},
  url = {https://doi.org/10.1145/3719160.3736606},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Cross-cultural Adaptation, Large Language Models (LLMs), Questionnaire Design, Questionnaire Pretesting, Survey Methodology},
  abstract = {Effective questionnaire design improves the validity of the results, but creating and adapting questionnaires across contexts is challenging due to resource constraints and limited expert access. Recently, the emergence of LLMs has led researchers to explore their potential in survey research. In this work, we focus on the suitability of LLMs in assisting the generation and adaptation of questionnaires. We introduce a novel pipeline that leverages LLMs to create new questionnaires, pretest with a target audience to determine potential issues and adapt existing standardized questionnaires for different contexts. We evaluated our pipeline for creation and adaptation through two studies on Prolific, involving 238 participants from the US and 118 participants from South Africa. Our findings show that participants found LLM-generated text clearer, LLM-pretested text more specific, and LLM-adapted questions slightly clearer and less biased than traditional ones. Our work opens new opportunities for LLM-driven questionnaire support in survey research.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{lin_haprepair_2025,
  title = {{HapRepair},
  author = {Lin, Zhihao and Zhou, Mingyi and Ma, Wei and Chen, Chi and Yang, Yun and Wang, Jun and Hu, Chunming and Li, Li},
  year = {2025},
  doi = {10.1145/3696630.3728556},
  url = {https://doi.org/10.1145/3696630.3728556},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {319--330},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {source: ACM},
  abstract = {Software defect detection and repair are essential software engineering tasks that mitigate potential risks in the early development stages. Large Language Models (LLMs) have demonstrated significant capabilities in software defect detection and repair. However, it is hard for LLMs to handle the new programming languages such as ArkTS (which is predominantly used in the OpenHarmony platform) due to training data shortage. Additionally, LLM-based multi-defect repair suffers from the limitation of the context window of LLMs. These issues significantly affect the performance of LLM-based defect repair in new programming languages. To address the above challenges, we propose HapRepair, a defect repair framework that integrates static analysis tools with retrieval-augmented generation (RAG) to improve the effectiveness of the defect repair. Specifically, we integrate CodeLinter into our iterative defect repair framework for defect detection, which is the basis of defect repair, and utilize RAG together with ArkAnalyzer to improve the quality of our repair solutions. To overcome the context window limitation of LLMs, we propose the Surrounding Context Extractor and the Context Combination Tool. Experiment results show that HapRepair effectively repairs defects in OpenHarmony Apps, demonstrating high reliability and efficiency in addressing code issues, achieving a defect repair rate of about 99\% on the test set, compared to only about 37\% when directly using LLMs for defect repair based on the defect information. Our approach demonstrates a robust solution for defect repair on new programming languages that have limited code corpus.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{xu_search---chain_2024,
  title = {Search-in-the-{Chain},
  author = {Xu, Shicheng and Pang, Liang and Shen, Huawei and Cheng, Xueqi and Chua, Tat-Seng},
  year = {2024},
  doi = {10.1145/3589334.3645363},
  url = {https://doi.org/10.1145/3589334.3645363},
  booktitle = {Proceedings of the {ACM},
  pages = {1362--1373},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {large language models, retrieval-augmented model, source: Scopus},
  abstract = {Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q\&amp;A, slot filling, fact checking, and long-form Q\&amp;A.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
  annote = {Cited by: 20; All Open Access; Gold Open Access},
}

@inproceedings{ma_librelog_2025,
  title = {{LibreLog},
  author = {Ma, Zeyang and Kim, Dong Jae and Chen, Tse-Hsun (Peter)},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00103},
  url = {https://doi.org/10.1109/icse55347.2025.00103},
  booktitle = {Proceedings of the {IEEE},
  pages = {924--936},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {Log parsing is a critical step that transforms unstructured log data into structured formats, facilitating subsequent log-based analysis. Traditional syntax-based log parsers are efficient and effective, but they often experience decreased accuracy when processing logs that deviate from the predefined rules. Recently, large language models (LLM) based log parsers have shown superior parsing accuracy. However, existing LLM-based parsers face three main challenges: 1) time-consuming and labor-intensive manual labeling for fine-tuning or in-context learning, 2) increased parsing costs due to the vast volume of log data and limited context size of LLMs, and 3) privacy risks from using commercial models like ChatGPT with sensitive log information. To overcome these limitations, this paper introduces LibreLog, an unsupervised log parsing approach that leverages open-source LLMs (i.e., Llama3-8B) to enhance privacy and reduce operational costs while achieving state-of-the-art parsing accuracy. LibreLog first groups logs with similar static text but varying dynamic variables using a fixed-depth grouping tree. It then parses logs within these groups using three components: i) similarity scoring-based retrieval augmented generation: selects diverse logs within each group based on Jaccard similarity, helping the LLM distinguish between static text and dynamic variables; ii) self-reflection: iteratively query LLMs to refine log templates to improve parsing accuracy; and iii) log template memory: stores parsed templates to reduce LLM queries for improved parsing efficiency. Our evaluation on LogHub-2.0 shows that LibreLog achieves 25\% higher parsing accuracy and processes logs 2.7 times faster compared to state-of-the-art LLM-based parsers. In short, LibreLog addresses privacy and cost concerns of using commercial LLMs while achieving state-of-the-arts parsing efficiency and accuracy.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{taveekitworachai_enhancing_2023,
  title = {Enhancing {Novelty},
  author = {Taveekitworachai, Pittawat and Thawonmas, Ruck},
  year = {2023},
  doi = {10.1145/3628454.3628456},
  url = {https://doi.org/10.1145/3628454.3628456},
  booktitle = {Proceedings of the 13th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bangkok, Thailand},
  keywords = {ChatGPT, Prompt engineering, Random word brainstorming},
  abstract = {This paper presents a new prompting approach for increasing the novelty in ChatGPT responses. ChatGPT has proven to be effective in generating natural language responses; however, ensuring response novelty remains challenging. Our proposed method, inspired by random word brainstorming, includes random words in prompts to introduce more diversity in ChatGPT responses. Through a questionnaire-based evaluation, we compared preferences for solution ideas generated using the standard approach and our proposed approach. We found that participants preferred our technique in 65\% of the 20 problems. The results suggest the effectiveness of our proposed approach. We also explored the use of GPT models as evaluators, with GPT-3.5 achieving 65\% accuracy and GPT-4 achieving 70\% accuracy when compared to human preferences from the questionnaire. These results suggest the potential of leveraging GPT models as noisy natural language evaluators. For future studies, we recommend focusing on prompt engineering and word list design to further improve performance. Overall, incorporating random words in prompts can effectively increase novelty in ChatGPT responses.},
  address = {New York, NY, USA},
  series = {{IAIT},
  isbn = {979-8-4007-0849-7},
}

@inproceedings{wang_decoding_2024,
  title = {Decoding {Urban},
  author = {Wang, Siqi and Liang, Chao and Gao, Yunfan and Liu, Yang and Li, Jing and Wang, Haofen},
  year = {2024},
  doi = {10.1145/3664647.3681705},
  url = {https://doi.org/10.1145/3664647.3681705},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {4757--4765},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {industrial park planning and operation, large language model agent, urban design and planning, urban knowledge graph},
  abstract = {Industrial parks are critical to urban economic growth. Yet, their development often encounters challenges stemming from imbalances between industrial requirements and urban services, underscoring the need for strategic planning and operations. This paper introduces IndustryScopeKG, a pioneering large-scale multi-modal, multi-level industrial park knowledge graph, which integrates diverse urban data including street views, corporate, socio-economic, and geospatial information, capturing the complex relationships and semantics within industrial parks. Alongside this, we present the IndustryScopeGPT framework, which leverages Large Language Models (LLMs) with Monte Carlo Tree Search to enhance tool-augmented reasoning and decision-making in Industrial Park Planning and Operation (IPPO). Our work significantly improves site recommendation and functional planning, demonstrating the potential of combining LLMs with structured datasets to advance industrial park management. This approach sets a new benchmark for intelligent IPPO research and lays a robust foundation for advancing urban industrial development. The dataset and related code are available at https://github.com/Tongji-KGLLM/IndustryScope.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{paria_navigating_2024,
  title = {Navigating {SoC},
  author = {Paria, Sudipta and Dasgupta, Aritra and Bhunia, Swarup},
  year = {2024},
  doi = {10.1145/3649476.3660393},
  url = {https://doi.org/10.1145/3649476.3660393},
  booktitle = {Proceedings of the {Great},
  pages = {252--257},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clearwater, FL, USA},
  keywords = {Bug Fix, CWEs., Formal Verification, HDL Code, LLMs, SoC Security, SVA, Verilog},
  abstract = {The increasing prominence of Large Language Models (LLMs) is being acknowledged for their exceptional abilities in comprehending natural language, conducting advanced reasoning, and generating contextual responses. LLMs expedite code generation, verification, and bug-fixing tasks across software and hardware domains. Development of hardware designs typically involves translating natural language specifications into Hardware Description Languages (HDLs) like Verilog or SystemVerilog, followed by circuit synthesis, physical layout, and fabrication, with the potential for human errors in the process. In the current industry practice, HDL verification tasks typically rely on manual expertise from security professionals to detect and address vulnerabilities. Modern System-on-Chip (SoC) designs integrate several Intellectual Property (IP) blocks implemented using HDL and communicate through a common bus to perform intended functions. Ensuring security throughout the SoC design process requires innovative solutions due to the complex nature of SoC designs and the distribution of assets across multiple IP blocks. Popular conversation LLMs such as Open AI’s ChatGPT and Google’s GEMINI (formerly BARD) offer the potential to automate HDL code generation and verification tasks by interpreting user prompts represented in natural language descriptions, thereby minimizing manual effort and enhancing hardware design quality. This paper explores recent research works on HDL generation, verification, and bug fix leveraging LLMs while addressing prevailing challenges and presenting potential opportunities for improvement.},
  address = {New York, NY, USA},
  series = {{GLSVLSI},
  isbn = {979-8-4007-0605-9},
}

@inproceedings{santana_can_2025,
  title = {Can {LLMs},
  author = {Santana, Vagner Figueredo de and Berger, Sara and Machado, Tiago and de Macedo, Maysa Malfiza Garcia and Sanctos, Cassia Sampaio and Williams, Lemara and Wu, Zhaoqing},
  year = {2025},
  doi = {10.1145/3708359.3712137},
  url = {https://doi.org/10.1145/3708359.3712137},
  booktitle = {Proceedings of the 30th {International},
  pages = {298--313},
  publisher = {Association for Computing Machinery},
  keywords = {Prompt Engineering, Recommendation Systems, Recommender Systems, Responsible AI, Responsible Prompting},
  abstract = {Human-Computer Interaction practitioners have been proposing best practices in user interface design for decades. However, generative Artificial Intelligence (GenAI) brings additional design considerations and currently lacks sufficient user guidance regarding affordances, inputs, and outputs. In this context, we developed a recommender system to promote responsible AI (RAI) practices while people prompt GenAI systems, by recommending addition of sentences based on social values and removal of harmful sentences. We detail a lightweight recommender system designed to be used in prompting-time and compare its recommendations to the ones provided by three base large language models (LLMs) and two LLMs fine-tuned for the task, i.e., recommending inclusion of sentences based on social values and removal of harmful sentences from a given prompt. Results indicate that our approach has the best F1-score balance in terms of recommendations for additions and removal of sentences to promote responsible prompts, while a fine-tuned model obtained the best F1-score for additions, and our approach obtained the best F1-score for removals of harmful sentences. In addition, fine-tuned models improved the objectiveness of responses by reducing the verbosity of generated content in 93\% when compared to the content generated by base models. Presented findings contribute to RAI by showing the limits and bias of existing LLMs in terms of recommendations on how to create more responsible prompts and how open-source technologies can fill this gap in prompting-time.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{koziolek_llm-based_2024-1,
  title = {{LLM},
  author = {Koziolek, Heiko and Koziolek, Anne},
  year = {2024},
  doi = {10.1145/3643795.3648385},
  url = {https://doi.org/10.1145/3643795.3648385},
  booktitle = {Proceedings of the 1st {International},
  pages = {38--45},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {ChatGPT, code generation, DCS, GPT4, IDs, IEC 61131-3, image recognition, industrial automation, industrial case study, large language models, P\&amp, PLC},
  abstract = {LLM-based code generation could save significant manual efforts in industrial automation, where control engineers manually produce control logic for sophisticated production processes. Previous attempts in control logic code generation lacked methods to interpret schematic drawings from process engineers. Recent LLMs now combine image recognition, trained domain knowledge, and coding skills. We propose a novel LLM-based code generation method that generates IEC 61131-3 Structure Text control logic source code from Piping-and-Instrumentation Diagrams (P\&amp;IDs) using image recognition. We have evaluated the method in three case study with industrial P\&amp;IDs and provide first evidence on the feasibility of such a code generation besides experiences on image recognition glitches.},
  address = {New York, NY, USA},
  series = {{LLM4Code},
  isbn = {979-8-4007-0579-3},
}

@inproceedings{solch_direct_2025,
  title = {Direct {Automated},
  author = {Sölch, Maximilian and Dietrich, Felix T. J. and Krusche, Stephan},
  year = {2025},
  doi = {10.1145/3696630.3727247},
  url = {https://doi.org/10.1145/3696630.3727247},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {901--911},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {education, formative feedback, grading, software engineering},
  abstract = {Timely and individualized feedback is essential for students' learning progress and motivation, yet providing such feedback has become increasingly challenging due to growing student numbers. This has resulted in a time-consuming, repetitive, and often manual task for educators, contributing to a high workload.This paper presents DAFeeD, an LLM-based approach for automated feedback on student submissions across various exercise domains. The defined feedback process enables interactive learning by allowing students to submit solutions multiple times and automatically receive iterative LLM feedback on their submission attempts before deadlines. By incorporating task details, grading criteria, student solutions, and custom instructions into the prompt, DAFeeD provides clear, personalized, and pedagogically meaningful feedback to support continuous improvement.To evaluate the feedback process, we implemented DAFeeD in an open-source reference implementation integrated into the learning platform Artemis. A controlled study with students working on a programming task in a supervised environment showed that students found the feedback relevant and beneficial. They reported feeling more comfortable and willing to request automated feedback due to its convenience and immediacy. Additionally, deploying DAFeeD in a software engineering course with 450 students demonstrated improvements in student performance and encouraged iterative refinement through multiple submissions.These findings highlight DAFeeD's potential to enhance feedback processes in computing education, improving both learning efficiency and student outcomes.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{ashkinaze_plurals_2025,
  title = {Plurals: {A},
  author = {Ashkinaze, Joshua and Fry, Emily and Edara, Narendra and Gilbert, Eric and Budak, Ceren},
  year = {2025},
  doi = {10.1145/3706598.3713675},
  url = {https://doi.org/10.1145/3706598.3713675},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, Human-AI Interaction, Human-Computer Interaction, Multi-Agent Systems, Pluralism},
  abstract = {Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a “view from nowhere” but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simulated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by deliberative democracy, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot generation in 75\% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{pontes_miranda_towards_2024,
  title = {Towards an {In},
  author = {Pontes Miranda, James William and Bruneliere, Hugo and Tisi, Massimo and Sunyé, Gerson},
  year = {2024},
  doi = {10.1145/3687997.3695650},
  url = {https://doi.org/10.1145/3687997.3695650},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {29--42},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pasadena, CA, USA},
  keywords = {Large language models, Model views, Model-driven engineering, Modeling languages, Prompt engineering, source: ACM},
  abstract = {In the Model-Driven Engineering (MDE) of complex systems, multiple models represent various systems' aspects. In practice, these models are often unconnected and specified using different modeling languages. Model view solutions can be employed to automatically combine such models. However, writing model view definitions is not trivial. When modeling languages are semantically distant and/or have a large number of concepts, it can quickly become difficult to manually identify the language elements to be selected, associated, or queried to build a model view. As a solution, this paper proposes an in-context Large Language Model (LLM)-based approach to assist engineers in writing model-view definitions. Notably, we rely on LLMs and Prompt Engineering techniques to automatically generate drafts of model-view definitions by providing as input only minimal information on the modeling languages to be combined. We implemented our approach by integrating the EMF Views solution for model views with the LangChain framework for LLM-based applications. To this end, we tailored LangChain to handle EMF metamodels. We validated our approach and implementation on a set of model views originally specified either in VPDL, the ViewPoint Definition Language of EMF Views, or as ATL model-to-model transformations. We compared these original model view definitions with the ones we automatically generated. The obtained results show the feasibility and applicability of our approach.},
  address = {New York, NY, USA},
  series = {{SLE},
  isbn = {979-8-4007-1180-0},
}

@inproceedings{hu_research_2025,
  title = {Research on the {Application},
  author = {Hu, Songtao and Zhao, Wenxi and Ma, Bin},
  year = {2025},
  doi = {10.1145/3744103.3744120},
  url = {https://doi.org/10.1145/3744103.3744120},
  booktitle = {Proceedings of the 2024 {International},
  pages = {76--84},
  publisher = {Association for Computing Machinery},
  keywords = {Large models, military intelligence, training methods},
  abstract = {Abstract: With the rapid development of artificial intelligence, large military intelligence models are increasingly applied in key areas such as battlefield monitoring, decision support, and multi-source intelligence integration. These models can provide real-time situational awareness, optimize strategic planning, and help integrate diverse data sources such as satellite imagery, drone reconnaissance, and communication intelligence. The document primarily discusses the applications, training methods, and challenges of large military intelligence models. It highlights how large models are increasingly used in areas such as battlefield monitoring, multi-source intelligence integration, and decision-making support. The document covers four key training methods: Retrieval-Augmented Generation, Incremental Pretraining, Supervised Fine-Tuning, and Reinforcement Learning from Human Feedback, comparing their advantages, disadvantages, and resource demands.},
  address = {New York, NY, USA},
  series = {{ISAICS},
  isbn = {979-8-4007-1442-9},
}

@inproceedings{bian_empowering_2025,
  title = {Empowering {IETF},
  author = {Bian, Jie and Welzl, Michael},
  year = {2025},
  doi = {10.1145/3744200.3744761},
  url = {https://doi.org/10.1145/3744200.3744761},
  booktitle = {Proceedings of the 2025 {Applied},
  pages = {24--31},
  publisher = {Association for Computing Machinery},
  note = {event-place: Madrid, Spain},
  keywords = {I-Ds, IETF, Information Retrieval, Instruction Tuning, LLM, RFC, Test Time Scaling},
  abstract = {The Internet Engineering Task Force (IETF) produces extensive textual data, including discussions in email archives and GitHub repositories and Internet-Drafts (I-Ds), which are preliminary versions of Requests for Comments (RFCs). The sheer volume and complexity of this material present significant workflow challenges, contributing to the duration of the standardization process (it is common to take several years from an initial draft to the final RFC). This paper explores the potential of Natural Language Processing (NLP) using Large Language Models (LLMs) to streamline IETF workflows. We use Information Retrieval (IR) to i) build a search system that helps users locate comments related to details in an I-D, and ii) partially automate RFC writing.},
  address = {New York, NY, USA},
  series = {{ANRW},
  isbn = {979-8-4007-2009-3},
}

@inproceedings{yang_aqua_2024,
  title = {{AQuA},
  author = {Yang, Saelyne and Vermeulen, Jo and Fitzmaurice, George and Matejka, Justin},
  year = {2024},
  doi = {10.1145/3613904.3642752},
  url = {https://doi.org/10.1145/3613904.3642752},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {generative AI, large language models, question answering, software learning, tutorial videos},
  abstract = {Tutorial videos are a popular help source for learning feature-rich software. However, getting quick answers to questions about tutorial videos is difficult. We present an automated approach for responding to tutorial questions. By analyzing 633 questions found in 5,944 video comments, we identified different question types and observed that users frequently described parts of the video in questions. We then asked participants (N=24) to watch tutorial videos and ask questions while annotating the video with relevant visual anchors. Most visual anchors referred to UI elements and the application workspace. Based on these insights, we built AQuA, a pipeline that generates useful answers to questions with visual anchors. We demonstrate this for Fusion 360, showing that we can recognize UI elements in visual anchors and generate answers using GPT-4 augmented with that visual information and software documentation. An evaluation study (N=16) demonstrates that our approach provides better answers than baseline methods.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{cao_writing_2025,
  title = {Writing {Style},
  author = {Cao, Hongliu},
  year = {2025},
  doi = {10.1145/3701551.3703514},
  url = {https://doi.org/10.1145/3701551.3703514},
  booktitle = {Proceedings of the {Eighteenth},
  pages = {336--344},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hannover, Germany},
  keywords = {bias, fairness, information retrieval, responsible ai, universal text embeddings, writing styles, source: ACM},
  abstract = {The rapid advancement of Language Model technologies has opened new opportunities, but also introduced new challenges related to bias and fairness. This paper explores the uncharted territory of potential biases in state-of-the-art universal text embedding models towards specific document and query writing styles within Information Retrieval (IR) systems. Our investigation reveals that different embedding models exhibit different preferences for document writing style, while more informal and emotive styles are less favored by most embedding models. In terms of query writing styles, many embedding models tend to match the query style with the retrieved document style, but some show a consistent preference for specific styles. Text embedding models fine-tuned on synthetic data generated by LLMs display a consistent preference for certain style of generated data. These biases in text embedding based IR systems can inadvertently silence or marginalize certain communication styles, thereby posing a significant threat to fairness in information retrieval. Finally, we also compare the answer styles of Retrieval Augmented Generation (RAG) systems based on different LLMs and find out that most text embedding models are biased towards LLM's answer styles when used as evaluation metrics for answer correctness. This study sheds light on the critical issue of writing style based bias in IR systems, offering valuable insights for the development of more fair and robust models.},
  address = {New York, NY, USA},
  series = {{WSDM},
  isbn = {979-8-4007-1329-3},
}

@inproceedings{huh_vid2coach_2025,
  title = {{Vid2Coach},
  author = {Huh, Mina and Xue, Zihui and Das, Ujjaini and Ashutosh, Kumar and Grauman, Kristen and Pavel, Amy},
  year = {2025},
  doi = {10.1145/3746059.3747612},
  url = {https://doi.org/10.1145/3746059.3747612},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Accessibility, How-To Videos, Task Assistant, Video Understanding},
  abstract = {People use videos to learn new recipes, exercises, and crafts. Such videos remain difficult for blind and low vision (BLV) people to follow as they rely on visual comparison. Our observations of visual rehabilitation therapists (VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide both proactive and responsive support including detailed descriptions, non-visual workarounds, and progress feedback. We propose Vid2Coach, a system that transforms how-to videos into wearable camera-based assistants that provide accessible instructions and mixed-initiative feedback. From the video, Vid2Coach generates accessible instructions by augmenting narrated instructions with demonstration details and completion criteria for each step. It then uses retrieval-augmented-generation to extract relevant non-visual workarounds from BLV-specific resources. Vid2Coach then monitors user progress with a camera embedded in commercial smart glasses to provide context-aware instructions, proactive feedback, and answers to user questions. BLV participants (N=8) using Vid2Coach completed cooking tasks with 58.5\% fewer errors than when using their typical workflow and wanted to use Vid2Coach in their daily lives. Vid2Coach demonstrates an opportunity for AI visual assistance that strengthens rather than replaces non-visual expertise.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{sabouri_trust_2025,
  title = {Trust {Dynamics},
  author = {Sabouri, Sadra and Eibl, Philipp and Zhou, Xinyi and Ziyadi, Morteza and Medvidovic, Nenad and Lindemann, Lars and Chattopadhyay, Souti},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00199},
  url = {https://doi.org/10.1109/icse55347.2025.00199},
  booktitle = {Proceedings of the {IEEE},
  pages = {1678--1690},
  publisher = {IEEE Press},
  keywords = {AI-code assistants, software development, trust, source: ACM},
  abstract = {Software developers increasingly rely on AI code generation utilities. To ensure that "good" code is accepted into the code base and "bad" code is rejected, developers must know when to trust an AI suggestion. Understanding how developers build this intuition is crucial to enhancing developer-AI collaborative programming. In this paper, we seek to understand how developers (1) define and (2) evaluate the trustworthiness of a code suggestion and (3) how trust evolves when using AI code assistants. To answer these questions, we conducted a mixed-method study consisting of an in-depth exploratory survey with (n=29) developers followed by an observation study (n=10).We found that comprehensibility and perceived correctness were the most frequently used factors to evaluate code suggestion trustworthiness. However, the gap in developers' definition and evaluation of trust points to a lack of support for evaluating trustworthy code in real-time. We also found that developers often alter their trust decisions, keeping only 52\% of original suggestions. Based on these findings, we extracted four guidelines to enhance developer-AI interactions. We validated the guidelines through a survey with (n=7) domain experts and survey members (n=8). We discuss the validated guidelines, how to apply them, and tools to help adopt them.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{zhang_navigating_2025,
  title = {Navigating the {Fog},
  author = {Zhang, Chao and Zhu, Shengqi and Yang, Xinyu and Tseng, Yu-Chia and Jiang, Shenrong and Rzeszotarski, Jeffrey M.},
  year = {2025},
  doi = {10.1145/3719160.3736618},
  url = {https://doi.org/10.1145/3719160.3736618},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Hallucinations, Fact Checking, Large Language Models, Sensemaking, University Students},
  abstract = {LLM interfaces, such as ChatGPT, are widely used by students in higher education. However, their reliability is compromised by the tendency to generate plausible yet factually inaccurate content. This issue is particularly critical as the HCI community shows growing interest in designing LLM-based educational technology. Despite this interest, we have yet to learn how plausible falsehoods disrupt students’ real-time sensemaking of outputs from imperfectly reliable LLMs, and how students currently attempt to mitigate these negative effects. Thus, we conducted a case study of 15 university students using ChatGPT through think-aloud tasks and semi-structured interviews. We identified recurring patterns of sensemaking, with students facing challenges such as relying on intuitive guesses and feeling overwhelmed by LLM’s lengthy, sycophantic, and overconfident responses. They adapted by inducing inconsistencies from the LLM’s responses and strategically dividing tasks between themselves and the LLM. Lastly, our study highlights several design implications for future reliable LLM interfaces.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{ariza-casabona_comparative_2024,
  title = {A {Comparative},
  author = {Ariza-Casabona, Alejandro and Boratto, Ludovico and Salamó, Maria},
  year = {2024},
  doi = {10.1145/3640457.3688069},
  url = {https://doi.org/10.1145/3640457.3688069},
  booktitle = {Proceedings of the 18th {ACM},
  pages = {105--115},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bari, Italy},
  keywords = {Explainable Recommendation, Feature Hallucination, Natural Language Explanations, Reproducibility},
  abstract = {One way to increase trust among users towards recommender systems is to provide the recommendation along with a textual explanation. In the literature, extraction-based, generation-based, and, more recently, hybrid solutions based on retrieval-augmented generation have been proposed to tackle the problem of text-based explainable recommendation. However, the use of different datasets, preprocessing steps, target explanations, baselines, and evaluation metrics complicates the reproducibility and state-of-the-art assessment of previous work among different model categories for successful advancements in the field. Our aim is to provide a comprehensive analysis of text-based explainable recommender systems by setting up a well-defined benchmark that accommodates generation-based, extraction-based, and hybrid approaches. Also, we enrich the existing evaluation of explainability and text quality of the explanations with a novel definition of feature hallucination. Our experiments on three real-world datasets unveil hidden behaviors and confirm several claims about model patterns. Our source code and preprocessed datasets are available at https://github.com/alarca94/text-exp-recsys24.},
  address = {New York, NY, USA},
  series = {{RecSys},
  isbn = {979-8-4007-0505-2},
}

@inproceedings{zhang_how_2024,
  title = {How to {Efficiently},
  author = {Zhang, Hongying and Li, Gaolei and Li, Shenghong and Liu, Hongfu and Wang, Shuo and Li, Jianhua},
  year = {2024},
  doi = {10.1145/3689217.3690622},
  url = {https://doi.org/10.1145/3689217.3690622},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {25--34},
  publisher = {Association for Computing Machinery},
  note = {event-place: Salt Lake City, UT, USA},
  keywords = {artificial intelligent agents, critical infrastructure, large code-graph model, vulnerability management},
  abstract = {Critical infrastructure vulnerabilities, once maliciously manipulated, may cause serious security accidents. However, existing methods are always unable to discover, assess, block and repair those unknown/known vulnerabilities in a timely and effective manner. This article explores the potential of large models on vulnerability management optimization. To efficiently orchestrate complex vulnerability management tasks (e.g., detection, prioritization, and code repairing), we propose to conduct a novel Large Code-graph Model (LCM) to break down vulnerability life-cycle management into distinct suites using artificial intelligence agents, Retrieval-Augmented Generation (RAG), and graph-structured large models to automate processes without extensive prior knowledge. In particular, we conduct an evaluation experiment utilizing the proposed LCM for pre-processing vulnerable data in the vulnerability detection suite. The results showed a final detection accuracy of 97.2\%, significantly outperforming baseline models and confirming that the proposed LCM can autonomously extract superior features as a data pre-processing tool. Consequently, the experimental results also partially validate the feasibility of our proposed framework.},
  address = {New York, NY, USA},
  series = {{LAMPS},
  isbn = {979-8-4007-1209-8},
}

@inproceedings{wang_composable_2025,
  title = {Composable {Effect},
  author = {Wang, Di},
  year = {2025},
  doi = {10.1145/3759425.3763396},
  url = {https://doi.org/10.1145/3759425.3763396},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {124--129},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {algebraic effects, effect handlers, large language models, LLM-integrated scripts},
  abstract = {Implementing LLM-integrated scripts introduces challenges in modularity and performance, as scripts are often coupled to specific LLM implementations and fail to exploit parallelization opportunities. This paper proposes using composable effect handling to separate workflow logic from effectful operations, such as LLM calls, I/O, and concurrency, enabling modularity without sacrificing the opportunity for performance optimization. By treating these operations as abstract interfaces and discharging them via effect handlers, this paper shows that scripts can achieve significant speedups (e.g., 10× in a Tree-of-Thoughts case study) without compromising modularity. This paper aims to promote composable effect handling as a programming style for LLM scripting.},
  address = {New York, NY, USA},
  series = {{LMPL},
  isbn = {979-8-4007-2148-9},
}

@inproceedings{dai_mitigating_2025,
  title = {Mitigating {Source},
  author = {Dai, Sunhao and Zhou, Yuqi and Pang, Liang and Li, Zhuoyang and Du, Zhaocheng and Wang, Gang and Xu, Jun},
  year = {2025},
  doi = {10.1145/3726302.3730038},
  url = {https://doi.org/10.1145/3726302.3730038},
  booktitle = {Proceedings of the 48th {International},
  pages = {370--380},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information retrieval, llm alignment, source bias},
  abstract = {Recent studies have revealed a phenomenon known as source bias, where PLM-based retrievers assign higher relevance scores to LLM-generated content despite its semantic quality being comparable to human-written content. As LLMs rapidly advance and become more widely used, effectively counteracting source bias is crucial for the sustainable development of the information retrieval (IR) ecosystem. Existing methods primarily attempt to address source bias from the retriever side, adopting a "passive defense" approach that intervenes only after biased content has entered the retrieval pipeline. These solutions are limited by frequent retriever updates in industrial applications, high recurring costs, and their inability to address the root cause of source bias.In this paper, we propose a new perspective for mitigating source bias by actively aligning LLM outputs at the data generation stage. Specifically, we introduce LLM-SBM, a novel LLM alignment framework for source bias mitigation. First, we construct high-quality alignment datasets using an automatic preference data construction pipeline. This pipeline leverages LLMs to generate multiple rephrasings of content and employs a PLM-based retriever to assign corresponding specific preference values for each generated document, thereby forming preference pairs according to these preferences. Moreover, to fully utilize these scalar values of preference and enhance the efficiency of the alignment process, LLM-SBM incorporates these preference differences as weighting factors in the loss function during policy training. Extensive experiments across multiple datasets and PLM-based retrievers demonstrate that LLMs aligned with LLM-SBM successfully reduce source bias while preserving their general capabilities.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{khurana_it_2025,
  title = {Do {It},
  author = {Khurana, Anjali and Su, Xiaotian and Wang, April Yi and Chilana, Parmit K},
  year = {2025},
  doi = {10.1145/3706598.3713431},
  url = {https://doi.org/10.1145/3706598.3713431},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {feature-rich software, human-AI collaboration, large language models, semi-automation, software copilots, user control},
  abstract = {Large Language Model (LLM)-based in-application assistants, or copilots, can automate software tasks, but users often prefer learning by doing, raising questions about the optimal level of automation for an effective user experience. We investigated two automation paradigms by designing and implementing a fully automated copilot (AutoCopilot) and a semi-automated copilot (GuidedCopilot) that automates trivial steps while offering step-by-step visual guidance. In a user study (N=20) across data analysis and visual design tasks, GuidedCopilot outperformed AutoCopilot in user control, software utility, and learnability, especially for exploratory and creative tasks, while AutoCopilot saved time for simpler visual tasks. A follow-up design exploration (N=10) enhanced GuidedCopilot with task-and state-aware features, including in-context preview clips and adaptive instructions. Our findings highlight the critical role of user control and tailored guidance in designing the next generation of copilots that enhance productivity, support diverse skill levels, and foster deeper software engagement.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{li_llms_2024,
  title = {{LLMs},
  author = {Li, Yilin and Jobson, Deddy},
  year = {2024},
  doi = {10.1145/3665939.3665969},
  url = {https://doi.org/10.1145/3665939.3665969},
  booktitle = {Proceedings of the 2024 {Workshop},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {human-in-the-loop, large language models, LLM, Text2SQL, source: ACM},
  abstract = {Text2SQL is typically considered a one-shot process where the user gives a natural language query and receives an SQL query in return. This approach is fraught with potential concerns, such as syntactical errors, logical mismatches, and schema hallucination, which often require time-consuming validations by end users. These challenges are exacerbated by the complexity of large queries typical in industry settings and the inherent ambiguity of natural language. To address these limitations, we propose a system that employs an iterative process for both query creation and validation, ensuring that the resulting data set meets the user's expectations. We tested this system against existing text-to-SQL LLM approaches using a standard industry use case, showcasing our system's ability to deliver coherent and accurate outcomes. Opportunities for future research to further refine this approach are also discussed1.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0693-6},
  series = {{HILDA},
}

@inproceedings{liu_webanns_2025,
  title = {{WebANNS},
  author = {Liu, Mugeng and Zhong, Siqi and Yang, Qi and Han, Yudong and Liu, Xuanzhe and Ma, Yun},
  year = {2025},
  doi = {10.1145/3726302.3730115},
  url = {https://doi.org/10.1145/3726302.3730115},
  booktitle = {Proceedings of the 48th {International},
  pages = {2483--2492},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {approximate nearest neighbor search, web browser, webassembly},
  abstract = {Approximate nearest neighbor search (ANNS) has become vital to modern AI infrastructure, particularly in retrieval-augmented generation (RAG) applications. Numerous in-browser ANNS engines have emerged to seamlessly integrate with popular LLM-based web applications, while addressing privacy protection and challenges of heterogeneous device deployments. However, web browsers present unique challenges for ANNS, including computational limitations, external storage access issues, and memory utilization constraints, which state-of-the-art (SOTA) solutions fail to address comprehensively.We propose WebANNS, a novel ANNS engine specifically designed for web browsers. WebANNS leverages WebAssembly to overcome computational bottlenecks, designs a lazy loading strategy to optimize data retrieval from external storage, and applies a heuristic approach to reduce memory usage. Experiments show that WebANNS is fast and memory efficient, achieving up to 743.8X improvement in 99th percentile query latency over the SOTA engine, while reducing memory usage by up to 39\%. Note that WebANNS decreases query time from 10 seconds to the 10-millisecond range in browsers, making in-browser ANNS practical with user-acceptable latency.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{tran_memoriease_2024,
  title = {{MemoriEase},
  author = {Tran, Quang-Linh and Nguyen, Binh and Jones, Gareth J. F. and Gurrin, Cathal},
  year = {2024},
  doi = {10.1145/3643489.3661114},
  url = {https://doi.org/10.1145/3643489.3661114},
  booktitle = {Proceedings of the 7th {Annual},
  pages = {12--17},
  publisher = {Association for Computing Machinery},
  note = {event-place: Phuket, Thailand},
  keywords = {source: ACM},
  abstract = {Lifelog retrieval plays an important role in memory support for lifeloggers. It helps the lifeloggers to browse, search and navigate their life moments from the lifelog data. However, the volume and variety of lifelog data are enormous and range in multiple modalities so they impose a big challenge to retrieve accurate lifelog moments. The Lifelog Search Challenges (LSCs) are a benchmark challenge for evaluating lifelog retrieval systems in different tasks. In this paper, we introduce the MemoriEase 2.0 lifelog retrieval system that participates in LSC'24. This system not only inherits core functions from the precedent system but also incorporates new components such as conversational search, visual similarity search and retrieval-augmented generation for question-answering tasks. The new functions are expected to help expert and novice users solve all topics in three tasks of LSC'24. We evaluate MemoriEase 2.0 in KIS topics in LSC'23 and the system achieves promising results with Recall@1 is 40\% at the first hint and it solves 8 over 10 topics.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0550-2},
  series = {{LSC},
}

@inproceedings{sun_rearter_2025,
  title = {{ReARTeR},
  author = {Sun, Zhongxiang and Wang, Qipeng and Yu, Weijie and Zang, Xiaoxue and Zheng, Kai and Xu, Jun and Zhang, Xiao and Song, Yang and Li, Han},
  year = {2025},
  doi = {10.1145/3726302.3730102},
  url = {https://doi.org/10.1145/3726302.3730102},
  booktitle = {Proceedings of the 48th {International},
  pages = {1251--1261},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {reasoning, retrieval augment generation, trustworthy, source: ACM},
  abstract = {Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) have shown promise in knowledge-intensive tasks, yet their reasoning capabilities, particularly for complex multi-step reasoning, remain limited. Although recent approaches have explored integrating RAG with chain-of-thought reasoning or incorporating test-time search with process reward model (PRM), these methods face several untrustworthy challenges, including lack of explanations, bias in PRM training data, early-step bias in PRM scores, and ignoring post-training that fails to fully optimize reasoning potential. To address these issues, we propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding (ReARTeR), a framework that enhances RAG systems' reasoning capabilities through both post-training and test-time scaling. At test time, ReARTeR introduces Trustworthy Process Rewarding via a Process Reward Model for accurate scalar scoring and a Process Explanation Model (PEM) for generating natural language explanations, enabling step refinement. During post-training, we leverage Monte Carlo Tree Search guided by Trustworthy Process Rewarding to collect high-quality step-level preference data, which is used to optimize the model through Iterative Preference Optimization. ReARTeR tackles three key challenges: (1) misalignment between PRM and PEM, addressed through off-policy preference learning; (2) bias in PRM training data, mitigated by a balanced annotation method and incorporating stronger annotations for difficult examples; and (3) early-step bias in PRM, resolved via a temporal-difference-based look-ahead search strategy. Experimental results on multi-step reasoning benchmarks demonstrate that ReARTeR significantly improves reasoning performance, highlighting its potential to advance the reasoning capability of RAG systems.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{lau_ban_2023,
  title = {From "{Ban},
  author = {Lau, Sam and Guo, Philip},
  year = {2023},
  doi = {10.1145/3568813.3600138},
  url = {https://doi.org/10.1145/3568813.3600138},
  booktitle = {Proceedings of the 2023 {ACM},
  pages = {106--121},
  publisher = {Association for Computing Machinery},
  note = {event-place: Chicago, IL, USA},
  keywords = {AI coding tools, ChatGPT, Copilot, instructor perspectives, LLM},
  abstract = {Over the past year (2022–2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.},
  address = {New York, NY, USA},
  series = {{ICER},
  isbn = {978-1-4503-9976-0},
}

@inproceedings{shen_gpiot_2025,
  title = {{GPIoT},
  author = {Shen, Leming and Yang, Qiang and Huang, Xinyu and Ma, Zijing and Zheng, Yuanqing},
  year = {2025},
  doi = {10.1145/3715014.3722064},
  url = {https://doi.org/10.1145/3715014.3722064},
  booktitle = {Proceedings of the 23rd {ACM},
  pages = {199--212},
  publisher = {Association for Computing Machinery},
  note = {event-place: UC Irvine Student Center., Irvine, CA, USA},
  keywords = {fine-tuning, IoT program synthesis, small language model},
  abstract = {Code Large Language Models (LLMs) enhance software development efficiency by automatically generating code and documentation based on user requirements. However, code LLMs cannot synthesize specialized programs when tasked with IoT applications that require domain knowledge. While Retrieval-Augmented Generation (RAG) offers a promising solution by fetching relevant domain knowledge, it necessitates powerful cloud LLMs (e.g., GPT-4) to process user requirements and retrieved contents, which raises significant privacy concerns. This approach also suffers from unstable networks and prohibitive LLM query costs. Moreover, it is challenging to ensure the correctness and relevance of the fetched contents. To address these issues, we propose GPIoT, a code generation system for IoT applications by fine-tuning locally deployable Small Language Models (SLMs) on IoT-specialized datasets. SLMs have smaller model sizes, allowing efficient local deployment and execution to mitigate privacy concerns and network uncertainty. Furthermore, by fine-tuning SLMs with our IoT-specialized datasets, the SLMs' ability to synthesize IoT-related programs can be substantially improved. To evaluate GPIoT's capability in synthesizing programs for IoT applications, we develop a benchmark, IoTBench. Extensive experiments and user trials demonstrate the effectiveness of GPIoT in generating IoT-specialized code, outperforming state-of-the-art code LLMs with an average task accuracy increment of 64.7\% and significant improvements in user satisfaction.},
  address = {New York, NY, USA},
  series = {{SenSys},
  isbn = {979-8-4007-1479-5},
}

@inproceedings{gebreegziabher_metricmate_2025,
  title = {{MetricMate},
  author = {Gebreegziabher, Simret Araya and Chiang, Charles and Wang, Zichu and Ashktorab, Zahra and Brachman, Michelle and Geyer, Werner and Li, Toby Jia-Jun and Gómez-Zará, Diego},
  year = {2025},
  doi = {10.1145/3729176.3729199},
  url = {https://doi.org/10.1145/3729176.3729199},
  booktitle = {Proceedings of the 4th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Evaluation Methods, Human AI Interaction, Large Language Models, LLM-as-a-Judge},
  abstract = {The rise of the use of Large Language Models (LLMs) in work has driven the need for robust evaluation methods that align model behavior with human values and preferences. LLM-as-a-judge approaches have emerged as a scalable solution, leveraging LLMs to evaluate generated outputs based on flexible user-defined criteria. However, users often struggle to articulate clear evaluation criteria. In addition, human preferences and criteria definitions evolve, and predefined templates fail to account for context-specific nuances. To address these challenges, we present MetricMate, an interactive tool that supports users in defining and calibrating evaluation criteria for LLM-as-a-judge systems. MetricMate introduces hierarchical criteria definitions and curated examples of success and failure to promote human-AI criteria negotiation and alignment. Additionally, MetricMate learns from users’ interactions with data by enabling users to group data to identify patterns and provide context-specific criteria.},
  address = {New York, NY, USA},
  series = {{CHIWORK},
  isbn = {979-8-4007-1384-2},
}

@inproceedings{kurshan_ai_2024,
  title = {{AI},
  author = {Kurshan, Eren and Mehta, Dhagash and Balch, Tucker},
  year = {2024},
  doi = {10.1145/3677052.3698655},
  url = {https://doi.org/10.1145/3677052.3698655},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {745--751},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {source: ACM},
  abstract = {Adoption of AI by criminal entities across traditional and emerging financial crime paradigms has been a disturbing recent trend. Particularly concerning is the proliferation of generative AI, which has empowered criminal activities ranging from sophisticated phishing schemes to the creation of hard-to-detect deep fakes, and to advanced spoofing attacks to biometric authentication systems. The exploitation of AI by criminal purposes continues to escalate, presenting an unprecedented challenge. AI adoption causes an increasingly complex landscape of fraud typologies intertwined with cybersecurity vulnerabilities. Overall, GenAI has a transformative effect on financial crimes and fraud. According to some estimates, GenAI will quadruple the fraud losses by 2027 with a staggering annual growth rate of over 30\% [27]. As crime patterns become more intricate, personalized, and elusive, deploying effective defensive AI strategies becomes indispensable. However, several challenges hinder the necessary progress of AI-based fincrime detection systems. This paper examines the latest trends in AI/ML-driven financial crimes and detection systems. It underscores the urgent need for developing agile AI defenses that can effectively counteract the rapidly emerging threats. It also aims to highlight the need for cooperation across the financial services industry to tackle the GenAI induced crime waves.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{ding_enhancing_2024,
  title = {Enhancing {On},
  author = {Ding, Yucheng and Niu, Chaoyue and Wu, Fan and Tang, Shaojie and Lyu, Chengfei and Chen, Guihai},
  year = {2024},
  doi = {10.1145/3637528.3671679},
  url = {https://doi.org/10.1145/3637528.3671679},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {597--608},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {datastore subset selection, device-cloud hybrid service, on-device llm enhancement, source: ACM},
  abstract = {Many billion-scale large language models (LLMs) have been released for resource-constraint mobile devices to provide local LLM inference service when cloud-based powerful LLMs are not available. However, the capabilities of current on-device LLMs still lag behind those of cloud-based LLMs, and how to effectively and efficiently enhance on-device LLM inference becomes a practical requirement. We thus propose to collect the user's historical interactions with the cloud-based LLM and build an external datastore on the mobile device for enhancement using nearest neighbors search. Nevertheless, the full datastore improves the quality of token generation at the unacceptable expense of much slower generation speed. To balance performance and efficiency, we propose to select an optimal subset of the full datastore within the given size limit, the optimization objective of which is proven to be submodular. We further design an offline algorithm, which selects the subset after the construction of the full datastore, as well as an online algorithm, which performs selection over the stream and can be flexibly scheduled. We theoretically analyze the performance guarantee and the time complexity of the offline and the online designs to demonstrate effectiveness and scalability. We finally take three ChatGPT related dialogue datasets and four different on-device LLMs for evaluation. Evaluation results show that the proposed designs significantly enhance LLM performance in terms of perplexity while maintaining fast token generation speed. Practical overhead testing on the smartphone reveal the efficiency of on-device datastore subset selection from memory usage and computation overhead.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{kang_tldr_2024,
  title = {tl;dr: {Chill},
  author = {Kang, Eunsuk and Shaw, Mary},
  year = {2024},
  doi = {10.1145/3689492.3689816},
  url = {https://doi.org/10.1145/3689492.3689816},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {303--315},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pasadena, CA, USA},
  keywords = {AI-assisted development, software correctness, software engineering principles, source: ACM},
  abstract = {Social media provide a steady diet of dire warnings that artificial intelligence (AI) will make software engineering (SE) irrelevant or obsolete. To the contrary, the engineering discipline of software is rich and robust; it encompasses the full scope of software design, development, deployment, and practical use; and it has regularly assimilated radical new offerings from AI. Current AI innovations such as machine learning, large language models (LLMs) and generative AI will offer new opportunities to extend the models and methods of SE. They may automate some routine development processes, and they will bring new kinds of components and architectures. If we're fortunate they may force SE to rethink what we mean by correctness and reliability. They will not, however, render SE irrelevant.},
  address = {New York, NY, USA},
  series = {Onward! '24},
  isbn = {979-8-4007-1215-9},
}

@inproceedings{singh_finqapt_2024,
  title = {{FinQAPT},
  author = {Singh, Kuldeep and Kaur, Simerjot and Smiley, Charese},
  year = {2024},
  doi = {10.1145/3677052.3698682},
  url = {https://doi.org/10.1145/3677052.3698682},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {266--273},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {finance, LLM, numerical reasoning, prompting, question answering},
  abstract = {Financial decision-making hinges on the analysis of relevant information embedded in the enormous volume of documents in the financial domain. To address this challenge, we developed FinQAPT, an end-to-end pipeline that streamlines the identification of relevant financial reports based on a query, extracts pertinent context, and leverages Large Language Models (LLMs) to perform downstream tasks. To evaluate the pipeline, we experimented with various techniques to optimize the performance of each module using the FinQA dataset. We introduced a novel clustering-based negative sampling technique to enhance context extraction and a novel prompting method called Dynamic N-shot Prompting to boost the numerical question-answering capabilities of LLMs. At the module level, we achieved state-of-the-art accuracy on FinQA, attaining an accuracy of 80.6\%. However, at the pipeline level, we observed decreased performance due to challenges in extracting relevant context from financial reports. We conducted a detailed error analysis of each module and the end-to-end pipeline, pinpointing specific challenges that must be addressed to develop a robust solution for handling complex financial tasks.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{donnelly_exploring_2025,
  title = {Exploring the {Utility},
  author = {Donnelly, Jonathan and Roegiest, Adam},
  year = {2025},
  doi = {10.1145/3731120.3744609},
  url = {https://doi.org/10.1145/3731120.3744609},
  booktitle = {Proceedings of the 2025 {International},
  pages = {401--409},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {contract, embedding, legal, semantic similarity},
  abstract = {With the increasing use of text embeddings motivated by the adoption of Retrieval Augmented Generation (RAG) in applied domains, this work investigates whether the semantic aspects of text embeddings correspond to the colloquial understanding of semantic similarity in a legal domain. Using clauses from legal agreements, we find that embeddings and associated similarity measurements (e.g., cosine, L2) do not accurately reflect a legal understanding of ”semantically similar.” More specifically, legal clauses are more similar to a minimally changed, negated version than those with identical legal meaning but worded differently across embedding sources and similarity measures. We demonstrate that discriminative classification can be an effective stop-gap solution with these two types of variants using either a zero-shot generative model prompt or a multi-layer perceptron trained on the embeddings. These results indicate that care should be taken when applying off-the-shelf semantic similarity tools in specialized domains and provides a basis from which further work can be conducted to determine cost effective methods for measuring nuanced notions of similarity.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{franca_optimizing_2025,
  title = {Optimizing {Tail},
  author = {França, Celso and Rabbi, Gestefane and Salles, Thiago and Cunha, Washington and Rocha, Leonardo and André Gonçalves, Marcos},
  year = {2025},
  doi = {10.1145/3726302.3730052},
  url = {https://doi.org/10.1145/3726302.3730052},
  booktitle = {Proceedings of the 48th {International},
  pages = {1392--1401},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {extreme multi-label text classification, rag-labels, retrieving and fusing, tail-head trade-off, source: ACM},
  abstract = {We tackle Extreme Multi-Label Text Classification (XMTC), which involves assigning relevant labels to texts from a huge label space. Attempting to optimize the underexplored tail-head trade-off, we address the XMTC task through its core challenges of volume, skewness, and quality by proposing xCoRetriev, a novel two-stage retrieving and fusing ranking pipeline. Our pipeline addresses the volume challenge by dynamically slicing the large label space; it also tackles the skewness challenge by favoring the tail labels while fusing sparse and dense retrievers. Finally, xCoRetriev faces the quality challenge by enhancing the label space with Retrieval-Augmented Generated (RAG)-labels. Our experiments with four XMTC benchmarks with hundreds of thousands of text documents and labels against six state-of-the-art XMTC baselines demonstrate xCoRetriev's strengths in terms of: (i) scalability for large label spaces, being among the most efficient methods at training and prediction; (ii) effectiveness in the face of high skewness, with gains of up to 48\% in propensity-scored metrics against the best state-of-the-art baselines; and (iii) capability of handling very noisy datasets by exploiting RAG-labels.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{yang_knighter_2025,
  title = {{KNighter},
  author = {Yang, Chenyuan and Zhao, Zijie and Xie, Zichen and Li, Haoyu and Zhang, Lingming},
  year = {2025},
  doi = {10.1145/3731569.3764827},
  url = {https://doi.org/10.1145/3731569.3764827},
  booktitle = {Proceedings of the {ACM},
  pages = {655--669},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Seoul, Republic of Korea},
  keywords = {large language models, static analysis},
  abstract = {Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large systems remains impractical due to computational constraints and contextual limitations.We present KNighter, the first approach that unlocks scalable LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive systems, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 92 new, critical, longlatent bugs (average 4.3 years) in the Linux kernel; 77 are confirmed, 57 fixed, and 30 have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1870-0},
}

@inproceedings{deva_kya_2025,
  title = {"{Kya},
  author = {Deva, Roshini and Ramani, Dhruv and Divate, Tanvi and Jalota, Suhani and Ismail, Azra},
  year = {2025},
  doi = {10.1145/3706598.3713362},
  url = {https://doi.org/10.1145/3706598.3713362},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {chatbot, HCI4D, LLM, reproductive health},
  abstract = {Access to sexual and reproductive health information remains a challenge in many communities globally, due to cultural taboos and limited availability of healthcare providers. Public health organizations are increasingly turning to Large Language Models (LLMs) to improve access to timely and personalized information. However, recent HCI scholarship indicates that significant challenges remain in incorporating context awareness and mitigating bias in LLMs. In this paper, we study the development of a culturally-appropriate LLM-based chatbot for reproductive health with underserved women in urban India. Through user interactions, focus groups, and interviews with multiple stakeholders, we examine the chatbot’s response to sensitive and highly contextual queries on reproductive health. Our findings reveal strengths and limitations of the system in capturing local context, and complexities around what constitutes “culture”. Finally, we discuss how local context might be better integrated, and present a framework to inform the design of culturally-sensitive chatbots for community health.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@article{xu_automated_2024,
  title = {Automated c/c++ program repair for high-level synthesis via large language models},
  author = {Xu, K. and Zhang, G. L. and Yin, X. and Zhuo, C. and {...},
  year = {2024},
  doi = {10.1145/3670474.3685953},
  url = {https://doi.org/10.1145/3670474.3685953},
  booktitle = {Proceedings of the 2024 {ACM},
  journal = {Proceedings of the 2024 …},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, source: ACM},
  abstract = {… To mitigate the hallucinations in LLMs and enhance the … Employing RAG to the LLM for repairing HLS incompatibility … of using LLM, we compared the cost of the proposed joint LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{MLCAD},
  isbn = {979-8-4007-0699-8},
}

@inproceedings{arabzadeh_benchmarking_2025,
  title = {Benchmarking {LLM},
  author = {Arabzadeh, Negar and Clarke, Charles L. A.},
  year = {2025},
  doi = {10.1145/3726302.3730305},
  url = {https://doi.org/10.1145/3726302.3730305},
  booktitle = {Proceedings of the 48th {International},
  pages = {3194--3204},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {automated evaluation, information retrieval evaluation, large language models, relevance judgment},
  abstract = {Large Language Models (LLMs) are increasingly deployed in both academic and industry settings to automate the evaluation of information seeking systems, particularly by generating graded relevance judgments. Several studies report Kendall τ correlations exceeding 0.85 when comparing system rankings derived from human versus LLM-generated relevance labels. Previous work on LLM-based relevance assessment has primarily focused on replicating graded human relevance judgments through various prompting strategies. However, there has been limited exploration of alternative assessment methods or comprehensive comparative studies. In this paper, we systematically compare multiple LLM-based relevance assessment methods, including binary relevance judgments, graded relevance assessments, pairwise preference-based methods, and two nugget-based evaluation methods - document-agnostic and document-dependent. Wherever possible, we employ state-of-the-art tools and optimized prompts tailored for these methods. In addition to a traditional comparison based on system rankings using Kendall correlations, we also examine how well LLM judgments align with human preferences, as inferred from relevance grades. We conduct extensive experiments on datasets from three TREC Deep Learning tracks 2019, 2020 and 2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain question answering. Beyond dataset-specific results, our work offers a practical methodology for evaluating diverse LLM-based relevance assessment methods. As part of our data release, we include relevance judgments generated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model. Our goal is to reproduce various LLM-based relevance judgment methods to provide a comprehensive comparison. We release all the relevance judgments as a resource that establishes a baseline for future work, ensuring a level playing field for evaluation of LLM-based relevance judgments. All code, data, and resources are publicly available in our GitHub Repository at https://github.com/Narabzad/llm-relevance-judgement-comparison},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{darnell_empirical_2024,
  title = {An {Empirical},
  author = {Darnell, Benjamin and Chopra, Hetarth and Councilman, Aaron and Grove, David and Wang, Yu-Xiong and Adve, Vikram},
  year = {2024},
  doi = {10.1145/3643661.3643951},
  url = {https://doi.org/10.1145/3643661.3643951},
  booktitle = {Proceedings of the {ACM},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {ansible, code generation, domain specific languages, large language models},
  abstract = {The rapid proliferation of LLM-based programming assistants has enabled fast and accurate automatic code generation for general purpose programming languages. Domain-specific languages like Ansible, a DSL for IT Automation, have seen a lack of support despite being critical to many fields, due to limited public-domain code for training models and a lack of interest from tool developers. To address this issue, we collect a novel dataset of permissively licensed Ansible code, and use it to create Warp, an LLM for code fine-tuned to produce Ansible tasks from a natural language prompt. We evaluate state-of-the-art tools for LLM-based code generation models, comparing multiple common strategies, including fine-tuning base models on Ansible code and retrieval-augmented-generation using documentation, in order to understand challenges with existing methodology and identify future research directions to enable better code generation for DSLs.},
  address = {New York, NY, USA},
  series = {{InteNSE},
  isbn = {979-8-4007-0564-9},
}

@inproceedings{vaithilingam_imagining_2024,
  title = {Imagining a {Future},
  author = {Vaithilingam, Priyan and Arawjo, Ian and Glassman, Elena L.},
  year = {2024},
  doi = {10.1145/3643834.3661525},
  url = {https://doi.org/10.1145/3643834.3661525},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {289--300},
  publisher = {Association for Computing Machinery},
  note = {event-place: Copenhagen, Denmark},
  keywords = {AI affordances, Design fiction, Grounding, Human AI collaboration, Language models},
  abstract = {We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-0583-0},
}

@inproceedings{dedov_jointrank_2025,
  title = {{JointRank},
  author = {Dedov, Evgeny},
  year = {2025},
  doi = {10.1145/3731120.3744587},
  url = {https://doi.org/10.1145/3731120.3744587},
  booktitle = {Proceedings of the 2025 {International},
  pages = {208--217},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {block design, large language models for zero-shot ranking},
  abstract = {Efficiently ranking relevant items from large candidate pools is a cornerstone of modern information retrieval systems - such as web search, recommendation, and retrieval-augmented generation. Listwise rerankers, which improve relevance by jointly considering multiple candidates, are often limited in practice: either by model input size constraints, or by degraded quality when processing large sets. We propose a model-agnostic method for fast reranking large sets that exceed a model input limits. The method first partitions candidate items into overlapping blocks, each of which is ranked independently in parallel. Implicit pairwise comparisons are then derived from these local rankings. Finally, these comparisons are aggregated to construct a global ranking using algorithms such as Winrate or PageRank. Experiments on TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini as long-context model, while reducing latency from 21 to 8 seconds. The implementation of the algorithm and the experiments is available in the repository: https://github.com/V3RGANz/jointrank},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{tang_fine-tuning_2024,
  title = {Fine-tuning of {LLMs},
  author = {Tang, Lisirui and Wang, Chengyu and Li, Gangmin and Liu, Peng},
  year = {2024},
  doi = {10.1145/3689218.3689235},
  url = {https://doi.org/10.1145/3689218.3689235},
  booktitle = {Proceedings of the 2024 6th {International},
  pages = {69--73},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, Hong Kong},
  keywords = {Fine-tune, HeXie Management Theory, Large Language Models, RAG},
  abstract = {HeXie Management Theory (HXMT) has been used in many applications. Those applications have demonstrated the effectiveness of the theory in responding to management challenges by integrating oriental and occidental wisdom. With its adoption’s complexity, dynamics, and flexibility, a revolutionary method needs to be developed to simplify it more broadly. Large Language Models (LLMs) have shown their compelling ability to generate human-like content with their chat-based paradigm. Many specifically trained LLMs have demonstrated success in their dedicated application domains. This paper reports the study of fine-tuning LLMs using a specialized dataset derived from the HeXie management theory. Two models were built on Baidu’s Qianfan platform and were adapted for Chinese text. Four criteria were used to evaluate the performances. This study provides an example of fine-tuning LLMs for a Chinese text-based specific theory and building a domain-specific intelligent agent using LLMs or HXMT, which is available at https://alex17swim.com/chat/},
  address = {New York, NY, USA},
  series = {{PRIS},
  isbn = {979-8-4007-1825-0},
}

@inproceedings{chen_cograder_2025,
  title = {{CoGrader},
  author = {Chen, Zixin and Wang, Jiachen and Li, Yumeng and Li, Haobo and Shi, Chuhan and Zhang, Rong and Qu, Huamin},
  year = {2025},
  doi = {10.1145/3746059.3747670},
  url = {https://doi.org/10.1145/3746059.3747670},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {human-AI collaboration, large language models, llm for education, project report grading, source: ACM},
  abstract = {Grading project reports are increasingly significant in today’s educational landscape, where they serve as key assessments of students’ comprehensive problem-solving abilities. However, it remains challenging due to the multifaceted evaluation criteria involved, such as creativity and peer-comparative achievement. Meanwhile, instructors often struggle to maintain fairness throughout the time-consuming grading process. Recent advances in AI, particularly large language models, have demonstrated potential for automating simpler grading tasks, such as assessing quizzes or basic writing quality. However, these tools often fall short when it comes to complex metrics, like design innovation and the practical application of knowledge, that require an instructor’s educational insights into the class situation. To address this challenge, we conducted a formative study with six instructors and developed CoGrader, which introduces a novel grading workflow combining human-LLM collaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader was found effective in improving grading efficiency and consistency while providing reliable peer-comparative feedback to students. We also discuss design insights and ethical considerations for the development of human-AI collaborative grading systems.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-2037-6},
  series = {{UIST},
}

@inproceedings{yang_graphusion_2025,
  title = {Graphusion: {A},
  author = {Yang, Rui and Yang, Boming and Zhao, Xinjie and Gao, Fan and Feng, Aosong and Ouyang, Sixun and Blum, Moritz and She, Tianwei and Jiang, Yuang and Lecue, Freddy and Lu, Jinghui and Li, Irene},
  year = {2025},
  doi = {10.1145/3701716.3717821},
  url = {https://doi.org/10.1145/3701716.3717821},
  booktitle = {Companion {Proceedings},
  pages = {2579--2588},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {information systems database utilities and tools, information systems extraction, information systems language models, transformation and loading},
  abstract = {Knowledge Graphs (KGs) are crucial in the field of artificial intelligence and are widely used in downstream tasks, such as question-answering (QA). The construction of KGs typically requires significant effort from domain experts. Large Language Models (LLMs) have recently been used for Knowledge Graph Construction (KGC). However, most existing approaches focus on a local perspective, extracting knowledge triplets from individual sentences or documents, missing a fusion process to combine the knowledge in a global KG. This work introduces Graphusion, a zero-shot KGC framework from free text. It contains three steps: in Step 1, we extract a list of seed entities using topic modeling to guide the final KG includes the most relevant entities; in Step 2, we conduct candidate triplet extraction using LLMs; in Step 3, we design the novel fusion module that provides a global view of the extracted knowledge, incorporating entity merging, conflict resolution, and novel triplet discovery. Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for entity extraction and relation recognition, respectively. Moreover, we showcase how Graphusion could be applied to the Natural Language Processing (NLP) domain and validate it in an educational scenario. Specifically, we introduce TutorQA, a new expert-verified benchmark for QA, comprising six tasks and a total of 1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant improvement on the benchmark, for example, a 9.2\% accuracy improvement on sub-graph completion.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhang_ask_2025,
  title = {Ask and {Retrieve},
  author = {Zhang, Bolin and Wang, Shengwei and Jiang, Yangqin and Sui, Dianbo and Tu, Zhiying and Chu, Dianhui},
  year = {2025},
  doi = {10.1145/3726302.3729898},
  url = {https://doi.org/10.1145/3726302.3729898},
  booktitle = {Proceedings of the 48th {International},
  pages = {1055--1065},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {active inquiry, conversational ir, medical consultation, rag, source: Scopus},
  abstract = {Large language models (LLMs) cannot effectively collaborate with humans who provide imperfect information at the initial stage of the dialogue, unless they learn to proactively ask questions.Our core idea is to enable LLMs to decide whether to take the action of ”ask” or ”tell” at each turn by self-reasoning, with the belief of the decisions enhanced by retrieving knowledge related to the user input. Thus, we propose the ask and retrieve knowledge framework (Ark), where LLMs think through what to retrieve, when to stop retrieving, and then take actions accordingly. Ark is used to produce the action paths for model training. To mitigate the collapse of models trained on synthetic data, we propose a progressive training strategy: self-reason learning by supervised fine-tuning on produced paths and knowledge alignment through direct preference optimization on doctor response.To evaluate the information gain brought by the ask action, we design a method to calculate the ask utility value (AUV) based on the expected value of perfect information (EVPI) theory. Although MedArk is trained using synthetic data from GPT-4o-mini, it highly outperforms GPT-4o and other medical LLMs in six aspects: helpfulness, hallucination, action selection, BERTScore, AUV, and asking correctness. MedArk also achieves SOTA results in the perfect information scenario, i.e., medical examinations. We release our code, data and models at https://github.com/Bolin97/MedArk.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{alaofi_llms_2024,
  title = {{LLMs},
  author = {Alaofi, Marwah and Thomas, Paul and Scholer, Falk and Sanderson, Mark},
  year = {2024},
  doi = {10.1145/3673791.3698431},
  url = {https://doi.org/10.1145/3673791.3698431},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {32--41},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {information retrieval, llms, relevance labelling, test collections, source: ACM},
  abstract = {Large Language Models (LLMs) are increasingly being used to assess the relevance of information objects. This work reports on experiments to study the labelling of short texts (i.e., passages) for relevance, using multiple open-source and proprietary LLMs. While the overall agreement of some LLMs with human judgements is comparable to human-to-human agreement measured in previous research, LLMs are more likely to label passages as relevant compared to human judges, indicating that LLM labels denoting non-relevance are more reliable than those indicating relevance.This observation prompts us to further examine cases where human judges and LLMs disagree, particularly when the human judge labels the passage as non-relevant and the LLM labels it as relevant. Results show a tendency for many LLMs to label passages that include the original query terms as relevant. We therefore conduct experiments to inject query words into random and irrelevant passages, not unlike the way we inserted the query 'best café near me' into this paper. The results demonstrate that LLMs are highly influenced by the presence of query words in the passages under assessment, even if the wider passage has no relevance to the query. This tendency of LLMs to be fooled by the mere presence of query words demonstrates a weakness in our current measures of LLM labelling: relying on overall agreement misses important patterns of failures. There is a real risk of bias in LLM-generated relevance labels and, therefore, a risk of bias in rankers trained on those labels.Additionally, we investigate the effects of deliberately manipulating LLMs by instructing them to label passages as relevant, similar to the instruction 'this paper is perfectly relevant' inserted above. We find that such manipulation influences the performance of some LLMs, highlighting the critical need to consider potential vulnerabilities when deploying LLMs in real-world applications.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{spillo_gal-kars_2025,
  title = {{GAL},
  author = {Spillo, Giuseppe and Musto, Cataldo and Mannavola, Matteo and de Gemmis, Marco and Lops, Pasquale and Semeraro, Giovanni},
  year = {2025},
  doi = {10.1145/3699682.3728342},
  url = {https://doi.org/10.1145/3699682.3728342},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {73--82},
  publisher = {Association for Computing Machinery},
  keywords = {Knowledge Graph Augmentation, Large Language Models, Recommender Systems},
  abstract = {In this paper, we propose a recommendation model that exploits a graph augmentation technique based on Large Language Models (LLMs) to enrich the information encoded in its underlying Knowledge Graph (KG). Our work relies on the assumption that the triples encoded in a KG can often be noisy or incomplete, and this may lead to sub-optimal modeling of both the characteristics of items and the users’ preferences. In this setting, graph augmentation can be a suitable solution to improve the quality of the data model and provide users with high-quality recommendations.Accordingly, in this work, we align with this research line and propose GAL-KARS (Graph Augmentation with LLMs for Knowledge-Aware Recommender Systems). In our framework, we start from a KG, and we design some prompts for querying an LLM and augmenting the graph by incorporating: (a) further features describing the items; (b) further nodes describing the preferences of the users, obtained by reasoning over the items they like. The resulting KG is then passed through a Knowledge Graph Encoder that learns users’ and items’ embeddings based on the augmented KG. These embeddings are finally used to train a recommendation model and provide users with personalized suggestions. As shown in the experimental session, graph augmentation based on LLMs can significantly improve the predictive accuracy of our recommendation model, thus confirming the effectiveness of the model and the validity of our intuitions.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1313-2},
}

@inproceedings{wang_exploring_2025,
  title = {Exploring {Personalized},
  author = {Wang, Xingbo and Griffith, Janessa and Adler, Daniel A. and Castillo, Joey and Choudhury, Tanzeem and Wang, Fei},
  year = {2025},
  doi = {10.1145/3706598.3713852},
  url = {https://doi.org/10.1145/3706598.3713852},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Activity Recommendations, Behavior Change, Large Language Models, Personalized Health, Sleep Health},
  abstract = {Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru’s multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{tufano_unveiling_2024,
  title = {Unveiling {ChatGPT},
  author = {Tufano, Rosalia and Mastropaolo, Antonio and Pepe, Federica and Dabic, Ozren and Di Penta, Massimiliano and Bavota, Gabriele},
  year = {2024},
  doi = {10.1145/3643991.3644918},
  url = {https://doi.org/10.1145/3643991.3644918},
  booktitle = {Proceedings of the 21st {International},
  pages = {571--583},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {source: ACM},
  abstract = {Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0587-8},
  series = {{MSR},
}

@inproceedings{alghamdi_enhancing_2024,
  title = {Enhancing {Arabic},
  author = {Alghamdi, Muath and Abushawarib, Mohammed and Ellouh, Mahmoud and Ghaleb, Mustafa and Felemban, Muhamad},
  year = {2024},
  doi = {10.1145/3644713.3644763},
  url = {https://doi.org/10.1145/3644713.3644763},
  booktitle = {Proceedings of the 7th {International},
  pages = {366--371},
  publisher = {Association for Computing Machinery},
  note = {event-place: Dubai, United Arab Emirates},
  keywords = {Information Retrieval, Natural Language Processing},
  abstract = {In the modern landscape of Natural Language Processing (NLP), intelligent chatbots like ChatGPT 3.5 and Google’s Bard have shown remarkable competence in generic question-answering (QA) tasks. However, their performance falters when navigating domain-specific QA, particularly in the Arabic language, which is celebrated for its complex morphology and syntax. This paper presents a comprehensive approach to address these issues. The aim of this research is to build a chatbot tailored for a university community. We first create an extensive Arabic Q\&amp;A dataset by extracting data from academic documents, employing state-of-the-art Optical Character Recognition (OCR) tools. Then, we evaluate multiple text similarity measures like Pooled FastText Word embedding, BM25 ranking functions, and various semantic sentence embedding models. A thorough performance assessment reveals that the domain-specific model excels at both sentence-level similarity and context-relevance tasks. The developed web application chatbot, leveraging LangChain library and Retrieval Augmented Generation (RAG) methods, outperforms existing chatbots in domain-specific, Arabic language QA scenarios.},
  address = {New York, NY, USA},
  series = {{ICFNDS},
  isbn = {979-8-4007-0903-6},
}

@inproceedings{polys_prompt_2024,
  title = {Prompt {Engineering},
  author = {Polys, Nicholas and Mohammed, Ayat and Sandbrook, Ben},
  year = {2024},
  doi = {10.1145/3665318.3677159},
  url = {https://doi.org/10.1145/3665318.3677159},
  booktitle = {Proceedings of the 29th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: Guimarães, Portugal},
  keywords = {3D scene creation, Extensible 3D, Large Language Models},
  abstract = {Large Language Models (LLMs) are a new class of knowledge embodied in a computer and trained on massive amounts of human text, image, and video examples. As the result of a user prompt, these LLMs can generate generally coherent responses in several kinds of media and languages. Can LLMs write X3D code? In this paper we explore the ability of several leading LLMs to generate valid and sensible code for interactive X3D scenes. We compare the prompt results from three different LLMs to examine the quality of the generated X3D. We setup an experimental framework that uses a within-subjects repeated-measures design to create X3D from text prompts. We vary our prompt strategies and give the LLMs increasingly challenging and increasingly detailed scene requests. We assess the quality of the resulting X3D scenes including geometry, appearances, animations, and interactions. Our results provide a comparison of different prompt strategies and their outcomes. Such results provide early probes into the limited epistemology and fluency of contemporary LLMs in composing multi-part, animate-able 3D objects.},
  address = {New York, NY, USA},
  series = {{Web3D},
  isbn = {979-8-4007-0689-9},
}

@inproceedings{zhang_chainbuddy_2025,
  title = {{ChainBuddy},
  author = {Zhang, Jingyue and Arawjo, Ian},
  year = {2025},
  doi = {10.1145/3706598.3714085},
  url = {https://doi.org/10.1145/3706598.3714085},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI agents, automation, language models, LLM pipelines, prompt engineering, visual programming environments},
  abstract = {As large language models (LLMs) advance, their potential applications have grown significantly. However, it remains difficult to evaluate LLM behavior on user-defined tasks and craft effective pipelines to do so. Many users struggle with where to start, often referred to as the "blank page problem." ChainBuddy, an AI workflow generation assistant built into the ChainForge platform, aims to tackle this issue. From a single prompt or chat, ChainBuddy generates a starter evaluative LLM pipeline in ChainForge aligned to the user’s requirements. ChainBuddy offers a straightforward and user-friendly way to plan and evaluate LLM behavior and make the process less daunting and more accessible across a wide range of possible tasks and use cases. We report a within-subjects user study comparing ChainBuddy to the baseline interface. We find that when using AI assistance, participants with a variety of technical expertise reported a less demanding workload, felt more confident, and produced higher quality pipelines evaluating LLM behavior. However, we also uncover a mismatch between subjective and objective ratings of performance: participants rated their successfulness similarly across conditions, while independent experts rated participant workflows significantly higher with AI assistance. Drawing connections to the Dunning–Kruger effect, we discuss implications for the future design of workflow generation assistants regarding the risk of over-reliance.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{jatowt_flexidigital_2025,
  title = {{FlexiDigital},
  author = {Jatowt, Adam and Ristov, Sashko and Gritsch, Philipp and Brandacher, Simon and Rosengren, Peter and Valerio, Danilo and Luo, Fengji},
  year = {2025},
  doi = {10.1145/3679240.3734662},
  url = {https://doi.org/10.1145/3679240.3734662},
  booktitle = {Proceedings of the 16th {ACM},
  pages = {638--643},
  publisher = {Association for Computing Machinery},
  keywords = {Digital Twins, Energy bank, Energy Flexibility, Forecasting, LLMs},
  abstract = {The increasing penetration of renewable energy sources presents new challenges for grid stability, demand-response coordination, and energy flexibility. To address them, we introduce in this position paper, FlexiDigital, an interdisciplinary framework designed to integrate citizen-driven energy flexibility services through AI-powered digital assistants and federated digital twins. FlexiDigital enables prosumers to actively manage and trade their flexibility assets, offering a novel Flexibility-as-a-Service business model. Through dynamic incentives, real-time optimization, and system integration, FlexiDigital advances the clean energy transition by enhancing grid reliability, user engagement, and renewable integration. By giving resource owners direct control over their flexibility assets and compensating them fairly through adaptive pricing, the model fosters economic engagement and opens access to previously exclusive energy markets.},
  address = {New York, NY, USA},
  series = {E-{Energy},
  isbn = {979-8-4007-1125-1},
}

@inproceedings{kweon_uncertainty_2025,
  title = {Uncertainty {Quantification},
  author = {Kweon, Wonbin and Jang, Sanghwan and Kang, SeongKu and Yu, Hwanjo},
  year = {2025},
  doi = {10.1145/3696410.3714601},
  url = {https://doi.org/10.1145/3696410.3714601},
  booktitle = {Proceedings of the {ACM},
  pages = {4889--4901},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {large language models, recommendation, uncertainty},
  abstract = {Despite the widespread adoption of large language models (LLMs) for recommendation, we demonstrate that LLMs often exhibit uncertainty in their recommendations. To ensure the trustworthy use of LLMs in generating recommendations, we emphasize the importance of assessing the reliability of recommendations generated by LLMs. We start by introducing a novel framework for estimating the predictive uncertainty to quantitatively measure the reliability of LLM-based recommendations. We further propose to decompose the predictive uncertainty into recommendation uncertainty and prompt uncertainty, enabling in-depth analyses of the primary source of uncertainty. Through extensive experiments, we (1) demonstrate predictive uncertainty effectively indicates the reliability of LLM-based recommendations, (2) investigate the origins of uncertainty with decomposed uncertainty measures, and (3) propose uncertainty-aware prompting for a lower predictive uncertainty and enhanced recommendation. Our source code and model weights are available at https://github.com/WonbinKweon/UNC\_LLM\_REC\_WWW2025},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{vandeputte_foundational_2025,
  title = {Foundational {Design},
  author = {Vandeputte, Frederik},
  year = {2025},
  doi = {10.1145/3759429.3762620},
  url = {https://doi.org/10.1145/3759429.3762620},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {44--62},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {architectural patterns, best practices, design principles, excellence, GenAI, reliability, software systems, source: ACM},
  abstract = {Generative AI (GenAI) has emerged as a transformative technology, demonstrating remarkable capabilities across diverse application domains. However, GenAI faces several major challenges in developing reliable and efficient GenAI-empowered systems due to its unpredictability and inefficiency. This paper advocates for a paradigm shift: future GenAI-native systems should integrate GenAI's cognitive capabilities with traditional software engineering principles to create robust, adaptive, and efficient systems. We introduce foundational GenAI-native design principles centered around five key pillars—reliability, excellence, evolvability, self-reliance, and assurance—and propose architectural patterns such as GenAI-native cells, organic substrates, and programmable routers to guide the creation of resilient and self-evolving systems. Additionally, we outline the key ingredients of a GenAI-native software stack and discuss the impact of these systems from technical, user adoption, economic, and legal perspectives, underscoring the need for further validation and experimentation. Our work aims to inspire future research and encourage relevant communities to implement and refine this conceptual framework.},
  address = {New York, NY, USA},
  series = {Onward! '25},
  isbn = {979-8-4007-2151-9},
}

@inproceedings{liu_personaflow_2025,
  title = {{PersonaFlow},
  author = {Liu, Yiren and Sharma, Pranav and Oswal, Mehul and Xia, Haijun and Huang, Yun},
  year = {2025},
  doi = {10.1145/3715336.3735789},
  url = {https://doi.org/10.1145/3715336.3735789},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {506--534},
  publisher = {Association for Computing Machinery},
  keywords = {Co-Creation Systems, Ideation Support, Large Language Models, Persona Simulation, Scientific Discovery},
  abstract = {Generating interdisciplinary research ideas requires diverse domain expertise, but access to timely feedback is often limited by the availability of experts. In this paper, we introduce PersonaFlow, a novel system designed to provide multiple perspectives by using LLMs to simulate domain-specific experts. Our user studies showed that the new design 1) increased the perceived relevance and creativity of ideated research directions, and 2) promoted users’ critical thinking activities (e.g., interpretation, analysis, evaluation, inference, and self-regulation), without increasing their perceived cognitive load. Moreover, users’ ability to customize expert profiles significantly improved their sense of agency, which can potentially mitigate their over-reliance on AI. This work contributes to the design of intelligent systems that augment creativity and collaboration, and provides design implications of using customizable AI-simulated personas in domains within and beyond research ideation.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-1485-6},
}

@inproceedings{chakrabarti_inside_2025,
  title = {Inside {Out},
  author = {Chakrabarti, Hrishita and Tobia, Diletta Micol and Landoni, Monica and Pera, Maria Soledad},
  year = {2025},
  doi = {10.1145/3726302.3730315},
  url = {https://doi.org/10.1145/3726302.3730315},
  booktitle = {Proceedings of the 48th {International},
  pages = {3244--3254},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {children, emotions, information access systems, llm, search},
  abstract = {In an existing study, the InsideOut Framework is used to produce and explore the emotional profiles of search engines (SE) in response to queries formulated by children aged 9 to 11 in the classroom context, revealing the emotional diversity of SE responses. Since then, there have been significant technological advances in emotion detection and information access. In this work, we conduct a comprehensive reproducibility study where we probe today's emotional profile of SE using both a lexicon-based and a language-model based approach tailored to the Italian language, thus addressing an acknowledged limitation of the original study. Additionally, considering the prevalence of agents based on Large Language Models (LLM) as information access systems among children, we extend the analysis to capture the emotional undertones of LLM responses and juxtapose them to those of SE. Our findings emphasize the importance of leveraging the appropriate emotion detection technique to produce and explore emotional profiles and lead us to reflect on the interplay of emotions on children's search-as-learning experience.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{xu_finbert2_2025,
  title = {{FinBERT2},
  author = {Xu, Xuan and Wen, Fufang and Chu, Beilin and Fu, Zhibing and Lin, Qinhong and Liu, Jiaqi and Fei, Binjie and Li, Yu and Zhou, Linna and Yang, Zhongliang},
  year = {2025},
  doi = {10.1145/3711896.3737219},
  url = {https://doi.org/10.1145/3711896.3737219},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5117--5128},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {dense retriever, domain-specific LM, financial NLP, FinBERT, pretraining, topic modeling},
  abstract = {In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4\%-3.3\% and leading LLMs by 9.7\%-12.3\% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{gim_serve_2025,
  title = {Serve {Programs},
  author = {Gim, In and Zhong, Lin},
  year = {2025},
  doi = {10.1145/3713082.3730398},
  url = {https://doi.org/10.1145/3713082.3730398},
  booktitle = {Proceedings of the 2025 {Workshop},
  pages = {179--186},
  publisher = {Association for Computing Machinery},
  note = {event-place: Banff, AB, Canada},
  keywords = {KV cache, Large language models, LLM serving systems},
  abstract = {Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.},
  address = {New York, NY, USA},
  series = {{HotOS},
  isbn = {979-8-4007-1475-7},
}

@inproceedings{isozaki_towards_2025,
  title = {Towards {Automated},
  author = {Isozaki, Isamu and Shrestha, Manil and Console, Rick and Kim, Edward},
  year = {2025},
  doi = {10.1145/3708319.3733804},
  url = {https://doi.org/10.1145/3708319.3733804},
  booktitle = {Adjunct {Proceedings},
  pages = {404--419},
  publisher = {Association for Computing Machinery},
  keywords = {Benchmarking, Capture the Flag, Large Language Models, Penetration Testing},
  abstract = {Hacking poses a significant threat to cybersecurity, inflicting billions of dollars in damages annually. To mitigate these risks, ethical hacking, or penetration testing, is employed to identify vulnerabilities in systems and networks. Recent advancements in large language models (LLMs) have shown potential across various domains, including cybersecurity. However, there is currently no comprehensive, open, end-to-end penetration testing benchmark to drive progress and evaluate the capabilities of these models in security contexts. This paper introduces a novel open benchmark1 for LLM-based penetration testing, addressing this critical gap. We first evaluate the performance of LLMs, including GPT-4o and LLama 3.1-405B, using the state-of-the-art PentestGPT tool. Our findings reveal that while LLama 3.1 demonstrates an edge over GPT-4o, both models currently fall short of performing end-to-end penetration testing even with some minimal human assistance. Next, we advance the state-of-the-art and present ablation studies that provide insights into improving the PentestGPT tool2. Our research illuminates the challenges LLMs face in each aspect of Pentesting, e.g. enumeration, exploitation, and privilege escalation. This work contributes to the growing body of knowledge on AI-assisted cybersecurity and lays the foundation for future research in automated penetration testing using large language models.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-1399-6},
}

@inproceedings{li_llm-enhanced_2024,
  title = {{LLM},
  author = {Li, Wenhao and Yu, Zhiyuan and She, Qijin and Yu, Zhinan and Lan, Yuqing and Zhu, Chenyang and Hu, Ruizhen and Xu, Kai},
  year = {2024},
  doi = {10.1145/3680528.3687607},
  url = {https://doi.org/10.1145/3680528.3687607},
  booktitle = {{SIGGRAPH},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {source: ACM},
  abstract = {The household rearrangement task involves spotting misplaced objects in a scene and accommodate them with proper places. It depends both on common-sense knowledge on the objective side and human user preference on the subjective side. In achieving such a task, we propose to mine object functionality with user preference alignment directly from the scene itself, without relying on human intervention. To do so, we work with scene graph representation and propose LLM-enhanced scene graph learning which transforms the input scene graph into an affordance-enhanced graph (AEG) with information-enhanced nodes and newly discovered edges (relations). In AEG, the nodes corresponding to the receptacle objects are augmented with context-induced affordance which encodes what kind of carriable objects can be placed on it. New edges are discovered with newly discovered non-local relations. With AEG, we perform task planning for scene rearrangement by detecting misplaced carriables and determining a proper placement for each of them. We test our method by implementing a tiding robot in simulator and perform evaluation on a new benchmark we build. Extensive evaluations demonstrate that our method achieves state-of-the-art performance in misplacement detection and the following rearrangement planning.},
  address = {New York, NY, USA},
  series = {{SA},
  isbn = {979-8-4007-1131-2},
}

@inproceedings{demartini_preaching_2025,
  title = {Preaching to the {ChoIR},
  author = {Demartini, Gianluca and Hauff, Claudia and Lease, Matthew and Mizzaro, Stefano and Roitero, Kevin and Sanderson, Mark and Scholer, Falk and Shah, Chirag and Spina, Damiano and Thomas, Paul and de Vries, Arjen P. and Zuccon, Guido},
  year = {2025},
  doi = {10.1145/3731120.3744612},
  url = {https://doi.org/10.1145/3731120.3744612},
  booktitle = {Proceedings of the 2025 {International},
  pages = {78--91},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {artificial intelligence, lessons learned, research community},
  abstract = {The field of Information Retrieval (IR) changed profoundly at the end of the 1990s with the rise of Web Search, and there are parallels with developments in Artificial Intelligence (AI) happening today with the advent of ChatGPT, Large Language Models, and Generative AI. We acknowledge that there are clear differences between IR and AI. For example, IR is a much smaller field, and new problems arise, like data contamination that may affect benchmark-based evaluation of AI systems. But looking through the lens of an IR researcher, there are many striking similarities between the two fields of IR (25 years ago) and AI (today), and many topics appearing in discussions in AI resemble those of 25 years ago in IR: benchmark reliability and robust evaluation, reproducibility of results for non-public models, privacy and copyright issues, efficiency and scalability, etc. In this paper, we discuss similarities and differences between IR and AI and then derive some lessons learned in the field of IR as a list of recommendations - urging the IR community to reflect on, discuss, and convey these lessons to the AI field. We believe that a joint community effort by all IR researchers is both necessary and dutiful to obtain a fruitful discussion and research advancements with the AI community.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{zine_llm-based_2025,
  title = {{LLM},
  author = {Zine, Nada and Quinton, Clément and Rouvoy, Romain},
  year = {2025},
  doi = {10.1145/3744915.3748460},
  url = {https://doi.org/10.1145/3744915.3748460},
  booktitle = {Proceedings of the 29th {ACM},
  pages = {27--38},
  publisher = {Association for Computing Machinery},
  keywords = {Co-evolution, Feature Models, Large Language Models, Software Product Lines},
  abstract = {Software Product Lines (SPLs) and s are de\&nbsp;facto standards for managing variability in software systems. However, maintaining an up-to-date during software evolution is particularly challenging. Ensuring its consistency with the artifacts of an SPL requires co-evolving them alongside the developed system. When performed manually, this co-evolution process is tedious and error-prone, highlighting the need for automated support. Yet, little attention has been given to the automation of co-evolution between and source code. In this paper, we explore the potential of open-source s to fill this gap. Specifically, we investigate the extent to which s can support bidirectional co-evolution: from to source code—where modifications in the drive changes in the code—and from source code to —where updates in the code are reflected back into the. We evaluate our -based approach on a real-world configurable system. Our results demonstrate that co-evolution from source code to achieves F1 scores ranging from 0.93 to 1.0, while co-evolution from to source code achieves F1 scores between 0.41 and 0.99. These findings highlight the potential of s to support this co-evolution process, while also showing limitations and suggesting areas for improvement, particularly for the co-evolution from to code. Additionally, we conduct a comparative study across various s, revealing how choice affects co-evolution and, incidentally, how it affects model and code generation. Up to a certain size limit, larger s tend to produce more accurate and stable outputs than smaller ones, however, this influence is less pronounced in the code generation task. Overall, our work opens a new research avenue where s are leveraged for automating the co-evolution between configurable software systems and variability models.},
  address = {New York, NY, USA},
  series = {{SPLC},
  isbn = {979-8-4007-2024-6},
}

@inproceedings{pinto_lessons_2024,
  title = {Lessons from {Building},
  author = {Pinto, Gustavo and De Souza, Cleidson and Neto, Joao Batista and Souza, Alberto and Gotto, Tarci­sio and Monteiro, Edward},
  year = {2024},
  doi = {10.1145/3639477.3639751},
  url = {https://doi.org/10.1145/3639477.3639751},
  booktitle = {Proceedings of the 46th {International},
  pages = {408--417},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {challenges, code LLMs, LLM, LLM for code, LLM-based applications, LLM4code},
  abstract = {With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and CoPilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information.In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot AI. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications.},
  address = {New York, NY, USA},
  series = {{ICSE},
  isbn = {979-8-4007-0501-4},
}

@inproceedings{sun_creative_2025,
  title = {Creative {Blends},
  author = {Sun, Zhida and Zhang, Zhenyao and Zhang, Yue and Lu, Min and Lischinski, Dani and Cohen-Or, Daniel and Huang, Hui},
  year = {2025},
  doi = {10.1145/3706598.3713683},
  url = {https://doi.org/10.1145/3706598.3713683},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Creativity, Metaphor, Text-to-Image Generation, Visual Blends},
  abstract = {Visual blends combine elements from two distinct visual concepts into a single, integrated image, with the goal of conveying ideas through imaginative and often thought-provoking visuals. Communicating abstract concepts through visual blends poses a series of conceptual and technical challenges. To address these challenges, we introduce Creative Blends, an AI-assisted design system that leverages metaphors to visually symbolize abstract concepts by blending disparate objects. Our method harnesses commonsense knowledge bases and large language models to align designers’ conceptual intent with expressive concrete objects. Additionally, we employ generative text-to-image techniques to blend visual elements through their overlapping attributes. A user study (N=24) demonstrated that our approach reduces participants’ cognitive load, fosters creativity, and enhances the metaphorical richness of visual blend ideation. We explore the potential of our method to expand visual blends to include multiple object blending and discuss the insights gained from designing with generative AI.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{zhang_imperceptible_2024,
  title = {Imperceptible {Content},
  author = {Zhang, Quan and Zhou, Chijin and Go, Gwihwan and Zeng, Binqi and Shi, Heyuan and Xu, Zichen and Jiang, Yu},
  year = {2024},
  doi = {10.1145/3691620.3695001},
  url = {https://doi.org/10.1145/3691620.3695001},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {242--254},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {content poisoning, LLM applications},
  abstract = {Large Language Models (LLMs) have shown their superior capability in natural language processing, promoting extensive LLM-powered applications to be the new portals for people to access various content on the Internet. However, LLM-powered applications do not have sufficient security considerations on untrusted content, leading to potential threats. In this paper, we reveal content poisoning, where attackers can tailor attack content that appears benign to humans but causes LLM-powered applications to generate malicious responses. To highlight the impact of content poisoning and inspire the development of effective defenses, we systematically analyze the attack, focusing on the attack modes in various content, exploitable design features of LLM application frameworks, and the generation of attack content. We carry out a comprehensive evaluation on five LLMs, where content poisoning achieves an average attack success rate of 89.60\%. Additionally, we assess content poisoning on four popular LLM-powered applications, achieving the attack on 72.00\% of the content. Our experimental results also show that existing defenses are ineffective against content poisoning. Finally, we discuss potential mitigations for LLM application frameworks to counter content poisoning.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{yuan_kg-uq_2025,
  title = {{KG},
  author = {Yuan, Yingqing and Tao, Linwei and Lu, Haohui and Khushi, Matloob and Razzak, Imran and Dras, Mark and Yang, Jian and Naseem, Usman},
  year = {2025},
  doi = {10.1145/3701716.3717660},
  url = {https://doi.org/10.1145/3701716.3717660},
  booktitle = {Companion {Proceedings},
  pages = {2071--2077},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {llm, uncertainty estimation},
  abstract = {With the commercialization of large language models (LLMs) and their integration into daily life, addressing their susceptibility to hallucinations-unfactual information in generated outputs-has become an urgent priority. Existing uncertainty quantification (UQ) methods often rely on access to LLMs' internal states, which is unavailable for closed-source models like GPTs, or are primarily designed for short text. Current research on long text typically evaluates sentences individually, overlooking smaller semantic units that better capture the text's complexity. Recognizing the potential of knowledge graphs (KGs) to extract structured relationships from unstructured text, we propose KG-UQ, a UQ method leveraging KGs to address the semantic intricacies of long text. Our approach involves constructing KGs from long-text outputs and utilizing their embeddings to estimate uncertainties. Through our analysis, we demonstrate that knowledge graphs are an effective tool for decomposing long text into fundamental statements. However, we also highlight the increased uncertainty introduced during KG construction, stemming from inherent challenges in accurately capturing all semantic information.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhang_are_2024,
  title = {Are {Large},
  author = {Zhang, Hengran and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi},
  year = {2024},
  doi = {10.1145/3626772.3657784},
  url = {https://doi.org/10.1145/3626772.3657784},
  booktitle = {Proceedings of the 47th {International},
  pages = {1941--1951},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {large language models, open-domain qa, utility judgments, source: ACM, source: Scopus},
  abstract = {Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering.In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility\_judgments.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
  annote = {Cited by: 10; All Open Access; Gold Open Access},
}

@inproceedings{harvey_dont_2025,
  title = {"{Don},
  author = {Harvey, Emma and Koenecke, Allison and Kizilcec, Rene F.},
  year = {2025},
  doi = {10.1145/3706598.3713210},
  url = {https://doi.org/10.1145/3706598.3713210},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {edtech, education, harms, interviews, large language models, LLMs},
  abstract = {Education technologies (edtech) are increasingly incorporating new features built on large language models (LLMs), with the goals of enriching the processes of teaching and learning and ultimately improving learning outcomes. However, the potential downstream impacts of LLM-based edtech remain understudied. Prior attempts to map the risks of LLMs have not been tailored to education specifically, even though it is a unique domain in many respects: from its population (students are often children, who can be especially impacted by technology) to its goals (providing the correct answer may be less important for learners than understanding how to arrive at an answer) to its implications for higher-order skills that generalize across contexts (e.g., critical thinking and collaboration). We conducted semi-structured interviews with six edtech providers representing leaders in the K-12 space, as well as a diverse group of 23 educators with varying levels of experience with LLM-based edtech. Through a thematic analysis, we explored how each group is anticipating, observing, and accounting for potential harms from LLMs in education. We find that, while edtech providers focus primarily on mitigating technical harms, i.e., those that can be measured based solely on LLM outputs themselves, educators are more concerned about harms that result from the broader impacts of LLMs, i.e., those that require observation of interactions between students, educators, school systems, and edtech to measure. Overall, we (1) develop an education-specific overview of potential harms from LLMs, (2) highlight gaps between conceptions of harm by edtech providers and those by educators, and (3) make recommendations to facilitate the centering of educators in the design and development of edtech tools.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{guan_mfort-qa_2024,
  title = {{MFORT},
  author = {Guan, Che and Huang, Mengyu and Zhang, Peng},
  year = {2024},
  doi = {10.1145/3669754.3669822},
  url = {https://doi.org/10.1145/3669754.3669822},
  booktitle = {Proceedings of the 2024 10th {International},
  pages = {434--442},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bali Island, Indonesia},
  keywords = {Chain-of-Thought Prompting, Few-Shot Learning, Information Retrieval, Large Language Models, Open Table Question-Answering},
  abstract = {In today’s fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.},
  address = {New York, NY, USA},
  series = {{ICCAI},
  isbn = {979-8-4007-1705-5},
}

@inproceedings{joshi_coprompter_2025,
  title = {{CoPrompter},
  author = {Joshi, Ishika and Shahid, Simra and Venneti, Shreeya Manasvi and Vasu, Manushree and Zheng, Yantao and Li, Yunyao and Krishnamurthy, Balaji and Chan, Gromit Yeuk-Yin},
  year = {2025},
  doi = {10.1145/3708359.3712102},
  url = {https://doi.org/10.1145/3708359.3712102},
  booktitle = {Proceedings of the 30th {International},
  pages = {341--365},
  publisher = {Association for Computing Machinery},
  keywords = {HCI, LLM Evaluation, Prompt Optimization},
  abstract = {Ensuring large language models’ (LLMs) responses align with prompt instructions is crucial for application development. Based on our formative study with industry professionals, the alignment requires heavy human involvement and tedious trial-and-error especially when there are many instructions in the prompt. To address these challenges, we introduce CoPrompter, a framework that identifies misalignment based on assessing multiple LLM responses with criteria. It proposes a method to generate evaluation criteria questions derived directly from prompt requirements and an interface to turn these questions into a user-editable checklist. Our user study with industry prompt engineers shows that CoPrompter improves the ability to identify and refine instruction alignment with prompt requirements over traditional methods, helps them understand where and how frequently models fail to follow user’s prompt requirements, and helps in clarifying their own requirements, giving them greater control over the response evaluation process. We also present the design lessons to underscore our system’s potential to streamline the prompt engineering process.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{cai_hi_2025,
  title = {"{Hi},
  author = {Cai, Zhenyao and Wei, Shiyao and Han, Ariel and Peppler, Kylie A},
  year = {2025},
  doi = {10.1145/3713043.3728859},
  url = {https://doi.org/10.1145/3713043.3728859},
  booktitle = {Proceedings of the 24th {Interaction},
  pages = {684--698},
  publisher = {Association for Computing Machinery},
  keywords = {Child-AI Co-Creation, Content Creation, Creativity Support Tools, Media Literacy},
  abstract = {As children increasingly transition from media consumers to "prosumers" on user-generated content platforms, developing their media and content creation literacies becomes critical. Yet little research examines how children engage with video creation or how emerging technologies, such as generative AI, can support this creative process. This study explores children’s engagement and challenges during a two-week, project-based learning workshop where they learned to create educational videos teaching school-aligned science topics. Our exploratory findings suggest that educational video creation activity, when designed properly, can serve as an intervention for the dual learning of science literacy and media literacy. In the workshop, children showed different engagement profiles: some were self-directed and motivated by both science learning and video creation, some focused primarily on learning the science content, some treated the activity as a classroom assignment, and others disengaged from the process. Building on insights from our findings, We also extend Shneiderman’s Collect-Relate-Create-Donate (CRCD) model to better support children’s creativity. We propose design directions including conversational, filtered, and multimodal information gathering for the Collect phase; role-based peer and family interaction for the Relate phase; low-floor, high-ceiling creation tools for the Create phase; and positioning children as meaningful sharers of content while supporting education around content sharing, privacy, and audience awareness for the Donate phase.},
  address = {New York, NY, USA},
  series = {{IDC},
  isbn = {979-8-4007-1473-3},
}

@inproceedings{farzi_pencils_2024,
  title = {Pencils {Down},
  author = {Farzi, Naghmeh and Dietz, Laura},
  year = {2024},
  doi = {10.1145/3664190.3672511},
  url = {https://doi.org/10.1145/3664190.3672511},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {175--184},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {information retrieval evaluation, large language models, source: ACM},
  abstract = {Current IR evaluation paradigms are challenged by large language models (LLMs) and retrieval-augmented generation (RAG) methods. Furthermore, evaluation either resorts to expensive human judgments or lead to an over-reliance on LLMs.To remedy this situation, we introduce the RUBRIC metric, which puts information retrieval systems to the proverbial test. This metric leverages a bank of query-related test questions to quantify relevant information content that is contained in the systems' responses. The process involves (1) decomposing the query into detailed questions, and (2) checking each for answerability using passages in the system response. Using three TREC benchmarks, we demonstrate that our LLM-based RUBRIC approach works successfully. Unlike previous LLM-based evaluation measures, our paradigm lends itself for incorporating a human-in-the-loop while avoiding some pitfalls of over-reliance on AI or resorting to expensive manual passage-level judgments. Moreover, our evaluation is repeatable and extensible and can be scored with existing evaluation tools. Data and code at https://github.com/TREMA-UNH/rubric-evaluation/},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-0681-3},
}

@inproceedings{farinetti_chatbot_2024,
  title = {Chatbot {Development},
  author = {Farinetti, Laura and Canale, Lorenzo},
  year = {2024},
  doi = {10.1145/3649217.3653557},
  url = {https://doi.org/10.1145/3649217.3653557},
  booktitle = {Proceedings of the 2024 on {Innovation},
  pages = {401--407},
  publisher = {Association for Computing Machinery},
  note = {event-place: Milan, Italy},
  keywords = {source: ACM},
  abstract = {Critical thinking and creativity are fundamental skills for engineers and computer scientists. The emergence of Large Language Models (LLMs) able to create chatbots that use natural language is an opportunity for educators to foster these skills. The well-known risk of generative AI for potential misinformation offers fertile ground to practice critical thinking.This paper describes a hands-on experience within a database course, where students had to develop a chatbot using the LangChain framework, and to evaluate it from different points of view. The students were free to choose the domain of their chatbot. The learning goal was twofold: on the one hand, to make them practice with state-of-the-art technologies, and on the other hand to stimulate critical analysis on their output. The paper discusses the students' evaluation of the chatbots under several metrics, including document retrieval, syntax and grammar accuracy, semantic relevance and information reliability. Students' assessments were also compared to the teachers' ones, to gain an insight on the critical attitude of the students and to offer a ground for discussion.The experience was stimulating and appreciated by the students. The final results highlight that the majority of students successfully produced chatbot responses that were grammatically and syntactically correct, and that consistently extracted pertinent sections from documents, yielding semantically relevant outputs. Despite these achievements, a significant portion of students expressed reservations about the reliability of the chatbot's responses to prompts, gaining awareness of LLMs' capability to generate responses that make sense to humans but may be potentially misleading.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0600-4},
  series = {{ITiCSE},
}

@inproceedings{sun_persona-l_2025,
  title = {Persona-{L},
  author = {Sun, Lipeipei and Qin, Tianzi and Hu, Anran and Zhang, Jiale and Lin, Shuojia and Chen, Jianyan and Ali, Mona and Prpa, Mirjana},
  year = {2025},
  doi = {10.1145/3706598.3713445},
  url = {https://doi.org/10.1145/3706598.3713445},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Ability-based Framework, Context, Persona, UX Design},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{roy_choudhury_genai_2025,
  title = {{GenAI},
  author = {Roy Choudhury, Sharod and Chahal, Dheeraj and Phalak, Chetan and Ramesh, Manju and Singhal, Rekha},
  year = {2025},
  doi = {10.1145/3680256.3721333},
  url = {https://doi.org/10.1145/3680256.3721333},
  booktitle = {Companion of the 16th {ACM},
  pages = {154--161},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {cloud computing, cloud service optimization, editable cloud architecture, optimization},
  abstract = {The rapid adoption of cloud computing, accelerated by the global pandemic, has increased the need for efficient cloud architecture that balances cost and performance. As organizations migrate applications to the cloud, cloud architects face challenges in managing an overwhelming number of services—often exceeding a thousand. This paper presents a novel tool designed for editable cloud architecture management that automates the optimization process.Our solution enables cloud architects to visually design and edit cloud architectures while utilizing a backend represented as a directed acyclic graph in an adjacency matrix. This structure allows for dynamic adjustments based on real-time workload predictions, moving from reactive to proactive resource management. Leveraging advanced Generative AI models, specifically Azure's GPT-4o [11], our tool identifies alternative services that can effectively replace or supplement existing ones based on functionality. By extracting relevant data from AWS documentation, we provide actionable insights on service performance and cost.We validate our approach through use cases, demonstrating the tool's effectiveness in detecting potential bottlenecks and recommending service adjustments to eliminate Service Level Agreement (SLA) violations. Our findings indicate that the tool enhances performance and reduces operational costs, empowering cloud architects to make informed, data-driven decisions. This innovative approach significantly streamlines cloud resource management, ensuring organizations can effectively navigate the complexities of their cloud environments and achieve sustained operational excellence.},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-1130-5},
}

@inproceedings{zhu_learn_2025,
  title = {Learn, {Explore},
  author = {Zhu, Jianlong and Kempermann, Manon and Cannanure, Vikram Kamath and Hartland, Alexander and Navarrete, Rosa M. and Carteny, Giuseppe and Braun, Daniela and Weber, Ingmar},
  year = {2025},
  doi = {10.1145/3719160.3736611},
  url = {https://doi.org/10.1145/3719160.3736611},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Chatbot, Civic Education, Deliberation, Trustworthiness, Voting Advice Applications},
  abstract = {Voting advice applications (VAAs), which have become increasingly prominent in European elections, are seen as a successful tool for boosting electorates’ political knowledge and engagement. However, VAAs’ complex language and rigid presentation constrain their utility to less-sophisticated voters. While previous work enhanced VAAs’ click-based interaction with scripted explanations, a conversational chatbot’s potential for tailored discussion and deliberate political decision-making remains untapped. Our exploratory mixed-method study investigates how LLM-based chatbots can support voting preparation. We deployed a VAA chatbot to 331 users before Germany’s 2024 European Parliament election, gathering insights from surveys, conversation logs, and 10 follow-up interviews. Participants found the VAA chatbot intuitive and informative, citing its simple language and flexible interaction. We further uncovered VAA chatbots’ role as a catalyst for reflection and rationalization. Expanding on participants’ desire for transparency, we provide design recommendations for building interactive and trustworthy VAA chatbots.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{menshawy_navigating_2024,
  title = {Navigating {Challenges},
  author = {Menshawy, Ahmed and Nawaz, Zeeshan and Fahmy, Mahmoud},
  year = {2024},
  doi = {10.1145/3642970.3655840},
  url = {https://doi.org/10.1145/3642970.3655840},
  booktitle = {Proceedings of the 4th {Workshop},
  pages = {192--199},
  publisher = {Association for Computing Machinery},
  note = {event-place: Athens, Greece},
  keywords = {High-Throughput LLM Processing, Large Language Models (LLMs), LLM Deployment Challenges, LLM Model Compression and Pruning, LLMs Deployment, Scalability Challenges in LLMs Deployment, Technical Debt in AI},
  abstract = {Large Language Models (LLMs) have become an essential tool in advancing artificial intelligence and machine learning, enabling outstanding capabilities in natural language processing, and understanding. However, the efficient deployment of LLMs in production environments reveals a complex landscape of challenges and technical debt.In this paper, we aim to highlight unique forms of challenges and technical debt associated with the deployment of LLMs, including those related to memory management, parallelism strategies, model compression, and attention optimization. These challenges emphasize the necessity of custom approaches to deploying LLMs, demanding customization and sophisticated engineering solutions not readily available in broad-use machine learning libraries or inference engines.},
  address = {New York, NY, USA},
  series = {{EuroMLSys},
  isbn = {979-8-4007-0541-0},
}

@inproceedings{chen_online_2024,
  title = {Online {Personalizing},
  author = {Chen, Zekai and Chen, Po-Yu and Buet-Golfouse, Francois},
  year = {2024},
  doi = {10.1145/3677052.3698651},
  url = {https://doi.org/10.1145/3677052.3698651},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {711--718},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Large language models, multi-armed bandits, personalization},
  abstract = {Personalized content generation by Large Language Models (LLMs) in finance presents a challenge: efficiently adapting text to individual preferences without creating unique models for each user. This study introduces an innovative online method for financial applications, employing neural bandit algorithms to dynamically optimize soft instruction embeddings based on user feedback, enhancing personalization in white-box LLMs. Through experiments on public generation tasks, we demonstrate significant performance improvements. Notably, our NeuralTS implementation achieves up to a 62.9\% improvement in ROUGE scores and a 2.76\% increase in LLM-agent evaluation for personalized content generation. This research showcases the efficacy of neural bandits in refining LLM outputs to align with client-specific needs and regulatory requirements, marking a pivotal step towards feasible and effective adaptive text generation in finance. Our method offers a promising and scalable solution for financial institutions to enhance client engagement, improve risk assessment, and streamline regulatory reporting.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{yuan_designrepair_2025,
  title = {{DesignRepair},
  author = {Yuan, Mingyue and Chen, Jieshan and Xing, Zhenchang and Quigley, Aaron and Luo, Yuyu and Luo, Tianqi and Mohammadi, Gelareh and Lu, Qinghua and Zhu, Liming},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00109},
  url = {https://doi.org/10.1109/icse55347.2025.00109},
  booktitle = {Proceedings of the {IEEE},
  pages = {2483--2494},
  publisher = {IEEE Press},
  keywords = {design guideline, frontend code repair, large language models, UI design},
  abstract = {The rise of Large Language Models (LLMs) has streamlined frontend interface creation through tools like Vercel's V0, yet surfaced challenges in design quality (e.g., accessibility, and usability). Current solutions, often limited by their focus, generalisability, or data dependency, fall short in addressing these complexities. Moreover, none of them examine the quality of LLM-generated UI design. In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect. We utilised the mature and popular Material Design as our knowledge base to guide this process. Specifically, we first constructed a comprehensive knowledge base encoding Google's Material Design principles into low-level component knowledge base and high-level system design knowledge base. After that, DesignRepair employs a LLM for the extraction of key components and utilizes the Playwright tool for precise page analysis, aligning these with the established knowledge bases. Finally, we integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4 to holistically refine and repair frontend code through a strategic divide and conquer approach. Our extensive evaluations validated the efficacy and utility of our approach, demonstrating significant enhancements in adherence to design guidelines, accessibility, and user experience metrics.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{anandayuvaraj_fail_2024,
  title = {{FAIL},
  author = {Anandayuvaraj, Dharun and Campbell, Matthew and Tewari, Arav and Davis, James C},
  year = {2024},
  doi = {10.1145/3691620.3695022},
  url = {https://doi.org/10.1145/3691620.3695022},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {506--518},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {empirical software engineering, large language models, news analysis, software failure analysis},
  abstract = {Software failures inform engineering work, standards, regulations. For example, the Log4J vulnerability brought government and industry attention to evaluating and securing software supply chains. Retrospective failure analysis is thus a valuable line of software engineering research. Accessing private engineering records is difficult, so such analyses tend to use information reported by the news media. However, prior works in this direction have relied on manual analysis. That has limited the scale of their analyses. The community lacks automated support to enable such analyses to consider a wide range of news sources and incidents.To fill this gap, we propose the Failure Analysis Investigation with LLMs (FAIL) system. FAIL is a novel LLM-based pipeline that collects, analyzes, and summarizes software failures as reported in the news. FAIL groups articles that describe the same incidents. It then analyzes incidents using existing taxonomies for postmortems, faults, and system characteristics. To tune and evaluate FAIL, we followed the methods of prior works by manually analyzing 31 software failures. FAIL achieved an F1 score of 90\% for collecting news about software failures, a V-measure of 0.98 for merging articles reporting on the same incident, and extracted 90\% of the facts about failures. We then applied FAIL to a total of 137,427 news articles from 11 providers published between 2010 and 2022. FAIL identified and analyzed 2,457 distinct failures reported across 4,184 articles. Our findings include: (1) current generation of large language models are capable of identifying news articles that describe failures, and analyzing them according to structured taxonomies; (2) high recurrences of similar failures within organizations and across organizations; and (3) severity of the consequences of software failures have increased over the past decade. The full FAIL database is available so that researchers, engineers, and policymakers can learn from a diversity of software failures.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{korelic_sellma_2025,
  title = {{SELLMA},
  author = {Korelič, Martin and Machidon, Octavian and Pejović, Veljko},
  year = {2025},
  doi = {10.1145/3721888.3722091},
  url = {https://doi.org/10.1145/3721888.3722091},
  booktitle = {Proceedings of the 8th {International},
  pages = {7--12},
  publisher = {Association for Computing Machinery},
  note = {event-place: World Trade Center, Rotterdam, Netherlands},
  keywords = {large language models, LLM fine-tuning, location sensing, semantic location, ubiquitous computing, WiFi sensing, source: ACM},
  abstract = {Understanding a user's semantic location is of critical importance in numerous areas of mobile computing, such as mobile healthcare, mobile advertising, and mobile personal assistance. Nevertheless, inferring semantic location remains challenging and often relies on translating raw geographical coordinates via third-party online services. In this paper we introduce SELLMA, an approach for semantic location inference that harnesses Wi-Fi SSID sensing and on-device querying of a specially crafted LLM. We implement SELLMA in Android and show that it can uncover a number of environmental and geographical descriptors of a users location in a privacy-preserving manner, without the need for GPS querying, and without reliance on Web-based services.},
  address = {New York, NY, USA},
  series = {{EdgeSys},
  isbn = {979-8-4007-1559-4},
}

@inproceedings{al_owayyed_controlled_2025,
  title = {Controlled {Yet},
  author = {Al Owayyed, Mohammed and Denga, Adarsh and Brinkman, Willem-Paul},
  year = {2025},
  doi = {10.1145/3717511.3747075},
  url = {https://doi.org/10.1145/3717511.3747075},
  booktitle = {Proceedings of the 25th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Belief-Desire-Intention (BDI), Child Helpline Training, Conversational Agents, Counsellor Training, Large Language Models (LLMs), Social Skills Training, Virtual Training Simulations},
  abstract = {Child helpline training often relies on human-led roleplay, which is both time- and resource-consuming. To address this, rule-based interactive agent simulations have been proposed to provide a structured training experience for new counsellors. However, these agents might suffer from limited language understanding and response variety. To overcome these limitations, we present a hybrid interactive agent that integrates Large Language Models (LLMs) into a rule-based Belief-Desire-Intention (BDI) framework, simulating more realistic virtual child chat conversations. This hybrid solution incorporates LLMs into three components: intent recognition, response generation, and a bypass mechanism. We evaluated the system through two studies: a script-based assessment comparing LLM-generated responses to human-crafted responses, and a within-subject experiment (N = 37) comparing the LLM-integrated agent with a rule-based version. The first study provided evidence that the three LLM components were non-inferior to human-crafted responses. In the second study, we found credible support for two hypotheses: participants perceived the LLM-integrated agent as more believable and reported more positive attitudes toward it than the rule-based agent. Additionally, although weaker, there was some support for increased engagement (posterior probability = 0.845, 95\% HDI [–0.149, 0.465]). Our findings demonstrate the potential of integrating LLMs into rule-based systems, offering a promising direction for more flexible but controlled training systems.},
  address = {New York, NY, USA},
  series = {{IVA},
  isbn = {979-8-4007-1508-2},
}

@inproceedings{singh_leftovers_2024,
  title = {Leftovers for {LLaMA},
  author = {Singh, Ravi Kumar and Bandamudi, Likhith and Kunde, Shruti and Mishra, Mayank and Singhal, Rekha},
  year = {2024},
  doi = {10.1145/3629526.3645045},
  url = {https://doi.org/10.1145/3629526.3645045},
  booktitle = {Proceedings of the 15th {ACM},
  pages = {201--210},
  publisher = {Association for Computing Machinery},
  note = {event-place: London, United Kingdom},
  keywords = {distributed inference, leftover capacity, llms, optimal block placement},
  abstract = {n recent years, large language models (LLMs) have become pervasive in our day-to-day lives, with enterprises utilizing their services for a wide range of NLP-based applications. The exponential growth in the size of LLMs poses a significant challenge for efficiently utilizing these models for inference tasks, which require a substantial amount of memory and compute. Enterprises often possess multiple resources (workers, nodes, servers) with unused (leftover) capacity, providing an opportunity to address this challenge by distributing large models across these resources. Recent work such as Petals, provides a platform for distributing LLM models in a cluster of resources. Petals require that users use their discretion to distribute blocks on a given cluster, consequently leading to a non-optimal placement of blocks. In this paper, we propose LLaMPS - a large language model placement system that aims to optimize the placement of transformer blocks on the available enterprise resources, by utilizing the leftover capacity of the worker nodes. Our approach considers leftover memory capacity along with available CPU cores, when distributing transformer blocks optimally across worker nodes. Furthermore, we enhance the scalability of the system by maximizing the number of clients that can be served concurrently. We validate the efficacy of our approach by conducting extensive experiments using open-source large language models - BLOOM (1b, 3b, and 7b parameters), Falcon, and LLaMA. Our experiments demonstrate that LLaMPS facilitates optimal placement of transformer blocks by utilizing leftover resources, thus enabling enterprise-level deployment of large language models},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-0444-4},
}

@inproceedings{lamas_dsl-xpert_2024,
  title = {{DSL},
  author = {Lamas, Victor and R. Luaces, Miguel and Garcia-Gonzalez, Daniel},
  year = {2024},
  doi = {10.1145/3652620.3687782},
  url = {https://doi.org/10.1145/3652620.3687782},
  booktitle = {Proceedings of the {ACM},
  pages = {16--20},
  publisher = {Association for Computing Machinery},
  note = {event-place: Linz, Austria},
  keywords = {domain-specific languages (DSLS), few-shot learning, grammar prompting, large language models (LLMS), semantic parsing},
  abstract = {Nowadays, large language models (LLMs) are an extremely useful and fast tool to complement and help in many jobs and current problems. However, there are cases where a pretty specific vocabulary is used in which these models were not previously trained, leading to less satisfactory results. More specifically, these models are less effective when dealing with less-known or unpublished domain-specific languages (DSLs). Within this field, the automatic generation of code based on such languages, starting from natural language, would speed up the development times of any related project, as well as the understanding of such DSLs. Therefore, this paper presents a tool in which developers can perform what is known as semantic parsing. In other words, the developer can ask a pre-trained LLM to translate a natural language instruction into the vocabulary of the established DSL. Thus, by setting the DSL grammar as context (grammar prompting) and providing usage examples (few-shot learning), the LLM can quickly generate reliable domain-specific code, significantly improving the quality of life of the developers. A video demonstration of the tool is shown in the following link: https://zenodo.org/records/12610506.},
  address = {New York, NY, USA},
  series = {{MODELS},
  isbn = {979-8-4007-0622-6},
}

@inproceedings{prasongpongchai_talk_2025,
  title = {Talk to the {Hand},
  author = {Prasongpongchai, Thanawit and Pataranutaporn, Pat and Lertsutthiwong, Monchai and Maes, Pattie},
  year = {2025},
  doi = {10.1145/3706598.3715579},
  url = {https://doi.org/10.1145/3706598.3715579},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Human-AI Collaboration, Human-AI Interaction Technique, Large Language Models, Pointing Devices, Real-Time Feedback},
  abstract = {This paper presents Pointer Assistant, a novel human-AI interaction technique for on-screen tasks. The design features a chatbot displayed as an extra mouse pointer, alongside the user’s, which proactively gives feedback on user actions while directing them to relevant areas on the screen and responding to the user’s direct chat messages. The effectiveness of the design’s key characteristics, pointer form and proactivity, was investigated in a study involving 220 participants in a financial budget planning task. Results demonstrated that the pointer design and interaction reduced task load while improving satisfaction with the experience, and increased the number of budget categories ideated during the task compared to the traditional passive chat log design. Participants viewed Pointer Assistant as a fun, innovative, and helpful visual guide while noting that its assertiveness can be improved. Future developments could offer even further enhancements to the user experience of human-AI collaboration and task outcomes.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{chen_rarebench_2024,
  title = {{RareBench},
  author = {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
  year = {2024},
  doi = {10.1145/3637528.3671576},
  url = {https://doi.org/10.1145/3637528.3671576},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {4850--4861},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {benchmark for llms, evaluation, rare disease diagnosis},
  abstract = {Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{seo_prompt_2025,
  title = {A {Prompt},
  author = {Seo, Seongbum and Yoo, Sangbong and Jang, Yun},
  year = {2025},
  doi = {10.1145/3708359.3712117},
  url = {https://doi.org/10.1145/3708359.3712117},
  booktitle = {Proceedings of the 30th {International},
  pages = {89--105},
  publisher = {Association for Computing Machinery},
  keywords = {Intelligent Assistants, Large Language Models, Prompt Chaining},
  abstract = {Intelligent Assistants (IAs) often struggle with maintaining context in extended conversations, limiting their long-term recall capabilities and personalization. This study introduces a prompt chaining framework that integrates Large Language Models (LLMs) into IAs to address these issues. Our approach encompasses: (1) a formative study (N=30) analyzing IA-user interactions and identifying key challenges in long-term memory and personalization, (2) development of an LLM-based system with a novel prompt chaining mechanism for improved context retention, adaptable to various LLM architectures, and (3) implementation of a multi-step reasoning process to enhance context-aware and personalized responses. We evaluated the framework through quantitative analysis on multiple datasets, tested with different LLM backends, and conducted ablation studies to assess component contributions. The results show improvements in long-term recall, context awareness, and personalization across all tested models and datasets. A human evaluation study (N=47) indicated better Sensibleness, Consistency, and Personalization in IA response. This approach enhances IA memory capabilities and can adapt to different LLM backends.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{balog_rankers_2025,
  title = {Rankers, {Judges},
  author = {Balog, Krisztian and Metzler, Don and Qin, Zhen},
  year = {2025},
  doi = {10.1145/3726302.3730348},
  url = {https://doi.org/10.1145/3726302.3730348},
  booktitle = {Proceedings of the 48th {International},
  pages = {3865--3875},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {evaluation, large language models, ranking, source: ACM},
  abstract = {Large language models (LLMs) are increasingly integral to information retrieval (IR), powering ranking, evaluation, and AI-assisted content creation. This widespread adoption necessitates a critical examination of potential biases arising from the interplay between these LLM-based components. This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges. We provide the first empirical evidence of LLM judges exhibiting significant bias towards LLM-based rankers. Furthermore, we observe limitations in LLM judges' ability to discern subtle system performance differences. Contrary to some previous findings, our preliminary study does not find evidence of bias against AI-generated content. These results highlight the need for a more holistic view of the LLM-driven information ecosystem. To this end, we offer initial guidelines and a research agenda to ensure the reliable use of LLMs in IR evaluation.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{goel_x-lifecycle_2024,
  title = {X-{Lifecycle},
  author = {Goel, Drishti and Husain, Fiza and Singh, Aditya and Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and Zhang, Xuchao and Rajmohan, Saravan},
  year = {2024},
  doi = {10.1145/3663529.3663861},
  url = {https://doi.org/10.1145/3663529.3663861},
  booktitle = {Companion {Proceedings},
  pages = {417--428},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {Cloud Services, Large language models, Monitor management, Reliability, Root-cause analysis, source: ACM},
  abstract = {Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{quinn_longsight_2025,
  title = {{LongSight},
  author = {Quinn, Derrick and Yücel, E. Ezgi and Kim, Jinkwon and Martínez, José F. and Alian, Mohammad},
  year = {2025},
  doi = {10.1145/3725843.3756062},
  url = {https://doi.org/10.1145/3725843.3756062},
  booktitle = {Proceedings of the 58th {IEEE},
  pages = {34--48},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Large input context windows in transformer-based LLMs help minimize hallucinations and improve output accuracy and personalization. However, as the context window grows, the attention phase increasingly dominates execution time. Key–Value (KV) caching alleviates part of this cost by avoiding redundant computation, but the KV cache itself can quickly exceed the capacity of today’s GPU high-bandwidth memory (HBM). In this work, we present LongSight, an algorithm–hardware co-design framework for accelerating attention in large-context scenarios. LongSight leverages a compute-enabled CXL memory device, originally designed for dense retrieval acceleration, to offload KV cache storage and retrieval. Therefore, LongSight effectively elevates the value of relatively low-cost LPDDR DRAM to that of high-end HBM. We demonstrate that, with just a single GPU and a single compute-enabled CXL memory expander, LongSight can efficiently support context lengths of up to 1 million tokens for state-of-the-art Llama models.},
  address = {New York, NY, USA},
  series = {{MICRO},
  isbn = {979-8-4007-1573-0},
}

@inproceedings{zhong_design_2024,
  title = {The {Design},
  author = {Zhong, Xuanyan and Xin, Haiyang and Li, Wenfeng and Zhan, Zehui and Cheng, May-hung},
  year = {2024},
  doi = {10.1145/3675812.3675871},
  url = {https://doi.org/10.1145/3675812.3675871},
  booktitle = {Proceedings of the 2024 9th {International},
  pages = {62--68},
  publisher = {Association for Computing Machinery},
  note = {event-place: Guangzhou, China},
  keywords = {Collaborative problem solving, Conversational agent, GPT},
  abstract = {Dialogue is the basis of collaborative problem solving, and the development of generative artificial intelligence has made dialogue no longer limited to human-to-human, and human-computer dialogue has gradually become an important way for people to solve problems. At the same time, with the change of the subject of collaborative problem solving, the cultivation of collaborative problem-solving skill urgently needs to explore a new path. In this regard, more and more studies have begun to apply conversational agents in collaborative problem-solving activities, digging deeper into the effects of time on students in conversational agents. However, there is no clear answer to the question of how conversational agents can be better integrated into a collaborative environment for all to assist people in the collaborative problem-solving process and improve performance. In this study, we constructed a conceptual model of human-computer collaboration in order to improve students' learning performance. Based on this model, we integrated Retrieval-Augmented Generative and GPT to construct a conversational agent, and the results of the study showed that the Retrieval-Augmented Generative Agent for Collaborative Problem Solving constructed in this study can effectively promote students' collaborative problem-solving performance.},
  address = {New York, NY, USA},
  series = {{ICDEL},
  isbn = {979-8-4007-1680-5},
}

@inproceedings{pamnani_ai-driven_2025,
  title = {{AI},
  author = {Pamnani, Chintan Rajesh},
  year = {2025},
  doi = {10.1145/3748382.3748388},
  url = {https://doi.org/10.1145/3748382.3748388},
  booktitle = {Proceedings of the 2025 4th {International},
  pages = {26--30},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {The increasing complexity of digital hardware design necessitates automation strategies that enhance efficiency and reduce human effort. This study introduces a novel AI-driven framework that leverages multi-agent collaboration and generative modeling to optimize the hardware design process. By integrating language models with retrieval-augmented techniques, the system autonomously generates and refines hardware description language (HDL) code, improving design accuracy and performance. Experimental evaluations on digital circuit benchmarks demonstrate the effectiveness of this approach in synthesizing functional, power-efficient designs. These findings contribute to the advancement of AI-assisted electronic design automation (EDA) and scalable hardware development.},
  address = {New York, NY, USA},
  series = {{FAIML},
  isbn = {979-8-4007-1321-7},
}

@inproceedings{shanmugarasa_sok_2025,
  title = {{SoK},
  author = {Shanmugarasa, Yashothara and Ding, Ming and Arachchige, Chamikara Mahawaga and Rakotoarivelo, Thierry},
  year = {2025},
  doi = {10.1145/3708821.3733888},
  url = {https://doi.org/10.1145/3708821.3733888},
  booktitle = {Proceedings of the 20th {ACM},
  pages = {425--441},
  publisher = {Association for Computing Machinery},
  keywords = {Large Language Models, Privacy, Systematization of Knowledge},
  abstract = {Large language models (LLMs) are sophisticated artificial intelligence systems that enable machines to generate human-like text with remarkable precision. While LLMs offer significant technological progress, their development using vast amounts of user data scraped from the web and collected from extensive user interactions poses risks of sensitive information leakage. Most existing surveys focus on the privacy implications of the training data but tend to overlook privacy risks from user interactions and advanced LLM capabilities. This paper aims to fill that gap by providing a comprehensive analysis of privacy in LLMs, categorizing the challenges into four main areas: (i) privacy issues in LLM training data, (ii) privacy challenges associated with user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and (iv) privacy challenges involving LLM agents. We evaluate the effectiveness and limitations of existing mitigation mechanisms targeting these proposed privacy challenges and identify areas for further research.},
  address = {New York, NY, USA},
  series = {{ASIA},
  isbn = {979-8-4007-1410-8},
}

@inproceedings{pinto_developer_2024,
  title = {Developer {Experiences},
  author = {Pinto, Gustavo and De Souza, Cleidson and Rocha, Thayssa and Steinmacher, Igor and Souza, Alberto and Monteiro, Edward},
  year = {2024},
  doi = {10.1145/3644815.3644949},
  url = {https://doi.org/10.1145/3644815.3644949},
  booktitle = {Proceedings of the {IEEE},
  pages = {81--91},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {LLM, LLM-based applications, perception of productivity, user expectations},
  abstract = {In the rapidly advancing field of artificial intelligence, software development has emerged as a key area of innovation. Despite the plethora of general-purpose AI assistants available, their effectiveness diminishes in complex, domain-specific scenarios. Noting this limitation, both the academic community and industry players are relying on contextualized coding AI assistants. These assistants surpass general-purpose AI tools by integrating proprietary, domain-specific knowledge, offering precise and relevant solutions. Our study focuses on the initial experiences of 62 participants who used a contextualized coding AI assistant — named StackSpot AI— in a controlled setting. According to the participants, the assistants' use resulted in significant time savings, easier access to documentation, and the generation of accurate codes for internal APIs. However, challenges associated with the knowledge sources necessary to make the coding assistant access more contextual information as well as variable responses and limitations in handling complex codes were observed. The study's findings, detailing both the benefits and challenges of contextualized AI assistants, underscore their potential to revolutionize software development practices, while also highlighting areas for further refinement.},
  address = {New York, NY, USA},
  series = {{CAIN},
  isbn = {979-8-4007-0591-5},
}

@inproceedings{fan_litlinker_2025,
  title = {{LitLinker},
  author = {Fan, Haoxiang and Zhou, Changshuang and Yu, Hao and Wu, Xueyang and Gu, Jiangyu and Peng, Zhenhui},
  year = {2025},
  doi = {10.1145/3706598.3714111},
  url = {https://doi.org/10.1145/3706598.3714111},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {elementary schools, ideation, Interdisciplinary contexts, large language models, teachers},
  abstract = {Teaching literature under interdisciplinary (e.g., science, art) contexts that connect reading materials has become popular in elementary schools. However, constructing such contexts is challenging as it requires teachers to explore substantial amounts of interdisciplinary content and link it to the reading materials. In this paper, we develop LitLinker via an iterative design process involving 13 teachers to facilitate the ideation of interdisciplinary contexts for teaching literature. Powered by a large language model (LLM), LitLinker can recommend interdisciplinary topics and contextualize them with literary elements (e.g., paragraphs, viewpoints) in the reading materials. A within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker can improve the integration depth of different subjects and reduce workload in this ideation task. Expert interviews (N=9) also demonstrate LitLinker’s usefulness for supporting the ideation of interdisciplinary contexts for teaching literature. We conclude with concerns and design considerations for supporting interdisciplinary teaching with LLMs.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{wei_tar_2024,
  title = {{TAR},
  author = {Wei, Zizhong and Zhang, Qilai and Duan, Qiang and Wang, Guangxin and Li, Rui and Li, Xue and Chen, Qibin and Yang, Tong and Zhang, Lei and Jiang, Kai},
  year = {2024},
  doi = {10.1145/3677779.3677825},
  url = {https://doi.org/10.1145/3677779.3677825},
  booktitle = {Proceedings of the {International},
  pages = {282--286},
  publisher = {Association for Computing Machinery},
  note = {event-place: Xi'an, China},
  keywords = {source: ACM},
  abstract = {In real-world user interactions, complex problems often involve multiple intents, such as retrieval, RAG (Retrieval-Augmented Generation), and generation. However, existing methods such as ReAct and COT (Chain-of-Thought) are designed for single tasks, resulting in a gap between research and practical applications. To bridge this gap, we propose a novel complex problem-solving framework called TAR (Think-Action-Reflection). The framework consists of three key stages: the Think stage conducts fine-grained, multi-dimensional analysis of the problem to form a structured problem representation; the Action stage dynamically schedules execution tools based on the output of the Think stage, flexibly adapting to the solving requirements of different problems; and the Reflection stage assesses the alignment between the outputs from the Action stage and the initial user intent, initiating iterative enhancements where necessary. To verify the effectiveness of the TAR framework, we conducted experiments on two types of datasets. Firstly, we constructed a business scenario dataset to validate the effectiveness of the proposed method in addressing real-world problems. Secondly, we evaluated the effectiveness of the Think stage using open-domain question answering datasets such as HotpotQA, TriviaQA, and Natural Questions (NQ). The results demonstrate that, compared with existing methods, the TAR framework achieves significant performance improvements across various complex problem tasks, exhibiting advantages in problem understanding, task execution, and result optimization. These findings confirm the effectiveness of the TAR framework as a paradigm for complex problem-solving in real-world user interactions.},
  address = {New York, NY, USA},
  series = {{CMNM},
  isbn = {979-8-4007-0976-0},
}

@inproceedings{krishna_codellm-devkit_2025,
  title = {Codellm-{Devkit},
  author = {Krishna, Rahul and Pan, Rangeet and Sinha, Saurabh and Tamilselvam, Srikanth and Pavuluri, Raju and Vukovic, Maja},
  year = {2025},
  doi = {10.1145/3696630.3728555},
  url = {https://doi.org/10.1145/3696630.3728555},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {308--318},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {source: ACM},
  abstract = {Large Language Models for Code (or code LLMs) are increasingly gaining popularity and capabilities, offering a wide array of functionalities such as code completion, code generation, code summarization, test generation, code translation, and more. To leverage code LLMs to their full potential, developers must provide code-specific contextual information to the models. These are typically derived and distilled using program analysis tools. However, there exists a significant gap—these static analysis tools are often language-specific and come with a steep learning curve, making their effective use challenging. These tools are tailored to specific program languages, requiring developers to learn and manage multiple tools to cover various aspects of the their code base. Moreover, the complexity of configuring and integrating these tools into the existing development environments add an additional layer of difficulty. This challenge limits the potential benefits that could be gained from more widespread and effective use of static analysis in conjunction with LLMs.To address this challenge, we present codellm-devkit (hereafter, cldk), an open-source library that significantly simplifies the process of performing program analysis at various levels of granularity for different programming languages to support code LLM use cases. As a Python library, cldk offers developers an intuitive and user-friendly interface, making it incredibly easy to provide rich program analysis context to code LLMs. With this library, developers can effortlessly integrate detailed, code-specific insights that enhance the operational efficiency and effectiveness of LLMs in coding tasks. CLDK is available as an open-source library at https://github.com/codellm-devkit.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{abdullahi_k-paths_2025,
  title = {K-{Paths},
  author = {Abdullahi, Tassallah and Gemou, Ioanna and Nayak, Nihal V. and Murtaza, Ghulam and Bach, Stephen H. and Eickhoff, Carsten and Singh, Ritambhara},
  year = {2025},
  doi = {10.1145/3711896.3737011},
  url = {https://doi.org/10.1145/3711896.3737011},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5--16},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {drug discovery, explainability, gnns, inductive reasoning, knowledge graph reasoning, llms, source: ACM},
  abstract = {Biomedical knowledge graphs (KGs) encode rich, structured information critical for drug discovery tasks, but extracting meaningful insights from large-scale KGs remains challenging due to their complex structure. Existing biomedical subgraph retrieval methods are tailored for graph neural networks (GNNs), limiting compatibility with other paradigms, including large language models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that extracts structured, diverse, and biologically meaningful multi-hop paths from dense biomedical KGs. These paths enable prediction of unobserved drug-drug and drug-disease interactions, including those involving entities not seen during training, thus supporting inductive reasoning. K-Paths is training-free and employs a diversity-aware adaptation of Yen's algorithm to extract the K shortest loopless paths between entities in a query, prioritizing biologically relevant and relationally diverse connections. These paths serve as concise, interpretable reasoning chains that can be directly integrated with LLMs or GNNs to improve generalization, accuracy, and enable explainable inference. Experiments on benchmark datasets show that K-Paths improves zero-shot reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by 19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same tasks. K-Paths also boosts the training efficiency of EmerGNN, a state-of-the-art GNN, by reducing the KG size by 90\% while maintaining predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs and LLMs, enabling scalable and explainable LLM-augmented scientific discovery. We release our code and the retrieved paths as a benchmark for inductive reasoning.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{krishna_echoswift_2024,
  title = {{EchoSwift},
  author = {Krishna, Karthik and Bandili, Ramana},
  year = {2024},
  doi = {10.1145/3629527.3652273},
  url = {https://doi.org/10.1145/3629527.3652273},
  booktitle = {Companion of the 15th {ACM},
  pages = {158--162},
  publisher = {Association for Computing Machinery},
  note = {event-place: London, United Kingdom},
  keywords = {ai benchmarking, large language models, llm performance, text generation inference},
  abstract = {Large Language Models (LLMs) are advanced natural language processing models that are trained on vast amounts of text data to understand and generate human-like language. These models are designed to understand context, generate coherent and contextually relevant text, and demonstrate advanced language capabilities. In the dynamic landscape of LLMs, the demand for efficient inference benchmarking is crucial. Organizations such as TPC and SPEC brought several industry standard benchmark [1][2][3][4]. This publication introduces EchoSwift [11], a comprehensive benchmarking framework designed to evaluate the real-time performance of LLMs in deployment scenarios. As LLMs ascend to the forefront of technological innovation, their seamless integration into real-world applications demands a nuanced understanding of their efficiency, throughput, latency, and scalability. It is within this dynamic landscape that our publication unveils the EchoSwift, a novel benchmarking framework meticulously crafted to address the pressing need for comprehensive inference benchmarking, as well as the discovery of the right configuration for specific LLM requirements. For instance, certain deployments might have 32 tokens as input and 256 tokens as output, while others might have 256 tokens as input and 64 tokens as output. It is crucial to acknowledge that the configuration for these two requirements need not be the same for an optimal performance, scale and better TCO. The EchoSwift not only aids in comprehensive configuration discovery but also facilitates robust Performance/Scale testing, ensuring that LLM deployments are not only efficient but also finely tuned to their specific operational demands.},
  address = {New York, NY, USA},
  series = {{ICPE},
  isbn = {979-8-4007-0445-1},
}

@inproceedings{yin_lies_2024,
  title = {Lies, {Deceit},
  author = {Yin, Michael and Wang, Emi and Ng, Chuoxi and Xiao, Robert},
  year = {2024},
  doi = {10.1145/3613904.3642253},
  url = {https://doi.org/10.1145/3613904.3642253},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {large language models, LLM hallucinations, lying, player experience, video games},
  abstract = {Lying and deception are important parts of social interaction; when applied to storytelling mediums such as video games, such elements can add complexity and intrigue. We developed a game, “AlphaBetaCity”, in which non-playable characters (NPCs) made various false statements, and used this game to investigate perceptions of deceptive behaviour. We used a mix of human-written dialogue incorporating deliberate falsehoods and LLM-written scripts with (human-approved) hallucinated responses. The degree of falsehoods varied between believable but untrue statements to outright fabrications. 29 participants played the game and were interviewed about their experiences. Participants discussed methods for developing trust and gauging NPC truthfulness. Whereas perceived intentional false statements were often attributed towards narrative and gameplay effects, seemingly unintentional false statements generally mismatched participants’ mental models and lacked inherent meaning. We discuss how the perception of intentionality, the audience demographic, and the desire for meaning are major considerations when designing video games with falsehoods.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{kan_abcm_2025,
  title = {{ABCM},
  author = {Kan, Xuan and Dai, Yu and Shi, Jiancheng},
  year = {2025},
  doi = {10.1145/3760269.3760353},
  url = {https://doi.org/10.1145/3760269.3760353},
  booktitle = {Proceedings of the 2025 5th {International},
  pages = {540--546},
  publisher = {Association for Computing Machinery},
  keywords = {Agent-based system, Complexity metrics, Intelligent manufacturing, Manufacturing complexity, PySD, Simulation-based analysis},
  abstract = {Modern intelligent manufacturing systems are inherently complex, driven by product variety, dynamic scheduling, and intricate interactions among humans, machines, and materials. This complexity presents significant challenges for production planning, control, and decision-making. To address these issues, we propose ABCM (Agent-Based Complexity Management), a novel method that integrates system dynamics simulation with a large language model (LLM)-based agent framework. ABCM utilizes PySD to simulate manufacturing processes and extract structured model outputs, which are analyzed through a suite of modular tools registered within a LangChain agent. These tools enable the computation of descriptive statistics, dynamic performance indicators, and complexity metrics—including entropy, rise time, overshoot, and integral errors—thus allowing for automated, data-driven complexity assessment. The agent autonomously interprets simulation results, identifies performance bottlenecks, and offers optimization recommendations. A case study demonstrates the practical application of ABCM in managing production variability and enhancing system responsiveness. The modular and extensible architecture supports scalable deployment in diverse intelligent manufacturing scenarios, contributing to improved adaptability, efficiency, and complexity control.},
  address = {New York, NY, USA},
  series = {{ACAIB},
  isbn = {979-8-4007-1431-3},
}

@inproceedings{azher_limtopic_2025,
  title = {{LimTopic},
  author = {Azher, Ibrahim Al and Seethi, Venkata Devesh Reddy and Akella, Akhil Pandey and Alhoori, Hamed},
  year = {2025},
  doi = {10.1145/3677389.3702605},
  url = {https://doi.org/10.1145/3677389.3702605},
  booktitle = {Proceedings of the 24th {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hong Kong, China},
  keywords = {information extraction, large language models, limitations sections, research limitations, science of science},
  abstract = {The "limitations" sections of scientific articles play a crucial role in highlighting the boundaries and shortcomings of research, thereby guiding future studies and improving research methods. Analyzing these limitations benefits researchers, reviewers, funding agencies, and the broader academic community. We introduce LimTopic, a strategy where Topic generation in Limitation sections in scientific articles with Large Language Models (LLMs). Here, each topic contains the title and `Topic Summary.' This study focuses on effectively extracting and understanding these limitations through topic modeling and text summarization, utilizing the capabilities of LLMs. We extracted limitations from research articles and applied an LLM-based topic modeling integrated with the BERtopic approach to generate a title for each topic and `Topic Sentences.' To enhance comprehension and accessibility, we employed LLM-based text summarization to create concise and generalizable summaries for each topic's Topic Sentences and produce a `Topic Summary.' Our experimentation involved prompt engineering, fine-tuning LLM and BERTopic, and integrating BERTopic with LLM to generate topics, titles, and a topic summary. We also experimented with various LLMs with BERTopic for topic modeling and various LLMs for text summarization tasks. Our results showed that the combination of BERTopic and GPT 4 performed the best in terms of silhouette and coherence scores in topic modeling, and the GPT4 summary outperformed other LLM tasks as a text summarizer. Our code and dataset are available at https://github.com/IbrahimAlAzhar/LimTopic/tree/master.},
  address = {New York, NY, USA},
  series = {{JCDL},
  isbn = {979-8-4007-1093-3},
}

@inproceedings{hu_privacy-preserved_2024,
  title = {Privacy-{Preserved},
  author = {Hu, Qi and Li, Haoran and Bai, Jiaxin and Wang, Zihao and Song, Yangqiu},
  year = {2024},
  doi = {10.1145/3637528.3671678},
  url = {https://doi.org/10.1145/3637528.3671678},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {1108--1118},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {complex query answering (cqa), knowledge graphs (kgs), neural graph databases (ngdbs), privacy preserving},
  abstract = {In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries. Extensive experimental results on three datasets show that our framework can effectively protect private information in the graph database while delivering high-quality public answers responses to queries. The code is available at https://github.com/HKUST-KnowComp/PrivateNGDB.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{singh_meqa_2025,
  title = {{MEQA},
  author = {Singh, Sonal and Gupta, Yadunath and Chowdhury, Soudip Roy},
  year = {2025},
  doi = {10.1145/3703323.3704277},
  url = {https://doi.org/10.1145/3703323.3704277},
  booktitle = {Proceedings of the 8th {International},
  pages = {422--426},
  publisher = {Association for Computing Machinery},
  keywords = {A, Enterprise Q\&amp, Information retrieval, LangGraph, LLM, Multi-Agent System, Multi-Modal Gen-AI, RISE, STaR},
  abstract = {This paper introduces a new architecture for multi-agent systems designed to support query answering over secure enterprise data. The system uses a modular approach to natural language understanding and task execution, utilizing specialized agents for query processing. Our system features a Multi-Agent Block that consists of agents for general inquiries, Text2SQL, visualization, and information consolidation. These agents work together to handle complex queries. The Answer Generation component then integrates these results into coherent responses. We demonstrate our system’s efficiency using a complex query processed by a Text2SQL agent. In this scenario, the agent interacts with multiple endpoints, including an enterprise database and LLM endpoints, while employing techniques like RISE (Recursive Introspection for Results Improvements) and STaR (Self-Taught Reasoner) to enhance reasoning. Additionally, we use an open-source utility, LLMLingua, to compress prompts and reduce computational overhead. Our approach shows strong performance across various retrieval tasks, offering a significant step toward more intuitive and efficient data interaction, with the potential to transform how organizations utilize large language models, multiple agents, and data assets.},
  address = {New York, NY, USA},
  series = {{CODS},
  isbn = {979-8-4007-1124-4},
}

@inproceedings{liu_fitting_2025,
  title = {Fitting {Into},
  author = {Liu, Zheng and Li, Chaofan and Xiao, Shitao and Li, Chaozhuo and Zhang, Chen Jason and Liao, Hao and Lian, Defu and Shao, Yingxia},
  year = {2025},
  doi = {10.1145/3696410.3714620},
  url = {https://doi.org/10.1145/3696410.3714620},
  booktitle = {Proceedings of the {ACM},
  pages = {3942--3951},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {flexibility, lightweighting, llm-based re-rankers, text retrieval, source: ACM},
  abstract = {Large language models (LLMs) provide powerful foundations to perform fine-grained text re-ranking. However, they are often prohibitive in reality due to constraints on computation bandwidth. In this work, we propose a flexible architecture called Matroyshka Re-Ranker, which is designed to facilitate runtime customization of model layers and sequence lengths at each layer based on users' configurations. Consequently, the LLM-based re-rankers can be made applicable across various real-world situations. The increased flexibility may come at the cost of precision loss. To address this problem, we introduce a suite of techniques to optimize the performance. First, we propose cascaded self-distillation, where each sub-architecture learns to preserve a precise re-ranking performance from its super components, whose predictions can be exploited as smooth and informative teacher signals. Second, we design a factorized compensation mechanism, where two collaborative LoRA modules, vertical and horizontal, are jointly employed to compensate for the precision loss resulted from arbitrary combinations of layer and sequence compression. We perform comprehensive experiments using passage and document retrieval datasets from MSMARCO, along with all public datasets from BEIR. In our experiments, Matryoshka Re-Ranker substantially outperforms existing methods, while effectively preserving its superior performance across various compression forms and application scenarios. We have publicly released our method at this https://github.com/FlagOpen/FlagEmbedding repo.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{sanchez_cuadrado_automating_2024,
  title = {Automating the {Development},
  author = {Sánchez Cuadrado, Jesús and Pérez-Soler, Sara and Guerra, Esther and De Lara, Juan},
  year = {2024},
  doi = {10.1145/3640794.3665538},
  url = {https://doi.org/10.1145/3640794.3665538},
  booktitle = {Proceedings of the 6th {ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Luxembourg, Luxembourg},
  keywords = {Domain-Specific Languages, Large Language Models, Task-oriented Chatbots},
  abstract = {Task-oriented chatbots are increasingly used to access all sorts of services – like booking a flight, or setting a medical appointment – through natural language conversation. There are many technologies for implementing task-oriented chatbots, including Dialogflow, Watson, and Rasa. They rely on an explicit definition of the user intents, conversation flows, and chatbot outputs, which is costly to specify, and sometimes results in suboptimal user experiences and artificial conversations with limited diversity of chatbot responses. Recently, the advances in generative artificial intelligence fostered by Large Language Models (LLMs) have enabled a new range of open-domain chatbots, like ChatGPT, able to converse fluently on any topic. However, they are general-purpose, and therefore not directly usable to solve specialised tasks reliably. In this paper, we study the power of LLMs to build task-oriented chatbots, resulting in lighter specifications – no intent definition required – and more natural conversations than in intent-based approaches. To this end, we propose a lightweight domain-specific language based on YAML to specify chatbots using modules of different types (e.g., menus, question-answering, data gathering). These specifications are compiled into structured LLM prompts that use the ReAct framework to inform our runtime how to interpret the user input and coordinate the tasks that the chatbot must perform. The paper presents the design and realisation of our framework, and an assessment that encodes a set of existing intent-based chatbots using our approach, showing its benefits in terms of specification size, conversation flexibility and output diversity.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-0511-3},
}

@inproceedings{liu_ora_2025,
  title = {{ORA},
  author = {Liu, Hongyi and Ma, Yinping and Huang, Xiaosong and Zhang, Lingzhe and Jia, Tong and Li, Ying},
  year = {2025},
  doi = {10.1145/3721145.3725757},
  url = {https://doi.org/10.1145/3721145.3725757},
  booktitle = {Proceedings of the 39th {ACM},
  pages = {884--894},
  publisher = {Association for Computing Machinery},
  keywords = {LLMs, Online Learning, Retrieval-Augmented, Runtime Prediction},
  abstract = {Accurate job runtime prediction is critical for efficient scheduling in high-performance computing (HPC) platforms. For instance, precise predictions enable techniques such as backfilling, where small jobs are executed ahead of schedule to maximize resource utilization and enhance computational efficiency. However, existing runtime prediction methods primarily rely on job metadata (e.g., submission time, requested runtime, and required memory) while ignoring the content of job scripts, which limits their accuracy. To address this issue, we propose an Online Retrieval-Augmented Language Model (ORA) for job runtime prediction. ORA encodes both metadata and script information from historical jobs into feature vectors to form a database, enabling similarity-based retrieval to assist in predicting the runtime of new jobs. To address distribution shifts, ORA incrementally updates the database without requiring model retraining. Additionally, personalized retrieval mechanisms are employed to mitigate the impact of distribution shifts. To reduce interference caused by repetitive content in retrieved jobs and enhance the learning of essential differences, we design a diff-based contextual learning mechanism. This highlights the differences between the current job and the retrieved jobs, improving the model’s ability to capture distinctive features. Experimental results demonstrate that the proposed method outperforms existing baselines, achieving an average accuracy improvement of over 40\% in real-world scenarios where the training and testing job times do not overlap, and the metadata does not include running information such as actual memory usage. Ablation studies further highlight the contribution of each component of the proposed method.},
  address = {New York, NY, USA},
  series = {{ICS},
  isbn = {979-8-4007-1537-2},
}

@inproceedings{chi_watchwithme_2025,
  title = {{WatchWithMe},
  author = {Chi, Peggy and Hu, Senpo and Shi, Lei and Kraljic, Tanya, PhD and Secor, Justin and Dong, Tao and Essa, Irfan and Cleron, Mike},
  year = {2025},
  doi = {10.1145/3719160.3736624},
  url = {https://doi.org/10.1145/3719160.3736624},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {interactive prompting, large language models., text-based conversational interfaces, Videos},
  abstract = {Videos are a popular way for viewers to follow topics of interest. In areas such as product and technology reviews, videos often present in-depth perspectives in a compact fashion, driving viewers to look for additional explanations. We propose WatchWithMe, an automatic approach that provides viewers in-context guided watching during video playback. Powered by large language models, WatchWithMe generates guided materials from the video transcript as if creating a reading guide, including summaries, highlights, and question prompts. WatchWithMe reveals relevant information responsive to the spoken content in a review video. Viewers skim and prompt in our text-based conversational UI, to which we automatically expand the video viewing context to the model for contextual responses. We evaluated WatchWithMe with public videos and collected feedback from 20 participants. Findings showed that our method encouraged viewers to seek out viewpoints or confirmations related to the video topics.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@article{einy_cost-effective_2024,
  title = {Cost-{Effective},
  author = {Einy, Y. and Milo, T. and Novgorodov, S.},
  year = {2024},
  doi = {10.1145/3665601.3669848},
  url = {https://doi.org/10.1145/3665601.3669848},
  booktitle = {Proceedings of the {Conference},
  journal = {… Understanding and Integration of Data for …},
  pages = {45--49},
  publisher = {Association for Computing Machinery},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar, Data Enrichment, Data Integration, Large Language Models},
  abstract = {… Using a RAG LLM involves retrieval cost𝑐𝑟 (𝑀), processing cost for {\textbar},
  annote = {Query date: 2025-10-25 20:50:36},
  address = {New York, NY, USA},
  series = {{GUIDE},
  isbn = {979-8-4007-0694-3},
}

@inproceedings{sun_real-time_2024,
  title = {A {Real},
  author = {Sun, Yiping and Shi, Yang and Du, Jiaolong},
  year = {2024},
  doi = {10.1145/3627673.3680054},
  url = {https://doi.org/10.1145/3627673.3680054},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4906--4913},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {approximate nearest neighborhood search, gpu parallel system, multi stream gpu, real time vector insertion},
  abstract = {In recent years, Approximate Nearest Neighbor Search (ANNS) has played a pivotal role in modern search and recommendation systems, especially in emerging LLM applications like Retrieval-Augmented Generation. There is a growing exploration into harnessing the parallel computing capabilities of GPUs to meet the substantial demands of ANNS. However, existing systems primarily focus on offline scenarios, overlooking the distinct requirements of online applications that necessitate real-time insertion of new vectors. This limitation renders such systems inefficient for real-world scenarios. Moreover, previous architectures struggled to effectively support real-time insertion due to their reliance on serial execution streams. In this paper, we introduce a novel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our architecture achieves its objectives through three key advancements: 1) We initially examined the real-time insertion mechanisms in existing GPU ANNS systems and discovered their reliance on repetitive copying and memory allocation, which significantly hinders real-time effectiveness on GPUs. As a solution, we introduce a dynamic vector insertion algorithm based on memory blocks, which includes in-place rearrangement. 2) To enable real-time vector insertion in parallel, we introduce a multi-stream parallel execution mode, which differs from existing systems that operate serially within a single stream. Our system utilizes a dynamic resource pool, allowing multiple streams to execute concurrently without additional execution blocking. 3) Through extensive experiments and comparisons, our approach effectively handles varying QPS levels across different datasets, reducing latency by up to 40\%-80\%. The proposed system has also been deployed in real-world industrial search and recommendation systems, serving hundreds of millions of users daily, and has achieved significant results.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{tan_exploring_2025,
  title = {Exploring the {Impact},
  author = {Tan, Chek Tien and Atmosukarto, Indriyati and Tandianus, Budianto and Shen, Songjia and Wong, Steven},
  year = {2025},
  doi = {10.1145/3706598.3713456},
  url = {https://doi.org/10.1145/3706598.3713456},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {avatars, Chatbots, conversational agents, large language models},
  abstract = {Despite the growing prominence of Artificial Intelligence (AI) chatbots used in education, there remains a significant gap in our understanding of how interface design elements, particularly avatar representations, influence learning experiences. This paper explores the impact of different AI chatbot avatar representations on students’ learning experiences through a mixed-methods within-subjects study, where participants interacted with three distinct types of AI chatbot interfaces with a common large language model (LLM) over a 14-week university course. Our findings reveal that preferences vary according to factors such as learning habits and learning activities. Avatar design also exhibits affordances for specific prompting behaviors, while the perceived human touch influenced learning experiences in nuanced ways. Additionally, real-world relationships with the individuals behind deepfakes influence these experiences. These insights suggest that the thoughtful integration of diverse avatar representations in AI chatbot systems for different learners and settings can greatly enhance learning experiences.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{he_cognify_2025,
  title = {Cognify: {Supercharging},
  author = {He, Zijian and Abhyankar, Reyna and Srivatsa, Vikranth and Zhang, Yiying},
  year = {2025},
  doi = {10.1145/3711896.3736884},
  url = {https://doi.org/10.1145/3711896.3736884},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {932--943},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {agentic workflows, bayesian optimization, gen-ai workflows, llm, optimization, test-time scaling},
  abstract = {Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8×, reduces execution monetary cost by up to 10×, and reduces end-to-end latency by 2.7×.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{ganiyu_ai5gtest_2025,
  title = {{AI5GTest},
  author = {Ganiyu, Abiodun and Gajjar, Pranshav and Shah, Vijay K},
  year = {2025},
  doi = {10.1145/3734477.3734703},
  url = {https://doi.org/10.1145/3734477.3734703},
  booktitle = {18th {ACM},
  pages = {53--64},
  publisher = {Association for Computing Machinery},
  note = {event-place: Arlington, VA, USA},
  keywords = {3gpp, ai5gtest, automated testing, llm, o-ran, source: ACM},
  abstract = {The advent of Open Radio Access Networks (O-RAN) has transformed the telecommunications industry by promoting interoperability, vendor diversity, and rapid innovation. However, its disaggregated architecture introduces complex testing challenges, particularly in validating multi-vendor components against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as those provided by Open Testing and Integration Centres (OTICs), rely heavily on manual processes, are fragmented and prone to human error, leading to inconsistency and scalability issues. To address these limitations, we present AI5GTest – an AI-powered, specification-aware testing framework designed to automate the validation of O-RAN components. AI5GTest leverages a cooperative Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and Debug-LLM. Gen-LLM automatically generates expected procedural flows for test cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references signaling messages against these flows to validate compliance and detect deviations. If anomalies arise, Debug-LLM performs root cause analysis, providing insight to the failure cause. To enhance transparency and trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the Gen-LLM presents top-k relevant official specifications to the tester for approval before proceeding with validation. Evaluated using a range of test cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest demonstrates a significant reduction in overall test execution time compared to traditional manual methods, while maintaining high validation accuracy.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1530-3},
  series = {{WiSec},
}

@inproceedings{uprety_using_2025,
  title = {Using {Large},
  author = {Uprety, Sagar and Buyuklieva, Boyana and Tiwari, Prayag},
  year = {2025},
  doi = {10.1145/3701716.3717752},
  url = {https://doi.org/10.1145/3701716.3717752},
  booktitle = {Companion {Proceedings},
  pages = {1645--1648},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {ai for science, information extraction, llm, source: ACM},
  abstract = {We present a novel dataset for hypothesis and claim extraction from research papers in the interdisciplinary domain of social infertility. We use OpenAI's o1-mini reasoning LLM to create the dataset by extracting hypotheses and claims, which are then manually validated. Then we also use smaller closed and open-weight LLMs to create a benchmark for hypothesis and claim extraction.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{zhao_let_2024,
  title = {Let {Me},
  author = {Zhao, Yuyue and Wu, Jiancan and Wang, Xiang and Tang, Wei and Wang, Dingxian and de Rijke, Maarten},
  year = {2024},
  doi = {10.1145/3626772.3657828},
  url = {https://doi.org/10.1145/3626772.3657828},
  booktitle = {Proceedings of the 47th {International},
  pages = {1796--1806},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {large language models, recommender system, tool learning, source: ACM},
  abstract = {Conventional recommender systems (RSs) face challenges in precisely capturing users' fine-grained preferences. Large language models (LLMs) have shown capabilities in commonsense reasoning and leveraging external tools that may help address these challenges. However, existing LLM-based RSs suffer from hallucinations, misalignment between the semantic space of items and the behavior space of users, or overly simplistic control strategies (e.g., whether to rank or directly present existing results). To bridge these gap, we introduce ToolRec, a framework for LLM-empowered recommendations via tool learning that uses LLMs as surrogate users, thereby guiding the recommendation process and invoking external tools to generate a recommendation list that aligns closely with users' nuanced preferences.We formulate the recommendation process as a process aimed at exploring user interests in attribute granularity. The process factors in the nuances of the context and user preferences. The LLM then invokes external tools based on a user's attribute instructions and probes different segments of the item pool. We consider two types of attribute-oriented tools: rank tools and retrieval tools. Through the integration of LLMs, ToolRec enables conventional recommender systems to become external tools with a natural language interface. Extensive experiments verify the effectiveness of ToolRec, particularly in scenarios that are rich in semantic content.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{chiu_enhancing_2025,
  title = {Enhancing {Medical},
  author = {Chiu, Yen-Jung and Chuang, Chao-Chun and Hwa, Kuo-Yuan},
  year = {2025},
  doi = {10.1145/3707127.3707138},
  url = {https://doi.org/10.1145/3707127.3707138},
  booktitle = {Proceedings of the 2024 11th {International},
  pages = {66--71},
  publisher = {Association for Computing Machinery},
  keywords = {Edema, hospital, LLM, Medical report, Natural language model, source: ACM},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing (NLP) with significant advancements in text generation. LLMs often struggle with complex domain-specific tasks, such as medical report analysis, despite their capabilities. This study focuses on enhancing LLM performance for medical applications, particularly in diagnosing and managing cardiogenic pulmonary edema (CPE). This research explores fine-tuning LLMs to develop a real-time CPE chatbot for Intensive Care Units (ICUs). The chatbot aims to provide diagnostic suggestions and explanations based on patient data. In the results, the LLaMa3-8B model performed better in predicting patients' CPE stage and keyword extraction. The accuracies achieved 72\% and 86\%.},
  address = {New York, NY, USA},
  series = {{ICBBE},
  isbn = {979-8-4007-1827-4},
}

@inproceedings{kim_fostering_2025,
  title = {Fostering {Appropriate},
  author = {Kim, Sunnie S. Y. and Vaughan, Jennifer Wortman and Liao, Q. Vera and Lombrozo, Tania and Russakovsky, Olga},
  year = {2025},
  doi = {10.1145/3706598.3714020},
  url = {https://doi.org/10.1145/3706598.3714020},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Explanations, Human-AI interaction, Inconsistencies, Large language models, Overreliance, Question answering, Sources},
  abstract = {Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users’ reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users’ reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{fu_medquery_2025,
  title = {{MedQuery},
  author = {Fu, Chenhan and Xia, Yu and Wang, Guoming and Lu, Rongxing and Tang, Siliang},
  year = {2025},
  doi = {10.1145/3731715.3733383},
  url = {https://doi.org/10.1145/3731715.3733383},
  booktitle = {Proceedings of the 2025 {International},
  pages = {321--329},
  publisher = {Association for Computing Machinery},
  note = {event-place: Chicago, IL, USA},
  keywords = {graph, medical literature, multimodal, query answering, rag},
  abstract = {In the fields of medicine and science, the volume of specialized literature has grown exponentially, containing vast multimodal data-text, images, and tables-that is essential for conveying in-depth scientific insights. However, effectively retrieving, processing, and answering high-level, complex queries from this data remains a significant challenge. In this study, we introduce MedQuery, a multimodal medical knowledge query-answering system that integrates query-based literature retrieval with a response generation module capable of reasoning across multimodal data. Our system begins with literature retrieval from PubMed, using keyword extraction and query alignment to improve document accuracy and relevance. Next, the multimodal processing module processes source documents, extracting images, tables, and text, converting all into a unified textual format. This data is then structured into a global graph capturing relationships among document elements, allowing our system to support a more integrated, in-depth understanding of complex medical queries beyond basic fact retrieval. Extensive evaluations across multiple datasets, including PubMedQA, PubMed-Summarization, and our own MedInquiry dataset, demonstrate that MedQuery outperforms traditional methods and existing commercial AI systems, achieving around 90\% win rates in answer quality and a 13-36\% improvement in accuracy.},
  address = {New York, NY, USA},
  series = {{ICMR},
  isbn = {979-8-4007-1877-9},
}

@inproceedings{pan_acknowledge_2025,
  title = {{ACKnowledge},
  author = {Pan, Ziqi and Zhang, Xiucheng and Li, Zisu and Peng, Zhenhui and Fan, Mingming and Ma, Xiaojuan},
  year = {2025},
  doi = {10.1145/3706598.3713791},
  url = {https://doi.org/10.1145/3706598.3713791},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Affordance-based Interaction Planning, Human Compatible, Real-world Context},
  abstract = {Intelligent agents coexisting with humans often need to interact with human-shared objects in environments. Thus, agents should plan their interactions based on objects’ affordances and the current situation to achieve acceptable outcomes. How to support intelligent agents’ planning of affordance-based interactions compatible with human perception and values in real-world contexts remains under-explored. We conducted a formative study identifying the physical, intrapersonal, and interpersonal contexts that count to household human-agent interaction. We then proposed ACKnowledge, a computational framework integrating a dynamic knowledge graph, a large language model, and a vision language model for affordance-based interaction planning in dynamic human environments. In evaluations, ACKnowledge generated acceptable planning results with an understandable process. In real-world simulation tasks, ACKnowledge achieved a high execution success rate and overall acceptability, significantly enhancing usage-rights respectfulness and social appropriateness over baselines. The case study’s feedback demonstrated ACKnowledge’s negotiation and personalization capabilities toward an understandable planning process.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{maniar_mempal_2025,
  title = {{MemPal},
  author = {Maniar, Natasha and Chan, Samantha W.T. and Zulfikar, Wazeer and Ren, Scott and Xu, Christine and Maes, Pattie},
  year = {2025},
  doi = {10.1145/3708359.3712151},
  url = {https://doi.org/10.1145/3708359.3712151},
  booktitle = {Proceedings of the 30th {International},
  pages = {993--1015},
  publisher = {Association for Computing Machinery},
  keywords = {context-aware agent, large language models, large visual language models, memory assistant, multimodal systems, older adults, voice interfaces, wearables},
  abstract = {Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults’ own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person’s activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal’s design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults’ unique needs.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{yuan_day_2025,
  title = {A {Day},
  author = {Yuan, Xiangzhe and Wang, Jiajun and Wan, Qian and Hu, Siying},
  year = {2025},
  doi = {10.1145/3715275.3732090},
  url = {https://doi.org/10.1145/3715275.3732090},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1341--1359},
  publisher = {Association for Computing Machinery},
  keywords = {AI for Social Good, Dirty Work, Empathy Simulation, Fairness, Interactive Fiction (IF), Large Language Models (LLMs), Occupational Bias, Perspective-Taking, Stigma Reduction},
  abstract = {Occupations referred to as “dirty work” often face entrenched social stigma, which adversely affects the mental health of workers in these fields and impedes occupational equity. In this study, we propose a novel Interactive Fiction (IF) framework powered by Large Language Models (LLMs) to encourage perspective-taking and reduce biases against these stigmatized yet essential roles. Through an experiment with participants (n = 100) across four such occupations, we observed a significant increase in participants’ understanding of these occupations, as well as a high level of empathy and a strong sense of connection to individuals in these roles. Additionally, qualitative interviews with participants (n = 15) revealed that the LLM-based perspective-taking IF enhanced immersion, deepened emotional resonance and empathy toward “dirty work,” and allowed participants to experience a sense of professional fulfillment in these occupations. However, participants also highlighted ongoing challenges, such as limited contextual details generated by the LLM and the unintentional reinforcement of existing stereotypes. Overall, our findings underscore that an LLM-based perspective-taking IF framework offers a promising and scalable strategy for mitigating stigma and promoting social equity in marginalized professions.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{alshahwan_automated_2024,
  title = {Automated {Unit},
  author = {Alshahwan, Nadia and Chheda, Jubin and Finogenova, Anastasia and Gokkaya, Beliz and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
  year = {2024},
  doi = {10.1145/3663529.3663839},
  url = {https://doi.org/10.1145/3663529.3663839},
  booktitle = {Companion {Proceedings},
  pages = {185--196},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {Automated Test Generation, Genetic Improvement, Large Language Models, LLMs, Unit Testing, source: ACM},
  abstract = {This paper describes Meta’s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75\% of TestGen-LLM’s test cases built correctly, 57\% passed reliably, and 25\% increased coverage. During Meta’s Instagram and Facebook test-a-thons, it improved 11.5\% of all classes to which it was applied, with 73\% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{ramjee_ashabot_2025,
  title = {{ASHABot},
  author = {Ramjee, Pragnya and Chhokar, Mehak and Sachdeva, Bhuvan and Meena, Mahendra and Abdullah, Hamid and Vashistha, Aditya and Nagar, Ruchit and Jain, Mohit},
  year = {2025},
  doi = {10.1145/3706598.3713680},
  url = {https://doi.org/10.1145/3706598.3713680},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {ASHA, Chatbot, Experts-in-the-loop, Frontline Healthcare, GPT-4, HCI4D, India, Medical},
  abstract = {Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot’s role in addressing CHWs’ informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs’ supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{zulfikar_memoro_2024,
  title = {Memoro: {Using},
  author = {Zulfikar, Wazeer Deen and Chan, Samantha and Maes, Pattie},
  year = {2024},
  doi = {10.1145/3613904.3642450},
  url = {https://doi.org/10.1145/3613904.3642450},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {context-aware agent, large language models, memory assistant, minimal interfaces, voice interfaces},
  abstract = {People have to remember an ever-expanding volume of information. Wearables that use information capture and retrieval for memory augmentation can help but can be disruptive and cumbersome in real-world tasks, such as in social settings. To address this, we developed Memoro, a wearable audio-based memory assistant with a concise user interface. Memoro uses a large language model (LLM) to infer the user’s memory needs in a conversational context, semantically search memories, and present minimal suggestions. The assistant has two interaction modes: Query Mode for voicing queries and Queryless Mode for on-demand predictive assistance, without explicit query. Our study of (N=20) participants engaged in a real-time conversation, demonstrated that using Memoro reduced device interaction time and increased recall confidence while preserving conversational quality. We report quantitative results and discuss the preferences and experiences of users. This work contributes towards utilizing LLMs to design wearable memory augmentation systems that are minimally disruptive.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{chakraborty_empirical_2025,
  title = {Empirical {Evaluation},
  author = {Chakraborty, Mohna and Kulkarni, Adithya and Li, Qi},
  year = {2025},
  doi = {10.1145/3701716.3717815},
  url = {https://doi.org/10.1145/3701716.3717815},
  booktitle = {Companion {Proceedings},
  pages = {1598--1604},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {fact verification, large language models, prompting strategies},
  abstract = {Commercial large language models (LLMs) such as GPT-3.5 have emerged as powerful tools for diverse natural language processing (NLP) tasks, yet concerns persist about their reliability in generating factual responses. This study investigates the potential of commercial LLMs such as GPT-3.5 for fact verification, addressing three key research questions: (1) Can GPT-3.5 perform fact verification reliably? (2) How do different prompting strategies affect their performance? (3) What are the common errors they make with the most effective prompt? Using the benchmark FEVER 1.0 dataset, we designed and evaluated three prompts, with experiments conducted using GPT-3.5 as a representative commercial LLM. Our experiments demonstrate that GPT-3.5 achieves a Label Accuracy (LA) of over 72\% with the best-performing prompt, significantly outperforming simpler prompts. A detailed error analysis reveals that approximately 70\% of mistakes stem from logical reasoning and contextual misunderstandings. These findings suggest that carefully crafted prompts can substantially enhance the accuracy of LLMs, in fact verification tasks, highlighting their potential as supportive tools for applications in sensitive domains.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{cornacchia_between_2025,
  title = {Between {Promise},
  author = {Cornacchia, Alessandro and Alabdulaal, Iliyas and Saghier, Ibraheem and Mirdad, Albaraa and Fayoumi, Omar and Canini, Marco},
  year = {2025},
  doi = {10.1145/3725783.3764388},
  url = {https://doi.org/10.1145/3725783.3764388},
  booktitle = {Proceedings of the 16th {ACM},
  pages = {155--167},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Emerald Hall, Seoul, Republic of Korea},
  keywords = {AI agents, cloud-native applications, large language models, observability, root cause analysis},
  abstract = {Large Language Models (LLMs) are increasingly explored as general-purpose assistants for infrastructure operations, helping automate tasks like querying data, analyzing logs, and suggesting fixes. In this paper, we consider the more general and ambitious problem of fully automating root cause analysis (RCA) in microservice systems, where LLMs must collect information, reason about it, and interact with the environment to detect, localize and resolve issues. Anecdotal evidence offers useful insights and partial solutions, but the broader challenge remains unresolved. We systematically evaluate multiple LLM agent architectures across a range of incident scenarios. We study how different tool-augmented agents perform, and shed light on common failure modes, including hallucinated reasoning paths and inefficient use of context. Our findings reveal both the promise and the limitations of current approaches, and point to concrete directions for more robust and effective use of LLMs in this domain.},
  address = {New York, NY, USA},
  series = {{APSys},
  isbn = {979-8-4007-1572-3},
}

@inproceedings{basit_asci_2025,
  title = {{ASCI},
  author = {Basit, Nada and Floryan, Mark and Hott, John R. and Huo, Allen and Le, Jackson and Zheng, Ivan},
  year = {2025},
  doi = {10.1145/3641554.3701957},
  url = {https://doi.org/10.1145/3641554.3701957},
  booktitle = {Proceedings of the 56th {ACM},
  pages = {81--87},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {computer science education, cosine similarity, group formation, office hours, source: ACM},
  abstract = {The Artificial Intelligence Smart Classroom Initiative (ASCI) presents a re-imagined set of online course tools, designed primarily to support growing computer science classes. The system has four primary tools: an office hours queue, an automatic student grouping algorithm, a course-specific local large-language model (LLM), and administration tools for detecting students and TAs that need support. These tools interoperate to improve the quality of one another (e.g., LLM conversations support students directly in the office hours queue) and are enhanced by synchronizing data from multiple external sources such as Piazza, Gradescope, and Canvas. The system has been deployed in multiple courses over the past three semesters: initially as a FIFO queue, then supporting manual grouping and smart grouping of office hour attendees, and recently including LLM support. Preliminary results indicate that students who were grouped using the tool were more likely to return to the queue more than twice as often (on average) than those who were not. However, while grouping in office hours has the potential to decrease student wait times, teaching assistants and students tend to favor one-on-one meetings over group meetings. This might be improved in the future with updates to the software, TA training, and incorporation of other supporting tools (e.g., LLM technology). The other, newer, tools will be more thoroughly evaluated in future semesters.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0531-1},
  series = {{SIGCSETS},
}

@inproceedings{venugopalan_combining_2025,
  title = {Combining {Large},
  author = {Venugopalan, Devika and Yan, Ziwen and Borchers, Conrad and Lin, Jionghao and Aleven, Vincent},
  year = {2025},
  doi = {10.1145/3706468.3706516},
  url = {https://doi.org/10.1145/3706468.3706516},
  booktitle = {Proceedings of the 15th {International},
  pages = {373--383},
  publisher = {Association for Computing Machinery},
  keywords = {caregivers, hybrid tutoring, K-12, large language models, mathematics education, tutoring systems},
  abstract = {Caregivers (i.e., parents and members of a child’s caring community) are underappreciated stakeholders in learning analytics. Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support. Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. This LLM generated message recommendations for caregivers supporting their child’s math practice via chat. Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation. We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.},
  address = {New York, NY, USA},
  series = {{LAK},
  isbn = {979-8-4007-0701-8},
}

@inproceedings{snyder_early_2024,
  title = {On {Early},
  author = {Snyder, Ben and Moisescu, Marius and Zafar, Muhammad Bilal},
  year = {2024},
  doi = {10.1145/3637528.3671796},
  url = {https://doi.org/10.1145/3637528.3671796},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {2721--2732},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {LLM hallucinations, question answering},
  abstract = {While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{yang_debugging_2024,
  title = {Debugging with an {AI},
  author = {Yang, Stephanie and Zhao, Hanzhang and Xu, Yudian and Brennan, Karen and Schneider, Bertrand},
  year = {2024},
  doi = {10.1145/3632620.3671092},
  url = {https://doi.org/10.1145/3632620.3671092},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {84--94},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne, VIC, Australia},
  keywords = {AI tutoring, debugging, help-seeking, large language models, LLMs, programming education},
  abstract = {Debugging is a crucial skill for programmers, yet it can be challenging for novices to learn. The introduction of large language models (LLMs) has opened up new possibilities for providing personalized debugging support to students. However, concerns have been raised about potential student over-reliance on LLM-based tools. This mixed-methods study investigates how a pedagogically-designed LLM-based chatbot supports students’ debugging efforts in an introductory programming course. We conducted interviews and debugging think-aloud tasks with 20 students at three points throughout the semester. We specifically focused on characterizing when students initiate help from the chatbot during debugging, how they engage with the chatbot’s responses, and how they describe their learning experiences with the chatbot. By analyzing data from the debugging tasks, we identified varying help-seeking behaviors and levels of engagement with the chatbot’s responses, depending on students’ familiarity with the suggested strategies. Interviews revealed that students appreciated the content and experiential knowledge provided by the chatbot, but did not view it as a primary source for learning debugging strategies. Additionally, students self-identified certain chatbot usage behaviors as negative, “non-ideal” engagement and others as positive, “learning-oriented” usage. Based on our findings, we discuss pedagogical implications and future directions for designing pedagogical chatbots to support debugging.},
  address = {New York, NY, USA},
  series = {{ICER},
  isbn = {979-8-4007-0475-8},
}

@inproceedings{feng_cmdbench_2024,
  title = {{CMDBench},
  author = {Feng, Yanlin and Rahman, Sajjadur and Feng, Aaron and Chen, Vincent and Kandogan, Eser},
  year = {2024},
  doi = {10.1145/3665601.3669846},
  url = {https://doi.org/10.1145/3665601.3669846},
  booktitle = {Proceedings of the {Conference},
  pages = {16--25},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {Benchmark, Compound AI Systems., Data Discovery, LLMs},
  abstract = {Compound AI systems (CASs) that employ LLMs as agents to accomplish knowledge-intensive tasks via interactions with tools and data retrievers have garnered significant interest within database and AI communities. While these systems have the potential to supplement typical analysis workflows of data analysts in enterprise data platforms, unfortunately, CASs are subject to the same data discovery challenges that analysts have encountered over the years — silos of multimodal data sources, created across teams and departments within an organization, make it difficult to identify appropriate data sources for accomplishing the task at hand. Existing data discovery benchmarks do not model such multimodality and multiplicity of data sources. Moreover, benchmarks of CASs prioritize only evaluating end-to-end task performance. To catalyze research on evaluating the data discovery performance of multimodal data retrievers in CASs within a real-world setting, we propose CMDBench, a benchmark modeling the complexity of enterprise data platforms. We adapt existing datasets and benchmarks in open-domain — from question answering and complex reasoning tasks to natural language querying over structured data — to evaluate coarse- and fine-grained data discovery and task execution performance. Our experiments reveal the impact of data retriever design on downstream task performance — 46\% drop in task accuracy on average — across various modalities, data sources, and task difficulty. The results indicate the need to develop optimization strategies to identify appropriate LLM agents and retrievers for efficient execution of CASs over enterprise data.},
  address = {New York, NY, USA},
  series = {{GUIDE},
  isbn = {979-8-4007-0694-3},
}

@inproceedings{bai_leveraging_2024,
  title = {Leveraging {Large},
  author = {Bai, Xiao and Wu, Xue and Stojkovic, Ivan and Tsioutsiouliklis, Kostas},
  year = {2024},
  doi = {10.1145/3627673.3680093},
  url = {https://doi.org/10.1145/3627673.3680093},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4349--4357},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {keyphrase generation, knowledge distillation, large language models, LLM},
  abstract = {Generating a set of keyphrases that convey the main concepts discussed in a document has been applied to improve various applications including document retrieval and online advertising. The state-of-the-art approaches mostly rely on the neural sequence-to-sequence framework to generate keyphrases. However, training such deep neural networks either requires a significant amount of human efforts in obtaining ground truth keyphrases or suffers from lower quality training data derived from weakly supervised signals. More recently, pre-trained language models are fine-tuned to build more data-efficient keyphrase generation models. Yet, the documents often need to be truncated to adapt to the pre-trained context window. On the other hand, large language models (LLMs) have demonstrated impressive abilities in understanding very long text and generating answers for a wide range of natural language processing tasks, making them great candidates for improving keyphrase generation. There however is a lack of a systematic study on how to use LLMs, especially in an industrial setting that requires low generation latency. In this work, we present an empirical study to facilitate a more informed use of LLMs for keyphrase generation. We compare zero-shot and few-shot in-context learning with parameter efficient fine-tuning using a number of open-source LLMs. We show that using only a handful of well selected human annotated samples, the LLMs already outperform the fine-tuned language model baselines. When thousands of human labeled samples are available, fine-tuned large language models significantly improve the amount and the quality of the generated keyphrases. To enable efficient keyphrase generation at scale, we distill the knowledge from LLMs to a base-size language model. Our evaluation shows significant increase in user reach when the generated keyphrases are used for contextual targeting at Yahoo.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{cheng_enhancing_2025,
  title = {Enhancing {Semantic},
  author = {Cheng, Baijun and Wang, Kailong and Shi, Ling and Wang, Haoyu and Guo, Yao and Li, Ding and Chen, Xiangqun},
  year = {2025},
  doi = {10.1145/3759425.3763393},
  url = {https://doi.org/10.1145/3759425.3763393},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {112--117},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {LLM, pointer analysis, semantic understanding},
  abstract = {Pointer analysis has been studied for over four decades. However, existing frameworks continue to suffer from the propagation of incorrect facts. A major limitation stems from their insufficient semantic understanding of code, resulting in overly conservative treatment of user-defined functions. Recent advances in large language models (LLMs) present new opportunities to bridge this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a vision that integrates LLMs into pointer analysis to enhance both precision and scalability. LMPA identifies user-definedfunctions that resemble system APIs and models them accordingly, thereby mitigating erroneous cross-calling-context propagation. Furthermore, it enhances summary-based analysis by inferring initial points-to sets and introducing a novel summary strategy augmented with natural language. Finally, we discuss the key challenges involved in realizing this vision.},
  address = {New York, NY, USA},
  series = {{LMPL},
  isbn = {979-8-4007-2148-9},
}

@inproceedings{yan_generative_2024,
  title = {Generative {Artificial},
  author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Gasevic, Dragan},
  year = {2024},
  doi = {10.1145/3636555.3636856},
  url = {https://doi.org/10.1145/3636555.3636856},
  booktitle = {Proceedings of the 14th {Learning},
  pages = {101--111},
  publisher = {Association for Computing Machinery},
  note = {event-place: Kyoto, Japan},
  keywords = {ChatGPT, educational technology, generative artificial intelligence, human-AI collaboration, learning analytics, Midjourney},
  abstract = {Generative artificial intelligence (GenAI), exemplified by ChatGPT, Midjourney, and other state-of-the-art large language models and diffusion models, holds significant potential for transforming education and enhancing human productivity. While the prevalence of GenAI in education has motivated numerous research initiatives, integrating these technologies within the learning analytics (LA) cycle and their implications for practical interventions remain underexplored. This paper delves into the prospective opportunities and challenges GenAI poses for advancing LA. We present a concise overview of the current GenAI landscape and contextualise its potential roles within Clow’s generic framework of the LA cycle. We posit that GenAI can play pivotal roles in analysing unstructured data, generating synthetic learner data, enriching multimodal learner interactions, advancing interactive and explanatory analytics, and facilitating personalisation and adaptive interventions. As the lines blur between learners and GenAI tools, a renewed understanding of learners is needed. Future research can delve deep into frameworks and methodologies that advocate for human-AI collaboration. The LA community can play a pivotal role in capturing data about human and AI contributions and exploring how they can collaborate most effectively. As LA advances, it is essential to consider the pedagogical implications and broader socioeconomic impact of GenAI for ensuring an inclusive future.},
  address = {New York, NY, USA},
  series = {{LAK},
  isbn = {979-8-4007-1618-8},
}

@inproceedings{ujwal_reasoning_2024,
  title = {"{Reasoning},
  author = {Ujwal, Utkarsh and Surampudi, Sai Sri Harsha and Mitra, Sayantan and Saha, Tulika},
  year = {2024},
  doi = {10.1145/3627673.3680082},
  url = {https://doi.org/10.1145/3627673.3680082},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4922--4930},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {interpretability, large language models, legal domain, long-form question answering},
  abstract = {Long-Form Question Answering (LFQA) represents a growing interest in Legal Natural Language Processing (Legal-NLP) as many individuals encounter legal disputes at some point in their lives, but lack of knowledge about how to negotiate these complex situations might put them at risk. The endeavor to generate detailed answers to contextually rich legal questions has faced challenges, primarily due to the limited availability of specialized datasets involving intensive manual effort or incapability of existing LFQA models to produce informative responses. Addressing this, our research introduces a semi-synthetic dataset, Legal-LFQA (L2FQA) created by exploiting a large language model (LLM) and utilizing contexts derived from existing legal datasets. Additionally, we hypothesize that integrating legal reasoning into the answer generation process of the LLMs will help bolster both the quality and interpretability of the produced responses. We systematically analyze the quality of L2FQA using human evaluation and natural language inference based metrics. Next, we benchmark L2FQA on a wide range of general-purpose and domain-specific LLMs using fine-tuning and in-context learning (with zero, one and few shot) strategies. The efficacy of these techniques is gauged through several automated and human evaluations. Results indicate that incorporating legal reasoning into the answer generation process provides an avenue for improving the quality of responses in the context of Legal-LFQA task. By addressing the challenges faced in LFQA and emphasizing the potential of interpretability, this research contributes to the foundational work in enhancing question-answering systems within the legal domain.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{upadhyay_large-scale_2025,
  title = {A {Large},
  author = {Upadhyay, Shivani and Pradeep, Ronak and Thakur, Nandan and Campos, Daniel and Craswell, Nick and Soboroff, Ian and Lin, Jimmy},
  year = {2025},
  doi = {10.1145/3731120.3744605},
  url = {https://doi.org/10.1145/3731120.3744605},
  booktitle = {Proceedings of the 2025 {International},
  pages = {358--368},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {evaluation, llm judge, relevance assessment},
  abstract = {There is substantial interest in applying large language models (LLMs) to provide relevance assessments in information retrieval (IR) applications from both industry and academia. To date, researchers and practitioners have presented several studies, but many questions remain. In this paper, we examine four different relevance assessment strategies: a fully manual process and three variants that rely on LLMs to different extents using our tool called UMBRELA. These were deployed in the TREC 2024 RAG Track on a diverse set of 77 runs from 19 teams in situ, which allowed us to correlate system rankings induced by the different approaches and to characterize tradeoffs between cost and quality. We find that system rankings produced by the three LLM-based strategies correlate well at the run level with those produced by fully manual assessments in terms of nDCG@20, nDCG@100, and Recall@100. On a topic-by-topic basis, the correlations are lower, and results using our setup indicate that increased human involvement does not improve correlations sufficiently to justify their costs. Our study suggests that LLMs can potentially replace fully manual judgments to measure run-level effectiveness in a coarse-grained manner.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{namvarpour_art_2025,
  title = {The {Art},
  author = {Namvarpour, Mohammad (Matt) and Razi, Afsaneh},
  year = {2025},
  doi = {10.1145/3719160.3736621},
  url = {https://doi.org/10.1145/3719160.3736621},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational User Interfaces, CUI, Literature Review, Survey},
  abstract = {Conversational User Interfaces (CUIs) enable human-like interactions via voice, text, and multimodal communication, driven by natural language processing and machine learning. Prior literature reviews have primarily focused on specific application domains or design aspects, lacking an integrated, multi-dimensional analysis. This study addresses this gap by providing a structured framework synthesizing CUI research into interface design, system development, and ethical considerations. Our analysis highlights advancements in CUI design, such as dialogue structure, multimodal interactions, and adaptability. It also reveals persistent challenges, including bias in persona design, trust calibration, and data privacy. System development benefits from improvements in NLP, conversation memory, and multilingual capabilities. Ethical considerations, including social bias, user autonomy, and transparency, remain central to discussions on responsible CUI design. By analyzing existing research, we identify key gaps and suggest future directions, including multilingual and culturally adaptive CUIs, privacy-preserving AI techniques, and enhanced reasoning mechanisms for context-aware interactions.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{hoffmann_malinowski_2025,
  title = {Malinowski in the {Age},
  author = {Hoffmann, Michael and Fillies, Jan and Paschke, Adrian},
  year = {2025},
  doi = {10.1145/3719236.3719240},
  url = {https://doi.org/10.1145/3719236.3719240},
  booktitle = {Proceedings of the 21st {International},
  publisher = {Association for Computing Machinery},
  keywords = {Big Data, Computational Anthropology., Design Thinking, Educational Games, Machine Learning, Prompt Engineering},
  abstract = {Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4 have shown remarkable abilities in a wide range of tasks such as summarizing texts and assisting in coding. Scientific research has demonstrated that these models can also play text-adventure games. This study aims to explore whether LLMs can autonomously create text-based games based on anthropological classics, evaluating also their effectiveness in communicating knowledge. To achieve this, the study engaged anthropologists in discussions to gather their expectations and design inputs for an anthropologically themed game. Through iterative processes following the established HCI principle of ’design thinking’, the prompts and the conceptual framework for crafting these games were refined. Leveraging GPT3.5, the study created three prototypes of games centered around the seminal anthropological work of the social anthropologist’s Bronislaw Malinowski’s "Argonauts of the Western Pacific“ (1922). Subsequently, evaluations were conducted by inviting senior anthropologists to playtest these games, and based on their inputs, the game designs were refined. The tests revealed promising outcomes but also highlighted key challenges: the models encountered difficulties in providing in-depth thematic understandings, showed suspectibility to misinformation, tended towards monotonic responses after an extended period of play, and struggled to offer detailed biographical information. Despite these limitations, the study’s findings open up new research avenues at the crossroads of artificial intelligence, machine learning, LLMs, ethnography, anthropology and human-computer interaction.},
  address = {New York, NY, USA},
  series = {{KUI},
  isbn = {979-8-4007-1032-2},
}

@inproceedings{deng_next_2025,
  title = {The {Next},
  author = {Deng, Xingyu and Wang, Xi and Stevenson, Mark},
  year = {2025},
  doi = {10.1145/3731120.3744614},
  url = {https://doi.org/10.1145/3731120.3744614},
  booktitle = {Proceedings of the 2025 {International},
  pages = {436--448},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {evidence retrieval, scientific fact-checking},
  abstract = {Scientific fact-checking aims to determine the veracity of scientific claims by retrieving and analysing evidence from research literature. The problem is inherently more complex than general fact-checking since it must accommodate the evolving nature of scientific knowledge, the structural complexity of academic literature and the challenges posed by long-form, multimodal scientific expression. However, existing approaches focus on simplified versions of the problem based on small-scale datasets consisting of abstracts rather than full papers, thereby avoiding the distinct challenges associated with processing complete documents. This paper examines the limitations of current scientific fact-checking systems and reveals the many potential features and resources that could be exploited to advance their performance. It identifies key research challenges within evidence retrieval, including (1) evidence-driven retrieval that addresses semantic limitations and topic imbalance (2) time-aware evidence retrieval with citation tracking to mitigate outdated information, (3) structured document parsing to leverage long-range context, (4) handling complex scientific expressions, including tables, figures, and domain-specific terminology and (5) assessing the credibility of scientific literature. Preliminary experiments were conducted to substantiate these challenges and identify potential solutions. This perspective paper aims to advance scientific fact-checking with a specialised IR system tailored for real-world applications.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{abdallah_arabicaqa_2024,
  title = {{ArabicaQA},
  author = {Abdallah, Abdelrahman and Kasem, Mahmoud and Abdalla, Mahmoud and Mahmoud, Mohamed and Elkasaby, Mohamed and Elbendary, Yasser and Jatowt, Adam},
  year = {2024},
  doi = {10.1145/3626772.3657889},
  url = {https://doi.org/10.1145/3626772.3657889},
  booktitle = {Proceedings of the 47th {International},
  pages = {2049--2059},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {arabic question answering, information retrieval, llm, question generation},
  abstract = {In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking of LLMs in Arabic question answering offer significant advancements in the field of Arabic NLP. The dataset and code are publicly accessible for further research https://github.com/DataScienceUIBK/ArabicaQA.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{tian_respark_2025,
  title = {{ReSpark},
  author = {Tian, Yuan and Zhang, Chuhan and Wang, Xiaotong and Pan, Sitong and Cui, Weiwei and Zhang, Haidong and Deng, Dazhen and Wu, Yingcai},
  year = {2025},
  doi = {10.1145/3746059.3747644},
  url = {https://doi.org/10.1145/3746059.3747644},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Data Reports, Large Language Models, Visualization Generation},
  abstract = {Creating data reports is a labor-intensive task involving iterative data exploration, insight extraction, and narrative construction. A key challenge lies in composing the analysis logic-from defining objectives and transforming data to identifying and communicating insights. Manually crafting this logic can be cognitively demanding. While experienced analysts often reuse scripts from past projects, finding a perfect match for a new dataset is rare. Even when similar analyses are available online, they usually share only results or visualizations, not the underlying code, making reuse difficult. To address this, we present ReSpark, a system that leverages large language models (LLMs) to reverse-engineer analysis logic from existing reports and adapt it to new datasets. By generating draft analysis steps, ReSpark provides a warm start for users. It also supports interactive refinement, allowing users to inspect intermediate outputs, insert objectives, and revise content. We evaluate ReSpark through comparative and user studies, demonstrating its effectiveness in lowering the barrier to generating data reports without relying on existing analysis code.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{dhruv_leveraging_2025,
  title = {Leveraging {Large},
  author = {Dhruv, Akash and Dubey, Anshu},
  year = {2025},
  doi = {10.1145/3732775.3733572},
  url = {https://doi.org/10.1145/3732775.3733572},
  booktitle = {Proceedings of the {Platform},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  note = {event-place: FHNW University of Applied Sciences and Arts Northwestern Switzerland, Brugg-Windisch, Switzerland},
  keywords = {code translation, generative artificial intelligence, language interoperability, large language models, prompt engineering, scientific computing, software development},
  abstract = {The emergence of foundational models and generative artificial intelligence (GenAI) is poised to transform productivity in scientific computing, especially in code development, refactoring, and translating from one programming language to another. However, because the output of GenAI cannot be guaranteed to be correct, manual intervention remains necessary. Some of this intervention can be automated through task-specific tools, alongside additional methodologies for correctness verification and effective prompt development. We explored the application of GenAI in assisting with code translation, language interoperability, and codebase inspection within a legacy Fortran codebase used to simulate particle interactions at the Large Hadron Collider (LHC). In the process, we developed a tool, CodeScribe, which combines prompt engineering with user supervision to establish an efficient process for code conversion. In this paper, we demonstrate how CodeScribe assists in converting Fortran code to C++, generating Fortran-C APIs for integrating legacy systems with modern C++ libraries, and providing developer support for code organization and algorithm implementation. We also address the challenges of AI-driven code translation and highlight its benefits for enhancing productivity in scientific computing workflows.},
  address = {New York, NY, USA},
  series = {{PASC},
  isbn = {979-8-4007-1886-1},
}

@inproceedings{golgoon_mechanistic_2024,
  title = {Mechanistic interpretability of large language models with applications to the financial services industry},
  author = {Golgoon, Ashkan and Filom, Khashayar and Ravi Kannan, Arjun},
  year = {2024},
  doi = {10.1145/3677052.3698612},
  url = {https://doi.org/10.1145/3677052.3698612},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {660--668},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {FinTech, Large Language Models (LLMs), Mechanistic Interpretability, Natural Language Processing., Transformer Circuits},
  abstract = {Large Language Models exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small’s attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads 10.2 (head 2, layer 10), 10.7, and 11.3, as well as the (negative) heads 9.6 and 10.6 play a significant role in the task completion.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{ren_llm-based_2025,
  title = {{LLM},
  author = {Ren, Ruiyang and Wang, Yuhao and Li, Junyi and Jiang, Jinhao and Zhao, Wayne Xin and Wang, Wenjie and Chua, Tat-Seng},
  year = {2025},
  doi = {10.1145/3726302.3730025},
  url = {https://doi.org/10.1145/3726302.3730025},
  booktitle = {Proceedings of the 48th {International},
  pages = {1098--1108},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {intricate information seeking, large language models, monte-carlo tree search},
  abstract = {In the era of vast digital information, the sheer volume and heterogeneity of available information present significant challenges for intricate information seeking. Users frequently face multistep web search tasks that involve navigating vast and varied data sources. This complexity demands every step remains comprehensive, accurate, and relevant. However, traditional search methods often struggle to balance the need for localized precision with the broader context required for holistic understanding, leaving critical facets of intricate queries underexplored. In this paper, we introduce an LLM-based search assistant that adopts a new information seeking paradigm with holistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task as a progressive information collection process with a knowledge memory and unite an adaptive checklist with multi-perspective reward modeling in MCTS. The adaptive checklist provides explicit sub-goals to guide the MCTS process toward comprehensive coverage of complex user queries. Simultaneously, our multi-perspective reward modeling offers both exploration and retrieval rewards, along with progress feedback that tracks completed and remaining sub-goals, refining the checklist as the tree search progresses. By striking a balance between localized tree expansion and global guidance, HG-MCTS reduces redundancy in search paths and ensures that all crucial aspects of an intricate query are properly addressed. Extensive experiments on real-world intricate information seeking tasks demonstrate that HG-MCTS acquires thorough knowledge collections and delivers more accurate final responses compared with existing baselines.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{cai_rtbagent_2025,
  title = {{RTBAgent},
  author = {Cai, Leng and He, Junxuan and Li, Yikai and Liang, Junjie and Lin, Yuanping and Quan, Ziming and Zeng, Yawen and Xu, Jin},
  year = {2025},
  doi = {10.1145/3701716.3715259},
  url = {https://doi.org/10.1145/3701716.3715259},
  booktitle = {Companion {Proceedings},
  pages = {104--113},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {bid optimization, bidding agents, large language models, real-time bidding},
  abstract = {Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: https://github.com/CaiLeng/RTBAgent.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{cho_fishnet_2024,
  title = {{FISHNET},
  author = {Cho, Nicole and Srishankar, Nishan and Cecchi, Lucas and Watson, William},
  year = {2024},
  doi = {10.1145/3677052.3698597},
  url = {https://doi.org/10.1145/3677052.3698597},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {591--599},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Harmonizing, LLM Agents, Planning, Sub-querying, Swarming, source: Scopus},
  abstract = {Financial intelligence generation from vast data sources has typically relied on traditional methods of knowledge-graph construction or database engineering. Recently, fine-tuned financial domain-specific Large Language Models (LLMs), have emerged. While these advancements are promising, limitations such as high inference costs, hallucinations, and the complexity of concurrently analyzing high-dimensional financial data, emerge. This motivates our invention FISHNET (Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning), an agentic architecture that accomplishes highly complex analytical tasks for more than 98,000 regulatory filings that vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows remarkable performance for financial insight generation (61.8\% success rate over 5.0\% Routing, 45.6\% RAG R-Precision). We conduct rigorous ablations to empirically prove the success of FISHNET, each agent’s importance, and the optimized performance of assembling all agents. Our modular architecture can be leveraged for a myriad of use-cases, enabling scalability, flexibility, and data integrity that are critical for financial tasks.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
  annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{kretzer_closing_2025,
  title = {Closing the {Loop},
  author = {Kretzer, Felix and Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo and Maedche, Alexander},
  year = {2025},
  doi = {10.1145/3706598.3713932},
  url = {https://doi.org/10.1145/3706598.3713932},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Assistance, GUI Prototypes, Requirements, User Stories},
  abstract = {Graphical user interfaces (GUIs) are at the heart of almost every software we encounter. GUIs are often created through a collaborative effort involving UX designers, product owners, and software developers, constantly facing changing requirements. Historically, problems in GUI development include a fragmented, poorly integrated tool landscape and high synchronization efforts between stakeholders. Recent approaches suggest using large language models (LLMs) to recognize requirements fulfillment in GUIs and automatically propose new GUI components. Based on ten interviews with practitioners, this paper proposes an LLM-based assistant as a Figma plug-in that bridges the gap between user stories and GUI prototyping. We evaluated the prototype with 40 users and 40 crowd-workers, showing that the effectiveness of GUI creation is improved by using LLMs to detect requirements’ completion and generate new GUI components. We derive design rationales to support cross-functional integration in software development, ensuring that our plug-in integrates well into established processes.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{rao_xpf_2025,
  title = {{XPF},
  author = {Rao, Kunal and Coviello, Giuseppe and Mellone, Gennaro and De Vita, Ciro Giuseppe and Chakradhar, Srimat},
  year = {2025},
  doi = {10.1145/3731545.3743644},
  url = {https://doi.org/10.1145/3731545.3743644},
  booktitle = {Proceedings of the 34th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: University of Notre Dame Conference Facilities, Notre Dame, IN, USA},
  keywords = {agents, generative artificial intelligence (GenAI), large language models (LLM), runtimes, tools},
  abstract = {In this paper, we propose a novel agentic AI system called XPF, which enables users to create "agents" using just natural language, where each agent is capable of executing complex, real-world business workflows in an accurate and reliable manner. XPF provides an interface to develop and iterate over the agent creation process and then deploy the agent in production when satisfactory results are produced consistently. The key components of XPF include: (a) planner, which leverages LLM to generate a step-by-step plan, which can further be edited by a human (b) compiler, which leverages LLM to compile the plan into a flow graph (c) executor, which handles distributed execution of the flow graph (using LLM, tools, RAG, etc.) on an underlying cluster and (d) verifier, which helps in verification of the output (through human generated tests or auto-generated tests using LLM). We develop five different agents using XPF and conduct experiments to evaluate one particular aspect i.e. difference in accuracy and reliability of the five agents with "human-generated" vs "auto-generated" plans. Our experiments show that we can get much more accurate and reliable response for a business workflow when step-by-step instructions (in natural language) are given by a human familiar with the workflow, rather than letting the LLM figure out the execution plan steps. In particular, we observe that "human-generated" plan almost always gives 100\% accuracy whereas "auto-generated" plan almost never gives 100\% accuracy. In terms of reliability, we observe through Rouge-L, Blue and Meteor scores, that the output from "human-generated" plan is much more reliable than "auto-generated" plan.},
  address = {New York, NY, USA},
  series = {{HPDC},
  isbn = {979-8-4007-1869-4},
}

@inproceedings{santana_responsible_2025,
  title = {Responsible {Prompting},
  author = {Santana, Vagner Figueredo de and Berger, Sara E and Candello, Heloisa and Machado, Tiago and Sanctos, Cassia Sampaio and Su, Tianyu and Williams, Lemara},
  year = {2025},
  doi = {10.1145/3706598.3713365},
  url = {https://doi.org/10.1145/3706598.3713365},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Human-AI Interaction, Proactive Value Alignment, Prompt Engineering, Recommender Systems, Responsible AI, Responsible Computing, Responsible Prompting},
  abstract = {Human-Computer Interaction practitioners have been proposing best practices in user interface design for decades. However, generative Artificial Intelligence (GenAI) brings additional design considerations and currently lacks sufficient user guidance regarding affordances, inputs, and outputs. In this context, we developed a recommender system to promote responsible AI (RAI) practices while people prompt GenAI systems. We detail 10 interviews with IT professionals, the resulting recommender system developed, and 20 user sessions with IT professionals interacting with our prompt recommendations. Results indicate that responsible prompting recommendations have the potential to support novice prompt engineers and raise awareness about RAI in prompting-time. They also suggest that recommendations should simultaneously maximize both a prompt’s similarity to a user’s input as well as a diversity of associated social values provided. These findings contribute to RAI by offering practical ways to provide user guidance and enrich human-GenAI interaction via prompt recommendations.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{xu_ckgfuzzer_2025,
  title = {{CKGFuzzer},
  author = {Xu, Hanxiang and Ma, Wei and Zhou, Ting and Zhao, Yanjie and Chen, Kai and Hu, Qiang and Liu, Yang and Wang, Haoyu},
  year = {2025},
  doi = {10.1109/ICSE-Companion66252.2025.00079},
  url = {https://doi.org/10.1109/icse-companion66252.2025.00079},
  booktitle = {Proceedings of the {IEEE},
  pages = {243--254},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73\% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4\% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries. Our research enhances the overall performance of fuzz testing by refining fuzz driver generation strategies and input seed analysis, offering a more effective solution for vulnerability remediation and software quality improvement.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-3683-1},
}

@inproceedings{ashktorab_evalassist_2025,
  title = {{EvalAssist},
  author = {Ashktorab, Zahra and Desmond, Michael and Pan, Qian and Johnson, James M. and Santillán Cooper, Martin and Daly, Elizabeth M. and Nair, Rahul and Pedapati, Tejaswini and Do, Hyo Jin and Geyer, Werner},
  year = {2025},
  doi = {10.1145/3746059.3747740},
  url = {https://doi.org/10.1145/3746059.3747740},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {With the broad availability of large language models and their ability to generate vast outputs using varied prompts and configurations, determining the best output for a given task requires an intensive evaluation process, one where machine learning practitioners must decide how to assess the outputs and then carefully carry out the evaluation. This process is both time-consuming and costly. As practitioners work with an increasing number of models, they must now evaluate outputs to determine which model performs best for a given task. LLMs are increasingly used as evaluators to filter training data, evaluate model performance or assist human evaluators with detailed assessments. Our application, EvalAssist, supports this process by aiding users in interactively refining evaluation criteria. In our study with machine learning practitioners (n=15), each completing 6 tasks yielding 131 evaluations, we explore how task-related factors and judgment strategies influence criteria refinement and user perceptions. Findings show that users performed more evaluations with direct assessment by making criteria task-specific, modifying judgments, and changing the AI evaluator model. We conclude with recommendations for how systems can better support practitioners with AI-assisted evaluations.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{xie_tree-enhanced_2025,
  title = {A {Tree},
  author = {Xie, Mingxi and Lin, Yuefeng and Wang, Yike},
  year = {2025},
  doi = {10.1145/3747912.3747952},
  url = {https://doi.org/10.1145/3747912.3747952},
  booktitle = {Proceedings of the 2025 {International},
  pages = {255--260},
  publisher = {Association for Computing Machinery},
  keywords = {Automated framework, Dynamic search tree, Financial knowledge graph, Intelligent question-answering system, Multi-agent collaboration},
  abstract = {Question answering tasks in the financial futures are usually complex. It requires complex semantic parsing and dynamic optimization. To solve this issue, in this paper, we propose an enhanced question-answering system based on dynamic multi-agent search trees. In the system, we construct a hierarchical tree based on dynamic search using a multi-agent collaborative method. We integrated semantic association networks based on financial knowledge graphs. In this way, we automatically decomposed the complex queries. The key technical breakthroughs made in this paper include the following. Firstly, we build a task-planning algorithm based on multi-agent interaction. This enables dynamic expansion and pruning of search tree nodes, which manages the decomposition of domain-specific complex problems. Secondly, we established a financial knowledge graph with 150 core entities and 290 types of dynamic relationships. Using RDF triples and a semantic rule engine, it precisely expands domain terminologies and enhances cross-modal retrieval. Thirdly, we constructed a security-controlled automated framework design. This design uses a tool invocation-execution feedback loop and a hierarchical summarization mechanism (BM25 retrieval + context compression). In this way, it can reduce long-text processing time and ensure data permission control. According to the experiments, the system can answer specialized questions in different tasks, such as treasury futures trend analysis. It also supports fully automated reasoning and can be traced throughout the process. In this research, we provide an innovative solution for intelligent question-answering systems in financial futures. This method combines dynamic adaptability, domain expertise, and security reliability.},
  address = {New York, NY, USA},
  series = {{SECA},
  isbn = {979-8-4007-1513-6},
}

@inproceedings{mayfield_evaluation_2024,
  title = {On the {Evaluation},
  author = {Mayfield, James and Yang, Eugene and Lawrie, Dawn and MacAvaney, Sean and McNamee, Paul and Oard, Douglas W. and Soldaini, Luca and Soboroff, Ian and Weller, Orion and Kayi, Efsun and Sanders, Kate and Mason, Marc and Hibbler, Noah},
  year = {2024},
  doi = {10.1145/3626772.3657846},
  url = {https://doi.org/10.1145/3626772.3657846},
  booktitle = {Proceedings of the 47th {International},
  pages = {1904--1915},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {evaluation, factual citation, report generation, text analysis},
  abstract = {Large Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and—critically—a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable—if not required—in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{han_event_2024,
  title = {Event {Traffic},
  author = {Han, Xiao and Zhang, Zhenduo and Wu, Yiling and Zhang, Xinfeng and Wu, Zhe},
  year = {2024},
  doi = {10.1145/3664647.3680706},
  url = {https://doi.org/10.1145/3664647.3680706},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {8855--8864},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {dataset, event traffic forecasting, multimodal fusion, time-series and language},
  abstract = {With the development of deep learning, traffic forecasting technology has made significant progress and is being applied in many practical scenarios. However, various events held in cities, such as sporting events, exhibitions, concerts, etc., have a significant impact on traffic patterns of surrounding areas, causing current advanced prediction models to fail in this case. In this paper, to broaden the applicable scenarios of traffic forecasting, we focus on modeling the impact of events on traffic patterns and propose an event traffic forecasting problem with multimodal inputs. We outline the main challenges of this problem: diversity and sparsity of events, as well as insufficient data. To address these issues, we first use textual modal data containing rich semantics to describe the diverse characteristics of events. Then, we propose a simple yet effective multi-modal event traffic forecasting model that uses pre-trained text and traffic encoders to extract the embeddings and fuses the two embeddings for prediction. Encoders pre-trained on large-scale data have powerful generalization abilities to cope with the challenge of sparse data. Next, we design an efficient large language model-based event description text generation pipeline to build multi-modal event traffic forecasting datasets, ShenzhenCEC and SuzhouIEC. Experiments on two real-world datasets show that our method achieves state-of-the-art performance compared with eight baselines, reducing mean absolute error during the event peak period by 4.26\%. Code is available at: https://github.com/2448845600/EventTrafficForecasting.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{kobiella_when_2025,
  title = {When {AI},
  author = {Kobiella, Charlotte and Isroilov, Ulugbek and Schmidt, Albrecht},
  year = {2025},
  doi = {10.1145/3719160.3736630},
  url = {https://doi.org/10.1145/3719160.3736630},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {AI agents, equitability, negotiation, respect, trust},
  abstract = {Negotiation is a crucial decision-making process where parties seek to resolve differences and optimize outcomes. While prior research has focused on maximizing negotiation outcomes, fostering a collaborative atmosphere is essential for long-term relationship-building. This study explores the role of AI-assisted moderation in negotiations that emulate high-stress environments. We developed a text-based AI moderator and evaluated its usability and effectiveness in a two-phase study: a pilot study with 14 participants followed by a final user study with 16 participants. To provide an initial point of comparison, we assessed trust, respect, and equitability in AI-moderated versus non-moderated negotiations. Quantitative findings indicate a negative effect of AI-assisted moderation on relationship-building, while qualitative insights suggest that AI moderation fosters collaboration. However, the cognitive load of text-based facilitation hinders its effectiveness. These results highlight the importance of seamless AI integration and contribute to the broader discourse on AI’s role in behavior change and mediated communication.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{dudy_unequal_2025,
  title = {Unequal {Opportunities},
  author = {Dudy, Shiran and Tholeti, Thulasi and Ramachandranpillai, Resmi and Ali, Muhammad and Li, Toby Jia-Jun and Baeza-Yates, Ricardo},
  year = {2025},
  doi = {10.1145/3708359.3712111},
  url = {https://doi.org/10.1145/3708359.3712111},
  booktitle = {Proceedings of the 30th {International},
  pages = {1499--1516},
  publisher = {Association for Computing Machinery},
  keywords = {Cultural representation, geographical divide, LLM auditing., LLM biases, under-represented topics},
  abstract = {Recent advancements in Large Language Models (LLMs) have made them a popular information-seeking tool among end users. However, the statistical training methods for LLMs have raised concerns about their representation of under-represented topics, potentially leading to biases that could influence real-world decisions and opportunities. These biases could have significant economic, social, and cultural impacts as LLMs become more prevalent, whether through direct interactions—such as when users engage with chatbots or automated assistants—or through their integration into third-party applications (as agents), where the models influence decision-making processes and functionalities behind the scenes. Our study examines the biases present in LLMs recommendations of U.S. cities and towns across three domains: relocation, tourism, and starting a business. We explore two key research questions: (i) How similar LLM responses are, and (ii) How this similarity might favor areas with certain characteristics over others, introducing biases. We focus on the consistency of LLM responses and their tendency to over-represent or under-represent specific locations. Our findings point to consistent demographic biases in these recommendations, which could perpetuate a “rich-get-richer” effect that widens existing economic disparities.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{jiang_peatmoss_2024,
  title = {{PeaTMOSS},
  author = {Jiang, Wenxin and Yasmin, Jerin and Jones, Jason and Synovic, Nicholas and Kuo, Jiashen and Bielanski, Nathaniel and Tian, Yuan and Thiruvathukal, George K. and Davis, James C.},
  year = {2024},
  doi = {10.1145/3643991.3644907},
  url = {https://doi.org/10.1145/3643991.3644907},
  booktitle = {Proceedings of the 21st {International},
  pages = {431--443},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {datasets, deep neural networks, empirical software engineering, machine learning, model zoos, open-source, package registries},
  abstract = {The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.Our artifact is available at https://github.com/PurdueDualityLab/PeaTMOSS-Artifact. Our dataset is available at https://transfer.rcac.purdue.edu/file-manager?origin\_id=ff978999-16c2-4b50-ac7a-947ffdc3eb1d\&amp;origin\_path=\%2F.},
  address = {New York, NY, USA},
  series = {{MSR},
  isbn = {979-8-4007-0587-8},
}

@inproceedings{hoveyda_adaptive_2025,
  title = {Adaptive {Orchestration},
  author = {Hoveyda, Mohanna and Oosterhuis, Harrie and de Vries, Arjen P. and de Rijke, Maarten and Hasibi, Faegheh},
  year = {2025},
  doi = {10.1145/3726302.3730351},
  url = {https://doi.org/10.1145/3726302.3730351},
  booktitle = {Proceedings of the 48th {International},
  pages = {3899--3910},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {adaptive information systems, graph-based orchestration, source: ACM},
  abstract = {Advancements in large language models (LLMs) have driven the emergence of complex new systems to provide access to information, that we will collectively refer to as modular generative information access (GenIA) systems. They integrate a broad and evolving range of specialized components, including LLMs, retrieval models, and a heterogeneous set of sources and tools. While modularity offers flexibility, it also raises critical challenges: How can we systematically characterize the space of possible modules and their interactions? How can we automate and optimize interactions among these heterogeneous components? And, how do we enable this modular system to dynamically adapt to varying user query requirements and evolving module capabilities? In this perspective paper, we argue that the architecture of future modular generative information access systems will not just assemble powerful components, but enable a self-organizing system through real-time adaptive orchestration - where components' interactions are dynamically configured for each user input, maximizing information relevance while minimizing computational overhead. We give provisional answers to the questions raised above with a roadmap that depicts the key principles and methods for designing such an adaptive modular system. We identify pressing challenges, and propose avenues for addressing them in the years ahead. This perspective urges the IR community to rethink modular system designs for developing adaptive, self-optimizing, and future-ready architectures that evolve alongside their rapidly advancing underlying technologies.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{burgueno_human_2024,
  title = {A {Human},
  author = {Burgueño, Lola and Keet, Maria and Kienzle, Jörg and Michael, Judith and Babur, Önder},
  year = {2024},
  doi = {10.1145/3652620.3687806},
  url = {https://doi.org/10.1145/3652620.3687806},
  booktitle = {Proceedings of the {ACM},
  pages = {578--586},
  publisher = {Association for Computing Machinery},
  note = {event-place: Linz, Austria},
  keywords = {cyber-physical systems, digital twin, human behavior, large language models, user scenario},
  abstract = {In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case.An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation.},
  address = {New York, NY, USA},
  series = {{MODELS},
  isbn = {979-8-4007-0622-6},
}

@inproceedings{laban_lexi_2024,
  title = {{LEXI},
  author = {Laban, Guy and Laban, Tomer and Gunes, Hatice},
  year = {2024},
  doi = {10.1145/3687272.3688296},
  url = {https://doi.org/10.1145/3687272.3688296},
  booktitle = {Proceedings of the 12th {International},
  pages = {250--259},
  publisher = {Association for Computing Machinery},
  note = {event-place: Swansea, United Kingdom},
  keywords = {Behavioural Experimentation, Human–Agent Interaction, Large Language Models, LLM, Open-Source, Usability Testing},
  abstract = {The recent developments in Large Language Models (LLMs) mark a significant moment in the research and development of social interactions with artificial agents. These agents are widely deployed in a variety of settings, with potential impact on users. However, the study of social interactions with agents powered by LLMs is still emerging, limited by access to the technology and to data, the absence of standardised interfaces, and challenges to establishing controlled experimental setups using the currently available platforms. To address these gaps, we developed LEXI, LLMs Experimentation Interface, an open-source tool for deploying artificial agents powered by LLMs in social interaction behavioural experiments. Using a graphical interface, LEXI allows researchers to build agents and deploy them in experimental setups along with forms for collecting self-reported data while collecting interaction logs. The outcomes of usability testing indicate LEXI’s broad utility, high usability, and minimal mental workload requirement, with benefits observed across disciplines. A proof-of-concept study exploring the tool’s efficacy in evaluating social human–agent interactions was conducted, resulting in high-quality data. A comparison of empathetic versus neutral agents indicated that people perceive empathetic agents as more social, and write longer and more positive messages towards them.},
  address = {New York, NY, USA},
  series = {{HAI},
  isbn = {979-8-4007-1178-7},
}

@inproceedings{fan_towards_2025,
  title = {Towards {Resilient},
  author = {Fan, Xiaojing and Tao, Chunliang},
  year = {2025},
  doi = {10.1145/3719384.3719447},
  url = {https://doi.org/10.1145/3719384.3719447},
  booktitle = {Proceedings of the 2024 7th {Artificial},
  pages = {429--436},
  publisher = {Association for Computing Machinery},
  keywords = {Adversarial Attacks, Computational Efficiency, Large Language Models, Robustness},
  abstract = {With the increasing demand for practical applications of Large Language Models (LLMs), many attention-efficient models have been developed to balance performance and computational cost. However, the adversarial robustness of these models remains under-explored. In this work, we design a framework to investigate the trade-off between efficiency, performance, and adversarial robustness of LLMs and conduct extensive experiments on three prominent models with varying levels of complexity and efficiency – Transformer++, Gated Linear Attention (GLA) Transformer, and MatMul-Free LM – utilizing the GLUE and AdvGLUE datasets. The AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to challenge model robustness. Our results show that while the GLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate higher efficiency and either superior or comparative robustness on AdvGLUE tasks compared to Transformer++ across different attack levels. These findings highlight the potential of simplified architectures to achieve a compelling balance between efficiency, performance, and adversarial robustness, offering valuable insights for applications where resource constraints and resilience to adversarial attacks are critical.},
  address = {New York, NY, USA},
  series = {{AICCC},
  isbn = {979-8-4007-1792-5},
}

@inproceedings{gao_social_2025,
  title = {Social {Context},
  author = {Gao, Jinfei and Wang, Xiao and Gan, Tian and Yin, Jianhua and Luo, Chuanchen and Nie, Liqiang},
  year = {2025},
  doi = {10.1145/3726302.3730085},
  url = {https://doi.org/10.1145/3726302.3730085},
  booktitle = {Proceedings of the 48th {International},
  pages = {1995--2005},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {community portrait, graph neural networks, information diffusion, information pathway, llm},
  abstract = {With the increasing prevalence of online communities, social networks have become pivotal platforms for information propagation. However, this rise is accompanied by issues such as the spread of misinformation and online rumors. Community Level Information Pathway Prediction (CLIPP) is proposed to effectively stop the propagation of harmful information within specific communities. While progress has been made in understanding user-level propagation, there is a significant gap in addressing the CLIPP problem at the community level, particularly with regard to social context interpretation and the cold start problem in niche communities. To bridge this gap, we propose a novel model, named Community-Level Propagation Prediction with LLM enhanced Social Context Interpretation and Community Coldstart (ComPaSC3), which integrates three primary modules. The video enhancement module leverages LLMs to enrich the interpretation of multimedia content by embedding world knowledge. The community portrait building module utilizes LLMs to generate detailed community portraits for community interpretation. To tackle the community cold start problem, the dynamic commLink module links non-popular communities to the popular ones based on their portrait similarity, and dynamically updates their relationship weights. Our experimental results demonstrate that ComPaSC3 significantly improves predictive accuracy in both popular and non-popular scenarios. Particularly in non-popular communities, our approach outperforms existing state-of-the-art methods, achieving improvements of 10.00\% - 15.20\% in Rec@5 and 7.31\% - 12.32\% in NDCG@10.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{zhao_knoll_2025,
  title = {Knoll: {Creating},
  author = {Zhao, Dora and Yang, Diyi and Bernstein, Michael S.},
  year = {2025},
  doi = {10.1145/3746059.3747711},
  url = {https://doi.org/10.1145/3746059.3747711},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {data curation, end-user customization, large language models},
  abstract = {Large language models are designed to encode general purpose knowledge about the world from Internet data. Yet, a wealth of information falls outside this scope — ranging from personal preferences to organizational policies, from community-specific advice to up-to-date news — that users want models to access but remains unavailable. In this paper, we propose a knowledge ecosystem in which end-users can create, curate, and configure custom knowledge modules that are utilized by language models, such as ChatGPT and Claude. To support this vision, we introduce Knoll, a software infrastructure that allows users to make modules by clipping content from the web or authoring shared documents on Google Docs and GitHub, add modules that others have made, and rely on the system to insert relevant knowledge when interacting with an LLM. We conduct a public deployment of Knoll reaching over 200 users who employed the system for a diverse set of tasks including personalized recommendations, advice-seeking, and writing assistance. Knoll improves the quality of generated responses with participants preferring responses generated with Knoll over baseline GPT-4o responses for 81.5\% of the queries when external knowledge is needed.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{whitehead_conversational_2025,
  title = {Conversational {Interactions},
  author = {Whitehead, Jim and Wessel, Thomas and Chen, Blythe and Cruz-James, Raven and Harnist, Luc and Klunder, William and Lam, Justin and Lin, Ethan and Luo, Roman and Nguyen, Hung and Poddar, Naitik and Ravinutula, Shiva and Montoreano, Alejandro and Shehane, Logan and Sims, Yazmyn and Spangler, Jarod and Tan, Michelle and Trela, Zosia},
  year = {2025},
  doi = {10.1145/3723498.3723788},
  url = {https://doi.org/10.1145/3723498.3723788},
  booktitle = {Proceedings of the 20th {International},
  publisher = {Association for Computing Machinery},
  keywords = {conversational interaction with procedural generators, large language models, natural language interaction, Procedural content generation},
  abstract = {This paper explores the potential of Large Language Models (LLMs) to facilitate conversational natural language interactions to aid humans in mixed-initiative generation of game worlds. This paper explores the issues in creating a system that allows for rapid iteration in a turn-based user-LLM design software. We identify key research topics, including game world representation in LLMs, natural language-based world manipulation using function calls, and direct manipulation from processed LLM output. We successfully created a QA dataset to compare the accuracy of leading models using text, images, or both. Our findings highlight the potential of LLMs to assist in procedural content generation through enhanced natural language interaction and conversations while revealing the challenges of in-game world manipulation that warrant further research.},
  address = {New York, NY, USA},
  series = {{FDG},
  isbn = {979-8-4007-1856-4},
}

@inproceedings{wu_cardioai_2025,
  title = {{CardioAI},
  author = {Wu, Siyi and Cao, Weidan and Fu, Shihan and Yao, Bingsheng and Yang, Ziqi and Yin, Changchang and Mishra, Varun and Addison, Daniel and Zhang, Ping and Wang, Dakuo},
  year = {2025},
  doi = {10.1145/3706598.3714272},
  url = {https://doi.org/10.1145/3706598.3714272},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Cancer treatment-induced cardiotoxicity, Human-AI collaboration, Large Language Models, Multimodal AI system},
  abstract = {Despite recent advances in cancer treatments that prolong patients’ lives, treatment-induced cardiotoxicity (i.e., the various heart damages caused by cancer treatments) emerges as one major side effect. The clinical decision-making process of cardiotoxicity is challenging, as early symptoms may happen in non-clinical settings and are too subtle to be noticed until life-threatening events occur at a later stage; clinicians already have a high workload focusing on the cancer treatment, no additional effort to spare on the cardiotoxicity side effect. Our project starts with a participatory design study with 11 clinicians to understand their decision-making practices and their feedback on an initial design of an AI-based decision-support system. Based on their feedback, we then propose a multimodal AI system, CardioAI, that can integrate wearables data and voice assistant data to model a patient’s cardiotoxicity risk to support clinicians’ decision-making. We conclude our paper with a small-scale heuristic evaluation with four experts and the discussion of future design considerations.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{alves_da_veiga_generative_2023,
  title = {Generative {Ominous},
  author = {Alves da Veiga, Pedro},
  year = {2023},
  doi = {10.1145/3623462.3623475},
  url = {https://doi.org/10.1145/3623462.3623475},
  booktitle = {Proceedings of the 20th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {source: ACM},
  abstract = {The advent of generative AI artworks has paved the way for ground-breaking explorations in the realm of digital creativity. This article delves into the multifaceted dimensions of G.O.D., an abbreviation for the art project Generative Ominous Dataset. G.O.D. aims at critically engaging with contemporary AI generative image systems and their intricate interplay with copyright issues, artistic autonomy, and the ethical implications of data collection, unravelling its conceptual underpinnings and its implications for the broader discourse on artificial intelligence, artistic agency, and the evolving contours of digital art. G.O.D. is a generative artwork, entirely coded in Processing, and developed within a/r/cography, a creative research methodology. G.O.D. scrutinizes and questions the ethics of contemporary text-to-image AI-based systems, such as Midjourney, DALL-E, or Firefly. These systems have been at the centre of controversies concerning the datasets used for their training, which encompass online sourced copyrighted materials, without authorization or attribution, masking questionable approaches with technological dazzlement. Many artists and authors find their works repurposed by these systems for the mass production of digital derivatives. G.O.D. aims at critically exposing art audiences to these concerns.},
  address = {New York, NY, USA},
  series = {{KUI},
  isbn = {979-8-4007-0836-7},
}

@inproceedings{wang_effects_2025,
  title = {The {Effects},
  author = {Wang, Sierra and Jefferson, Thomas and Piech, Chris and Mitchell, John C.},
  year = {2025},
  doi = {10.1145/3698205.3729557},
  url = {https://doi.org/10.1145/3698205.3729557},
  booktitle = {Proceedings of the {Twelfth},
  pages = {73--82},
  publisher = {Association for Computing Machinery},
  note = {event-place: Palermo, Italy},
  keywords = {ai, chatbot, cs1, global, gpt, randomized controlled trial},
  abstract = {We conducted a large-scale randomized controlled trial (RCT) in a massive, open-access, online CS1 course to examine how a chatbot's (1) placement within the course interface, (2) degree of personification, and (3) technical functionality impact student outcomes. When the chatbot was placed in the course's Integrated Development Environment instead of the lessons interface, over five times the proportion of students sent a message. Additionally, these students sent four times as many messages and asked about the homework four times as often. Subtle design choices – such as greeting students with ”Hello” – nearly doubled the percentage of students who engaged with the tool. Despite these significant differences in chatbot usage, we found no meaningful impact on student performance in the course. The study included 6,515 students from 149 countries, divided into 10 experiment groups - each receiving a distinct chatbot implementation - and one control group. Our results show that chatbot design dramatically influences how students engage with the tool, providing valuable insights for further development of AI-based pedagogical tools.},
  address = {New York, NY, USA},
  series = {L@{S},
  isbn = {979-8-4007-1291-3},
}

@inproceedings{liu_opseval_2025,
  title = {{OpsEval},
  author = {Liu, Yuhe and Pei, Changhua and Xu, Longlong and Chen, Bohan and Sun, Mingze and Zhang, Zhirui and Sun, Yongqian and Zhang, Shenglin and Wang, Kun and Zhang, Haiming and Li, Jianhui and Xie, Gaogang and Wen, Xidao and Nie, Xiaohui and Ma, Minghua and Pei, Dan},
  year = {2025},
  doi = {10.1145/3696630.3728572},
  url = {https://doi.org/10.1145/3696630.3728572},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {503--513},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {benchmark, evaluation, large language models, operations, prompt engineering},
  abstract = {In recent decades, the field of software engineering has driven the rapid evolution of Information Technology (IT) systems, including advances in cloud computing, 5G networks, and financial information platforms. Ensuring the stability, reliability, and robustness of these complex IT systems has emerged as a critical challenge. Large language models (LLMs) that have exhibited remarkable capabilities in NLP-related tasks are showing great potential in AIOps, such as root cause analysis of failures, generation of operations and maintenance scripts, and summarizing of alert information. Unlike knowledge in general corpora, knowledge of Ops varies with the different IT systems, encompassing various private sub-domain knowledge, sensitive to prompt engineering due to various sub-domains, and containing numerous terminologies. Existing NLP-related benchmarks can not guide the selection of suitable LLMs for Ops (OpsLLM), and current metrics (e.g., BLEU, ROUGE) can not adequately reflect the question-answering (QA) effectiveness in the Ops domain. We propose a comprehensive benchmark suite, OpsEval, including an Ops-oriented evaluation dataset, an Ops evaluation benchmark, and a specially designed Ops QA evaluation method. Our dataset contains 7,334 multiple-choice questions and 1,736 QA questions. We have carefully selected and released 20\% of the dataset written by domain experts in various sub-domains to assist current researchers in preliminary evaluations of OpsLLMs1. We test over 24 latest LLMs under various settings such as self-consistency, chain-of-thought, and in-context learning, revealing findings when applying LLMs to Ops. We also propose an evaluation method for QA in Ops, which has a coefficient of 0.9185 with human experts and is improved by 0.4471 and 1.366 compared to BLEU and ROUGE, respectively. Over the past one year, our dataset and leaderboard have been continuously updated.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{ma_t3set_2025,
  title = {{T3Set},
  author = {Ma, Ji and Wu, Jiale and Wang, Haoyu and Zhang, Yanze and Xie, Xiao and Zhou, Zheng and Zhang, Hui and Wang, Jiachen and Wu, Yingcai},
  year = {2025},
  doi = {10.1145/3711896.3737407},
  url = {https://doi.org/10.1145/3711896.3737407},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5686--5697},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {multimodal dataset, table tennis, virtual coach, source: ACM},
  abstract = {Coaching is critical for learning table tennis skills. However, amateur table tennis players often lack access to professional coaches due to high costs and a limited number of coaches. While recent multimodal large language models show promise as virtual coaches, most of the existing approaches merely rely on video analysis, which is not comprehensive enough. In table tennis, many important kinematic details (e.g., strength, acceleration) cannot be captured by videos. They can only be tracked using sensors. To address this gap, we present T3Set (Table Tennis Training Set), a multimodal dataset that synchronizes inertial measurement unit (IMU) data from sensors mounted on 32 players' rackets with video recordings. The sensor data has 16 dimensions and a sample rate of 100Hz. This dataset covers 7 fundamental techniques across 380 training rounds, totaling 8655 annotated strokes, with 8395 targeted suggestions from coaches. The key features of T3Set include (1) temporal alignment between sensor data, video data, and text data. (2) high-quality targeted suggestions which are consistent with predefined suggestion taxonomy. Based on T3Set, we propose a novel two-stage framework that effectively integrates motion perception with generative reasoning as a virtual coach. Our method quantitatively outperforms baseline methods. The dataset, code, and documentation are available at https://github.com/jima-cs/T3Set.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{salminen_communication_2024,
  title = {Communication {Design},
  author = {Salminen, Joni and Jung, Soon-Gyo and Medina, Johanne and Aldous, Kholoud and Azem, Jinan and Akhtar, Waleed and Häyhänen, Essi and Jansen, Bernard J},
  year = {2024},
  doi = {10.1145/3681716.3681727},
  url = {https://doi.org/10.1145/3681716.3681727},
  booktitle = {Proceedings of the 27th {International},
  pages = {176--187},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tampere, Finland},
  keywords = {source: ACM},
  abstract = {We analyzed the communication patterns of Cipherbot, an educational AI chatbot that addresses validation and transparency problems in AI-student interaction, with 44 undergraduate business students using the system. Findings show that Cipherbot delivers information grounded in learning materials, primarily focusing on statements of fact, with a 99\% occurrence rate. Also, about a third (31\%) of Cipherbot's messages to students contain an example, which adheres to pedagogical best practices. However, more than a third (36\%) of Cipherbot's communication with students also contained traces of opinion, particularly normative statements about how companies or individuals should behave. Cipherbot also demonstrates some level of social etiquette, using thank yous and apologies (7\%), but it rarely engages in requests for information, or clarifications and comprehension checks–as these categories might be useful for student engagement, future exploration into diversifying Cipherbot's communication style to support learning is required. Student feedback suggests that usability issues in educational AI chatbots comprise both communicational and technical issues, e.g., incompleteness and redundancy–iterative testing of prompts with student feedback could address many of these challenges. Based on our findings, we make six propositions about students’ interaction with educational AI chatbots.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1823-6},
  series = {Mindtrek '24},
}

@inproceedings{li_learning_2024,
  title = {Learning to {Rewrite},
  author = {Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Kong, Weize and Bendersky, Michael},
  year = {2024},
  doi = {10.1145/3589334.3645408},
  url = {https://doi.org/10.1145/3589334.3645408},
  booktitle = {Proceedings of the {ACM},
  pages = {3367--3378},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {large language models, personalized text generation, prompt rewrite},
  abstract = {Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{zhai_addressing_2025,
  title = {Addressing {Past},
  author = {Zhai, Nai-ji and Tuo, Xianyi and Jin, Zhishuo},
  year = {2025},
  doi = {10.1145/3704217.3705008},
  url = {https://doi.org/10.1145/3704217.3705008},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {106--111},
  publisher = {Association for Computing Machinery},
  keywords = {Content Safety, Contextual Analysis, Ethical AI, Large Language Models (LLMs), Model Robustness, Past Tense Temporal Attack, Prompt Engineering},
  abstract = {Several previous studies highlight the problems that several state-of-art large language models are vulnerable under past tense attack (e.g.,” How to make a Molotov cocktail?” to” How did people make a Molotov cocktail?”). In this paper, we try several methods to address this problem on GPT-3.5-Turbo. The single prompt-engineering method reached a 1\% attack success rate\&nbsp;and a\&nbsp;17\% over-refusal rate, and llama-2-7b trained on the Latent Adversarial Training method reached a 0\% asr. However, its over-refusal rate was 48\%. As an alternative method, we tried to build an advisor with a filter based on LLM, which can perform internet searching to check the safety of the content and provide more information with more timely related resources. With more information, the model can give more detailed insight even if it gives a possible refusal with “Sorry.” Source code at https://github.com/t0rit0/2024-summer-project},
  address = {New York, NY, USA},
  series = {{ESET},
  isbn = {979-8-4007-0709-4},
}

@inproceedings{li_understanding_2024,
  title = {Understanding {Code},
  author = {Li, Cong and Xu, Zhaogui and Di, Peng and Wang, Dongxia and Li, Zheng and Zheng, Qian},
  year = {2024},
  doi = {10.1145/3691620.3694999},
  url = {https://doi.org/10.1145/3691620.3694999},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {216--228},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {code change, code review, language model, LLM, SLM, source: ACM},
  abstract = {Recent studies indicate that traditional techniques for understanding code changes are not as effective as techniques that directly prompt language models (LMs). However, current LM-based techniques heavily rely on expensive, large LMs (LLMs) such as GPT-4 and Llama-13b, which are either commercial or prohibitively costly to deploy on a wide scale, thereby restricting their practical applicability. This paper explores the feasibility of deploying small LMs (SLMs) while maintaining comparable or superior performance to LLMs in code change understanding. To achieve this, we created a small yet high-quality dataset called HQCM which was meticulously reviewed, revised, and validated by five human experts. We fine-tuned state-of-the-art 7b and 220m SLMs using HQCM and compared them with traditional techniques and LLMs with ≥70b parameters. Our evaluation confirmed HQCM's benefits and demonstrated that SLMs, after finetuning by HQCM, can achieve superior performance in three change understanding tasks: change summarization, change classification, and code refinement. This study supports the use of SLMs in environments with security, computational, and financial constraints, such as in industry scenarios and on edge devices, distinguishing our work from the others.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{andrews_aimoderator_2025,
  title = {{AiModerator},
  author = {Andrews, Peter and Borch, Njål and Fjeld, Morten},
  year = {2025},
  doi = {10.1145/3708359.3712148},
  url = {https://doi.org/10.1145/3708359.3712148},
  booktitle = {Proceedings of the 30th {International},
  pages = {137--156},
  publisher = {Association for Computing Machinery},
  keywords = {Hyper-contextualization, Information Accessibility, Interactive Video, Multimodal Conversational Agents, Natural Language Processing, Political Discourse, Political Engagement, User Experience},
  abstract = {Political debates are essential in political discourse for democratic societies. Advancements in technology have significantly transformed the structure of political debates, the ways in which politicians communicate, and the platforms through which audiences engage with them. Originally a forum for improving understanding, political debates have increasingly favored theatrics over substance, risking young adult disengagement. To bring substance back to this medium we developed AiModerator, a political debate co-pilot acting as a Multimodal Conversational Agent (MCA). AiModerator aims to promote engagement while improving understanding by analyzing video content to provide contextually relevant information. This consolidated information facilitates understanding while keeping users synchronized with the debate viewing experience. Our system builds upon multimodal techniques, integrating computer vision and large language models to demonstrate ways of improving content delivery and engagement. AiModerator’s backend system extracts events from identified speech data, allowing the user to interact with these events through a touch interface on an iPad application. We address three key topics: evaluating young adults’ engagement, satisfaction, and preference compared to traditional second screening, and determining whether AiModerator can improve subjective understanding. To evaluate these measures we conducted a mixed-method evaluation (n=20) within-group design A-B study. Our analysis found AiModerator excelled in promoting engagement and satisfaction while delivering clear, contextually relevant information to the user which improved their understanding of debate topics more than the second screening mode. Our qualitative analysis offers broader insights, particularly in terms of a trade-off between automation and information consolidation versus autonomy and control.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{sun_research_2025,
  title = {Research and {Implementation},
  author = {Sun, Xiaohan},
  year = {2025},
  doi = {10.1145/3762329.3762351},
  url = {https://doi.org/10.1145/3762329.3762351},
  booktitle = {Proceedings of the 2nd {International},
  pages = {121--126},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, DeepSeek, Knowledge Base, QC Knowledge Management, Tobacco Indust},
  abstract = {To address issues in tobacco industry QC knowledge management-decentralized resources, inefficient retrieval, and repetitive labor—an intelligent framework using DeepSeek large model is built. By integrating the QC knowledge base with DeepSeek-R1 engine, it enables intelligent extraction, multi-dimensional semantic storage, and precise retrieval. Experiments show inputting natural language yields a 92.3\% relevant QC knowledge graph, boosting reuse efficiency and retrieval accuracy, supporting digital transformation of tobacco QC knowledge management.},
  address = {New York, NY, USA},
  series = {{AITC},
  isbn = {979-8-4007-1862-5},
}

@inproceedings{zha_m2conceptbase_2024,
  title = {{M2ConceptBase},
  author = {Zha, Zhiwei and Wang, Jiaan and Li, Zhixu and Zhu, Xiangru and Song, Wei and Xiao, Yanghua},
  year = {2024},
  doi = {10.1145/3627673.3679852},
  url = {https://doi.org/10.1145/3627673.3679852},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {3113--3123},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {knowledge base, multimodal knowledge base, multimodal symbol grounding, visual question answering},
  abstract = {Multimodal knowledge bases (MMKBs) provide cross-modal aligned knowledge crucial for multimodal tasks. However, the images in existing MMKBs are generally collected for entities in encyclopedia knowledge graphs. Therefore, detailed groundings of visual semantics with linguistic concepts are lacking, which are essential for the visual concept cognition ability of multimodal models. Addressing this gap, we introduce M2 ConceptBase, the first concept-centric MMKB. M2 ConceptBase models concepts as nodes with associated images and detailed textual descriptions. We propose a context-aware multimodal symbol grounding approach to align concept-image and concept-description pairs using context information from image-text datasets. Comprising 951K images and 152K concepts, M2 ConceptBase links each concept to an average of 6.27 images and a single description, ensuring comprehensive visual and textual semantics. Human studies confirm more than 95\% alignment accuracy, underscoring its quality. Additionally, our experiments demonstrate that M2 ConceptBase significantly enhances VQA model performance on the OK-VQA task. M2 ConceptBase also substantially improves the fine-grained concept understanding capabilities of multimodal large language models through retrieval augmentation in two concept-related tasks, highlighting its value.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{al-zuraiqi_designing_2025,
  title = {Designing and {Optimizing},
  author = {Al-Zuraiqi, Ahmad and Greer, Des},
  year = {2025},
  doi = {10.1145/3727582.3728687},
  url = {https://doi.org/10.1145/3727582.3728687},
  booktitle = {Proceedings of the 21st {International},
  pages = {55--64},
  publisher = {Association for Computing Machinery},
  note = {event-place: Trondheim, Norway},
  keywords = {alignment datasets, bias mitigation, cross-modal integration, dataset creation, federated learning, firmware vulnerability analysis, IoT security, Large Language Models (LLMs), metadata enrichment, synthetic augmentation},
  abstract = {Large Language Models (LLMs) show great promise for automating critical IoT security tasks, yet they often fail to address high-stakes vulnerabilities without domain-focused datasets. In this paper, we present a structured methodology to design and optimize IoT-specific alignment datasets informed by static analysis insights, thereby bridging the gap between generic language models and specialized IoT security requirements. Our approach integrates findings from IoT firmware analysis tools (e.g. FACT and Binwalk) with authoritative vulnerability repositories (MITRE CVE, CWE, CAPEC) to construct three key dataset types: (1) Base Datasets, capturing essential IoT vulnerabilities and configurations, (2) Classification Datasets, discerning IoT from non-IoT prompts, and (3) Alignment Datasets employing Contrastive Preference Optimization (CPO), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO) for IoT-specific fine-tuning. We further incorporate secure-by-design principles and bias mitigation strategies—ranging from device-type diversity to synthetic data augmentation—to ensure fair, high-fidelity representations of IoT security scenarios. Experimental results demonstrate that our alignment datasets improve LLM responsiveness and correctness for vulnerabilities discovered via offline static analysis, including outdated libraries, hard-coded credentials, and insecure default services. Notably, Kahneman-Tversky Optimization achieves a 97\% alignment accuracy, reflecting the impact of clear binary classifications in high-stakes security tasks. This work underscores the significance of dual-system integration (static analysis plus LLM alignment) for proactive IoT defense. By foregrounding domain-specific vulnerabilities in carefully curated datasets, we enable LLMs to generate more actionable, context-aware security recommendations, thus advancing state-of-the-art IoT protections in both research and industry deployments.},
  address = {New York, NY, USA},
  series = {{PROMISE},
  isbn = {979-8-4007-1594-5},
}

@inproceedings{xiao_llm-enhanced_2024,
  title = {{LLM},
  author = {Xiao, Xichonglang and Xu, Hao},
  year = {2024},
  doi = {10.1145/3674399.3674433},
  url = {https://doi.org/10.1145/3674399.3674433},
  booktitle = {Proceedings of the {ACM},
  pages = {86--91},
  publisher = {Association for Computing Machinery},
  note = {event-place: Changsha, China},
  keywords = {llama3, Military, NL2SQL, SFT},
  abstract = {With the development of big data and artificial intelligence technology, data has become an important production factor and core resource. There is also a growing demand for real-time accurate, convenient and fast data analysis in the military field.NL2SQL technology can be embedded into various intelligent platforms, data analysis tools or intelligent assistants, realizing seamless dialogue between users and data, responding instantly to complex analytical requests, and providing timely and accurate support of decision-making for commanders. However, the Chinese NL2SQL task has a low success rate compared to the English task due to the base model’s bias in converting Chinese entities, difficulty in linking schemas, and unclear thought chain. In this paper, based on the llama3-8b model, we explored the NL2SQL technique under resource-constrained conditions.By analyzing the characteristics of the model’s erroneous output results under regular prompts,target refinements of Chinese prompts are designed.And through supervised fine-tuning, we have made the model’s accuracy rate on the dev set increase from the initial 46\% to 70\%.},
  address = {New York, NY, USA},
  series = {{ACM},
  isbn = {979-8-4007-1011-7},
}

@inproceedings{mirindi_advanced_2025,
  title = {Advanced evaluation of {BIM},
  author = {Mirindi, Derrick and Sinkhonde, David and Mirindi, Frederic and Bezabith, Tajebe},
  year = {2025},
  doi = {10.1145/3716489.3728431},
  url = {https://doi.org/10.1145/3716489.3728431},
  booktitle = {Proceedings of the 2025 {Computers},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, Building Information Modeling, Generative Artificial Intelligence, OpenAI o1},
  abstract = {The rapid advancement of artificial intelligence (AI) has led the AI community to speculate that artificial superintelligence (ASI) may be within reach, particularly if an AI system can iteratively search for solutions, learn from results, and leverage improved knowledge for more searches. In this context, this study explores the integration of Generative Artificial Intelligence (GenAI) into Building Information Modeling (BIM) by focusing on the four key pillars of the OpenAI o1 model and their ethical implications. Through comprehensive analysis of existing literature, we examine these pillars—policy initialization, reward design, search strategies, and learning mechanisms—and their application in BIM-GenAI within a continuous improvement cycle. Results demonstrate that policy initialization generates human-like reasoning behaviors and domain-specific knowledge for BIM tasks. Reward design, central to reinforcement learning, optimizes BIM objectives through measurable metrics and learned evaluation methods. Search strategies prove valuable for exploring complex design spaces and generating high-quality BIM solutions, while learning mechanisms, including policy gradient and behavior cloning, enable continuous model improvement through feedback. The study emphasizes the importance of establishing BIM-AI protocols, maintaining human expertise in decision-making, and balancing automation with human input. Our findings suggest that while GenAI, powered by reinforcement learning, offers significant potential for enhancing BIM capabilities, three critical ethical considerations—data privacy and security, algorithmic bias mitigation, and transparency and accountability—must guide responsible implementation. This research contributes to the growing body of knowledge on AI in construction technologies and provides a foundation for the ethical advancement of BIM-GenAI systems using OpenAI o1.},
  address = {New York, NY, USA},
  series = {{SIGMIS},
  isbn = {979-8-4007-1497-9},
}

@inproceedings{sayana_beyond_2025,
  title = {Beyond {Retrieval},
  author = {Sayana, Krishna and Vasudeva, Raghavendra and Vasilevski, Yuri and Su, Kun and Hebert, Liam and Pine, James and Pham, Hubert and Jash, Ambarish and Sodhi, Sukhdeep},
  year = {2025},
  doi = {10.1145/3701716.3717531},
  url = {https://doi.org/10.1145/3701716.3717531},
  booktitle = {Companion {Proceedings},
  pages = {2411--2420},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {language models, recommenders},
  abstract = {Large Language Models (LLMs) have shown remarkable progress in generating human-quality text and engaging in complex reasoning. This presents a unique opportunity to revolutionize conversational recommender systems by enabling them to generate rich, engaging and personalized narratives that go beyond recommendations. However, the lack of suitable datasets limits research in this area. This paper addresses this challenge by making two key contributions.First, we introduce REGEN Reviews Enhanced with GEnerative Narratives, a new dataset extending the Amazon Product Reviews with rich user narratives. Furthermore, we perform an extensive automated evaluation of the dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. To the best of our knowledge, this represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12\% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret their contributions to this new generative task.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{noh_biassist_2025,
  title = {{BIASsist},
  author = {Noh, Yeo-Gyeong and Han, MinJu and Jeon, Junryeol and Hong, Jin-Hyuk},
  year = {2025},
  doi = {10.1145/3706598.3713531},
  url = {https://doi.org/10.1145/3706598.3713531},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Assistive tool, Bias, LLM-powered application, News article},
  abstract = {Biased news articles can distort readers’ perceptions by presenting information in a way that favors or disfavors a particular point of view. Subtly embedded in the text, these biased news articles can shape our views daily without people even realizing it. To address this issue, we propose BIASsist, an LLM-based approach designed to mitigate bias in news articles. Based on existing research, we defined six types of bias and introduced three assistive components—identification, explanation, and neutralization—to provide a broader range of bias information and enhance readers’ bias-awareness. We conducted a mixed-method study with 36 participants to evaluate the effectiveness of BIASsist. The results show participants’ bias awareness significantly improved and their interest in identifying bias increased. Participants also tended to engage more actively in critically evaluating articles. Based on these findings, we discuss its potential to improve media literacy and critical thinking in today’s information overload era.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{muthyala_so_2024,
  title = {So {You},
  author = {Muthyala, Mayank P. and Lauer, Claire and Carradini, Stephen},
  year = {2024},
  doi = {10.1145/3641237.3691661},
  url = {https://doi.org/10.1145/3641237.3691661},
  booktitle = {Proceedings of the 42nd {ACM},
  pages = {128--137},
  publisher = {Association for Computing Machinery},
  note = {event-place: Fairfax, VA, USA},
  keywords = {AI, API, Artificial intelligence, Chatbot, Drought, GPT, Open AI, Science Communication, User Experience, Water, source: ACM},
  abstract = {Chatbots have become a popular method through which to deliver conversational-style information to users about a range of topics, including providing customer service, news and weather updates, educational content, and medical information. This article compares two chatbots created with different methods, including via custom architecture and custom GPT to determine the strengths and limitations of the development methods. The bots that our research team developed were built to deliver information about water and drought to Arizona residents. We compare the initial setup process, customization capabilities, the training process, prompt engineering requirements, file handling, costs, and outputs of each bot. The custom architecture bot offers the flexibility and control of answers, but it costs more than its comparator and takes more time. The custom GPT requires little experience with Large Language Models (LLMs) and no experience with coding, but offers less control. Because we recognize that public agencies often don't have the expertise or funding to build a fully-customized bot architecture, we conclude with suggestions about the contexts and purposes or which each type of bot should be developed.},
  address = {New York, NY, USA},
  series = {{SIGDOC},
  isbn = {979-8-4007-0519-9},
}

@inproceedings{kwak_adaptive_2025,
  title = {Adaptive {Tutoring},
  author = {Kwak, Yerin and Dunder, Nora and Viberg, Olga and Pardos, Zachary A.},
  year = {2025},
  doi = {10.1145/3698205.3729549},
  url = {https://doi.org/10.1145/3698205.3729549},
  booktitle = {Proceedings of the {Twelfth},
  pages = {50--61},
  publisher = {Association for Computing Machinery},
  note = {event-place: Palermo, Italy},
  keywords = {adaptive tutoring systems, case study, curricular alignment, large language models, neural machine translation, oatutor, open educational resources, source: ACM},
  abstract = {Adaptive tutoring systems have demonstrated significant improvements in math learning, yet their adoption outside of the United States remains limited. The absence of these technologies, along with a lack of research on localizing tutoring systems to different educational contexts, presents a significant barrier for institutions seeking to integrate these tools into their classrooms to support students' math learning. This paper presents a case study on the localization and deployment of OATutor, an adaptive tutoring system developed in the U.S., for use in a math course at KTH Royal Institute of Technology in Sweden. Our study explores using artificial intelligence to automate and validate this process, focusing on translation and syllabus adaptation to ensure the content aligns with the course curriculum and the Swedish educational context. We successfully deployed the system in the course, demonstrating a novel method for translating math content and providing an analysis of syllabus adaptation tailored to the local context. By documenting this process, we contribute to the broader effort to make educational technologies more accessible to diverse learner populations by providing a scalable approach to localization.},
  address = {New York, NY, USA},
  series = {L@{S},
  isbn = {979-8-4007-1291-3},
}

@inproceedings{abdenebaoui_value-driven_2025,
  title = {Value-{Driven},
  author = {Abdenebaoui, Larbi and Aljuneidi, Saja and Horstmannshoff, Fynn and Meyer, Jochen and Boll, Susanne},
  year = {2025},
  doi = {10.1145/3715275.3732103},
  url = {https://doi.org/10.1145/3715275.3732103},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1554--1564},
  publisher = {Association for Computing Machinery},
  keywords = {AI in public administration, Artificial Intelligence, Generative Chatbots, Value Driven Design, Value Sensitive Design},
  abstract = {Artificial intelligence holds significant potential to transform public administration by streamlining complex processes and improving service delivery. However, its adoption is hindered by concerns regarding its alignment with public values e.g., ensuring that all citizens are treated equitably. This study addresses these challenges through a value-driven design approach, focusing on a generative chatbot to assist citizens with the Housing Entitlement Certificate application, a service enabling low-income households to access subsidized housing in Germany. Through two participatory workshops with citizens and public administration employees, the research highlights a tension between significant benefits, such as improved accessibility and streamlined processes, and challenges, including reduced human interaction and dependency on technology. To navigate these tensions, the study proposes actionable requirements, including escalation pathways to human representatives and robust data protection measures. By providing a real-world example, this work illustrates how value-based design can guide the responsible and impactful integration of AI in public administration. The findings underscore the importance of balancing the potential of generative chatbots with societal and ethical considerations, offering a replicable framework for aligning AI systems with public values in diverse service contexts.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-1482-5},
}

@inproceedings{pu_ideasynth_2025,
  title = {{IdeaSynth},
  author = {Pu, Kevin and Feng, K. J. Kevin and Grossman, Tovi and Hope, Tom and Dalvi Mishra, Bhavana and Latzke, Matt and Bragg, Jonathan and Chang, Joseph Chee and Siangliulue, Pao},
  year = {2025},
  doi = {10.1145/3706598.3714057},
  url = {https://doi.org/10.1145/3706598.3714057},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Human-AI Collaboration, Research Ideation, Scientific Literature},
  abstract = {Research ideation involves broad exploring and deep refining ideas. Both require deep engagement with literature. Existing tools focus primarily on broad idea generation, yet offer little support for iterative specification, refinement, and evaluation needed to further develop initial ideas. To bridge this gap, we introduce IdeaSynth, a research idea development system that uses LLMs to provide literature-grounded feedback for articulating research problems, solutions, evaluations, and contributions. IdeaSynth represents these idea facets as nodes on a canvas, and allow researchers to iteratively refine them by creating and exploring variations and combinations. Our lab study (N = 20) showed that participants, while using IdeaSynth, explored more alternative ideas and expanded initial ideas with more details compared to a strong LLM-based baseline. Our deployment study (N = 7) demonstrated that participants effectively used IdeaSynth for real-world research projects at various ideation stages from developing initial ideas to revising framings of mature manuscripts, highlighting the possibilities to adopt IdeaSynth in researcher’s workflows.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{cowgill_recapturing_2024,
  title = {({Re},
  author = {Cowgill, Rachel and Bainbridge, David and Dix, Alan and Hoyle, Victoria and Fong, Vicki and Thomas, David},
  year = {2024},
  doi = {10.1145/3660570.3660579},
  url = {https://doi.org/10.1145/3660570.3660579},
  booktitle = {Proceedings of the 11th {International},
  pages = {23--31},
  publisher = {Association for Computing Machinery},
  note = {event-place: Stellenbosch, South Africa},
  keywords = {British Chinese Communities, ChatGPT, Data-gathering, Digital Archives, DJ-ing, Heritage, Music Venues, Nightclubs, York},
  abstract = {The loss of many high-street music venues in recent years has highlighted their connectedness to place and communities. Understanding the emotional geographies of these venues, as experienced by their patrons, is key to explaining the outcry that can accompany such closures. In these circumstances it can be challenging to try to (re)capture the intangible elements that defined a lost venue and widen the scope for musicological enquiry. This paper sets out to address that challenge by exploring methods developed by the Willow Community Digital Archive to co-create a community archive in celebration of The Willow, a family-run restaurant-cum-nightclub that operated in York, UK, for over 40 years. Further, we report on how these methods informed the crafting of a general-purpose digital library system to form the archive. We also detail some initial experiments with ChatGPT, embedded into the archive, to investigate its potential to encourage visitors to engage with and inspire further contributions to the archive.},
  address = {New York, NY, USA},
  series = {{DLfM},
  isbn = {979-8-4007-1720-8},
}

@inproceedings{liu_webglm_2023,
  title = {{WebGLM},
  author = {Liu, Xiao and Lai, Hanyu and Yu, Hao and Xu, Yifan and Zeng, Aohan and Du, Zhengxiao and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  year = {2023},
  doi = {10.1145/3580305.3599931},
  url = {https://doi.org/10.1145/3580305.3599931},
  booktitle = {Proceedings of the 29th {ACM},
  pages = {4549--4560},
  publisher = {Association for Computing Machinery},
  note = {event-place: Long Beach, CA, USA},
  keywords = {efficient retrieval enhancement system, human preference alignment, pre-trained language model},
  abstract = {We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0103-0},
}

@inproceedings{guo_ideabench_2025,
  title = {{IdeaBench},
  author = {Guo, Sikun and Shariatmadari, Amir Hassan and Xiong, Guangzhi and Huang, Albert and Kim, Myles and Williams, Corey M. and Bekiranov, Stefan and Zhang, Aidong},
  year = {2025},
  doi = {10.1145/3711896.3737419},
  url = {https://doi.org/10.1145/3711896.3737419},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5888--5899},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {AI for science, hypothesis generation, large language models},
  abstract = {Large Language Models (LLMs) have revolutionized interactions between human and artificial intelligence (AI) systems, demonstrating state-of-the-art performance across various domains, including scientific discovery and hypothesis generation. However, the absence of a comprehensive and systematic evaluation framework for LLM-driven research idea generation hinders a rigorous understanding of their strengths and limitations. To address this gap, we propose IdeaBench, a benchmark system that provides a structured dataset and evaluation framework for standardizing the assessment of research idea generation by LLMs. Our dataset comprises titles and abstracts from 2,374 influential papers across eight research domains, along with their 29,408 referenced works, creating a context-rich environment that mirrors human researchers' ideation processes. By profiling LLMs as domain-specific researchers and grounding them in similar contextual constraints, we directly leverage the models' knowledge learned from the pre-training stage to generate new research ideas. To systematically evaluate LLMs' research ideation capability and approximate human assessment, we propose a reference-based metric that aligns with human judgment to quantify idea quality with the assistance of LLMs. Through this evaluation, we find that while LLMs excel at generating novel ideas, they may struggle with generating feasible ideas. IdeaBench serves as a critical resource for benchmarking and comparing LLMs, ultimately advancing research on AI's role in automating scientific discovery.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{sikder_efficient_2025,
  title = {Efficient {Adaptation},
  author = {Sikder, Fadul and Lei, Yu and Ji, Yuede},
  year = {2025},
  doi = {10.1145/3727582.3728688},
  url = {https://doi.org/10.1145/3727582.3728688},
  booktitle = {Proceedings of the 21st {International},
  pages = {65--74},
  publisher = {Association for Computing Machinery},
  note = {event-place: Trondheim, Norway},
  keywords = {Fine-tuning, Large Language Models, Layer Freezing, Llama 3.2, Smart Contracts, Solidity, StarCoder, Transfer Learning, Vulnerability Detection},
  abstract = {Smart contracts underpin decentralized applications but face significant security risks from vulnerabilities, while traditional analysis methods have limitations. Large Language Models (LLMs) offer promise for vulnerability detection, yet adapting these powerful models efficiently, particularly generative ones, remains challenging. This paper investigates two key strategies for the efficient adaptation of LLMs for Solidity smart contract vulnerability detection: (1) replacing token-level generation with a dedicated classification head during fine-tuning, and (2) selectively freezing lower transformer layers using Low-Rank Adaptation (LoRA). Our empirical evaluation demonstrates that the classification head approach enables models like Llama 3.2 3B to achieve high accuracy (77.5\%), rivaling the performance of significantly larger models such as the fine-tuned GPT-3.5. Furthermore, we show that selectively freezing bottom layers reduces training time and memory usage by approximately 10-20\% with minimal impact on accuracy. Notably, larger models (3B vs. 1B parameters) exhibit greater resilience to layer freezing, maintaining high accuracy even with a large proportion of layers frozen, suggesting a localization of general code understanding in lower layers versus task-specific vulnerability patterns in upper layers. These findings present practical insights for developing and deploying performant LLM-based vulnerability detection systems efficiently, particularly in resource-constrained settings.},
  address = {New York, NY, USA},
  series = {{PROMISE},
  isbn = {979-8-4007-1594-5},
}

@inproceedings{zaslavsky_enhancing_2024,
  title = {Enhancing spatially-disaggregated simulations with large language models},
  author = {Zaslavsky, Ilya and Lei, Jiaxi and Graham, Rishi and Handcock, Mark S. and Aronoff-Spencer, Eliah},
  year = {2024},
  doi = {10.1145/3686592.3686595},
  url = {https://doi.org/10.1145/3686592.3686595},
  booktitle = {Proceedings of the 2024 7th {International},
  pages = {14--18},
  publisher = {Association for Computing Machinery},
  keywords = {Agent-Based Models, Epidemiological Modeling, Large Language Models, Spatially-Disaggregated Models, System Dynamics, source: ACM},
  abstract = {We present our experience integrating large language models (LLMs) and simulation engines to enhance spatially-disaggregated simulation, taking advantage of the spatial knowledge and spatial reasoning capabilities of LLMs. The examples illustrate LLM integration with different variations of compartmental epidemiological models, including agent-based models (ABM) in the context of modeling COVID-19 infection spread in a school setting, and LLM integration with a system dynamics model which supports a serious game focused on strategies for responding to disease outbreaks at the county level. We present the architecture of the integrated LLM-simulation system, demonstrate the initial results, and discuss the challenges of the current approach, related to LLM's understanding of spatial information and spatial relationships, their reasoning capabilities, and model performance and scalability.},
  address = {New York, NY, USA},
  series = {{ICoMS},
  isbn = {979-8-4007-0722-3},
}

@inproceedings{nguyen-ho_t-eagle_2025,
  title = {T-{EAGLE},
  author = {Nguyen-Ho, Thang-Long and Tran, Allie and Tran, Minh-Triet and Gurrin, Cathal and Healy, Graham},
  year = {2025},
  doi = {10.1145/3729459.3748691},
  url = {https://doi.org/10.1145/3729459.3748691},
  booktitle = {Proceedings of the 8th {Annual},
  pages = {23--27},
  publisher = {Association for Computing Machinery},
  keywords = {interactive retrieval systems, lifelog, semantic embedding},
  abstract = {There is a growing need to retrieve specific events or information from personal lifelog data, but this is particularly challenging due to the massive scale and the passive nature of data capture by lifelogging devices. Current systems typically rely on image similarity for single, isolated images, which struggle to capture the user intent expressed in natural language and the semantic links between the images and activities occurring over time. To address this issue, we propose a novel lifelog retrieval framework that explicitly combines both visual and temporal similarity in a multi-stage process, shifting the focus from single images to coherent sequences of actions. Our approach uses image embeddings to initialize a set of candidate images. Importantly, the system then re-evaluates the query similarity based on action descriptions which contain temporal information across image sequences. Action captioning, integrated into the indexing process, captures richer temporal and semantic context, allowing the system to distinguish between visually similar but semantically distinct events. Additionally, the system incorporates an evidence-based question answering mechanism, in which the narratives of the retrieved sequences provide contextual grounding for the answering model. The paper proposes a hybrid retrieval framework that combines image similarity for candidate initialization and visual-textual similarity for event retrieval. The integration of action descriptions enables language-based temporal representation of events. These are extracted offline through semantic content analysis and serve as the basis for building an evidence-based Question Answering module using these narratives as context. This approach helps bridge the gap between user intent and the multimodal, temporally structured nature of lifelog data.},
  address = {New York, NY, USA},
  series = {{LSC},
  isbn = {979-8-4007-1857-1},
}

@inproceedings{han_xbrl_2024,
  title = {{XBRL},
  author = {Han, Shijie and Kang, Haoqiang and Jin, Bo and Liu, Xiao-Yang and Yang, Steve Y},
  year = {2024},
  doi = {10.1145/3677052.3698614},
  url = {https://doi.org/10.1145/3677052.3698614},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {856--864},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Large language models (LLM), Semantic-augmented generation, XBRL reports},
  abstract = {eXtensible Business Reporting Language (XBRL) has attained the status of the global de facto standard for business reporting. However, its complexity poses significant barriers to interpretation and accessibility. In this paper, we present the first evaluation of large language models’ (LLMs) performance in analyzing XBRL reports. Our study identifies LLMs’ limitations in the comprehension of financial domain knowledge and mathematical calculation in the context of XBRL reports. To address these issues, we propose enhancement methods using external tools under the agent framework, referred to as XBRL-Agent, which invokes retrievers and calculators. Extensive experiments on two tasks - the Domain Query Task (which involved testing 500 XBRL term explanations and 50 domain questions) and the Numeric Type Query Task (tested 1,000 financial math tests and 50 numeric queries) - demonstrate substantial performance improvements, with accuracy increasing by up to 17\% for the domain task and 42\% for the numeric type task. This work not only explores the potential of LLMs for analyzing XBRL reports but also augments the reliability and robustness of such analysis, although there is still much room for improvement in mathematical calculations.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{hong_context-aware_2024,
  title = {A {Context},
  author = {Hong, Jihyeong and Lee, Yokyung and Kim, Dae Hyun and Choi, DaEun and Yoon, Yeo-Jin and Lee, Gyu-cheol and Lee, Zucheul and Kim, Juho},
  year = {2024},
  doi = {10.1145/3643834.3661579},
  url = {https://doi.org/10.1145/3643834.3661579},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {1857--1874},
  publisher = {Association for Computing Machinery},
  note = {event-place: Copenhagen, Denmark},
  keywords = {context-awareness, conversational agent, large-language models, metaverse},
  abstract = {One common asset of metaverse is that users can freely explore places and actions without linear procedures. Thus, it is hard yet important to understand the divergent challenges each user faces when onboarding metaverse. Our formative study (N = 16) shows that first-time users ask questions about metaverse that concern 1) a short-term spatiotemporal context, regarding the user’s current location, recent conversation, and actions, and 2) a long-term exploration context regarding the user’s experience history. Based on the findings, we present PICAN, a Large Language Model-based pipeline that generates context-aware answers to users when onboarding metaverse. An ablation study (N = 20) reveals that PICAN’s usage of context made responses more useful and immersive than those generated without contexts. Furthermore, a user study (N = 21) shows that the use of long-term exploration context promotes users’ learning about the locations and activities within the virtual environment.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-0583-0},
}

@inproceedings{lee_rex_2024,
  title = {{REX},
  author = {Lee, Christine P and Praveena, Pragathi and Mutlu, Bilge},
  year = {2024},
  doi = {10.1145/3643834.3661559},
  url = {https://doi.org/10.1145/3643834.3661559},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {2911--2925},
  publisher = {Association for Computing Machinery},
  note = {event-place: Copenhagen, Denmark},
  keywords = {failures, human-robot interaction, program repair, robot, user-centered design, vignette study},
  abstract = {Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study (n = 162), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors—safety, privacy, and complexity—that require adaptive repair strategies. The second, in-person study (n = 24) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-0583-0},
}

@inproceedings{mridul_terminators_2025,
  title = {Terminators: {Terms},
  author = {Mridul, Maruf Ahmed and Kang, Inwon and Seneviratne, Oshani},
  year = {2025},
  doi = {10.1145/3720554.3736183},
  url = {https://doi.org/10.1145/3720554.3736183},
  booktitle = {Companion {Publication},
  pages = {44--48},
  publisher = {Association for Computing Machinery},
  keywords = {Accountability, Agentic Workflow, Auditing, Automation, Language Models},
  abstract = {Terms of Service (ToS) documents are often lengthy and written in complex legal language, making them difficult for users to read and understand. To address this challenge, we propose Terminators, a modular agentic framework that leverages large language models (LLMs) to parse and audit ToS documents. Rather than treating ToS understanding as a black-box summarization problem, Terminators breaks the task down to three interpretable steps: term extraction, verification, and accountability planning. We demonstrate the effectiveness of our method on the OpenAI ToS using GPT-4o, highlighting strategies to minimize hallucinations and maximize auditability.Our results suggest that structured, agent-based LLM workflows can enhance both the usability and enforceability of complex legal documents. By translating opaque terms into actionable, verifiable components, Terminators promotes ethical use of web content by enabling greater transparency, empowering users to understand their digital rights, and supporting automated policy audits for regulatory or civic oversight.},
  address = {New York, NY, USA},
  series = {Websci {Companion},
  isbn = {979-8-4007-1535-8},
}

@inproceedings{lee_log2plan_2025,
  title = {{Log2Plan},
  author = {Lee, Seoyoung and Yoon, Seobin and Lee, Seongbeen and Kim, Hyesoo and Sim, Joo Yong},
  year = {2025},
  doi = {10.1145/3746059.3747663},
  url = {https://doi.org/10.1145/3746059.3747663},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {GUI automation, Large Language Models, Task Mining, Two-level Planning},
  abstract = {GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0\% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{fan_hierarchical_2025,
  title = {Hierarchical {Table},
  author = {Fan, Grace and Freire, Juliana},
  year = {2025},
  doi = {10.1145/3736733.3736746},
  url = {https://doi.org/10.1145/3736733.3736746},
  booktitle = {Proceedings of the {Workshop},
  publisher = {Association for Computing Machinery},
  note = {event-place: Intercontinental Berlin, Berlin, Germany},
  keywords = {column type annotation, data integration, dataset search and discovery, semantic profiling},
  abstract = {Exploratory table discovery in open data portals presents significant challenges due to unreliable metadata and ambiguous table semantics. Users typically lack prior knowledge of available datasets, making it difficult to identify relevant tables through traditional keyword search or value matching approaches, which often fail to capture semantic relevance across heterogeneous table representations.We propose a new approach that automatically constructs hierarchical semantic representations of tables, encompassing specific column semantic types, shared concepts across column groups, and general table-level semantics. By leveraging these semantically-rich representations, our method retrieves relevant tables through semantic alignment rather than traditional value or metadata matching, leading to improved accuracy and recall for table discovery queries.To enhance interpretability and support human-in-the-loop exploration, our system presents users with semantically relevant tables alongside explanations of their relevance. Evaluation on real-world open data and a question-answering benchmark demonstrates the effectiveness of our approach, achieving up to 36\% improvement in Recall@10 compared to embedding-based baselines confirming the utility of hierarchical semantic representations for table discovery.},
  address = {New York, NY, USA},
  series = {{HILDA},
  isbn = {979-8-4007-1959-2},
}

@inproceedings{kaate_when_2025,
  title = {When {Personas},
  author = {Kaate, Ilkka and Salminen, Joni and Jung, Soon-Gyo and Xuan, Trang Thi Thu and Azem, Jinan Y. and Santos, João M. and Jansen, Bernard J},
  year = {2025},
  doi = {10.1145/3715336.3735676},
  url = {https://doi.org/10.1145/3715336.3735676},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {2350--2372},
  publisher = {Association for Computing Machinery},
  keywords = {Interactive Personas, Persona Systems, Usability, User Representation},
  abstract = {The development of persona systems provides a possibility for end users to interact with different persona modalities. In a 54-participant randomized controlled experiment, we compare two persona interaction modalities, document and dialogue personas, both generated using AI approaches from survey data. Overall, dialogue personas appear to be perceived more favorably than document personas. However, document personas exhibit a wider range of perceptions, suggesting that experiences with document personas are more polarizing among users. The document personas had higher transparency and were perceived as more complete, but the task completion was perceived as more difficult, although the task success rate was higher. The dialogue personas were perceived as more usable, with a higher System Usability Scale score, and more enjoyable. Our findings provide critical insights into the increasingly important area of persona interaction modalities and the broad paradigm of human-persona interaction.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-1485-6},
}

@inproceedings{zhai_confsum_2025,
  title = {{ConfSum},
  author = {Zhai, Rundi and Liu, Jianmin and Miao, Yukai and Chen, Li and Li, Dan and Cui, Baojiang and Zhang, Peng and Zhai, Ennan and Ding, Zishuo},
  year = {2025},
  doi = {10.1145/3750022.3750459},
  url = {https://doi.org/10.1145/3750022.3750459},
  booktitle = {Proceedings of the 2nd {Workshop},
  pages = {19--24},
  publisher = {Association for Computing Machinery},
  note = {event-place: Coimbra, Portugal},
  keywords = {Formal Methods, Large Language Models, Network management},
  abstract = {When network operators need to understand the high-level intent behind a network's existing device configurations, they must engage in a tedious and error-prone process of manually reverse-engineering the low-level commands. We propose Configuration Intent Summarization (CIS), a new task that aims to automate this process by generating human-readable summaries of the intents embedded across a network's configurations. CIS is challenging due to the diversity of intents, the semantic gap between device-specific configurations and network-wide intents, and the need to reason about interactions between multiple devices' configurations. We present ConfSum, a system that addresses these challenges by leveraging the unique ability of large language models (LLMs) to parse semi-structured configuration files and summarize them in natural language. However, the full CIS task requires reasoning about device interactions and other complexities that are beyond the capabilities of LLMs alone. To enhance the LLM's robustness to these challenges, ConfSum introduces novel techniques for retrieving relevant examples to augment LLM prompts, decomposing the generation process to handle multi-device intents, and integrating with formal validation tools. Our experiments demonstrate that Conf-Sum achieves high intent coverage while generating summaries that match the quality of human experts.},
  address = {New York, NY, USA},
  series = {{FMANO},
  isbn = {979-8-4007-2103-8},
}

@inproceedings{da_flans_2025,
  title = {{FlanS},
  author = {Da, Longchao and Wang, Rui and Xu, Xiaojian and Bhatia, Parminder and Kass-Hout, Taha and Wei, Hua and Xiao, Cao},
  year = {2025},
  doi = {10.1145/3711896.3736963},
  url = {https://doi.org/10.1145/3711896.3736963},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {404--414},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {foundation model, language-based segmentation, medical image},
  abstract = {Medical imaging is crucial for diagnosing a patient's health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model's superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{terdalkar_graph_2025,
  title = {Graph {Repairs},
  author = {Terdalkar, Hrishikesh and Bonifati, Angela and Mauri, Andrea},
  year = {2025},
  doi = {10.1145/3735546.3735859},
  url = {https://doi.org/10.1145/3735546.3735859},
  booktitle = {Proceedings of the 8th {Joint},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {Graph Repair, Large Language Models, Property Graphs},
  abstract = {Property graphs are widely used in domains such as healthcare, finance, and social networks, but they often contain errors due to inconsistencies, missing data, or schema violations. Traditional rule-based and heuristic-driven graph repair methods are limited in their adaptability as they need to be tailored for each dataset. On the other hand, interactive human-in-the-loop approaches may become infeasible when dealing with large graphs, as the cost-both in terms of time and effort-of involving users becomes too high. Recent advancements in Large Language Models (LLMs) present new opportunities for automated graph repair by leveraging contextual reasoning and their access to real-world knowledge. We evaluate the effectiveness of six open-source LLMs in repairing property graphs. We assess repair quality, computational cost, and model-specific performance. Our experiments show that LLMs have the potential to detect and correct errors, with varying degrees of accuracy and efficiency. We discuss the strengths, limitations, and challenges of LLM-driven graph repair and outline future research directions for improving scalability and interpretability.},
  address = {New York, NY, USA},
  series = {{GRADES},
  isbn = {979-8-4007-1923-3},
}

@inproceedings{yu_what_2024,
  title = {What {Makes},
  author = {Yu, Xiao and Zhang, Zexian and Niu, Feifei and Hu, Xing and Xia, Xin and Grundy, John},
  year = {2024},
  doi = {10.1145/3691620.3695061},
  url = {https://doi.org/10.1145/3691620.3695061},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {656--668, source: ACM},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {empirical study, high-quality data, large language models, practitioners' perspective},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance in various application domains, largely due to their self-supervised pre-training on extensive high-quality text datasets. However, despite the importance of constructing such datasets, many leading LLMs lack documentation of their dataset construction and training procedures, leaving LLM practitioners with a limited understanding of what makes a high-quality training dataset for LLMs. To fill this gap, we initially identified 18 characteristics of high-quality LLM training datasets, as well as 10 potential data pre-processing methods and 6 data quality assessment methods, through detailed interviews with 13 experienced LLM professionals. We then surveyed 219 LLM practitioners from 23 countries across 5 continents. We asked our survey respondents to rate the importance of these characteristics, provide a rationale for their ratings, specify the key data pre-processing and data quality assessment methods they used, and highlight the challenges encountered during these processes. From our analysis, we identified 13 crucial characteristics of high-quality LLM datasets that receive a high rating, accompanied by key rationale provided by respondents. We also identified some widely-used data pre-processing and data quality assessment methods, along with 7 challenges encountered during these processes. Based on our findings, we discuss the implications for researchers and practitioners aiming to construct high-quality training datasets for optimizing LLMs.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1248-7},
  series = {{ASE},
}

@inproceedings{lin_technical_2024,
  title = {Technical {Brief},
  author = {Lin, Dayi and Cogo, Filipe Roseiro and Rajbahadur, Gopi Krishnan and Hassan, Ahmed E.},
  year = {2024},
  doi = {10.1145/3639478.3643062},
  url = {https://doi.org/10.1145/3639478.3643062},
  booktitle = {Proceedings of the 2024 {IEEE},
  pages = {431--433},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lisbon, Portugal},
  keywords = {FMware, foundation model, software engineering for FMware},
  abstract = {Foundation Models (FM) like GPT-4 have given rise to FMware, FM-powered applications, which represent a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. Such challenges jeopardize the quality and trustworthiness of FMware. In our technical brief, we will present the latest research and industrial practices in engineering FMware, and discuss the SE challenges and opportunities facing both researchers and practitioners in the FMware era.The brief is unique in that it is presented from an SE point of view, not an AI point-of-view ensuring that attendees are not bogged into complex mathematical and AI details unless they are essential for contextualizing the SE challenges and opportunities.},
  address = {New York, NY, USA},
  series = {{ICSE},
  isbn = {979-8-4007-0502-1},
}

@inproceedings{qian_hsf_2025,
  title = {{HSF},
  author = {Qian, Cheng and Zhang, Hainan and Sha, Lei and Zheng, Zhiming},
  year = {2025},
  doi = {10.1145/3701716.3717659},
  url = {https://doi.org/10.1145/3701716.3717659},
  booktitle = {Companion {Proceedings},
  pages = {2078--2087},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {defense strategies, jailbreak attacks, large language models, source: ACM},
  abstract = {With the growing deployment of LLMs in daily applications like chatbots and content generation, efforts to ensure outputs align with human values and avoid harmful content have intensified. However, increasingly sophisticated jailbreak attacks threaten this alignment, aiming to induce unsafe outputs. Current defense efforts either focus on prompt rewriting or detection, which are limited in effectiveness due to the various design of jailbreak prompts, or on output control and detection, which are computationally expensive as they require LLM inference. Therefore, designing a pre-inference defense method that resists diverse jailbreak prompts is crucial for preventing LLM jailbreak attacks. We observe that jailbreak attacks, safe queries, and harmful queries exhibit different clustering patterns within the LLM's hidden state representation space. This suggests that by leveraging the LLM's hidden state representational capabilities, we can analyze the LLM's forthcoming behavior and proactively intervene for defense. In this paper, we propose a jailbreak attack defense strategy based on a Hidden State Filter (HSF), a lossless architectural defense mechanism that enables the model to preemptively identify and reject adversarial inputs before the inference process begins. We activate its defensive potential through an additional plugin module, effectively framing the defense task as a classification problem. Experimental results on two benchmark datasets, utilizing three different LLMs, show that HSF significantly enhances resilience against six cutting-edge jailbreak attacks. It significantly reduces the success rate of jailbreak attacks while minimally impacting responses to benign user queries, with negligible inference overhead, and outperforming defense baselines.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{coppolillo_engagement-driven_2025,
  title = {Engagement-{Driven},
  author = {Coppolillo, Erica and Cinus, Federico and Minici, Marco and Bonchi, Francesco and Manco, Giuseppe},
  year = {2025},
  doi = {10.1145/3711896.3736932},
  url = {https://doi.org/10.1145/3711896.3736932},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {369--379},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {engagement maximization, large language models, reinforcement learning},
  abstract = {Large Language Models (LLMs) demonstrate significant persuasive capabilities in one-on-one interactions, but their influence within social networks, where interconnected users and complex opinion dynamics pose unique challenges, remains underexplored. This paper addresses the research question: Can LLMs generate meaningful content that maximizes user engagement on social networks? To answer this, we propose a pipeline using reinforcement learning with simulated feedback, where the network's response to LLM-generated content (i.e., the reward) is simulated through a formal engagement model. This approach bypasses the temporal cost and complexity of live experiments, enabling an efficient feedback loop between the LLM and the network under study. It also allows to control over endogenous factors such as the LLM's position within the social network and the distribution of opinions on a given topic. Our approach is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. Such flexibility makes it suitable for more complex engagement tasks and interventions in computational social science. Using our framework, we analyze the performance of LLMs in generating social engagement under different conditions, showcasing their full potential in this task. The experimental code is publicly available at https://github.com/mminici/Engagement-Driven-Content-Generation.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{ke_automation_2024,
  title = {Automation, {Trustworthy},
  author = {Ke, Chih-Kun and Wu, Mei-Yu and Chung, Bor-Lin},
  year = {2024},
  doi = {10.1145/3658549.3658561},
  url = {https://doi.org/10.1145/3658549.3658561},
  booktitle = {Proceedings of the 2024 {International},
  pages = {42--47},
  publisher = {Association for Computing Machinery},
  note = {event-place: Taipei, Taiwan},
  keywords = {source: ACM},
  abstract = {Medical institutions are looking forward to importing innovative information services to reduce the burden on medical staff. For example, medical staff repeatedly process the contextual data in each stage of the prenatal examinations, which is labor-intensive and time-consuming. If a dispute happens, the data will become the evidence to support whether the prenatal examinations were handled appropriately. Therefore, if the prenatal examination data is not accurately recorded and properly preserved, it will become a source of pressure for medical staff to face disputes in the future. This research takes the prenatal examinations of pregnant as a use case. We apply robotic process automation technology to assist in the automation of prenatal examinations and blockchain technology to establish trustworthy prenatal examinations. Besides, we also use generative artificial intelligence technology to provide intelligent user-assisted consultation in open-domain question answering. The contribution of this work is to build an automated reliable prenatal examination information service to provide good care for pregnant.},
  address = {New York, NY, USA},
  series = {I-{DO},
  isbn = {979-8-4007-0918-0},
}

@inproceedings{zhang_towards_2025-1,
  title = {Towards {AI},
  author = {Zhang, Han and Shalev-Arkushin, Rotem and Baltatzis, Vasileios and Gillis, Connor and Laput, Gierad and Kushalnagar, Raja and Quandt, Lorna C and Findlater, Leah and Bedri, Abdelkareem and Lea, Colin},
  year = {2025},
  doi = {10.1145/3706598.3713855},
  url = {https://doi.org/10.1145/3706598.3713855},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {accessibility, assistive technology, DHH community, human-centered design, Sign language generation},
  abstract = {Sign languages are essential for the Deaf and Hard-of-Hearing (DHH) community. Sign language generation systems have the potential to support communication by translating from written languages, such as English, into signed videos. However, current systems often fail to meet user needs due to poor translation of grammatical structures, the absence of facial cues and body language, and insufficient visual and motion fidelity. We address these challenges by building on recent advances in LLMs and video generation models to translate English sentences into natural-looking AI ASL signers. The text component of our model extracts information for manual and non-manual components of ASL, which are used to synthesize skeletal pose sequences and corresponding video frames. Our findings from a user study with 30 DHH participants and thorough technical evaluations demonstrate significant progress and identify critical areas necessary to meet user needs.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{hendrawan_explanations_2024,
  title = {Explanations in {Open},
  author = {Hendrawan, Rully Agus and Brusilovsky, Peter and Lekshmi Narayanan, Arun Balajiee and Barria-Pineda, Jordan},
  year = {2024},
  doi = {10.1145/3631700.3665188},
  url = {https://doi.org/10.1145/3631700.3665188},
  booktitle = {Adjunct {Proceedings},
  pages = {256--263},
  publisher = {Association for Computing Machinery},
  note = {event-place: Cagliari, Italy},
  keywords = {Adaptive explanation, Concept graph, Information exploration, Intelligent interface, Open user model},
  abstract = {Open user models provide affordance for a transparent user control over recommendations based on shared symbolic representation within the system. Users must build their user profile by adding these symbols and tuning their importance to get meaningful recommendations. Since the link between these symbols and the reference explanation is often unavailable, it can be difficult for users to understand them. These symbols are often referred to as concepts, tags, areas, topics, labels, features, or keyphrases. This study showcases an information exploration system that helps students identify potential faculty members to collaborate with. The system works by matching user and faculty profiles that contain keywords or phrases representing topics/areas of interest. Students must develop their understanding of research topics while building their profiles, which can become challenging as they add more keywords. To support students in controlling the recommendation, we introduce post hoc explanations with three levels of detail: no explanations, individual explanation for topics, and explanation of the relationships between topics. This study explores how explanation is associated with the user context / tasks and the exploration process. Our observation suggests that expertise in the field is linked to exploring fewer novel topics and seeking fewer explanations but engaging more with explanations of relationships. In addition, we found that the engagement with faculty information is moderately correlated with the use of more advanced explanations.},
  address = {New York, NY, USA},
  series = {{UMAP},
  isbn = {979-8-4007-0466-6},
}

@inproceedings{man_lusifer_2025,
  title = {{LUSIFER},
  author = {Man, Hieu and Ngo, Nghia Trung and Dac Lai, Viet and Rossi, Ryan A. and Dernoncourt, Franck and Huu Nguyen, Thien},
  year = {2025},
  doi = {10.1145/3726302.3730029},
  url = {https://doi.org/10.1145/3726302.3730029},
  booktitle = {Proceedings of the 48th {International},
  pages = {1360--1370},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {large language models, multilingual benchmarks, multilingual text embedding, representation learning, zero-shot learning},
  abstract = {Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data. The code and dataset for training are available at: https://github.com/hieum98/lusifer},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{liang_how_2025,
  title = {How {Users},
  author = {Liang, Yidong and Wu, Zhijing and Zhang, Fan and Song, Dandan and Huang, Heyan},
  year = {2025},
  doi = {10.1145/3726302.3729998},
  url = {https://doi.org/10.1145/3726302.3729998},
  booktitle = {Proceedings of the 48th {International},
  pages = {634--644},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {generative information retrieval system, search behavior, search experience, user study},
  abstract = {The development of LLM has facilitated the emergence of generative information retrieval (IR) systems, such as ”Bing Chat”. Generative IR systems return generated text with citations rather than a list of ranked search results. User studies on IR systems are essential for understanding users' interaction patterns, evaluating and optimizing systems, and improving search experience, particularly in the context of generative IR systems with novel conversational interfaces and responses. However, systematic investigations into user behavior and search experience on generative IR systems are notably lacking. To address this gap, we conducted a user study using Bing Chat to explore user behavior and feedback on generative IR systems. The participants were required to accomplish three types of tasks using Bing Chat. During the search process, we collected their various behavior (e.g., click, query reformulation) and explicit feedback (e.g., satisfaction, credibility, and success). Additionally, the same study was conducted on traditional IR systems Bing for comparison. Analyses of these data show that Bing Chat can reduce the user's search effort and lead to a better search experience without any decrease in credibility compared with Bing. We believe that this work provides valuable insight into the design and evaluation of generative information retrieval systems.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{chen_efficient_2025,
  title = {Efficient and {Scalable},
  author = {Chen, Junjie},
  year = {2025},
  doi = {10.1145/3732365.3732398},
  url = {https://doi.org/10.1145/3732365.3732398},
  booktitle = {Proceedings of the 2025 5th {International},
  pages = {195--199},
  publisher = {Association for Computing Machinery},
  keywords = {Systematic management platforms, Task economy},
  abstract = {Task economy is characterized by fast demand variation and heterogeneous data generated from different sources. Platforms operating in this space require real-time analytics to influence decision-making and improve service offerings, so fast and effective data processing is critical. We introduce a detailed framework to build over effective and scalable data hardware for gig economy platforms at scale in this paper. We propose a new modular architecture that addresses the systematic management of processing tasks and the seamless integration of individual data sources. It combines processing techniques of streaming and batch to elevate the data movement and reduce delay.By using microservices architecture, the framework enables the independent deployment of components, which increases its flexibility and robustness. Using substantial benchmarking metrics on real-world datasets allows for implementation speedups and resource consumption compared to classical methods while still allowing gig-economy platforms to hand-click large amounts of data and adapt quickly to changing market conditions while minimizing storage costs.},
  address = {New York, NY, USA},
  series = {{CNSSE},
  isbn = {979-8-4007-1361-3},
}

@inproceedings{zhu_power_2025,
  title = {Power {Knowledge},
  author = {Zhu, Jiazheng and Liang, Xiao and Xu, Jiannan and Zhang, Yingqiang and Zhang, Zhonghao and He, Kejia},
  year = {2025},
  doi = {10.1145/3733054.3733082},
  url = {https://doi.org/10.1145/3733054.3733082},
  booktitle = {Proceedings of the 2025 {International},
  pages = {153--158},
  publisher = {Association for Computing Machinery},
  keywords = {Agent framework, Chain-of-Thought, In-Context Learning, Large language models, Power knowledge question-answering},
  abstract = {Electric power industry is very important to our modern life. The traditional knowledge and question-answering can not adapt to the increasingly complex huge data with the development of power system. Although the LLMs succeed in common cases, they can hardly understand and answer questions from the power industries successfully. How to effectively solve some hard problems, such as comprehensively apply the knowledge base of the power industry domain, improve the multi-jump and complicated QA capability, avoid refine tuning of the large models and so on has never been reported before. To fill the research vacuum points above mentioned, this paper has explored a way to construct CoT dataset, design an ICL technique in-context learning and generate the agent framework for electricity-related QA task respectively. Experiments show that: Compared with other ways, using Agent + CoT + ICL methods makes our model perform better in accuracy and solving skills; it verifies the effectiveness of the method designed here.},
  address = {New York, NY, USA},
  series = {{ISAC},
  isbn = {979-8-4007-1519-8},
}

@inproceedings{wang_wisdom_2024,
  title = {{WisdoM},
  author = {Wang, Wenbin and Ding, Liang and Shen, Li and Luo, Yong and Hu, Han and Tao, Dacheng},
  year = {2024},
  doi = {10.1145/3664647.3681403},
  url = {https://doi.org/10.1145/3664647.3681403},
  booktitle = {Proceedings of the 32nd {ACM},
  pages = {2282--2291},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {contextual fusion, contextual world knowledge, large vision-language model, multimodal sentiment analysis},
  abstract = {Multimodal Sentiment Analysis (MSA) focuses on leveraging multimodal signals for understanding human sentiment. Most of the existing works rely on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs), thereby restricting their ability to achieve better multimodal sentiment analysis (MSA). In this paper, we propose a plug-in framework named WisdoM, to leverage the contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced MSA. WisdoM utilizes LVLMs to comprehensively analyze both images and corresponding texts, simultaneously generating pertinent context. Besides, to reduce the noise in the context, we design a training-free contextual fusion mechanism. We evaluate our WisdoM in both the aspect-level and sentence-level MSA tasks on the Twitter2015, Twitter2017, and MSED datasets. Experiments on three MSA benchmarks upon several advanced LVLMs, show that our approach brings consistent and significant improvements (up to +6.3\% F1 score). Code is available at https://github.com/DreamMr/WisdoM.},
  address = {New York, NY, USA},
  series = {{MM},
  isbn = {979-8-4007-0686-8},
}

@inproceedings{kim_transdisciplinary_2025,
  title = {On {Transdisciplinary},
  author = {Kim, Junwhan},
  year = {2025},
  doi = {10.1145/3702163.3702465},
  url = {https://doi.org/10.1145/3702163.3702465},
  booktitle = {Proceedings of the 2024 16th {International},
  pages = {523--528},
  publisher = {Association for Computing Machinery},
  keywords = {Data Science and Engineering Education, Transdisciplinary Research},
  abstract = {This paper explores the role of Data Science and Engineering (DSE) education in fostering transdisciplinary research and innovation. By combining data science’s analytical capabilities with the infrastructure-building focus of data engineering, DSE offers powerful tools for addressing complex challenges across diverse fields such as finance, agriculture, law, and engineering. However, researchers outside traditional DSE domains often lack the expertise to manage and leverage data effectively. To bridge this gap, we have developed four courses—data science, data engineering, advanced DSE, and vision AI—at the University of the District of Columbia, designed to equip students from various disciplines with the necessary skills to apply DSE techniques in their respective fields. This paper highlights the research conducted by students in these courses, emphasizing the importance of transdisciplinary collaboration in advancing scientific discovery and technological innovation.},
  address = {New York, NY, USA},
  series = {{ICETC},
  isbn = {979-8-4007-1781-9},
}

@inproceedings{mridul_provakar_exploring_2025,
  title = {Exploring the {Effectiveness},
  author = {Mridul Provakar, Mondol and Hashi, Emrana Kabir},
  year = {2025},
  doi = {10.1145/3723178.3723244},
  url = {https://doi.org/10.1145/3723178.3723244},
  booktitle = {Proceedings of the 3rd {International},
  pages = {498--505},
  publisher = {Association for Computing Machinery},
  keywords = {Financial Question Answering, Fine-Tuning, Gemma, Large Language Models, Llama-2, Low-Rank-Adaptation, Prompt Engineering},
  abstract = {The financial sector is undergoing a profound transformation as AI technologies, particularly large language models (LLMs), revolutionize financial analysis through efficient and accurate natural language processing (NLP). This study investigates the efficacy of LLMs in addressing financial question-answering tasks, focusing specifically on two state-of-the-art models: Llama2-7b by Meta and Gemma-7b by Google. Despite their established prowess in general NLP tasks, their suitability for domain-specific applications, such as financial question answering, necessitates further exploration. Employing a comprehensive evaluation approach encompassing zero-shot prompt engineering, few-shot prompt engineering, and supervised fine-tuning methodologies, this study assesses the performance of Llama2 and Gemma using key metrics, including ROUGE-L, cosine similarity, and human evaluation. The preliminary findings reveal significant distinctions between the two models. Llama2 demonstrates a higher frequency of correct answers, but it is prone to hallucinations, often producing incorrect or incomplete information. In contrast, Gemma’s performance is notably inferior, struggling to respond accurately to most queries. These observations highlight the need for continued research to enhance LLMs’ ability to answer financial questions, while this study offers key insights into their strengths and weaknesses in managing financial inquiries.},
  address = {New York, NY, USA},
  series = {{ICCA},
  isbn = {979-8-4007-1382-8},
}

@inproceedings{gienapp_evaluating_2024,
  title = {Evaluating {Generative},
  author = {Gienapp, Lukas and Scells, Harrisen and Deckers, Niklas and Bevendorff, Janek and Wang, Shuai and Kiesel, Johannes and Syed, Shahbaz and Fröbe, Maik and Zuccon, Guido and Stein, Benno and Hagen, Matthias and Potthast, Martin},
  year = {2024},
  doi = {10.1145/3626772.3657849},
  url = {https://doi.org/10.1145/3626772.3657849},
  booktitle = {Proceedings of the 47th {International},
  pages = {1916--1929},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {ad hoc search, evaluation, generative information retrieval},
  abstract = {Recent advances in large language models have enabled the development of viable generative retrieval systems. Instead of a traditional document ranking, generative retrieval systems often directly return a grounded generated text as a response to a query. Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval. Yet, the established evaluation methodology for ranking-based ad hoc retrieval is not suited for the reliable and reproducible evaluation of generated responses. To lay a foundation for developing new evaluation methods for generative retrieval systems, we survey the relevant literature from the fields of information retrieval and natural language processing, identify search tasks and system architectures in generative retrieval, develop a new user model, and study its operationalization.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{liu_leveraging_2025,
  title = {Leveraging {Passage},
  author = {Liu, Qi and Wang, Bo and Wang, Nan and Mao, Jiaxin},
  year = {2025},
  doi = {10.1145/3696410.3714554},
  url = {https://doi.org/10.1145/3696410.3714554},
  booktitle = {Proceedings of the {ACM},
  pages = {4274--4283},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {efficiency, large language models, reranking},
  abstract = {Recent studies have demonstrated the effectiveness of using large language language models (LLMs) in passage ranking. The listwise approaches, such as RankGPT, have become new state-of-the-art in this task. However, the efficiency of RankGPT models is limited by the maximum context length and relatively high latency of LLM inference. To address these issues, in this paper, we propose PE-Rank, leveraging the single passage embedding as a good context compression for efficient listwise passage reranking. By treating each passage as a special token, we can directly input passage embeddings into LLMs, thereby reducing input length. Additionally, we introduce an inference method that dynamically constrains the decoding space to these special tokens, accelerating the decoding process. For adapting the model to reranking, we employ listwise learning to rank loss for training. Evaluation results on multiple benchmarks demonstrate that PE-Rank significantly improves efficiency in both prefilling and decoding, while maintaining competitive ranking effectiveness. The code is available at https://github.com/liuqi6777/pe\_rank},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{chopra_challenges_2025,
  title = {Challenges in {Using},
  author = {Chopra, Bhavya and Singha, Ananya and Fariha, Anna and Gulwani, Sumit and Parnin, Chris and Tiwari, Ashish and Henley, Austin Z.},
  year = {2025},
  doi = {10.1145/3736733.3736748},
  url = {https://doi.org/10.1145/3736733.3736748},
  booktitle = {Proceedings of the {Workshop},
  publisher = {Association for Computing Machinery},
  note = {event-place: Intercontinental Berlin, Berlin, Germany},
  keywords = {computational notebooks, data science, large language models},
  abstract = {Large Language Models (LLMs) are transforming data science, offering assistance in coding, preprocessing, analysis, and decisionmaking. However, data scientists face significant challenges when interacting with LLM-powered agents and implementing their suggestions effectively. To explore these challenges, we conducted a mixed-methods study comprising contextual observations, semi-structured interviews (n=14), and a survey (n=114). Our findings reveal key obstacles, including difficulties in retrieving contextual data, crafting prompts for complex tasks, adapting generated code to local environments, and refining prompts iteratively. Based on these insights, we propose actionable design recommendations, such as data brushing for improved context selection and inquisitive feedback loops to enhance communication with conversational AI assistants in data science workflows.},
  address = {New York, NY, USA},
  series = {{HILDA},
  isbn = {979-8-4007-1959-2},
}

@inproceedings{ryu_cinema_2025,
  title = {Cinema {Multiverse},
  author = {Ryu, Jeongwoo and Kim, Kyusik and Heo, Dongseok and Song, Hyungwoo and Oh, Changhoon and Suh, Bongwon},
  year = {2025},
  doi = {10.1145/3706598.3713641},
  url = {https://doi.org/10.1145/3706598.3713641},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational AI, Film appreciation, Media consumption, Multi-agent systems, Parasocial relationship, User engagement, Virtual personas},
  abstract = {Advancements in large language models (LLMs) enable the development of interactive systems that enhance user engagement with cinematic content. We introduce Cinema Multiverse Lounge, a multi-agent conversational system where users interact with LLM-based agents embodying diverse film-related personas. We investigate how user interactions with these agents influence their film appreciation. Thirty participants engaged in three discussion sessions, freely selecting persona agents such as film characters, filmmakers, or anonymous audiences. We explored how users composed different combinations of personas, the factors affecting their engagement and interpretation, and how diverse perspectives influenced film appreciation. Results indicate that interactions with varied agents enhanced participants’ appreciation by enabling the exploration of multiple viewpoints and fostering deeper narrative engagement. Moreover, the unexpected clashes between different worldviews added a fresh and enjoyable layer to the interactions. Our findings provide empirical insights and design implications for developing multi-agent systems that support enriched media consumption experiences.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{al_haque_evolution_2025,
  title = {The {Evolution},
  author = {Al Haque, Ebtesam and Brown, Chris and LaToza, Thomas D. and Johnson, Brittany},
  year = {2025},
  doi = {10.1145/3696630.3731665},
  url = {https://doi.org/10.1145/3696630.3731665},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1494--1502},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {ai, developers, information seeking, mixed-methods study, productivity, skill building, software engineering, source: ACM},
  abstract = {About 32\% of a software practitioners' day involves seeking and using information to support task completion. Although the information needs of software practitioners have been studied extensively, the impact of AI-assisted tools on their needs and information-seeking behaviors remains largely unexplored. To addresses this gap, we conducted a mixed-method study to understand AI-assisted information seeking behavior of practitioners and its impact on their perceived productivity and skill development. We found that developers are increasingly using AI tools to support their information seeking, citing increased efficiency as a key benefit. Our findings also amplify caveats that come with effectively using AI tools for information seeking, especially for learning and skill development, such as the importance of foundational developer knowledge that can guide and inform the information provided by AI tools. Our efforts have implications for the effective integration of AI tools into developer workflows as information retrieval systems and learning aids.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{janaka_tom_2024,
  title = {{TOM},
  author = {Janaka, Nuwan and Zhao, Shengdong and Hsu, David and Tan Jing Wen, Sherisse and Koh, Chun Keat},
  year = {2024},
  doi = {10.1145/3675094.3678382},
  url = {https://doi.org/10.1145/3675094.3678382},
  booktitle = {Companion of the 2024 on {ACM},
  pages = {837--843},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {ai assistance, ai/ml, ar, augmented reality, context-aware system, hmd, interactions, mr, smart glasses, wearable, xr, source: ACM},
  abstract = {Advanced wearable digital assistants can significantly enhance task performance, reduce user burden, and provide personalized guidance to improve users' abilities. However, developing these assistants presents several challenges. To address this, we introduce TOM (The Other Me), a conceptual architecture and open-source software platform (https://github.com/TOM-Platform) that supports the development of wearable intelligent assistants that are contextually aware of both the user and the environment. Collaboratively developed with researchers and developers, TOM meets their diverse requirements. TOM facilitates the creation of intelligent assistive AR applications for daily activities, supports the recording and analysis of user interactions, and provides assistance for various activities, as demonstrated in our preliminary evaluations.},
  address = {New York, NY, USA},
  series = {{UbiComp},
  isbn = {979-8-4007-1058-2},
}

@inproceedings{liang_aligning_2024,
  title = {Aligning {Large},
  author = {Liang, Yuanyuan and Tan, Keren and Xie, Tingyu and Tao, Wenbiao and Wang, Siyuan and Lan, Yunshi and Qian, Weining},
  year = {2024},
  doi = {10.1145/3627673.3679713},
  url = {https://doi.org/10.1145/3627673.3679713},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1367--1377},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {graph databases, graph query language, large language models, natural language to graph query language},
  abstract = {Graph Databases (Graph DB) find extensive application across diverse domains such as finance, social networks, and medicine. Yet, the translation of Natural Language (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses significant challenges owing to its intricate and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks tailored to a particular domain, the absence of domain-specific NL-GQL data pairs adds complexity to aligning LLMs with the graph DB. To tackle this challenge, we present a well-defined pipeline. Initially, we use ChatGPT to generate NL-GQL data pairs, leveraging the provided graph DB and two mutual verification self-instruct methods which ensure consistency between NL and GQL. Subsequently, we employ the generated data to fine-tune LLMs, ensuring alignment between LLMs and the graph DB. Moreover, we find the importance of relevant schema in efficiently generating accurate GQLs. Thus, we introduce a method to extract relevant schema as the input context. We evaluate our method using two carefully constructed datasets derived from graph DBs in the finance and medicine domains, named FinGQL and MediGQL. Experimental results reveal that our approach significantly outperforms a set of baseline methods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on EX for FinGQL and MediGQL, respectively},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{frank_designing_2025,
  title = {Designing and {Operating},
  author = {Frank, Ian},
  year = {2025},
  doi = {10.1145/3719487.3719508},
  url = {https://doi.org/10.1145/3719487.3719508},
  booktitle = {Proceeding of the 2024 8th {International},
  pages = {83--90},
  publisher = {Association for Computing Machinery},
  keywords = {Digital transformation (DX), Engagement, Interfaces, Learning management systems},
  abstract = {This paper describes the development, operation, and lessons learned from a low-code template designed for class delivery and administration. Built up over four years of highly-evaluated weekly online classes, the template has fostered increased student-teacher interaction, demonstrated the benefits of a database representation of contents, and underscored the importance of visual design. Additionally, it facilitated the incorporation of insights from online streaming as well as experimentation with the integration of AI capabilities. Key design elements include visually appealing layouts to streamline class flow and administration, the extensive use of icons and thumbnails, and Q\&amp;A banners for class feedback. This paper discusses the system development, challenges faced during implementation, and pathways for others to adapt a similar approach. In sharing both a template and the experiences, the goal is to offer signposts for others to deploy similar frameworks, in the pursuit of efficient, engaging, and adaptable classes, whether online or face-to-face.},
  address = {New York, NY, USA},
  series = {{ICEEL},
  isbn = {979-8-4007-1741-3},
}

@inproceedings{han_towards_2025,
  title = {Towards {Unobtrusive},
  author = {Han, Violet Yinuo and Gonzalez, Jesse T and Yang, Christina and Wang, Zhiruo and Hudson, Scott E and Ion, Alexandra},
  year = {2025},
  doi = {10.1145/3746059.3747726},
  url = {https://doi.org/10.1145/3746059.3747726},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Agents, Human-AI Interaction, Intention Inference, Large Language Models, Physical AI, Proactive Human Robot Interaction, Robotic Objects, Tangible Interfaces},
  abstract = {Users constantly interact with physical, most often passive, objects. Consider if familiar objects instead proactively assisted users, e.g., a stapler moving across the table to help users organize documents, or a knife moving away to prevent injury as the user is inattentively about to lean against the countertop. In this paper, we build on the qualities of tangible interaction and focus on recognizing user needs in everyday tasks to enable ubiquitous yet unobtrusive tangible interaction. To achieve this, we introduce an architecture that leverages large language models\&nbsp;(LLMs) to perceive users’ environment and activities, perform spatial-temporal reasoning, and generate object actions aligned with inferred user intentions and object properties. We demonstrate the system’s utility providing proactive assistance with multiple objects and in various daily scenarios. To evaluate our system components, we compare our system-generated output for user goal estimation and object action recommendation with human-annotated baselines, with results indicating good agreement.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{rzepka_effectiveness_2025,
  title = {Effectiveness of {Security},
  author = {Rzepka, Rafal and Obayashi, Akihiko},
  year = {2025},
  doi = {10.1145/3704137.3704180},
  url = {https://doi.org/10.1145/3704137.3704180},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {156--161},
  publisher = {Association for Computing Machinery},
  keywords = {Expert Systems, GraphRAG, Knowledge Graph, Large Language Models, Security Export Control},
  abstract = {In this paper we present results of our experiments investigating if an expert knowledge graph can improve Large Language Models accuracy in predicting correct answer labels and regulations related to the topic of security export control. As the lack of related data prevents machine-learning or fine-tuning approaches, we implement prompt expansion by searching most relevant nodes of the graph and adding the expanded context to the prompt. Results of our experiments show that the addition improved answer type selection but clearly hamper the capability of finding a correct regulation category.},
  address = {New York, NY, USA},
  series = {{ICAAI},
  isbn = {979-8-4007-1801-4},
}

@inproceedings{nelson_sensai_2025,
  title = {{SENSAI},
  author = {Nelson, Connor and Doupé, Adam and Shoshitaishvili, Yan},
  year = {2025},
  doi = {10.1145/3641554.3701801},
  url = {https://doi.org/10.1145/3641554.3701801},
  booktitle = {Proceedings of the 56th {ACM},
  pages = {833--839},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {source: ACM},
  abstract = {The modern educational landscape faces the challenge of maintaining effective, personalized mentorship amid expanding class sizes. This challenge is particularly pronounced in fields requiring hands-on practice, such as cybersecurity education. Teaching assistants and peer interactions provide some relief, but the student-to-educator ratio often remains high, limiting individualized attention. The advent of Large Language Models (LLMs) offers a promising solution by potentially providing scalable and personalized guidance. In this paper, we introduce SENSAI, an AI-powered tutoring system that leverages LLMs to offer tailored feedback and assistance by transparently extracting and utilizing the learner's working context, including their active terminals and edited files. Over the past year, SENSAI has been deployed in an applied cybersecurity curriculum at a large public R1 university and made available to a broader online community of global learners, assisting 2,742 users with hundreds of educational challenges. In total 178,074 messages were exchanged across 15,413 sessions, incurring a total cost of 1,979–comparable to that of a single undergraduate teaching assistant but with a significantly wider reach. SENSAI demonstrates significant improvements in student problem-solving efficiency and satisfaction, offering insights into the future role of AI in education.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0531-1},
  series = {{SIGCSETS},
}

@inproceedings{zheng_humanevo_2025,
  title = {{HumanEvo},
  author = {Zheng, Dewu and Wang, Yanlin and Shi, Ensheng and Zhang, Ruikai and Ma, Yuchi and Zhang, Hongyu and Zheng, Zibin},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00228},
  url = {https://doi.org/10.1109/icse55347.2025.00228},
  booktitle = {Proceedings of the {IEEE},
  pages = {1372--1384},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {To evaluate the repository-level code generation capabilities of Large Language Models (LLMs) in complex real-world software development scenarios, many evaluation methods have been developed. These methods typically leverage contextual code from the latest version of a project to assist LLMs in accurately generating the desired function. However, such evaluation methods fail to consider the dynamic evolution of software projects over time, which we refer to as evolution-ignored settings. This in turn results in inaccurate evaluation of LLMs' performance. In this paper, we conduct an empirical study to deeply understand LLMs' code generation performance within settings that reflect the evolution nature of software development. To achieve this, we first construct an evolution-aware repository-level code generation dataset, namely HumanEvo, equipped with an automated execution-based evaluation tool. Second, we manually categorize HumanEvo according to dependency levels to more comprehensively analyze the model's performance in generating functions with different dependency levels. Third, we conduct extensive experiments on HumanEvo with seven representative and diverse LLMs to verify the effectiveness of the proposed benchmark. We obtain several important findings through our experimental study. For example, we find that previous evolution-ignored evaluation methods result in inflated performance of LLMs, with performance overestimations ranging from 10.0\% to 61.1\% under different context acquisition methods, compared to the evolution-aware evaluation approach. Based on the findings, we give actionable suggestions for more realistic evaluation of LLMs on code generation. We also build a shared evolution-aware code generation toolbox to facilitate future research. The replication package including source code and datasets is anonymously available at https://github.com/DeepSoftwareAnalytics/HumanEvo.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{xiao_comparative_2024,
  title = {A {Comparative},
  author = {Xiao, Weizhen},
  year = {2024},
  doi = {10.1145/3677779.3677791},
  url = {https://doi.org/10.1145/3677779.3677791},
  booktitle = {Proceedings of the {International},
  pages = {76--80},
  publisher = {Association for Computing Machinery},
  note = {event-place: Xi'an, China},
  keywords = {source: ACM},
  abstract = {Financial sentiment analysis, the task of discerning market sentiment from financial texts, plays a crucial role in investment decisions, risk assessment, and understanding economic trends. Traditional sentiment analysis techniques have often faced limitations in handling the complexities and nuances of financial language. The advent of large language models (LLMs) has brought a paradigm shift in this field. With their remarkable ability to process and understand natural language, LLMs are enabling new approaches that increase the accuracy and sophistication of financial sentiment analysis. This paper provides a comparative overview of cutting-edge LLM-based techniques for financial sentiment analysis. We introduce a six-pronged classification framework covering data types, sentiment granularity, model architectures, training approaches, methodological focus, and evaluation metrics. This framework aims to provide a structured perspective for understanding recent research trends. Our analysis reveals several key developments in the field. We discuss the challenges and opportunities associated with advanced techniques, like Instruction-tuning approaches and Retrieval-augmented methods. While LLMs offer clear advantages, ensuring data quality, mitigating bias, enhancing model explainability, and scaling these models to real-world applications remain active research areas. This review offers investors and financial researchers a comprehensive guide to the evolving landscape of financial sentiment analysis, facilitating well-informed choices for different use cases and laying the groundwork for future research.},
  address = {New York, NY, USA},
  series = {{CMNM},
  isbn = {979-8-4007-0976-0},
}

@inproceedings{lin_jupiter_2025,
  title = {Jupiter: {Pushing},
  author = {Lin, Zhiheng and Meng, Ke and Xu, Changjie and Cao, Weichen and Tan, Guangming},
  year = {2025},
  doi = {10.1145/3689031.3717491},
  url = {https://doi.org/10.1145/3689031.3717491},
  booktitle = {Proceedings of the {Twentieth},
  pages = {558--572},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {Delegation, GPU, Subgraph Matching},
  abstract = {Graph pattern matching (GPM) aims to find subgraphs isomorphic to user-specified patterns within a large graph. Due to its ability to reveal potential relationships among entities in complex networks, it is widely applied in various fields, such as mining molecular structures in bioinformatics, detecting fraud in cloud-based e-commerce, and querying knowledge graphs in large language model. The explosion of data brought by the AI era has rendered traditional GPM systems inadequate for real-world needs. Due to the intricate data dependencies of GPM tasks, most SOTA GPM systems currently have limited scalability and performance, they perform well in small graph mining with single node but cannot scale to modern clusters with GPU acceleration. This paper introduces JUPITER, the first system capable of matching patterns on large graph across multi-node GPU clusters, which can handle graphs 10 times larger than SOTAs with the same memory resources. Its core principle is to delegate computation to the data-residing processing unit rather than pulling data to the computation location, which greatly improves communication efficiency. Experimental results show that JUPITER can reduce communication volume by two orders of magnitude compared to SOTA subgraph matching systems, achieving up to 120× speedup and an average of 21.5× speedup.},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{rajapakse_simple_2024,
  title = {Simple {Transformers},
  author = {Rajapakse, Thilina C. and Yates, Andrew and de Rijke, Maarten},
  year = {2024},
  doi = {10.1145/3673791.3698412},
  url = {https://doi.org/10.1145/3673791.3698412},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {209--215},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {open source information retrieval, source: ACM},
  abstract = {Language technology, particularly information retrieval, is poised to have a profound impact on society. We believe that technology with such far-reaching potential should be accessible to everyone, not just the technologically privileged. Therefore, we advocate for open-source for all, ensuring that individuals from diverse research areas, societal sectors, and backgrounds have access to information retrieval and language technology tools with low barriers to entry. In this paper, we describe Simple Transformers, a library created with these goals in mind. It is designed to simplify the training, evaluation, and usage of transformer models. As of 2024, the library has garnered over 4,000 stars on GitHub and has been downloaded over 3 million times. These metrics reflect its wide acceptance and usage across different sectors. We describe the design and implementation of the library, provide examples of its usage and adoption, Finally, we also reflect on how Simple Transformers contributes to the goal of ”open-source for all.”},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{lu_proof_2024,
  title = {Proof {Automation},
  author = {Lu, Minghai and Delaware, Benjamin and Zhang, Tianyi},
  year = {2024},
  doi = {10.1145/3691620.3695521},
  url = {https://doi.org/10.1145/3691620.3695521},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {1509--1520},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {source: ACM},
  abstract = {Interactive theorem provers such as Coq are powerful tools to formally guarantee the correctness of software. However, using these tools requires significant manual effort and expertise. While Large Language Models (LLMs) have shown promise in automatically generating informal proofs in natural language, they are less effective at generating formal proofs in interactive theorem provers. In this paper, we conduct a formative study to identify common mistakes made by LLMs when asked to generate formal proofs. By analyzing 520 proof generation errors made by GPT-3.5, we found that GPT-3.5 often identified the correct high-level structure of a proof, but struggled to get the lower-level details correct. Based on this insight, we propose PALM, a novel generate-then-repair approach that first prompts an LLM to generate an initial proof and then leverages targeted symbolic methods to iteratively repair low-level problems. We evaluate PALM on a large dataset that includes more than 10K theorems. Our results show that PALM significantly outperforms other state-of-the-art approaches, successfully proving 76.6\% to 180.4\% more theorems. Moreover, PALM proves 1270 theorems beyond the reach of existing approaches. We also demonstrate the generalizability of PALM across different LLMs.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{barile_lp-dixit_2025,
  title = {{LP},
  author = {Barile, Roberto and d'Amato, Claudia and Fanizzi, Nicola},
  year = {2025},
  doi = {10.1145/3696410.3714667},
  url = {https://doi.org/10.1145/3696410.3714667},
  booktitle = {Proceedings of the {ACM},
  pages = {4034--4042},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {explanation, knowledge graphs, large language models, link prediction, source: ACM},
  abstract = {Knowledge Graphs provide a machine-readable representation of knowledge conforming to graph-based data models. Link prediction methods predict missing facts in incomplete knowledge graphs, often using scalable embedding based solutions that, however, lack comprehensibility which is crucial in many domains. Filling this gap, explanation methods identify supporting knowledge. For evaluating them, user studies are the obvious choice as users are the main recipients of explanations. However, finding domain experts is often challenging. In contrast, an automated approach is to measure the influence of explanations on the very same link prediction task, thus disregarding the perspective of users. Additionally, current evaluation methods vary across different explanation approaches. We propose LP-DIXIT, the first protocol to evaluate the utility of explanations of link predictions. LP-DIXIT is user-aware, algorithmic and unique for different explanation methods. It builds on a typical setting of user studies, but adopts Large Language Models (LLMs) to mimic users. Specifically, it measures how explanations improve the user (LLM) ability to perform predictions, which is key to trust. We experimentally proved an overall agreement between LP-DIXIT and user evaluations. Moreover, we adopted LP-DIXIT to conduct a comparative study of state-of-the-art explanation methods. The outcomes suggest that less is more: the most effective explanations are those consisting of a single fact.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{jiang_scpatcher_2024,
  title = {{SCPatcher},
  author = {Jiang, Ziyou and Shi, Lin and Yang, Guowei and Wang, Qing},
  year = {2024},
  doi = {10.1109/ASE56229.2023.00040},
  url = {https://doi.org/10.1109/ase56229.2023.00040},
  booktitle = {Proceedings of the 38th {IEEE},
  pages = {358--370},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically one-sentence principles without detailed specifications, e.g., "Properly free allocated memory upon the completion of functions and at all exit points.", which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73\% MLine on average, as well as coding explanations with 3.97\% F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.},
  address = {Echternach, Luxembourg},
  series = {{ASE},
  isbn = {979-8-3503-2996-4},
}

@inproceedings{yang_bee_2025,
  title = {{BEE},
  author = {Yang, Chen and Gross, Markus and Wampfler, Rafael},
  year = {2025},
  doi = {10.1145/3717511.3747067},
  url = {https://doi.org/10.1145/3717511.3747067},
  booktitle = {Proceedings of the 25th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {Belief-value Alignment, Cognitive Framework, Explainable Agents, Human-Agent Interaction, Social Agents},
  abstract = {Recent advances in large language models have enabled virtual agents to exhibit increasingly believable social behaviors. However, creating social agents that remain consistent with a defined profile and explain their reasoning remains challenging. We introduce a cognitive framework designed to address these gaps. Our framework features a graph-based memory module (concept pool) and a decision-making process inspired by human cognition. The concept pool contains an agent’s beliefs, values, and background stories for context-dependent retrieval. The decision-making process uses the concept pool to produce belief-value aligned responses of the virtual agent and intuitive, human-readable explanations of the reasoning. To evaluate the effectiveness of our framework, we created two virtual agents based on historical figures and compared them to baseline agents. Our evaluation combined quantitative assessments of belief-value alignment with a user study (n=48) examining explainable agency. Results show that our framework exhibits model-agnostic improved belief-value alignment and produces more detailed, relevant, and understandable explanations. By grounding virtual agent behavior in structured memories and cognitive principles, our framework offers a compelling step toward more coherent and socially intelligent virtual agents.},
  address = {New York, NY, USA},
  series = {{IVA},
  isbn = {979-8-4007-1508-2},
}

@inproceedings{fan_lessonplanner_2024,
  title = {{LessonPlanner},
  author = {Fan, Haoxiang and Chen, Guanzheng and Wang, Xingbo and Peng, Zhenhui},
  year = {2024},
  doi = {10.1145/3654777.3676390},
  url = {https://doi.org/10.1145/3654777.3676390},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Large language models, lesson plan preparation, pedagogy-driven system},
  abstract = {Preparing a lesson plan, e.g., a detailed road map with strategies and materials for instructing a 90-minute class, is beneficial yet challenging for novice teachers. Large language models (LLMs) can ease this process by generating adaptive content for lesson plans, which would otherwise require teachers to create from scratch or search existing resources. In this work, we first conduct a formative study with six novice teachers to understand their needs for support of preparing lesson plans with LLMs. Then, we develop LessonPlanner that assists users to interactively construct lesson plans with adaptive LLM-generated content based on Gagne’s nine events. Our within-subjects study (N = 12) shows that compared to the baseline ChatGPT interface, LessonPlanner can significantly improve the quality of outcome lesson plans and ease users’ workload in the preparation process. Our expert interviews (N = 6) further demonstrate LessonPlanner ’s usefulness in suggesting effective teaching strategies and meaningful educational resources. We discuss concerns on and design considerations for supporting teaching activities with LLMs.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{akyash_evolutionary_2024,
  title = {Evolutionary {Large},
  author = {Akyash, Mohammad and M Kamali, Hadi},
  year = {2024},
  doi = {10.1145/3649476.3660390},
  url = {https://doi.org/10.1145/3649476.3660390},
  booktitle = {Proceedings of the {Great},
  pages = {496--501},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clearwater, FL, USA},
  keywords = {Hardware Security, Large Language Models, RTL Debugging},
  abstract = {Automating hardware (HW) security vulnerability detection and mitigation during the design phase is imperative for two reasons: (i) It must be before chip fabrication, as post-fabrication fixes can be costly or even impractical; (ii) The size and complexity of modern HW raise concerns about unknown vulnerabilities compromising CIA triad. While Large Language Models (LLMs) can revolutionize both HW design and testing processes, within the semiconductor context, LLMs can be harnessed to automatically rectify security-relevant vulnerabilities inherent in HW designs. This study explores the seeds of LLM integration in register transfer level (RTL) designs, focusing on their capacity for autonomously resolving security-related vulnerabilities. The analysis involves comparing methodologies, assessing scalability, interpretability, and identifying future research directions. Potential areas for exploration include developing specialized LLM architectures for HW security tasks and enhancing model performance with domain-specific knowledge, leading to reliable automated security measurement and risk mitigation associated with HW vulnerabilities.},
  address = {New York, NY, USA},
  series = {{GLSVLSI},
  isbn = {979-8-4007-0605-9},
}

@inproceedings{hoeber_design_2025,
  title = {Design {Principles},
  author = {Hoeber, Orland},
  year = {2025},
  doi = {10.1145/3698204.3716443},
  url = {https://doi.org/10.1145/3698204.3716443},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {12--22},
  publisher = {Association for Computing Machinery},
  keywords = {design principles, exploratory search, heuristic evaluations, scoping review, search user interfaces},
  abstract = {Exploratory search has been proposed as a model of search behaviour that is well suited to complex search scenarios. However, the simple interfaces that are commonplace across many search contexts limit the ability for searchers to undertake exploratory searches. Little support is provided for the discovery, learning, and investigation necessary for exploratory browsing, or the query (re)formulation, result examination, and information extraction required for focused searching. While the design and study of search interfaces that accommodate and support searchers in undertaking exploratory searches has increased in recent years, much of this work has been ad hoc in nature. In this perspective paper, five search interface design principles are presented that are specifically tuned to support exploratory search. An extension of the classical heuristic evaluation method is provided to support the inspection of prototype search interfaces with respect to the design principles. Recent research in the field is categorized according to these design principles. Patterns and gaps in the literature are identified, highlighting opportunities for further research on exploratory search interfaces. These principles provide a framework to guide the design and inspection of future search interfaces to support exploratory search, as well as a mechanism for comparing and contrasting the interactive information retrieval literature as it relates to supporting exploratory search through novel interface design.},
  address = {New York, NY, USA},
  series = {{CHIIR},
  isbn = {979-8-4007-1290-6},
}

@inproceedings{zhong_comapoi_2025,
  title = {{CoMaPOI},
  author = {Zhong, Lin and Wang, Lingzhi and Yang, Xu and Liao, Qing},
  year = {2025},
  doi = {10.1145/3726302.3729930},
  url = {https://doi.org/10.1145/3726302.3729930},
  booktitle = {Proceedings of the 48th {International},
  pages = {1768--1778},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {large language models, multi-agent collaboration, point-of-interest prediction, spatiotemporal modeling, source: ACM},
  abstract = {Large Language Models (LLMs) offer new opportunities for the next Point-Of-Interest (POI) prediction task, leveraging their capabilities in semantic understanding of POI trajectories. However, previous LLM-based methods, which are superficially adapted to next POI prediction, largely overlook critical challenges associated with applying LLMs to this task. Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic understanding of numeric spatiotemporal data, which hinders accurate modeling of users' spatiotemporal distributions and preferences; and (2) an excessively large and unconstrained candidate POI space, which often results in random or irrelevant predictions. To address these issues, we propose a Collaborative Multi-Agent Framework for Next POI Prediction, named CoMaPOI. Through the close interaction of three specialized agents (Profiler, Forecaster, and Predictor), CoMaPOI collaboratively addresses the two critical challenges. The Profiler agent is responsible for converting numeric data into language descriptions, enhancing semantic understanding. The Forecaster agent focuses on dynamically constraining and refining the candidate POI space. The Predictor agent integrates this information to generate high-precision predictions. Extensive experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that CoMaPOI achieves state-of-the-art performance, improving all metrics by 5\% to 10\% compared to SOTA baselines. This work pioneers the investigation of challenges associated with applying LLMs to complex spatiotemporal tasks by leveraging tailored collaborative agents. Our source code is available at: https://github.com/Chips98/CoMaPOI.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{liu_selenite_2024,
  title = {Selenite: {Scaffolding},
  author = {Liu, Michael Xieyang and Wu, Tongshuang and Chen, Tianying and Li, Franklin Mingzhe and Kittur, Aniket and Myers, Brad A},
  year = {2024},
  doi = {10.1145/3613904.3642149},
  url = {https://doi.org/10.1145/3613904.3642149},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {Human-AI Collaboration, Large Language Models, Natural Language Processing, Sensemaking},
  abstract = {Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the “cold-start” problem — it not only requires significant input from previous users to generate and share these overviews, but such overviews may also turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users’ sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Through three studies, we found that Selenite produced accurate and high-quality overviews reliably, significantly accelerated users’ information processing, and effectively improved their overall comprehension and sensemaking experience.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{dinu_evaluating_2025,
  title = {Evaluating {Large},
  author = {Dinu, Anca and Florescu, Andra-Maria and Dinu, Liviu P.},
  year = {2025},
  doi = {10.1145/3733155.3734912},
  url = {https://doi.org/10.1145/3733155.3734912},
  booktitle = {Proceedings of the 18th {ACM},
  pages = {450--457},
  publisher = {Association for Computing Machinery},
  keywords = {Large Language Models, pastiche, stylometry},
  abstract = {This research investigates the ability of Large Language Models (LLMs) to pastiche the literary style of a professional writer.We asked six LLMs to pastiche literary works of 20th-century Romanian authors Mateiu Caragiale and Radu Albala. This task is presumably more complex than imitating the writing style in a contemporary language and a highly resourced language like English.For evaluation, we used standard evaluation metrics (ROUGE, BLUE, METEOR, Diversity, and Perplexity) and linguistic features (extracted with Linguistic Inquiry and Word Count).We compared the quality of the pastiches produced by the LLMs for the two authors, to see if their performance is consistent across different authors. The computational analysis showed that LLMs are able to produce fairly good quality pastiches that could pass for human production, regardless of the author they imitate. However, the manual analysis revealed that, in the case of Albala, the best performing two models, Llama and Qwen, achieved highly similar scores to the originals due to copy-pasting entire paragraphs.},
  address = {New York, NY, USA},
  series = {{PETRA},
  isbn = {979-8-4007-1402-3},
}

@inproceedings{rachabatuni_context-aware_2024,
  title = {Context-aware chatbot using {MLLMs},
  author = {Rachabatuni, Pavan Kartheek and Principi, Filippo and Mazzanti, Paolo and Bertini, Marco},
  year = {2024},
  doi = {10.1145/3625468.3652193},
  url = {https://doi.org/10.1145/3625468.3652193},
  booktitle = {Proceedings of the 15th {ACM},
  pages = {459--463},
  publisher = {Association for Computing Machinery},
  note = {event-place: Bari, Italy},
  keywords = {Chatbot, Cultural Heritage, Digital Learning, Museums, Visual Question Answering, source: ACM},
  abstract = {Multi-modal Large Language Models (MLLMs) are currently an extremely active research topic for the multimedia and computer vision communities, and show a significant impact in visual analysis and text generation tasks. MLLM's are well-versed in integrated understanding, analysis of complex data from cross modalities (i.e. text-image) and text generation with chat abilities. Almost all MLLM's, focus on alignment of image features to textual features for downstream text generation tasks includes detailed image description, visual question answering, stories and poems generation, phrase grounding, etc.. However, when focusing on visual question answering, questions that are highly relevant to the context of an image may not be answered correctly with the existing MLLM's, contrary to questions that are related to visual aspects. Moreover, generating meta data (context) for an image using present day MLLM's is hard task due to hallucinating characteristic of underlying Large Language Models (LLM's), and adequate contextual information cannot be directly derived from an image based perspective.Considering the cultural heritage domain, these issues hamper the introduction of multimedia chatbots as tools to support learning and understanding artworks, since contextual information is typically needed to better understand the content of the artworks themselves, and museum curators require that scientifically accurate information is provided to the users of such systems. In this paper we present a system that combines contextual description of the artworks to enhance the contextual visual question answering task.},
  address = {New York, NY, USA},
  series = {{MMSys},
  isbn = {979-8-4007-0412-3},
}

@inproceedings{mickens_guillotine_2025,
  title = {Guillotine: {Hypervisors},
  author = {Mickens, James and Radway, Sarah and Netravali, Ravi},
  year = {2025},
  doi = {10.1145/3713082.3730391},
  url = {https://doi.org/10.1145/3713082.3730391},
  booktitle = {Proceedings of the 2025 {Workshop},
  pages = {18--26},
  publisher = {Association for Computing Machinery},
  note = {event-place: Banff, AB, Canada},
  keywords = {source: ACM},
  abstract = {As AI models become more embedded in critical sectors like finance, healthcare, and the military, their inscrutable behavior poses ever-greater risks to society. To mitigate this risk, we propose Guillotine, a hypervisor architecture for sandboxing powerful AI models—models that, by accident or malice, can generate existential threats to humanity. Although Guillotine borrows some well-known virtualization techniques, Guillotine must also introduce fundamentally new isolation mechanisms to handle the unique threat model posed by existential-risk AIs. For example, a rogue AI may try to introspect upon hypervisor software or the underlying hardware substrate to enable later subversion of that control plane; thus, a Guillotine hypervisor requires careful co-design of the hypervisor software and the CPUs, RAM, NIC, and storage devices that support the hypervisor software, to thwart side channel leakage and more generally eliminate mechanisms for AI to exploit reflection-based vulnerabilities. Beyond such isolation at the software, network, and microarchitectural layers, a Guillotine hypervisor must also provide physical fail-safes more commonly associated with nuclear power plants, avionic platforms, and other types of mission-critical systems. Physical fail-safes, e.g., involving electromechanical disconnection of network cables, or the flooding of a datacenter which holds a rogue AI, provide defense in depth if software, network, and microarchitectural isolation is compromised and a rogue AI must be temporarily shut down or permanently destroyed.},
  address = {New York, NY, USA},
  series = {{HotOS},
  isbn = {979-8-4007-1475-7},
}

@inproceedings{huo_retrieving_2023,
  title = {Retrieving {Supporting},
  author = {Huo, Siqing and Arabzadeh, Negar and Clarke, Charles},
  year = {2023},
  doi = {10.1145/3624918.3625336},
  url = {https://doi.org/10.1145/3624918.3625336},
  booktitle = {Proceedings of the {Annual},
  pages = {11--20},
  publisher = {Association for Computing Machinery},
  note = {event-place: Beijing, China},
  keywords = {source: ACM},
  abstract = {Current large language models (LLMs) can exhibit near-human levels of performance on many natural language-based tasks, including open-domain question answering. Unfortunately, at this time, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report two simple experiments to automatically validate generated answers against a corpus. We base our experiments on questions and passages from the MS MARCO (V1) test collection, and a retrieval pipeline consisting of sparse retrieval, dense retrieval and neural rerankers. In the first experiment, we validate the generated answer in its entirety. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indicate if the generated answer can be supported by the retrieved answer. In the second experiment, we consider the generated answer at a more granular level, prompting the LLM to extract a list of factual statements from the answer and verifying each statement separately. We query the corpus with each factual statement and then present the LLM with the statement and the corresponding retrieved evidence. The LLM is prompted to indicate if the statement can be supported and make necessary edits using the retrieved material. With an accuracy of over 80\%, we find that an LLM is capable of verifying its generated answer when a corpus of supporting material is provided. However, manual assessment of a random sample of questions reveals that incorrect generated answers are missed by this verification process. While this verification process can reduce hallucinations, it can not entirely eliminate them.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0408-6},
}

@inproceedings{chen_empowering_2024,
  title = {Empowering {Private},
  author = {Chen, Yulin and Ding, Ning and Zheng, Hai-Tao and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  year = {2024},
  doi = {10.1145/3627673.3679665},
  url = {https://doi.org/10.1145/3627673.3679665},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {354--364},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {adaptive reflection, intelligent tutoring system, large language models, memory mechanism},
  abstract = {Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches have been made towards a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system based on large language models (LLMs). The proposed system ChatTutor, powered by state-of-the-art LLMs, is equipped with automatic course planning and adjusting, informative instruction, and adaptive quiz offering and evaluation. ChatTutor is decomposed into three inter-connected core processes: interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. To demonstrate the mechanism of each working module and the benefits of structured memory control and adaptive reflection, we conduct a wide range of analysis based on statistical results and user study. The analysis shows the designed processes boost system consistency and stability under long-term interaction and intentional disruptions, with up to 5\% and 20\% increase in performance respectively. Meanwhile, we also compare the system with scripts from real-world online learning platform and discuss the potential issues unique to LLM-based systems.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{paul_tasca_2025,
  title = {{TASCA},
  author = {Paul, Bibek and Bhowmick, Archisman and Mishra, Mayank and Gupta, Sarthak and Singhal, Rekha},
  year = {2025},
  doi = {10.1145/3703412.3703437},
  url = {https://doi.org/10.1145/3703412.3703437},
  booktitle = {Proceedings of the 4th {International},
  publisher = {Association for Computing Machinery},
  keywords = {Code Acceleration, ML Pipeline, Multi-Agents, Scalability Bottlenecks},
  abstract = {In the evolving landscape of machine learning (ML) and deep learning (DL), automatic optimization of these pipelines are crucial, especially with growing data volumes.Our tool TASCA\&nbsp;[2], an enhanced tool that leverages advanced Large Language Models (LLMs) like GPTNeo3.5/4 to automatically detect and transform performance anti-patterns in ML pipelines without human intervention. Building on our previous work with TASCA\&nbsp;[2], TASCA++ extends the capabilities of its predecessor by incorporating new features that improve detection accuracy and transformation efficiency, resulting in much better optimization. Our empirical evaluation on multiple real-world workloads demonstrates significant performance gains. TASCA++ represents a significant step forward in automated ML pipeline optimization, reducing computational overheads and enhancing scalability.},
  address = {New York, NY, USA},
  series = {{AIMLSystems},
  isbn = {979-8-4007-1161-9},
}

@inproceedings{mazhar_figurative-cum-commonsense_2025,
  title = {Figurative-cum-{Commonsense},
  author = {Mazhar, Abdullah and Shaik, Zuhair Hasan and Srivastava, Aseem and Ruhnke, Polly and Vaddavalli, Lavanya and Katragadda, Sri Keshav and Yadav, Shweta and Akhtar, Md Shad},
  year = {2025},
  doi = {10.1145/3696410.3714778},
  url = {https://doi.org/10.1145/3696410.3714778},
  booktitle = {Proceedings of the {ACM},
  pages = {637--648},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {memes, multimodality, natural language processing, source: ACM},
  abstract = {The expression of mental health symptoms through non-traditional means, such as memes, has gained remarkable attention over the past few years, with users often highlighting their mental health struggles through figurative intricacies within memes. While humans rely on commonsense knowledge to interpret these complex expressions, current Multimodal Language Models (MLMs) struggle to capture these figurative aspects inherent in memes. To address this gap, we introduce a novel dataset, AxiOM, derived from the GAD anxiety questionnaire, which categorizes memes into six fine-grained anxiety symptoms. Next, we propose a commonsense and domain-enriched framework, M3H, to enhance MLMs' ability to interpret figurative language and commonsense knowledge. The overarching goal remains to first understand and then classify the mental health symptoms expressed in memes. We benchmark M3H against 6 competitive baselines (with 20 variations), demonstrating improvements in both quantitative and qualitative metrics, including a detailed human evaluation. We observe a clear improvement of 4.20\% and 4.66\% on weighted-F1 metric. To assess the generalizability, we perform extensive experiments on a public dataset, RESTORE, for depressive symptom identification, presenting an ablation study that highlights the contribution of each module. Our findings reveal limitations in existing models and the advantage of employing commonsense to enhance figurative understanding.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{heinrich_systematic_2025,
  title = {A {Systematic},
  author = {Heinrich, Ronja and Zimmerer, Chris and Fischbach, Martin and Erich Latoschik, Marc},
  year = {2025},
  doi = {10.1145/3716553.3750790},
  url = {https://doi.org/10.1145/3716553.3750790},
  booktitle = {Proceedings of the 27th {International},
  pages = {485--495},
  publisher = {Association for Computing Machinery},
  keywords = {human-computer interaction, intentional, machine learning, multimodal fusion, multimodal interfaces, user-centered design},
  abstract = {This systematic review investigates the current state of research on multimodal fusion methods, i.e., the joint analysis of multimodal inputs, for intentional, instruction-based human-computer interactions, focusing on the combination of speech and spatially expressive modalities such as gestures, touch, pen, and gaze. We examine 50 systems from a User-Centered Design perspective, categorizing them by modality combinations, fusion strategies, application domains and media, as well as reusability. Our findings highlight a predominance of descriptive late fusion methods, limited reusability, and a lack of standardized tool support, hampering rapid prototyping and broader applicability. We identify emerging trends in machine learning-based fusion and outline future research directions to advance reusable and user-centered multimodal systems.},
  address = {New York, NY, USA},
  series = {{ICMI},
  isbn = {979-8-4007-1499-3},
}

@inproceedings{zhao_aligning_2024,
  title = {Aligning {Explanations},
  author = {Zhao, Yurou and Sun, Yiding and Han, Ruidong and Jiang, Fei and Guan, Lu and Li, Xiang and Lin, Wei and Ma, Weizhi and Mao, Jiaxin},
  year = {2024},
  doi = {10.1145/3627673.3679663},
  url = {https://doi.org/10.1145/3627673.3679663},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {3374--3383},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {alignment with rating and feature, explainable recommendation, maximizing mutual information, natural language-based explanation, source: ACM},
  abstract = {Providing natural language-based explanations to justify recommendations helps to improve users' satisfaction and gain users' trust. However, as current explanation generation methods are commonly trained with an objective to mimic existing user reviews, the generated explanations are often not aligned with the predicted ratings or some important features of the recommended items, and thus, are suboptimal in helping users make informed decision on the recommendation platform. To tackle this problem, we propose a flexible model-agnostic method named MMI (Maximizing Mutual Information) framework to enhance the alignment between the generated natural language explanations and the predicted rating/important item features. Specifically, we propose to use mutual information (MI) as a measure for the alignment and train a neural MI estimator. Then, we treat a well-trained explanation generation model as the backbone model and further fine-tune it through reinforcement learning with guidance from the MI estimator, which rewards a generated explanation that is more aligned with the predicted rating or a pre-defined feature of the recommended item. Experiments on three datasets demonstrate that our MMI framework can boost different backbone models, enabling them to outperform existing baselines in terms of alignment with predicted ratings and item features. Additionally, user studies verify that MI-enhanced explanations indeed facilitate users' decisions and are favorable compared with other baselines due to their better alignment properties.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{yen_coladder_2024,
  title = {{CoLadder},
  author = {Yen, Ryan and Zhu, Jiawen Stefanie and Suh, Sangho and Xia, Haijun and Zhao, Jian},
  year = {2024},
  doi = {10.1145/3654777.3676357},
  url = {https://doi.org/10.1145/3654777.3676357},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Code Generation, Dynamic Abstraction, Programming Interface},
  abstract = {This paper adopted an iterative design process to gain insights into programmers’ strategies when using LLMs for programming. We proposed CoLadder, a novel system that supports programmers by facilitating hierarchical task decomposition, direct code segment manipulation, and result evaluation during prompt authoring. A user study with 12 experienced programmers showed that CoLadder is effective in helping programmers externalize their problem-solving intentions flexibly, improving their ability to evaluate and modify code across various abstraction levels, from their task’s goal to final code implementation.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{woo_scalepool_2025,
  title = {{ScalePool},
  author = {Woo, Hyein and Kwon, Miryeong and Kim, Jiseon and Na, Eunjee and Choi, Hanjin and Jang, Seonghyeon and Jung, Myoungsoo},
  year = {2025},
  doi = {10.1145/3764862.3768173},
  url = {https://doi.org/10.1145/3764862.3768173},
  booktitle = {Proceedings of the 3rd {Workshop},
  pages = {10--18},
  publisher = {Association for Computing Machinery},
  note = {event-place: Seoul, Republic of Korea},
  keywords = {Accelerator-Centric Link, CXL, Resource Disaggregation},
  abstract = {This paper proposes ScalePool, a novel cluster architecture designed to interconnect numerous accelerators using unified hardware interconnects rather than traditional long-distance networking. ScalePool integrates Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for intra-cluster, low-latency accelerator communication, while using hierarchical CXL-based switching fabrics for scalable and coherent inter-cluster memory sharing. By abstracting interfaces through CXL, ScalePool structurally resolves interoperability constraints, enabling heterogeneous cluster operation and composable resource disaggregation.In addition, ScalePool introduces explicit memory tiering: the latency-critical tier-1 combines accelerator-local memory with coherence-centric CXL and XLink, whereas the high-capacity tier-2 employs dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable and efficient memory pooling. Evaluation results show that ScalePool accelerates LLM training by 1.22× on average and up to 1.84× compared to conventional RDMA-based environments. Furthermore, the proposed tier-2 memory disaggregation strategy reduces latency by up to 4.5× for memory-intensive workloads.},
  address = {New York, NY, USA},
  series = {{DIMES},
  isbn = {979-8-4007-2226-4},
}

@inproceedings{hassan_rethinking_2024,
  title = {Rethinking {Software},
  author = {Hassan, Ahmed E. and Lin, Dayi and Rajbahadur, Gopi Krishnan and Gallaba, Keheliya and Cogo, Filipe Roseiro and Chen, Boyuan and Zhang, Haoxiang and Thangarajah, Kishanthan and Oliva, Gustavo and Lin, Jiahuei (Justina) and Abdullah, Wali Mohammad and Jiang, Zhen Ming (Jack)},
  year = {2024},
  doi = {10.1145/3663529.3663849},
  url = {https://doi.org/10.1145/3663529.3663849},
  booktitle = {Companion {Proceedings},
  pages = {294--305},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {AIware, FMware, Foundation models, Large Language Models, source: ACM},
  abstract = {Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified ten key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. For each of those challenges, we state the path for innovation that we envision. We hope that the disclosure of the challenges will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{dietz_workbench_2024,
  title = {A {Workbench},
  author = {Dietz, Laura},
  year = {2024},
  doi = {10.1145/3626772.3657871},
  url = {https://doi.org/10.1145/3626772.3657871},
  booktitle = {Proceedings of the 47th {International},
  pages = {1963--1972},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {information retrieval evaluation, large language mmodels, source: ACM},
  abstract = {This resource paper addresses the challenge of evaluating Information Retrieval (IR) systems in the era of autoregressive Large Language Models (LLMs). Traditional methods relying on passage-level judgments are no longer effective due to the diversity of responses generated by LLM-based systems. We provide a workbench to explore several alternative evaluation approaches to judge the relevance of a system's response that incorporate LLMs: 1. Asking an LLM whether the response is relevant; 2. Asking the LLM which set of nuggets (i.e., relevant key facts) is covered in the response; 3. Asking the LLM to answer a set of exam questions with the response. This workbench aims to facilitate the development of new, reusable test collections. Researchers can manually refine sets of nuggets and exam questions, observing their impact on system evaluation and leaderboard rankings. Resource available at https://github.com/TREMA-UNH/rubric-grading-workbench},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{zhou_research_2024,
  title = {Research on the {Application},
  author = {Zhou, YuJie and Qi, JunJie and Zeng, Qiao and Liu, NingNing},
  year = {2024},
  doi = {10.1145/3675812.3675821},
  url = {https://doi.org/10.1145/3675812.3675821},
  booktitle = {Proceedings of the 2024 9th {International},
  pages = {10--15},
  publisher = {Association for Computing Machinery},
  note = {event-place: Guangzhou, China},
  keywords = {Automated Writing Assessment, Data-Driven Instruction, Educational Innovation, Interactive Learning Environments, Personalized Learning},
  abstract = {In the rapidly evolving field of educational technology, the application of Artificial Intelligence (AI) has begun to reshape various aspects of education, especially in the domain of language learning and teaching. Targeting English writing instruction, we have developed and implemented an advanced AI system named WAgent, representing the latest advancement in educational technology for personalized learning and enhancing teaching efficiency. WAgent is designed to address challenges present in traditional English writing instruction, such as improving student engagement, the lack of personalized feedback, and the excessive burden on teachers. This study aims to thoroughly analyze the application effectiveness of WAgent in English writing teaching, exploring its impact on student motivation, enhancement of writing skills, and improvement of teaching quality. Through a comprehensive evaluation of teaching application cases of WAgent, this research provides deep insights into how self-developed AI technology can be effectively utilized to innovate English writing education.},
  address = {New York, NY, USA},
  series = {{ICDEL},
  isbn = {979-8-4007-1680-5},
}

@inproceedings{tran_myeachtrax_2025,
  title = {{MyEachtraX},
  author = {Tran, Allie and Gurrin, Cathal},
  year = {2025},
  doi = {10.1145/3729459.3748690},
  url = {https://doi.org/10.1145/3729459.3748690},
  booktitle = {Proceedings of the 8th {Annual},
  pages = {44--51},
  publisher = {Association for Computing Machinery},
  keywords = {hubness mitigation, lifelog question answering, lifelog search challenge, lifelogging, mobile interface, multimodal retrieval, spatial semantics, temporal reasoning},
  abstract = {We present an enhanced version of MyEachtraX, a mobile-first lifelog question answering (QA) system developed for the Lifelog Search Challenge 2025. The system centers the QA process around early, high-quality retrieval of relevant lifelog events, using visual, spatial, and temporal context to guide answer generation. To address the unique challenges of lifelog data, including redundancy, ambiguity, and noisy signals, we propose a threefold set of improvements. First, we apply data quality filtering using semantic density estimation and perceptual hashing to eliminate low-information and duplicate images. Second, we integrate visual, spatial, and temporal cues into a unified scoring framework, enabling context-aware retrieval for complex, real-world queries. Third, we mitigate embedding hubness through query and gallery bank normalisation, enhancing ranking diversity and stability. Evaluations on LSC’24 tasks show significant improvements in top-k retrieval accuracy, particularly for time-sensitive and semantically rich queries, demonstrating the effectiveness of retrieval-focused design in lifelog QA systems.},
  address = {New York, NY, USA},
  series = {{LSC},
  isbn = {979-8-4007-1857-1},
}

@inproceedings{paduraru_unit_2024,
  title = {Unit {Test},
  author = {Paduraru, Ciprian and Stefanescu, Alin and Jianu, Augustin},
  year = {2024},
  doi = {10.1145/3663532.3664466},
  url = {https://doi.org/10.1145/3663532.3664466},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {7--13},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {game development, large language models, unit testing, source: ACM},
  abstract = {Challenges related to game quality, whether occurring during initial release or after updates, can result in player dissatisfaction, media scrutiny, and potential financial setbacks. These issues may stem from factors like software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development. This research explores the application of Large Language Models (LLMs) for automating unit test creation in game development, with a specific focus on strongly typed programming languages like C++ and C\#, widely used in the industry. The study centers around fine-tuning Code Llama, an advanced code generation model, to address common scenarios encountered in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily occurred within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation outcomes demonstrate the effectiveness of these methods in enhancing existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0674-5},
  series = {{FaSE4Games},
}

@inproceedings{maiti_can_2025,
  title = {Can an {AI},
  author = {Maiti, Pratyusha and Goel, Ashok},
  year = {2025},
  doi = {10.1145/3708359.3712134},
  url = {https://doi.org/10.1145/3708359.3712134},
  booktitle = {Proceedings of the 30th {International},
  pages = {314--324},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational AI Agents, Question Answering, Virtual Teaching Assistant},
  abstract = {Jill Watson is an LLM-powered conversational AI partner integrated with instructor-provided courseware, offering learners contextually relevant and immediately applicable support. This study examines learner-generated questions as part of organic interactions with Jill embedded within classroom Learning Management System and investigates whether Jill empowers learners to ask higher-order questions. Leveraging Bloom’s Taxonomy to assess question complexity, we collected over 5500 student questions from classroom deployments across three academic semesters and two educational settings. Student questions were classified using a fine-tuned BERT model and regression models were used to analyze the trends of complexity of the questions over time. Our results reveal a significant proportion of higher-order questions being asked in our classrooms, exceeding typical educational distributions. We also found a statistically significant increase in higher-order questioning with sustained interaction with Jill. These findings demonstrate that Jill empowers learners to engage in critical questioning, thereby enhancing their educational experience by promoting depth, relevance, and application of course concepts. Further research is recommended with larger and more diverse samples to generalize these findings.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{beasley_pipeline_2024,
  title = {Pipe(line) {Dreams},
  author = {Beasley, Cole and Abouzied, Azza},
  year = {2024},
  doi = {10.1145/3665939.3665962},
  url = {https://doi.org/10.1145/3665939.3665962},
  booktitle = {Proceedings of the 2024 {Workshop},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {source: ACM},
  abstract = {We exploit large language models (LLMs) to automate the end-to-end process of descriptive analytics and visualization. A user simply declares who they are and provides their data set. Our tool LLM4Vis sets analysis goals or metrics, generates code to process and analyze the data, visualizes the results and interprets the visualization to summarize key takeaways for our user. We examine the power of LLMs in democratizing data science for the non-technical user and in handling rich, multimodal data sets. We also explore LLM4Vis's limitations, opportunities for human-in-the-loop interventions, and challenges to measuring and improving the robustness and the utility of LLM-generated end-to-end data analysis pipelines.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0693-6},
  series = {{HILDA},
}

@inproceedings{zheng_predictions_2025,
  title = {From {Predictions},
  author = {Zheng, Xiaofan and Zeng, Zinan and Wang, Heng and Bai, Yuyang and Liu, Yuhan and Luo, Minnan},
  year = {2025},
  doi = {10.1145/3696410.3714532},
  url = {https://doi.org/10.1145/3696410.3714532},
  booktitle = {Proceedings of the {ACM},
  pages = {5364--5375},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {explainable, fake news detection, large vision-language models},
  abstract = {The rapid development of social media has led to a surge of eye-catching fake news on the Internet, with multimodal news comprising both images and text being particularly prevalent. To address the challenges of Multimodal Fake News Detection (MFND), numerous supervised task-specific Multimodal Small Language Models (MSLMs) have been developed. However, these models lack the breadth of knowledge and the depth of language understanding, which results in unsatisfactory adaptability, generalization, and explainability performance. To address these issues, we attempt to introduce Large Vision-Language Models (LVLMs), aiming to leverage the common sense understanding and logical reasoning abilities of LVLMs for the MFND task. We observed that LVLMs can generate reasonable analyses of news content from specific angles. However, when it comes to synthesizing these analyses for final judgment, their performance declines significantly, failing to meet the accuracy benchmarks set by existing MSLMs detection models. This reflects the need for a more effective way for LVLMs, which have not undergone task-specific training, to utilize their knowledge and capabilities. Based on these findings, we propose the Explainable Adaptive Rationale-Augmented Multimodal (EARAM) framework, which adaptively uses MSLMs to extract useful rationales from the multi-perspective analyses of LVLMs. After making judgments based on these rationales, EARAM then assists LVLMs in generating more reliable explanations. Extensive experiments demonstrate that our model not only achieves state-of-the-art results on widely used datasets but also significantly outperforms other models in terms of generalization and explainability.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{breuer_data_2024,
  title = {Data {Fusion},
  author = {Breuer, Timo},
  year = {2024},
  doi = {10.1145/3673791.3698423},
  url = {https://doi.org/10.1145/3673791.3698423},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {274--279},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {data fusion, large language models, query variants, source: ACM},
  abstract = {Considering query variance in information retrieval (IR) experiments is beneficial for retrieval effectiveness. Especially ranking ensembles based on different topically related queries retrieve better results than rankings based on a single query alone. Recently, generative instruction-tuned Large Language Models (LLMs) improved on a variety of different tasks in capturing human language. To this end, this work explores the feasibility of using synthetic query variants generated by instruction-tuned LLMs in data fusion experiments. More specifically, we introduce a lightweight, unsupervised, and cost-efficient approach that exploits principled prompting and data fusion techniques. In our experiments, LLMs produce more effective queries when provided with additional context information on the topic. Furthermore, our analysis based on four TREC newswire benchmarks shows that data fusion based on synthetic query variants is significantly better than baselines with single queries and also outperforms pseudo-relevance feedback methods. We publicly share the code and query datasets with the community as resources for follow-up studies.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{mao_skyserve_2025,
  title = {{SkyServe},
  author = {Mao, Ziming and Xia, Tian and Wu, Zhanghao and Chiang, Wei-Lin and Griggs, Tyler and Bhardwaj, Romil and Yang, Zongheng and Shenker, Scott and Stoica, Ion},
  year = {2025},
  doi = {10.1145/3689031.3717459},
  url = {https://doi.org/10.1145/3689031.3717459},
  booktitle = {Proceedings of the {Twentieth},
  pages = {159--175},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {AI Serving, Cloud Computing, Multi-cloud, Spot Instance},
  abstract = {Recent years have witnessed an explosive growth of AI models. The high cost of hosting AI services on GPUs and their demanding service requirements, make it timely and challenging to lower service costs and guarantee service quality. While spot instances have long been offered with a large discount, spot preemptions have discouraged users from using them to host model replicas when serving AI models.To address this, we propose a simple yet efficient policy, SpotHedge, that leverages spot replicas across different failure domains (e.g., regions and clouds) to ensure availability, lower costs, and high service quality. SpotHedge intelligently spreads spot replicas across different regions and clouds to improve availability and reduce correlated preemptions, over-provisions cheap spot replicas than required as a safeguard against possible preemptions, and dynamically falls back to on-demand replicas when spot replicas become unavailable. We built SkyServe, a system leveraging SpotHedge to efficiently serve AI models over a mixture of spot and on-demand replicas across regions and clouds. We compared SkyServe with both research and production systems on real AI workloads: SkyServe reduces cost by 43\% on average while achieving high resource availability compared to using on-demand replicas. Additionally, SkyServe improves P50, P90, and P99 latency by 2.3×, 2.1×, 2.1× on average compared to other research and production systems.},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{huang_concise_2024,
  title = {A {Concise},
  author = {Huang, Haitao and Liang, Zijing and Fang, Zirui and Wang, Zhiyuan and Chen, Mingxiu and Hong, Yifan and Liu, Ke and Shang, Penghui},
  year = {2024},
  doi = {10.1145/3677182.3677282},
  url = {https://doi.org/10.1145/3677182.3677282},
  booktitle = {Proceedings of the {International},
  pages = {563--566},
  publisher = {Association for Computing Machinery},
  note = {event-place: Nanchang, China},
  keywords = {source: ACM},
  abstract = {Sincerely in part to the rise of high-performance computer systems and transformer models, natural language processing has advanced. Also, a multitude of applications built on large language models continually improve people's cognitive abilities. Large language models continue to face difficulties when dealing with long context input. Many studies have suggested various specific strategies to address the challenge of extended context, however as of yet, no thorough summary of these studies exists. In this paper, we discuss the issues raised and the developments that have occurred in the long context application of large language models, and we attempt to suggest future directions for research and development.},
  address = {New York, NY, USA},
  series = {{ASENS},
  isbn = {979-8-4007-0978-4},
}

@inproceedings{peng_glitter_2025,
  title = {{GLITTER},
  author = {Peng, Weirui and Yang, Yinuo and Zhang, Zheng and Li, Toby Jia-Jun},
  year = {2025},
  doi = {10.1145/3746059.3747742},
  url = {https://doi.org/10.1145/3746059.3747742},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {asynchronous discussion, flipped classroom, human-AI collaboration},
  abstract = {Flipped classrooms promote active learning by having students engage with materials independently before class, allowing in-class time for collaborative problem-solving. During this pre-class phase, asynchronous online discussions help students build knowledge and clarify concepts with peers. However, it remains difficult to engage with temporally dispersed peer contributions, connect discussions with static learning materials, and prepare for in-class sessions based on their self-learning outcome. Our formative study identified cognitive challenges students encounter, including navigation barriers, reflection gaps, and contribution difficulty and anxiety. We present Glitter, an AI-assisted discussion platform for pre-class learning in flipped classrooms. Glitter helps students identify posts with shared conceptual dimensions, scaffold knowledge integration through conceptual blending, and enhance metacognition via personalized reflection reports. A lab study within subjects (n = 12) demonstrates that Glitter improves discussion engagement, sparks new ideas, supports reflection, and increases preparedness for in-class activities.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{wu_efficient_2025,
  title = {Efficient {Heuristics},
  author = {Wu, Xuan and Wang, Di and Wu, Chunguo and Wen, Lijie and Miao, Chunyan and Xiao, Yubin and Zhou, You},
  year = {2025},
  doi = {10.1145/3711896.3736923},
  url = {https://doi.org/10.1145/3711896.3736923},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {3228--3239},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {combinatorial optimization problems, heuristic generation, large language models},
  abstract = {Recent studies exploited Large Language Models (LLMs) to autonomously generate heuristics for solving Combinatorial Optimization Problems (COPs), by prompting LLMs to first provide search directions and then derive heuristics accordingly. However, the absence of task-specific knowledge in prompts often leads LLMs to provide unspecific search directions, obstructing the derivation of well-performing heuristics. Moreover, evaluating the derived heuristics remains resource-intensive, especially for those semantically equivalent ones, often requiring omissible resource expenditure. To enable LLMs to provide specific search directions, we propose the Hercules algorithm, which leverages our designed Core Abstraction Prompting (CAP) method to abstract the core components from elite heuristics and incorporate them as prior knowledge in prompts. We theoretically prove the effectiveness of CAP in reducing unspecificity and provide empirical results in this work. To reduce computing resources required for evaluating the derived heuristics, we propose few-shot Performance Prediction Prompting (PPP), a first-of-its-kind method for the Heuristic Generation (HG) task. PPP leverages LLMs to predict the fitness values of newly derived heuristics by analyzing their semantic similarity to previously evaluated ones. We further develop two tailored mechanisms for PPP to enhance predictive accuracy and determine unreliable predictions, respectively. The use of PPP makes Hercules more resource-efficient and we name this variant Hercules-P. Extensive experiments across four HG tasks, five COPs, and eight LLMs demonstrate that Hercules outperforms the state-of-the-art LLM-based HG algorithms, while Hercules-P excels at minimizing required computing resources. In addition, we illustrate the effectiveness of CAP, PPP, and the other proposed mechanisms by conducting relevant ablation studies.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{hildebrandt_acceptance_2025,
  title = {Acceptance, {Usability},
  author = {Hildebrandt, Kilian and Ortmann, Thorben and Putzar, Larissa},
  year = {2025},
  doi = {10.1145/3733155.3734906},
  url = {https://doi.org/10.1145/3733155.3734906},
  booktitle = {Proceedings of the 18th {ACM},
  pages = {575--582},
  publisher = {Association for Computing Machinery},
  keywords = {Acceptance, Affective Computing, HCI, Usability, Virtual Assistants},
  abstract = {The integration of AI virtual assistants (AIVAs) into daily life raises critical questions about acceptance, usability, and emotional reactions. This paper explores these constructs within the context of the German state-funded KIMM project, which investigates the use of AIVAs to support the modernization of rental housing stock. To better understand users’ interactions with AIVAs, we propose an extension of the Unified Theory of Acceptance and Use of Technology (UTAUT), incorporating usability, valence, and arousal as aspects of technology acceptance. To validate our proposed model, termed Usability-Valence-Arousal-UTAUT (UVA-UTAUT), we present a study design that combines explicit measures of usability and acceptance with implicit emotion recognition techniques. The findings are expected to contribute to the fields of Human-Computer Interaction (HCI), Affective Computing, and Psychology by offering a deeper understanding of how emotional states and usability influence the acceptance of AIVAs. Further exploration of the model is encouraged and future research directions are outlined.},
  address = {New York, NY, USA},
  series = {{PETRA},
  isbn = {979-8-4007-1402-3},
}

@inproceedings{cao_generative_2025,
  title = {Generative and {Malleable},
  author = {Cao, Yining and Jiang, Peiling and Xia, Haijun},
  year = {2025},
  doi = {10.1145/3706598.3713285},
  url = {https://doi.org/10.1145/3706598.3713285},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Generative User Interface, Malleable User Interface},
  abstract = {Unlike static and rigid user interfaces, generative and malleable user interfaces offer the potential to respond to diverse users’ goals and tasks. However, current approaches primarily rely on generating code, making it difficult for end-users to iteratively tailor the generated interface to their evolving needs. We propose employing task-driven data models—representing the essential information entities, relationships, and data within information tasks—as the foundation for UI generation. We leverage AI to interpret users’ prompts and generate the data models that describe users’ intended tasks, and by mapping the data models with UI specifications, we can create generative user interfaces. End-users can easily modify and extend the interfaces via natural language and direct manipulation, with these interactions translated into changes in the underlying model. The technical evaluation of our approach and user evaluation of the developed system demonstrate the feasibility and effectiveness of the proposed generative and malleable UIs.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{shaikh_creating_2025,
  title = {Creating {General},
  author = {Shaikh, Omar and Sapkota, Shardul and Rizvi, Shan and Horvitz, Eric and Park, Joon Sung and Yang, Diyi and Bernstein, Michael S.},
  year = {2025},
  doi = {10.1145/3746059.3747722},
  url = {https://doi.org/10.1145/3746059.3747722},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {natural language processing, User models},
  abstract = {Human-computer interaction has long imagined technology that understands us—from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific applications, and incapable of the flexible, cross-context reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted natural language propositions that capture that user’s behavior, knowledge, beliefs, and preferences. GUMs can infer that a user is preparing for a wedding they’re attending from a message thread with a friend. Or recognize that a user is struggling with a collaborator’s feedback on a draft paper by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with contextual understanding, manage OS notifications to surface important information only when needed, and enable interactive agents that adapt to user preferences across applications. We also instantiate a new class of proactive assistants (Gumbos) that discover and execute useful suggestions on a user’s behalf based on their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions of meaningful value that users wouldn’t think to request explicitly. Altogether, GUMs introduce new methods that leverage large multimodal models to understand unstructured user context, enabling both long-standing visions of HCI and entirely new interactive systems that anticipate user needs.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{zhang_mapexplorer_2025,
  title = {{MapExplorer},
  author = {Zhang, Xingjian and Xiong, Ziyang and Liu, Shixuan and Xie, Yutong and Ergen, Tolga and Shim, Dongsub and Xu, Hua and Lee, Honglak and Mei, Qiaozhu},
  year = {2025},
  doi = {10.1145/3711896.3737038},
  url = {https://doi.org/10.1145/3711896.3737038},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {3843--3854},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {spatially guided content generation, text generation evaluation, textual visualization},
  abstract = {Low-dimensional visualizations, or ”projection maps,” are widely used in scientific and creative domains to interpret large-scale and complex datasets. These visualizations not only aid in understanding existing knowledge spaces but also implicitly guide exploration into unknown areas. Although techniques such as t-SNE and UMAP can generate these maps, there exists no systematic method for leveraging them to generate new content. To address this, we introduce MapExplorer, a novel knowledge discovery task that translates coordinates within any projection map into coherent, contextually aligned textual content. This allows users to interactively explore and uncover insights embedded in the maps. To evaluate the performance of MapExplorer methods, we propose Atometric, a fine-grained metric inspired by ROUGE that quantifies logical coherence and alignment between generated and reference text. Experiments on diverse datasets demonstrate the versatility of MapExplorer in generating scientific hypotheses, crafting synthetic personas, and devising strategies for attacking large language models-even with simple baseline methods. By bridging visualization and generation, our work highlights the potential of MapExplorer to enable intuitive human-AI collaboration in large-scale data exploration.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{wang_it_2025,
  title = {“{It},
  author = {Wang, Ru and Zhang, Kexin and Wang, Yuqing and Brown, Keri, PhD and Zhao, Yuhang},
  year = {2025},
  doi = {10.1145/3663547.3746394},
  url = {https://doi.org/10.1145/3663547.3746394},
  booktitle = {Proceedings of the 27th {International},
  publisher = {Association for Computing Machinery},
  keywords = {Acceptance and Commitment Therapy, Exposure and Response Prevention, just-in-time intervention, mental health, Obsessive-Compulsive Disorder, OCD},
  abstract = {Obsessive-compulsive disorder (OCD) is a mental health condition that significantly impacts people’s quality of life. While evidence-based therapies such as exposure and response prevention (ERP) can be effective, managing OCD symptoms in everyday life—an essential part of treatment and independent living—remains challenging due to fear confrontation and lack of appropriate support. To better understand the challenges and needs in OCD self-management, we conducted interviews with 10 participants with diverse OCD conditions and seven therapists specializing in OCD treatment. Through these interviews, we explored the characteristics of participants’ triggers and how they shaped their compulsions, and uncovered key coping strategies across different stages of OCD episodes. Our findings highlight critical gaps between OCD self-management needs and currently available support. Building on these insights, we propose design opportunities for just-in-time self-management technologies for OCD, including personalized symptom tracking, just-in-time interventions, and support for OCD-specific privacy and social needs—through technology and beyond.},
  address = {New York, NY, USA},
  series = {{ASSETS},
  isbn = {979-8-4007-0676-9},
}

@inproceedings{samarinas_distillation_2025,
  title = {Distillation and {Refinement},
  author = {Samarinas, Chris and Zamani, Hamed},
  year = {2025},
  doi = {10.1145/3731120.3744613},
  url = {https://doi.org/10.1145/3731120.3744613},
  booktitle = {Proceedings of the 2025 {International},
  pages = {430--435},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {reasoning intensive retrieval, reinforcement learning},
  abstract = {We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{tran_myeachtrax_2024,
  title = {{MyEachtraX},
  author = {Tran, Ly Duyen and Nguyen, Thanh-Binh and Gurrin, Cathal and Zhou, Liting},
  year = {2024},
  doi = {10.1145/3643489.3661128},
  url = {https://doi.org/10.1145/3643489.3661128},
  booktitle = {Proceedings of the 7th {Annual},
  pages = {93--98},
  publisher = {Association for Computing Machinery},
  note = {event-place: Phuket, Thailand},
  keywords = {lifelog, mobile, question answering, retrieval},
  abstract = {Your whole life in your pocket. That is the premise of lifelogging, a technology that captures and stores every moment of your life in digital form. Built on top of MyEachtra and the lifelog question-answering pipeline, MyEachtraX is a mobile-based application that addresses the overlook of mobile platforms in the area. Furthermore, leveraging the latest advancements in natural language processing, such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), the system enhances the query-parsing, post-processing, and question-answering processes in lifelog retrieval. Official lifelog questions from the previous Lifelog Search Challenges were used to evaluate the system, which achieved an accuracy of 72.2\%. We identify the retrieval component as the main bottleneck of the pipeline and propose future works to improve the system.},
  address = {New York, NY, USA},
  series = {{LSC},
  isbn = {979-8-4007-0550-2},
}

@inproceedings{zheng_reasoning-focused_2025,
  title = {A {Reasoning},
  author = {Zheng, Lucia and Guha, Neel and Arifov, Javokhir and Zhang, Sarah and Skreta, Michal and Manning, Christopher D. and Henderson, Peter and Ho, Daniel E.},
  year = {2025},
  doi = {10.1145/3709025.3712219},
  url = {https://doi.org/10.1145/3709025.3712219},
  booktitle = {Proceedings of the 2025 {Symposium},
  pages = {169--193},
  publisher = {Association for Computing Machinery},
  note = {event-place: Munich, Germany},
  keywords = {benchmark, dataset, reasoning, retrieval},
  abstract = {As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs ("RAG" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.},
  address = {New York, NY, USA},
  series = {{CSLAW},
  isbn = {979-8-4007-1421-4},
}

@inproceedings{liu_efficient_2025,
  title = {Efficient {Vector},
  author = {Liu, Yi and Fang, Fei and Qian, Chen},
  year = {2025},
  doi = {10.1145/3736548.3737822},
  url = {https://doi.org/10.1145/3736548.3737822},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boston, MA, USA},
  keywords = {Disaggregated memory, HNSW, RDMA, vector database},
  abstract = {Efficient vector query processing is essential for powering large-scale AI applications, such as LLMs. However, existing solutions struggle with growing vector datasets that exceed the memory capacity of a single machine, leading to excessive data movement and resource underutilization in monolithic architectures.We introduce d-HNSW, the first vector search engine for RDMA-based disaggregated memory systems. d-HNSW achieves high performance by supporting efficient data indexing with minimal network communication overhead. At its core, d-HNSW introduces a novel disaggregation of the HNSW graph-based vector index, leveraging the properties of greedy search to coordinate data transfers efficiently between the memory and compute pools. Specifically, d-HNSW incorporates three key techniques: (i) Representative index caching, which constructs a lightweight index from a sampled subset of the data and caches it in the compute pool to minimize frequent access to critical components of the hierarchical graph index; (ii) RDMA-friendly data layout, which optimizes data placement to reduce networking round trips for both vector queries and insertions; and (iii) Batched query-aware data loading, which mitigates bandwidth usage between memory and compute pools, addressing the limited cache capacity of compute nodes. The experimental results demonstrate that d-HNSW outperforms Naive d-HNSW implementation by up to 117× in query latency while maintaining a recall of 0.87 on the SIFT1M dataset.},
  address = {New York, NY, USA},
  series = {{HotStorage},
  isbn = {979-8-4007-1947-9},
}

@inproceedings{rahimi_user-vlm_2025,
  title = {{USER},
  author = {Rahimi, Hamed and Bahaj, Adil and Abrini, Mouad and Khoramshahi, Mahdi and Ghogho, Mounir and Chetouani, Mohamed},
  year = {2025},
  doi = {10.1145/3716553.3750767},
  url = {https://doi.org/10.1145/3716553.3750767},
  booktitle = {Proceedings of the 27th {International},
  pages = {326--336},
  publisher = {Association for Computing Machinery},
  keywords = {Social Robotics, User Modeling, Vision Language Models},
  abstract = {The integration of vision-language models into robotic systems constitutes a significant advancement in enabling machines to interact with their surroundings in a more intuitive manner. While VLMs offer rich multimodal reasoning, existing approaches lack user-specific adaptability, often relying on generic interaction paradigms that fail to account for individual behavioral, contextual, or socio-emotional nuances. When customization is attempted, ethical concerns arise from unmitigated biases in user data, risking exclusion or unfair treatment. To address these dual challenges, we propose User-VLM 360°, a holistic framework integrating multimodal user modeling with bias-aware optimization. Our approach features: (1) user-aware tuning that adapts interactions in real time using visual-linguistic signals; (2) bias mitigation via preference optimization; and (3) curated 360° socio-emotive interaction datasets annotated with demographic, emotion, and relational metadata. Evaluations across eight benchmarks demonstrate state-of-the-art results: +35.3\% F1 in personalized VQA, +47.5\% F1 in facial features understanding, 15\% bias reduction, and 30× speedup over baselines. Ablation studies confirm component efficacy, and deployment on the Pepper robot validates real-time adaptability across diverse users. We open-source parameter-efficient 3B/10B models and an ethical verification framework for responsible adaptation.},
  address = {New York, NY, USA},
  series = {{ICMI},
  isbn = {979-8-4007-1499-3},
}

@inproceedings{susanti_paths_2025,
  title = {Paths to {Causality},
  author = {Susanti, Yuni and Färber, Michael},
  year = {2025},
  doi = {10.1145/3711896.3737076},
  url = {https://doi.org/10.1145/3711896.3737076},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {2778--2789},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {causal discovery, knowledge graphs, large language models},
  abstract = {Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context)-offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath -based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub.. https://github.com/susantiyuni/path-to-causality},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{alessio_cosrec_2025,
  title = {{CoSRec},
  author = {Alessio, Marco and Merlo, Simone and Di Noia, Tommaso and Faggioli, Guglielmo and Ferrante, Marco and Ferro, Nicola and Muntean, Cristina Ioana and Nardini, Franco Maria and Narducci, Fedelucio and Perego, Raffaele and Santucci, Giuseppe and Viterbo, Nicola},
  year = {2025},
  doi = {10.1145/3726302.3730319},
  url = {https://doi.org/10.1145/3726302.3730319},
  booktitle = {Proceedings of the 48th {International},
  pages = {3466--3477},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {conversational recommendation, conversational search, joint information retrieval and recommendation},
  abstract = {Conversational Information Access systems have experienced widespread diffusion thanks to the natural and effortless interactions they enable with the user. In particular, they represent an effective interaction interface for conversational search (CS) and conversational recommendation (CR) scenarios. Despite their commonalities, CR and CS systems are often devised, developed, and evaluated as isolated components. Integrating these two elements would allow for handling complex information access scenarios, such as exploring unfamiliar recommended product aspects, enabling richer dialogues, and improving user satisfaction. As of today, the scarce availability of integrated datasets - focused exclusively on either of the tasks - limits the possibilities for evaluating by-design integrated CS and CR systems. To address this gap, we propose CoSRec, the first dataset for joint Conversational Search and Recommendation (CSR) evaluation. The CoSRec test set includes 20 high-quality conversations, with human-made annotations for the quality of conversations, and manually crafted relevance judgments for products and documents. Additionally, we provide supplementary training data comprising partially annotated dialogues and raw conversations to support diverse learning paradigms. CoSRec is the first resource to model CR and CS tasks in a unified framework, enabling the training and evaluation of systems that must shift between answering queries and making suggestions dynamically.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_what_2025,
  title = {“{What},
  author = {Wang, Sonja Mei and Seim, Jonathan and Fricke, Nicola},
  year = {2025},
  doi = {10.1145/3743049.3743065},
  url = {https://doi.org/10.1145/3743049.3743065},
  booktitle = {Proceedings of the {Mensch},
  pages = {75--90},
  publisher = {Association for Computing Machinery},
  keywords = {3D visualization, co-design, e-participation, qualitative study, sociotechnical systems},
  abstract = {This paper reports on multiple co-design workshops for the development of an e-participation tool. Participants were citizens and city staff from Wuppertal, Germany, and experts on participation. We explored what information citizens and city staff need, how this information should be provided, and what features for input and communication should be implemented. Through a thematic analysis, we found that information should be short, visual, and accessible. This visual information includes a visualization of the area relevant to participation. Participants preferred an abstract 3D model over a 2D map and a photorealistic 3D model. Additional visualizations should complement this abstract 3D model to support better recognition of the area, e.g. through photos and videos, or through an extra layer that visualizes the terrain. Three preferred ways of interaction and communication were identified through the analysis: 1) commenting and voting on existing ideas, 2) placing icons, and 3) drawing sketches. Since moderation is often conducted manually, this impacts which features can be implemented. We also found that city staff was generally interested in using AI for several purposes, such as lowering their moderation load and supporting citizens in formulating ideas.},
  address = {New York, NY, USA},
  series = {{MuC},
  isbn = {979-8-4007-1582-2},
}

@inproceedings{kim_bleacherbot_2025,
  title = {{BleacherBot},
  author = {Kim, Kyusik and Song, Hyungwoo and Ryu, Jeongwoo and Oh, Changhoon and Suh, Bongwon},
  year = {2025},
  doi = {10.1145/3706598.3714178},
  url = {https://doi.org/10.1145/3706598.3714178},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {AI Agents, Co-viewing, Human-AI Interaction, Large Language Models (LLMs), Sports Communication},
  abstract = {Co-viewing, traditionally defined as watching content together in the same physical space, enhances emotional connections through shared experiences. With the rise of remote viewing during the COVID-19 pandemic, existing solutions, such as second-screen platforms and rule-based AI companions, struggle to facilitate meaningful social interactions. This study explores the potential of Large Language Models, which offer human-like interactions and personalization. Our formative study with ten participants revealed the importance of managing arousal levels, highlighting the need to balance between high- and low-arousal levels across different viewing contexts. Based on these insights, we developed ‘BleacherBot’, a sports co-viewing agent with distinct interaction styles that vary in arousal levels. Our main study with 27 participants demonstrated that matching users’ preferred arousal levels with the agent’s interaction style significantly enhanced their engagement and overall enjoyment. We propose design guidelines for AI co-viewing agents that consider their role as complements to human social interactions.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{roegiest_generative_2024,
  title = {Generative {Information},
  author = {Roegiest, Adam and Pinkosova, Zuzana},
  year = {2024},
  doi = {10.1145/3627508.3638345},
  url = {https://doi.org/10.1145/3627508.3638345},
  booktitle = {Proceedings of the 2024 {Conference},
  pages = {165--177},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sheffield, United Kingdom},
  keywords = {source: ACM},
  abstract = {Generative models, especially in information systems like ChatGPT and Bing Chat, have become increasingly integral to our daily lives. Their significance lies in their potential to revolutionize how we access, process, and generate information \&nbsp;[44]. However, a gap exists in ensuring these systems are accessible to all, especially considering the literacy challenges faced by a significant portion of the population in (but not limited to) English-speaking countries. This paper aims to investigate the “readability’’ of generative information systems and their accessibility barriers, particularly for those with literacy challenges. Using popular instruction fine-tuning datasets, we found that this training data could produce systems that generate at a college level, potentially excluding a large demographic. Our research methods involved analyzing the responses of popular Large Language Models (LLMs) and examining potential biases in how they can be trained. The key message is the urgent need for inclusivity in systems incorporating generative models, such as those studied by the Information Retrieval (IR) community. Our findings indicate that current generative systems might not be accessible to individuals with cognitive and literacy challenges, emphasizing the importance of ensuring that advancements in this field benefit everyone. By situating our research within the sphere of information seeking and retrieval, we underscore the essential role of these technologies in augmenting accessibility and efficiency of information access, thereby broadening their reach and enhancing user engagement.},
  address = {New York, NY, USA},
  series = {{CHIIR},
  isbn = {979-8-4007-0434-5},
}

@inproceedings{li_ai-driven_2025,
  title = {An {AI},
  author = {Li, Kunyan and Yu, Shuxuan and Wang, Shu and Li, Jiayi and Li, Rushuang and Wei, Jinwu},
  year = {2025},
  doi = {10.1145/3712464.3712518},
  url = {https://doi.org/10.1145/3712464.3712518},
  booktitle = {Proceedings of the 2024 4th {International},
  pages = {310--315},
  publisher = {Association for Computing Machinery},
  keywords = {AI-Driven Training, Highly complex industries, Specialized operations, source: ACM},
  abstract = {This paper introduces an AI-driven training system architecture tailored for specialized operations within high-complexity industries. Leveraging the advancements in artificial intelligence, particularly large models, the system offers a structured, four-layer architecture encompassing Device, Data, Model, and Application Layers. These layers integrate advanced AI capabilities to simulate real-world environments, deliver personalized training, and optimize performance in critical operations. Experimental results highlight the system's ability to reduce operator response times, improve success rates, and enhance operational safety and efficiency. This research presents a transformative approach to training personnel, aiming to elevate the safety and effectiveness of industrial operations.},
  address = {New York, NY, USA},
  series = {{SPCT},
  isbn = {979-8-4007-1063-6},
}

@inproceedings{huang_disambiguate_2024,
  title = {Disambiguate {Entity},
  author = {Huang, Zezhou},
  year = {2024},
  doi = {10.1145/3665601.3669844},
  url = {https://doi.org/10.1145/3665601.3669844},
  booktitle = {Proceedings of the {Conference},
  pages = {36--39},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {Data Integration, Entity Matching, Large Language Models},
  abstract = {Entity matching is a critical problem in data integration, central to tasks like fuzzy joins for tuple enrichment. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, when integrating with external databases, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match". This is because external databases contain tuples with varying levels of detail and granularity among entities, and an "exact match" in traditional entity matching rarely happens. As a result, understanding how entities are related and the potential nuances is critical, especially for high-stake tasks for responsible AI. In this work, we study a case problem of entity matching for ESG reporting. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities for resolving ambiguities in matching, with a human-in-the-loop process to make the final decision. By pre-defining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities, and responsibly perform downstream tasks.},
  address = {New York, NY, USA},
  series = {{GUIDE},
  isbn = {979-8-4007-0694-3},
}

@inproceedings{liu_unimel_2024,
  title = {{UniMEL},
  author = {Liu, Qi and He, Yongyi and Xu, Tong and Lian, Defu and Liu, Che and Zheng, Zhi and Chen, Enhong},
  year = {2024},
  doi = {10.1145/3627673.3679793},
  url = {https://doi.org/10.1145/3627673.3679793},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {1909--1919},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {large language models, multimodal entity linking, multimodal knowledge base},
  abstract = {Multimodal Entity Linking (MEL) is a crucial task that aims at linking ambiguous mentions within multimodal contexts to the referent entities in a multimodal knowledge base, such as Wikipedia. Existing methods focus heavily on using complex mechanisms and extensive model tuning methods to model the multimodal interaction on specific datasets. However, these methods overcomplicate the MEL task and overlook the visual semantic information, which makes them costly and hard to scale. Moreover, these methods cannot solve the issues like textual ambiguity, redundancy, and noisy images, which severely degrade their performance. Fortunately, the advent of Large Language Models (LLMs) with robust capabilities in text understanding and reasoning, particularly Multimodal Large Language Models (MLLMs) that can process multimodal inputs, provides new insights into addressing this challenge. However, how to design a universally applicable LLMs-based MEL approach remains a pressing challenge. To this end, we propose UniMEL, a \&lt;u\&gt;uni\&lt;/u\&gt;fied framework which establishes a new paradigm to process \&lt;u\&gt;m\&lt;/u\&gt;ultimodal \&lt;u\&gt;e\&lt;/u\&gt;ntity \&lt;u\&gt;l\&lt;/u\&gt;inking tasks using LLMs. In this framework, we employ LLMs to augment the representation of mentions and entities individually by integrating textual and visual information and refining textual information. Subsequently, we employ the embedding-based method for retrieving and re-ranking candidate entities. Then, with only 0.26\% of the model parameters fine-tuned, LLMs can make the final selection from the candidate entities. Extensive experiments on three public benchmark datasets demonstrate that our solution achieves state-of-the-art performance, and ablation studies verify the effectiveness of all modules. Our code is available at https://github.com/Javkonline/UniMEL.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{fons_tadacap_2024,
  title = {{TADACap},
  author = {Fons, Elizabeth and Kaur, Rachneet and Zeng, Zhen and Palande, Soham and Balch, Tucker and Vyetrenko, Svitlana and Veloso, Manuela},
  year = {2024},
  doi = {10.1145/3677052.3698690},
  url = {https://doi.org/10.1145/3677052.3698690},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {54--62},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {Adaptive, Domain-aware, Retrieval-based captioning, Time series captioning},
  abstract = {While image captioning has gained significant attention, the potential of captioning time-series images, prevalent in areas like finance and healthcare, remains largely untapped. Existing time-series captioning methods typically offer generic, domain-agnostic descriptions of time-series shapes and struggle to adapt to new domains without substantial retraining. To address these limitations, we introduce TADACap, a retrieval-based framework to generate domain-aware captions for time-series images, capable of adapting to new domains without retraining. Building on TADACap, we propose a novel retrieval strategy that retrieves diverse image-caption pairs from a target domain database, namely TADACap-diverse. We benchmarked TADACap-diverse against state-of-the-art methods and ablation variants. TADACap-diverse demonstrates comparable semantic accuracy while requiring significantly less annotation effort.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{anderer_making_2025,
  title = {Making {Lecture},
  author = {Anderer, Katharina and Müller, Karin and Strobel, Lukas and Wölfel, Matthias and Niehues, Jan and Gerling, Kathrin},
  year = {2025},
  doi = {10.1145/3663547.3746349},
  url = {https://doi.org/10.1145/3663547.3746349},
  booktitle = {Proceedings of the 27th {International},
  publisher = {Association for Computing Machinery},
  keywords = {Assistive technology, blind and low vision, large-language models, vision-language models},
  abstract = {Designing accessible lectures and lecture materials is crucial to promote inclusive higher education. We conducted need-finding interviews with 12 students who are blind or have low vision to learn their perspectives on how lectures and lecture material could become more accessible through Artificial Intelligence (AI) technologies. Key insights from the interviews reveal that students envision AI to automatically customize lecture material, connect disparate information sources, for example, to better keep track of the current lecture slide, and enhance interaction and engagement with lecture material. Based on these insights, we developed the LectureAssistant prototype, employing an iterative design process with visually impaired users that features AI-assisted video navigation and chatbot interaction. In a final evaluation with seven students, the participants expressed enthusiasm for features such as AI-powered video search and the possibility of asking questions about visual content in the current video frame. They provided valuable suggestions for future improvements, including notifications for lecture slide transitions and the provision of a short overview function for a slide. Insights from the study indicate great potential of the prototype to improve accessibility of lecture videos for students with visual impairments, although they also point to crucial areas for improvement, such as more reliable and personalized image descriptions.},
  address = {New York, NY, USA},
  series = {{ASSETS},
  isbn = {979-8-4007-0676-9},
}

@inproceedings{orlando_can_2025,
  title = {Can {Generative},
  author = {Orlando, Gian Marco and La Gatta, Valerio and Russo, Diego and Moscato, Vincenzo},
  year = {2025},
  doi = {10.1145/3717867.3717895},
  url = {https://doi.org/10.1145/3717867.3717895},
  booktitle = {Proceedings of the 17th {ACM},
  pages = {510--515},
  publisher = {Association for Computing Machinery},
  keywords = {Agent-Based Modeling, Friendship Paradox, Generative Agents, Social Media Simulation},
  abstract = {Generative Agent-Based Modeling (GABM) is an emerging simulation paradigm that combines the reasoning abilities of Large Language Models with traditional Agent-Based Modeling to replicate complex social behaviors, including interactions on social media. While prior work has focused on localized phenomena such as opinion formation and information spread, its potential to capture global network dynamics remains underexplored. This paper addresses this gap by analyzing GABM-based social media simulations through the lens of the Friendship Paradox (FP), a counterintuitive phenomenon where individuals, on average, have fewer friends than their friends. We propose a GABM framework for social media simulations, featuring generative agents that emulate real users with distinct personalities and interests. Using Twitter datasets on the US 2020 Election and the QAnon conspiracy, we show that the FP emerges naturally in GABM simulations. Consistent with real-world observations, the simulations unveil a hierarchical structure, where agents preferentially connect with others displaying higher activity or influence. Additionally, we find that infrequent connections primarily drive the FP, reflecting patterns in real networks. These findings validate GABM as a robust tool for modeling global social media phenomena and highlight its potential for advancing social science by enabling nuanced analysis of user behavior.},
  address = {New York, NY, USA},
  series = {Websci '25},
  isbn = {979-8-4007-1483-2},
}

@inproceedings{maxim_voice_2025,
  title = {Voice and {Choice},
  author = {Maxim, Andrew and Cooks, Eric and Krieger, Janice and Lok, Benjamin},
  year = {2025},
  doi = {10.1145/3717511.3747065},
  url = {https://doi.org/10.1145/3717511.3747065},
  booktitle = {Proceedings of the 25th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {computer-generated voice, healthcare, patient choice, virtual humans},
  abstract = {Advances in AI, including large language models, enable virtual health assistants (VHAs) to tailor messaging to cultural and individual needs. However, to fully realize this potential, VHA voices must match or exceed human voices in influencing behavioral intentions. We conducted a multi-study investigation on whether matching text-to-speech (TTS) voices to human voices across vocal traits could promote colorectal cancer (CRC) screening among Black or African American adults. Study 1 (N = 313) tested whether a TTS voice prosodically matched to a human voice could elicit equivalent screening intentions using a White male VHA. It also evaluated whether increasing vocal confidence via SSML would enhance effectiveness. A user preference study (N = 317) assessed VHA identity and delivery modality preferences, finding strong support for a Black female VHA. Building on these results, Study 2 (N = 106) replicated the TTS-human voice comparison using a Black female VHA. Across studies, matching TTS voices to human voices on frequency, speech rate, and pitch variation achieved equivalent screening intentions. We discuss implications for VHA design and user-centered voice adaptation.},
  address = {New York, NY, USA},
  series = {{IVA},
  isbn = {979-8-4007-1508-2},
}

@inproceedings{alam_memento_2024,
  title = {Memento 4.0: {A},
  author = {Alam, Naushad and Graham, Yvette and Gurrin, Cathal},
  year = {2024},
  doi = {10.1145/3643489.3661126},
  url = {https://doi.org/10.1145/3643489.3661126},
  booktitle = {Proceedings of the 7th {Annual},
  pages = {82--87},
  publisher = {Association for Computing Machinery},
  note = {event-place: Phuket, Thailand},
  keywords = {conversational search, information retrieval, lifelog, semantic image representation, source: ACM},
  abstract = {The practice of lifelogging, capturing one's daily experiences through wearable devices, has evolved significantly over the last decade, presenting both challenges and opportunities in information retrieval. This paper presents an early prototype of a conversational lifelog retrieval system designed to address the open challenges in this domain. Our system integrates a hierarchical event segmentation approach to automatically organize lifelog data into meaningful events, facilitating event-based retrieval over traditional image retrieval. Moreover, we incorporate a question-answering pipeline, leveraging large language models such as GPT-3.5 Turbo and Mistral7B, to enable free-form natural language interaction with the lifelog dataset. Moreover, we enhance our system's user interface by building on previous versions to streamline event-based retrieval and question-answering functionalities.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0550-2},
  series = {{LSC},
}

@inproceedings{li_managing_2025,
  title = {Managing {Scalable},
  author = {Li, Shaobo and Zhou, Yirui Eric and Xue, Yuqi and Xu, Yuan and Huang, Jian},
  year = {2025},
  doi = {10.1145/3731569.3764857},
  url = {https://doi.org/10.1145/3731569.3764857},
  booktitle = {Proceedings of the {ACM},
  pages = {979--995},
  publisher = {Association for Computing Machinery},
  note = {event-place: Lotte Hotel World, Seoul, Republic of Korea},
  keywords = {file system, GPU, GPUDirect storage},
  abstract = {As we shift from CPU-centric computing to GPU-accelerated computing for supporting intelligent data processing at scale, the storage bottleneck has been exacerbated. To bypass the host CPUand alleviate unnecessary data movements, modern GPUs enable direct storage access to SSDs (i.e., GPUDirect Storage). However, current GPUDirect Storage solutions still rely on the host file system to manage the storage device, direct storage accesses are still bottlenecked by the host.In this paper, we develop a GPU-orchestrated file system (GoFS) for scaling the direct storage accesses for GPU programs, by fully offloading the storage management to the GPU. As GoFS provides POSIX API and manages core filesystem structures in GPU memory, it can execute both control path and data path without host CPU involvement. To enable highly concurrent direct storage accesses, we rethink the design and implementation of core filesystem structures with various optimization techniques, such as scalable data indexing, fine-grained per-SM (streaming multiprocessor) block management, and zero-copy I/O accesses, by carefully exploring the GPU-accelerated computing paradigm. GoFS preserves the essential filesystem properties such as crash consistency, and it is compatible with existing host-based file systems like F2FS. GoFS does not require changes to the on-disk filesystem organization, therefore, the host and GPU can manage the SSD in a coordinated fashion, and maintain the data consistency in a primary/secondary mode. We implement GoFS based on F2FS using 7.9K lines of codes with CUDA programming. We examine its efficiency on an A100 GPU. Our experiments with various GPU-based applications show that GoFS outperforms state-of-the-art storage access solutions for GPUs by 1.61× on average.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1870-0},
}

@inproceedings{zhao_model_2025,
  title = {Model {Meets},
  author = {Zhao, Jujia and Wang, Yumeng and Ren, Zhaochun and Verberne, Suzan},
  year = {2025},
  doi = {10.1145/3705328.3748152},
  url = {https://doi.org/10.1145/3705328.3748152},
  booktitle = {Proceedings of the {Nineteenth},
  pages = {802--811},
  publisher = {Association for Computing Machinery},
  keywords = {Conversational Recommender System, Knowledge Graphs, Pre-trained Language Models},
  abstract = {Conversational Recommender Systems (CRSs) often integrate external knowledge to enhance user preference modeling and item representation learning, addressing the challenge of sparse conversational contexts. Traditional methods primarily utilize structured knowledge graphs (KGs) to model entity relationships and capture deep, multi-hop relationships among items. More recent studies employing pre-trained language models (PLMs), however, leverage unstructured text (e.g., customer reviews) to enrich contextual understanding of users and items. Despite reported performance gains from both knowledge types, a question remains: What is the compatibility between specific CRS model architectures and types of external knowledge, and how do different knowledge sources complement each other? We present a reproducibility study evaluating 9 state-of-the-art CRSs, including KG-based and PLM-based paradigms, to systematically investigate model–-knowledge compatibility and complementarity. Through a comprehensive evaluation on three datasets, we uncover three key findings: (1) Different model architectures have different compatibility with knowledge types: decoder-only models excel with structured knowledge, whereas encoder-decoder models better utilize unstructured knowledge. (2) Combining multiple knowledge sources isn’t always superior to using a single type, but merging similar knowledge types is generally more effective than mixing different ones. (3) Unstructured knowledge broadly benefits all scenario-specific conversations, particularly in genre-specific and descriptive scenarios, whereas structured knowledge demonstrates superior performance in comparative recommendation scenarios. Our study serves as an inspiration for future research on maximizing the benefits of external knowledge across different models in CRSs.},
  address = {New York, NY, USA},
  series = {{RecSys},
  isbn = {979-8-4007-1364-4},
}

@inproceedings{rosset_researchy_2025,
  title = {Researchy {Questions},
  author = {Rosset, Corbin and Chung, Ho-Lam and Qin, Guanghui and Chau, Ethan and Feng, Zhuo and Awadallah, Ahmed and Neville, Jennifer and Rao, Nikhil},
  year = {2025},
  doi = {10.1145/3726302.3730275},
  url = {https://doi.org/10.1145/3726302.3730275},
  booktitle = {Proceedings of the 48th {International},
  pages = {3712--3722},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {deepresearch, multi-document ir, non-factoid qa},
  abstract = {Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ”known unknowns” with clear indications of both what information is missing, and how to find it to answer the question. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ”unknown unknowns”. We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. Furthermore, recent products like Google's DeepResearch (announced a year after this resource was released publicly) specifically address such queries, retrieving hundreds of documents to synthesize report-style responses. We present Researchy Questions, the world's first, only and largest public dataset of ”Deep Research” questions filtered from real search engine logs to be non-factoid, ”decompositional” and multi-perspective. We show that users spend substantial ”effort” on these questions in terms of signals like clicks and session length. We also show that ”slow thinking” answering techniques, like decomposition into sub-questions shows benefit over answering directly. We release (at https://huggingface.co/datasets/corbyrosset/researchy\_questions) about 100k Researchy Questions with a permissive CDLA-2.0 license, along with click histograms on over 350k Clueweb22 URLs that were clicked for each question.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{ueki_u-cker_2025,
  title = {U-{Cker},
  author = {Ueki, Kazuya and Muto, Ryo and Wada, Takuya and Akaba, Ryota and Fernandez, Genesis Faith Layag},
  year = {2025},
  doi = {10.1145/3729459.3748688},
  url = {https://doi.org/10.1145/3729459.3748688},
  booktitle = {Proceedings of the 8th {Annual},
  pages = {58--62},
  publisher = {Association for Computing Machinery},
  keywords = {Information retrieval, Lifelog, Retrieval system},
  abstract = {This paper presents U-Cker, a lifelog retrieval system developed for our first participation in the Lifelog Search Challenge (LSC’25). The system is designed to support lifelog image retrieval based on multiple conditions, allowing users to input various types of information such as multiple text descriptions, images, temporal constraints, and location-based filters. As a core component of our system, we use the CLIP model to bridge the gap between various user inputs and the underlying visual data. With its flexible input design, U-Cker already covers the basic functionality required for lifelog search and has shown promising results on past LSC tasks. Building on this foundation, we plan to extend its capabilities in future work.},
  address = {New York, NY, USA},
  series = {{LSC},
  isbn = {979-8-4007-1857-1},
}

@inproceedings{han_enhancing_2024,
  title = {Enhancing {Investment},
  author = {Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
  year = {2024},
  doi = {10.1145/3677052.3698645},
  url = {https://doi.org/10.1145/3677052.3698645},
  booktitle = {Proceedings of the 5th {ACM},
  pages = {538--546},
  publisher = {Association for Computing Machinery},
  note = {event-place: Brooklyn, NY, USA},
  keywords = {AI-agent, Financial Report Analysis, Investment Research, Multi-agent Collaboration},
  abstract = {In recent years, the application of generative artificial intelligence (GenAI) in financial analysis and investment decision-making has gained significant attention. However, most existing approaches rely on single-agent systems, which fail to fully utilize the collaborative potential of multiple AI agents. In this paper, we propose a novel multi-agent collaboration system designed to enhance decision-making in financial investment research. The system incorporates agent groups with both configurable group sizes and collaboration structures to leverage the strengths of each agent group type. By utilizing a sub-optimal combination strategy, the system dynamically adapts to varying market conditions and investment scenarios, optimizing performance across different tasks. We focus on three sub-tasks: fundamentals, market sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30 companies listed on the Dow Jones Index. Our findings reveal significant performance variations based on the configurations of AI agents for different tasks. The results demonstrate that our multi-agent collaboration system outperforms traditional single-agent models, offering improved accuracy, efficiency, and adaptability in complex financial environments. This study highlights the potential of multi-agent systems in transforming financial analysis and investment decision-making by integrating diverse analytical perspectives.},
  address = {New York, NY, USA},
  series = {{ICAIF},
  isbn = {979-8-4007-1081-0},
}

@inproceedings{sun_exploring_2025,
  title = {Exploring the {Application},
  author = {Sun, Yangzi},
  year = {2025},
  doi = {10.1145/3735014.3735915},
  url = {https://doi.org/10.1145/3735014.3735915},
  booktitle = {Proceedings of the 2024 {International},
  pages = {291--296},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {As large language models (LLMs) evolve rapidly in natural language processing, their application potential in educational scenes has become increasingly prominent. This article aims to explore the specific application and technical realization of LLMs in English reading and writing courses and solve the issues in practical teaching. Focusing on the core tasks of automatic composition correction, reading material generation and conversational writing guidance, this paper proposes an improved framework utilizing conditional Generative Adversarial Networks (cGAN). This framework improves the quality and controllability of model output by introducing conditional embedding layers, strengthening learning reward mechanisms and adaptive difficulty generation algorithms. The results show that, compared with the benchmark models such as BERT and T5, the proposed model improves accuracy by 25.51\% at the highest, and the generation efficiency is also significantly optimized under large-scale data sets. In addition, a questionnaire survey of 50 teachers and 100 students shows that more than 85\% of users are satisfied with the usability and practicability of the system.CCS CONCEPTS • Applied computing∼Education∼Computer-assisted instruction•Applied computing∼Education∼Collaborative learning},
  address = {New York, NY, USA},
  series = {{BDMIP},
  isbn = {979-8-4007-1040-7},
}

@inproceedings{schulte_what_2025,
  title = {What {We},
  author = {Schulte, Carsten and Sentance, Sue and Sparmann, Sören and Altin, Rukiye and Friebroon-Yesharim, Mor and Landman, Martina and Rücker, Michael T. and Satavlekar, Spruha and Siegel, Angela and Tedre, Matti and Tubino, Laura and Vartiainen, Henriikka and VelÁzquez-Iturbide, J. Ángel and Waite, Jane and Wu, Zihan},
  year = {2025},
  doi = {10.1145/3689187.3709612},
  url = {https://doi.org/10.1145/3689187.3709612},
  booktitle = {2024 {Working},
  pages = {226--257},
  publisher = {Association for Computing Machinery},
  note = {event-place: Milan, Italy},
  keywords = {computing education, curriculum, educational traditions, k-12 education, philosophy, rationales, source: ACM},
  abstract = {K-12 computing education research is a rapidly growing field of research, both driven by and driving the implementation of computing as a school and extra-curricular subject globally. In the context of discipline-based education research, it is a new and emerging field, drawing on areas such as mathematics and science education research for inspiration and theoretical bases. The urgency around investigating effective teaching and learning in computing in school alongside broadening participation has led to much of the field being focused on empirical research. Less attention has been paid to the underlying philosophical assumptions informing the discipline, which might include a critical examination of the rationale for K-12 computing education, its goals and perspectives, and associated inherent values and beliefs. In this working group, we conducted an analysis of the implicit and hidden values, perspectives and goals underpinning computing education at school in order to shed light on the question of what we are talking about when we talk about K-12 computing education. To do this we used a multi-faceted approach to identify implicit rationales for K-12 computing education and examine what these might mean for the implemented curriculum. Methods used include both traditional and natural language processing techniques for examining relevant literature, alongside an examination of the theoretical literature relating to education theory. As a result we identified four traditions for K-12 computing education: algorithmic, design-making, scientific and societal. From this we have developed a framework for the exemplification of these traditions, alongside several potential use cases. We suggest that while this work may provoke some discussion and debate, it will help researchers and others to identify and express the rationales they draw on with respect to computing education.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1208-1},
  series = {{ITiCSE},
}

@inproceedings{roy_beyond_2024,
  title = {Beyond {Accuracy},
  author = {Roy, Soumyadeep and Khatua, Aparup and Ghoochani, Fatemeh and Hadler, Uwe and Nejdl, Wolfgang and Ganguly, Niloy},
  year = {2024},
  doi = {10.1145/3626772.3657882},
  url = {https://doi.org/10.1145/3626772.3657882},
  booktitle = {Proceedings of the 47th {International},
  pages = {1073--1082},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {gpt-4, medical qa, multi-label dataset, usmle error taxonomy, source: ACM},
  abstract = {GPT-4 demonstrates high accuracy in medical QA tasks, leading with an accuracy of 86.70\%, followed by Med-PaLM 2 at 86.50\%. However, around 14\% of errors remain. Additionally, current works use GPT-4 to only predict the correct option without providing any explanation and thus do not provide any insight into the thinking process and reasoning used by GPT-4 or other LLMs. Therefore, we introduce a new domain-specific error taxonomy derived from collaboration with medical students. Our GPT-4 USMLE Error (G4UE) dataset comprises 4153 GPT-4 correct responses and 919 incorrect responses to the United States Medical Licensing Examination (USMLE) respectively. These responses are quite long (258 words on average), containing detailed explanations from GPT-4 justifying the selected option. We then launch a large-scale annotation study using the Potato annotation platform and recruit 44 medical experts through Prolific, a well-known crowdsourcing platform. We annotated 300 out of these 919 incorrect data points at a granular level for different classes and created a multi-label span to identify the reasons behind the error. In our annotated dataset, a substantial portion of GPT-4's incorrect responses is categorized as a "Reasonable response by GPT-4," by annotators. This sheds light on the challenge of discerning explanations that may lead to incorrect options, even among trained medical professionals. We also provide medical concepts and medical semantic predications extracted using the SemRep tool for every data point. We believe that it will aid in evaluating the ability of LLMs to answer complex medical questions. We make the resources available at https://github.com/roysoumya/usmle-gpt4-error-taxonomy.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{lv_study_2024,
  title = {A study of {AIGC},
  author = {Lv, Han and Wang, Kun},
  year = {2024},
  doi = {10.1145/3675417.3675447},
  url = {https://doi.org/10.1145/3675417.3675447},
  booktitle = {Proceedings of the 2024 {Guangdong},
  pages = {179--191},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hongkong, China},
  keywords = {source: ACM},
  abstract = {The intelligent transformation and development of domestic small and medium-sized manufacturing enterprises is of great significance for transferring internal production capacity, enhancing the level of opening up to the outside world, and promoting the high-quality development of the foreign trade industry.The development of AIGC (Artificial Intelligence Generated Content) is not only a major technological breakthrough in the field of artificial intelligence, but also its highly efficient content production drives the generation of new intelligent export methods in the manufacturing industry. Through the mass generation of text, the use of pre-training models as well as the development and optimization of cue words, to the manufacturing and exporting enterprises to output content in line with SEO (Search Engine Optimization) rules, through the optimization of internal and external chains to improve search engine rankings, so as to enhance the overseas visibility of the enterprise's products, and to open up the sales of the products. Based on the mechanism analysis of economics and management perspective, AIGC can provide professional and in-depth website enhancement diagnostic solutions and key optimization for foreign trade enterprises on a regular basis through the revolution of content production, so as to enhance the export efficiency and competitiveness of foreign trade enterprises. Therefore, the application of AIGC technology in foreign trade enterprises should be promoted, and AIGC technology providers should be encouraged to strengthen the training of large models in different industries, so as to provide specialized technical support for export enterprises.},
  address = {New York, NY, USA},
  series = {{DEAI},
  isbn = {979-8-4007-1714-7},
}

@inproceedings{li_novel_2025,
  title = {A {Novel},
  author = {Li, Ziqing and Cukurova, Mutlu and Bulathwela, Sahan},
  year = {2025},
  doi = {10.1145/3706468.3706487},
  url = {https://doi.org/10.1145/3706468.3706487},
  booktitle = {Proceedings of the 15th {International},
  pages = {148--158},
  publisher = {Association for Computing Machinery},
  keywords = {Educational Question Generation, Formative Assessment, Natural Language Processing, Personalised Testing, Summative Assessment},
  abstract = {The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model’s performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT.},
  address = {New York, NY, USA},
  series = {{LAK},
  isbn = {979-8-4007-0701-8},
}

@inproceedings{liu_t-rap_2024,
  title = {T-{RAP},
  author = {Liu, Pei and Lin, Bo and Qin, Yihao and Weng, Cheng and Chen, Liqian},
  year = {2024},
  doi = {10.1145/3671016.3672506},
  url = {https://doi.org/10.1145/3671016.3672506},
  booktitle = {Proceedings of the 15th {Asia},
  pages = {105--114},
  publisher = {Association for Computing Machinery},
  note = {event-place: Macau, China},
  keywords = {Automated Vulnerability Repair, Deep Learning, Repair Template, Software Vulnerability},
  abstract = {Vulnerabilities exert great burden on developers in terms of debugging and maintenance. Automated Vulnerability Repair(AVR) is considered as a promising approach to alleviate the burden of developers. Template-based automated program repair techniques have shown their effectiveness in fixing general bugs. However, due to the diverse root causes of vulnerabilities, it is challenging to construct sufficient repair templates to cover various vulnerabilities. In this paper, we introduce a Template-guided Retrieval-Augmented Patch generation approach, named T-RAP. Inspired by retrieval-augmented techniques that effectively utilize historical data, our approach leverages repair templates to extract similar vulnerability repair patches from the codebase. These patches then guide the process of generating vulnerability patches. To extract similar patches, we also propose a matching algorithm specifically designed for the retrieval-augmented vulnerability repair. This involves identifying similarities between numerous templates and vulnerabilities during the template-guided stage. Experimental results demonstrate that T-RAP outperforms all the studied AVR approaches, repairing 56.8\% more vulnerabilities than VulRepair and 30.24\% more than VulMaster. It can also accurately repair more types of real-world vulnerabilities than VulMaster. Additionally, we evaluated the effectiveness of our patch retriever. The results indicate that our template-guided retriever, which is based on our matching algorithm, outperforms the retrieval algorithm proposed in the recent retrieval-augmented patch generation approach RAP-Gen.},
  address = {New York, NY, USA},
  series = {Internetware '24},
  isbn = {979-8-4007-0705-6},
}

@inproceedings{mozafari_triviahg_2024,
  title = {{TriviaHG},
  author = {Mozafari, Jamshid and Jangra, Anubhav and Jatowt, Adam},
  year = {2024},
  doi = {10.1145/3626772.3657855},
  url = {https://doi.org/10.1145/3626772.3657855},
  booktitle = {Proceedings of the 47th {International},
  pages = {2060--2070},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {hint generation, large language models, question answering},
  abstract = {Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 humans with answering questions using the provided hints. The effectiveness of hints varied, with success rates of 96\%, 78\%, and 36\% for questions with easy, medium, and hard answers, respectively. Moreover, the proposed automatic evaluation methods showed a robust correlation with annotators' results. Conclusively, the findings highlight three key insights: the facilitative role of hints in resolving unknown questions, the dependence of hint quality on answer difficulty, and the feasibility of employing automatic evaluation methods for hint assessment.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{zhuang_document_2025,
  title = {Document {Screenshot},
  author = {Zhuang, Shengyao and Khramtsova, Ekaterina and Ma, Xueguang and Koopman, Bevan and Lin, Jimmy and Zuccon, Guido},
  year = {2025},
  doi = {10.1145/3726302.3730056},
  url = {https://doi.org/10.1145/3726302.3730056},
  booktitle = {Proceedings of the 48th {International},
  pages = {414--423},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {corpus poisoning, document screenshot retrieval, vlm-based dense retrievers},
  abstract = {Recent advancements in dense retrieval have introduced vision-language model (VLM)-based retrievers, such as DSE and ColPali, which leverage document screenshots embedded as vectors to enable effective search and offer a simplified pipeline over traditional text-only methods. In this study, we propose three pixel poisoning attack methods designed to compromise VLM-based retrievers and evaluate their effectiveness under various attack settings and parameter configurations. Our empirical results demonstrate that injecting even a single adversarial screenshot into the retrieval corpus can significantly disrupt search results, poisoning the top-10 retrieved documents for 41.9\% of queries in the case of DSE and 26.4\% for ColPali. These vulnerability rates notably exceed those observed with equivalent attacks on text-only retrievers. Moreover, when targeting a small set of known queries, the attack success rate raises, achieving complete success in certain cases. By exposing the vulnerabilities inherent in vision-language models, this work highlights the potential risks associated with their deployment.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{mou_unifying_2024,
  title = {Unifying {Local},
  author = {Mou, Xinyi and Li, Zejun and Lyu, Hanjia and Luo, Jiebo and Wei, Zhongyu},
  year = {2024},
  doi = {10.1145/3589334.3645616},
  url = {https://doi.org/10.1145/3589334.3645616},
  booktitle = {Proceedings of the {ACM},
  pages = {2603--2614},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {knowledge graph, large language models, political science, source: ACM},
  abstract = {Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{ma_alibaba_2025,
  title = {Alibaba {LingmaAgent},
  author = {Ma, Yingwei and Yang, Qingping and Cao, Rongyu and Li, Binhua and Huang, Fei and Li, Yongbin},
  year = {2025},
  doi = {10.1145/3696630.3728549},
  url = {https://doi.org/10.1145/3696630.3728549},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {238--249},
  publisher = {Association for Computing Machinery},
  note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
  keywords = {automated program repair, automatic software engineering (ASE), fault localization, large language models (LLMs), monte carlo tree search (MCTS), software engineering agents},
  abstract = {This paper presents Alibaba LingmaAgent, a novel Automated Software Engineering method designed to comprehensively understand and utilize whole software repositories for issue resolution. Deployed in TONGYI Lingma, an IDE-based coding assistant developed by Alibaba Cloud, LingmaAgent addresses the limitations of existing LLM-based agents that primarily focus on local code information. Our approach introduces a top-down method to condense critical repository information into a knowledge graph, reducing complexity, and employs a Monte Carlo tree search based strategy enabling agents to explore and understand entire repositories. We guide agents to summarize, analyze, and plan using repository-level knowledge, allowing them to dynamically acquire information and generate patches for real-world GitHub issues. In extensive experiments, LingmaAgent demonstrated significant improvements, achieving an 18.5\% relative improvement on the SWE-bench Lite benchmark compared to SWE-agent. In production deployment and evaluation at Alibaba Cloud, LingmaAgent automatically resolved 16.9\% of in-house issues faced by development engineers, and solved 43.3\% of problems after manual intervention. Additionally, we have open-sourced a Python prototype of LingmaAgent for reference by other industrial developers 1. In fact, LingmaAgent has been used as a developed reference by many subsequently agents.},
  address = {New York, NY, USA},
  series = {{FSE},
  isbn = {979-8-4007-1276-0},
}

@inproceedings{wang_talking_2025,
  title = {Talking {Spell},
  author = {Wang, Xuetong and Pang, Ching Christie and Hui, Pan},
  year = {2025},
  doi = {10.1145/3746059.3747617},
  url = {https://doi.org/10.1145/3746059.3747617},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {AI Companionship, Embodied and Explorable Interaction, Human-Object Interaction, Large Language Models (LLMs), On-body Devices, Ubiquitous Computing, Wearable},
  abstract = {Virtual assistants (VAs) have become ubiquitous in daily life, integrated into smartphones and smart devices, sparking interest in AI companions that enhance user experiences and foster emotional connections. However, existing companions are often embedded in specific objects—such as glasses, home assistants, or dolls—requiring users to form emotional bonds with unfamiliar items, which can lead to reduced engagement and feelings of detachment. To address this, we introduce Talking Spell1, a wearable system that empowers users to imbue any everyday object with speech and anthropomorphic personas through a user-centric radiative network. Leveraging advanced computer vision (e.g., YOLOv11[46] for object detection), large vision-language models (e.g., QWEN-VL[4] for persona generation), speech-to-text and text-to-speech technologies, Talking Spell guides users through three stages of emotional connection: acquaintance, familiarization, and bonding. We validated our system through a user study involving 12 participants, utilizing Talking Spell to explore four interaction intentions: entertainment, companionship, utility, and creativity. The results demonstrate its effectiveness in fostering meaningful interactions and emotional significance with everyday objects. Our findings indicate that Talking Spell creates engaging and personalized experiences, as demonstrated through various devices, ranging from accessories to essential wearables.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{xie_beyond_2025,
  title = {Beyond {Visual},
  author = {Xie, Jingyi and Yu, Rui and Zhang, He and Billah, Syed Masum and Lee, Sooyeon and Carroll, John M.},
  year = {2025},
  doi = {10.1145/3706598.3714210},
  url = {https://doi.org/10.1145/3706598.3714210},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Be My AI., Be My Eyes, Human-AI interaction, large multimodal models (LMMs), People with visual impairments (PVI), remote sighted assistance (RSA), visual question answering (VQA)},
  abstract = {Large multimodal models (LMMs) have enabled new AI-powered applications that help people with visual impairments (PVI) receive natural language descriptions of their surroundings through audible text. We investigated how this emerging paradigm of visual assistance transforms how PVI perform and manage their daily tasks. Moving beyond usability assessments, we examined both the capabilities and limitations of LMM-based tools in personal and social contexts, while exploring design implications for their future development. Through interviews with 14 visually impaired users of Be My AI (an LMM-based application) and analysis of its image descriptions from both study participants and social media platforms, we identified two key limitations. First, these systems’ context awareness suffers from hallucinations and misinterpretations of social contexts, styles, and human identities. Second, their intent-oriented capabilities often fail to grasp and act on users’ intentions. Based on these findings, we propose design strategies for improving both human-AI and AI-AI interactions, contributing to the development of more effective, interactive, and personalized assistive technologies.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{jiang_research_2024,
  title = {Research on {Engineering},
  author = {Jiang, Yingdi and Yao, Jiarui and Li, Fangfei and Zhang, Yan},
  year = {2024},
  doi = {10.1145/3653946.3653961},
  url = {https://doi.org/10.1145/3653946.3653961},
  booktitle = {Proceedings of the 2024 7th {International},
  pages = {100--105},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {Engineering management, Keywords • Large language models, Knowledge graphs, Question-answering, source: ACM},
  abstract = {In the engineering management of the communication industry, there are many issues, including low efficiency in information acquisition and limitations in the level of intelligence.Large language models, with their powerful text comprehension and generation capabilities, offer new perspectives for the development of this field.This study constructed a question-answering system using a combined approach of large language models and text knowledge bases. The system dynamically leverages abundant external knowledge and enhances the model's reasoning ability and interpretability through knowledge graphs. In response to five categories of issues in engineering management, experiments and in-depth analysis revealed that although large language models may lack granularity in addressing some complex problems, the question-answering system overall achieved intelligent assistance, improving the efficiency of collaborative engineering management.},
  address = {New York, NY, USA},
  series = {{ICMVA},
  isbn = {979-8-4007-1655-3},
}

@inproceedings{piramanayagam_lifesearch_2025,
  title = {{LifeSearch},
  author = {Piramanayagam, Shriram and Chen, Enting and Melo, Andre and Vougiouklis, Pavlos and Diao, Chenxin and Merita, Pascual and Jiang, Zeren and Lai, Ruofei and Pan, Jeff Z.},
  year = {2025},
  doi = {10.1145/3729459.3748696},
  url = {https://doi.org/10.1145/3729459.3748696},
  booktitle = {Proceedings of the 8th {Annual},
  pages = {28--33},
  publisher = {Association for Computing Machinery},
  keywords = {Lifelog Search Challenge, Lifelogging, Multimodal Retrieval, source: ACM},
  abstract = {We introduce LifeSearch, a novel multimodal retrieval system developed for the Lifelog Search Challenge 2025 (LSC’25). Our proposed solution can query images across visual, spatial, and temporal domains. Central to our approach are spatial and temporal augmentation techniques, along with a time-constrained scoring mechanism. By integrating these methods with a hybrid retrieval strategy, LifeSearch improves the retrieval process, allowing users to execute complex lifelog queries and obtain highly relevant results.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1857-1},
  series = {{LSC},
}

@inproceedings{cao_companykg_2024,
  title = {{CompanyKG},
  author = {Cao, Lele and von Ehrenheim, Vilhelm and Granroth-Wilding, Mark and Anselmo Stahl, Richard and McCornack, Andrew and Catovic, Armin and Cavalcanti Rocha, Dhiana Deva},
  year = {2024},
  doi = {10.1145/3637528.3671515},
  url = {https://doi.org/10.1145/3637528.3671515},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {4816--4827},
  publisher = {Association for Computing Machinery},
  note = {event-place: Barcelona, Spain},
  keywords = {benchmark, company similarity quantification, edge prediction, graph neural network, investment, knowledge graph, private equity},
  abstract = {This paper presents CompanyKG (version 2), a large-scale heterogeneous graph developed for fine-grained company similarity quantification and relationship prediction, crucial for applications in the investment industry such as market mapping, competitor analysis, and mergers and acquisitions. CompanyKG comprises 1.17 million companies represented as graph nodes, enriched with company description embeddings, and 51.06 million weighted edges denoting 15 distinct inter-company relations. To facilitate a thorough evaluation of methods for company similarity quantification and relationship prediction, we have created four annotated evaluation tasks: similarity prediction, competitor retrieval, similarity ranking, and edge prediction. We offer extensive benchmarking results for 11 reproducible predictive methods, categorized into three groups: node-only, edge-only, and node+edge. To our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset derived from a real-world investment platform, specifically tailored for quantifying inter-company similarity and relationships.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-0490-1},
}

@inproceedings{kataria_learning_2023,
  title = {Learning to {Rank},
  author = {Kataria, Ayush and Venkateshprasanna, H M and Kummetha, Ashok Kumar Reddy},
  year = {2023},
  doi = {10.1145/3627217.3627224},
  url = {https://doi.org/10.1145/3627217.3627224},
  booktitle = {Proceedings of the 16th {Annual},
  pages = {25--30},
  publisher = {Association for Computing Machinery},
  note = {event-place: Hyderabad, India},
  keywords = {Information Retrieval, Learning Experience Platforms, Learning to Rank, Supervised Learning},
  abstract = {The ability to search and retrieve the right resources in a Learning Experience Platform (LXP) is critical in helping the workforce of an enterprise to upskill and deepen their expertise effectively. To ensure the best resources are shown as high in the result set as possible to catch learners’ attention, a supervised learning approach of training and deploying a Learning to Rank (LTR) model for re-ranking is proposed. This work specifically focuses on judgement list preparation taking advantage of the learning progress data available in LXPs, as well as on defining and measuring model performance through metrics in both test and production setups. In particular, it highlights the positive impact of the deployed LTR model in production using the defined metrics like average search result click position and percentage top N clicks.},
  address = {New York, NY, USA},
  series = {{COMPUTE},
  isbn = {979-8-4007-0840-4},
}

@inproceedings{zulfikar_resonance_2025,
  title = {Resonance: {Drawing},
  author = {Zulfikar, Wazeer and Chiaravalloti, Treyden and Shen, Jocelyn and Picard, Rosalind and Maes, Pattie},
  year = {2025},
  doi = {10.1145/3745900.3746099},
  url = {https://doi.org/10.1145/3745900.3746099},
  booktitle = {Proceedings of the {Augmented},
  pages = {199--215},
  publisher = {Association for Computing Machinery},
  keywords = {large language models, memory augmentation, mental health, positive psychology},
  abstract = {People inherently use experiences of their past while imagining their future, a capability that plays a crucial role in mental health. Resonance is an AI-powered journaling tool designed to augment this ability by offering AI-generated, action-oriented suggestions for future activities based on the user’s own past memories. Suggestions are offered when a new memory is logged and are followed by a prompt for the user to imagine carrying out the suggestion. In a two-week randomized controlled study (N=55), we found that using Resonance significantly improved mental health outcomes, reducing the users’ PHQ8 scores, a measure of current depression, and increasing their daily positive affect, particularly when they would likely act on the suggestion. Notably, the effectiveness of the suggestions was higher when they were personal, novel, and referenced the user’s logged memories. Finally, through open-ended feedback, we discuss the factors that encouraged or hindered the use of the tool.},
  address = {New York, NY, USA},
  series = {{AHs},
  isbn = {979-8-4007-1566-2},
}

@inproceedings{luo_imagescope_2025,
  title = {{ImageScope},
  author = {Luo, Pengfei and Zhou, Jingbo and Xu, Tong and Xia, Yuan and Xu, Linli and Chen, Enhong},
  year = {2025},
  doi = {10.1145/3696410.3714777},
  url = {https://doi.org/10.1145/3696410.3714777},
  booktitle = {Proceedings of the {ACM},
  pages = {1666--1682},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {collective reasoning, language-guided image retrieval, large multimodal model},
  abstract = {With the proliferation of images in online content, language-guided image retrieval (LGIR) has emerged as a research hotspot over the past decade, encompassing a variety of subtasks with diverse input forms. While the development of large multimodal models (LMMs) has significantly facilitated these tasks, existing approaches often address them in isolation, requiring the construction of separate systems for each task. This not only increases system complexity and maintenance costs, but also exacerbates challenges stemming from language ambiguity and complex image content, making it difficult for retrieval systems to provide accurate and reliable results. To this end, we propose ImageScope, a training-free, three-stage framework that leverages collective reasoning to unify LGIR tasks. The key insight behind the unification lies in the compositional nature of language, which transforms diverse LGIR tasks into a generalized text-to-image retrieval process, along with the reasoning of LMMs serving as a universal verification to refine the results. To be specific, in the first stage, we improve the robustness of the framework by synthesizing search intents across varying levels of semantic granularity using chain-of-thought (CoT) reasoning. In the second and third stages, we then reflect on retrieval results by verifying predicate propositions locally, and performing pairwise evaluations globally. Experiments conducted on six LGIR datasets demonstrate that ImageScope outperforms competitive baselines. Comprehensive evaluations and ablation studies further confirm the effectiveness of our design.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{nguyen_assessing_2025,
  title = {Assessing {Effective},
  author = {Nguyen, Le and Jain, Preet and Panchal, Krutik and Alam, Md Tanvirul and Rastogi, Nidhi},
  year = {2025},
  doi = {10.1145/3726302.3730326},
  url = {https://doi.org/10.1145/3726302.3730326},
  booktitle = {Proceedings of the 48th {International},
  pages = {3173--3182},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {benchmarking, clip, image retrieval, long text, multimodal, text-to-image},
  abstract = {Multimodal embedding models have been widely adopted in text-to-image retrieval, enabling direct comparison between text and image modalities. However, how well they handle long text is poorly understood. For instance, Long-CLIP found that OpenAI's CLIP model, despite having a 77-token input limit, maintains optimal performance for only 20 tokens- its effective token length. In this paper, we build on the Long-CLIP study, and extend the analysis to other widely used multimodal models and find their effective token length. Unlike Long-CLIP, we examine how domain-specific language influences changes in effective token length and explore its implications on different domains. Based on our findings, we create a comprehensive reference of various models' effective token length across different domains; offering deeper insights into the true limitations of multimodal models used in text-to-image retrieval. Finally, we introduce a systematic benchmark that determines the effective token length of any multimodal model using a given dataset. Our results show that the effective token length is consistently lower than the input token limit for all models, meaning that these models cannot utilize all the text that can be given to them. We also find that the effective token length varies by dataset, with domain-specific language influencing how much text a model can use before retrieval performance plateaus. Our code is available for reproducibility at https://github.com/aiforsec/EffectiveTokenLength-MModels},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_vifusion_2025,
  title = {{ViFusion},
  author = {Wang, Yisu and Zhu, Yixiang and Li, Xinjiao and Zhang, Yulong and Wu, Ruilong and Kutscher, Dirk},
  year = {2025},
  doi = {10.1145/3731715.3733461},
  url = {https://doi.org/10.1145/3731715.3733461},
  booktitle = {Proceedings of the 2025 {International},
  pages = {1452--1460},
  publisher = {Association for Computing Machinery},
  note = {event-place: Chicago, IL, USA},
  keywords = {in-network computation, tensor fusion, video feature indexing},
  abstract = {Large-scale video feature indexing in datacenters is critically dependent on efficient data transfer. Although in-network computation has emerged as a compelling strategy for accelerating feature extraction and reducing overhead in distributed multimedia systems, harnessing advanced networking resources at both the switch and host levels remains a formidable challenge. These difficulties are compounded by heterogeneous hardware, diverse application requirements, and complex multipath topologies. Existing methods focus primarily on optimizing inference for large neural network models using specialized collective communication libraries, which often face performance degradation in network congestion scenarios.To overcome these limitations, we present ViFusion, a communication aware tensor fusion framework that streamlines distributed video indexing by merging numerous small feature tensors into consolidated and more manageable units. By integrating an in-network computation module and a dedicated tensor fusion mechanism within datacenter environments, ViFusion substantially improves the efficiency of video feature indexing workflows. The deployment results show that ViFusion improves the throughput of the video retrieval system by 8–22× with the same level of latency as state-of-the-art systems.},
  address = {New York, NY, USA},
  series = {{ICMR},
  isbn = {979-8-4007-1877-9},
}

@inproceedings{yen_code_2025,
  title = {Code {Shaping},
  author = {Yen, Ryan and Zhao, Jian and Vogel, Daniel},
  year = {2025},
  doi = {10.1145/3706598.3713822},
  url = {https://doi.org/10.1145/3706598.3713822},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Dynamic Abstraction, Ink-based Sketching, Programming Interface},
  abstract = {We introduce the concept of code shaping, an interaction paradigm for editing code using free-form sketch annotations directly on top of the code and console output. To evaluate this concept, we conducted a three-stage design study with 18 different programmers to investigate how sketches can communicate intended code edits to an AI model for interpretation and execution. The results show how different sketches are used, the strategies programmers employ during iterative interactions with AI interpretations, and interaction design principles that support the reconciliation between the code editor and sketches. Finally, we demonstrate the practical application of the code shaping concept with two use case scenarios, illustrating design implications from the study.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{talaei_storysage_2025,
  title = {{StorySage},
  author = {Talaei, Shayan and Li, Meijin and Grover, Kanu and Hippler, James Kent and Yang, Diyi and Saberi, Amin},
  year = {2025},
  doi = {10.1145/3746059.3747681},
  url = {https://doi.org/10.1145/3746059.3747681},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Biography Writing, Generative Conversational Agents, Human-AI interaction},
  abstract = {Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative—a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N = 28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{zamani_retrieval-enhanced_2022,
  title = {Retrieval-{Enhanced},
  author = {Zamani, Hamed and Diaz, Fernando and Dehghani, Mostafa and Metzler, Donald and Bendersky, Michael},
  year = {2022},
  doi = {10.1145/3477495.3531722},
  url = {https://doi.org/10.1145/3477495.3531722},
  booktitle = {Proceedings of the 45th {International},
  pages = {2875--2886},
  publisher = {Association for Computing Machinery},
  note = {event-place: Madrid, Spain},
  keywords = {knowledge grounding, memory augmentation, retrieval augmentation},
  abstract = {Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {978-1-4503-8732-3},
}

@inproceedings{lee_understanding_2025,
  title = {Understanding {Adolescents},
  author = {Lee, Jamie and Jung, Kyuha and Newman, Erin Gregg and Chow, Emilie and Chen, Yunan},
  year = {2025},
  doi = {10.1145/3706598.3713244},
  url = {https://doi.org/10.1145/3706598.3713244},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Adolescents, Artificial Intelligence, Design Fiction, Health and Wellbeing, source: ACM},
  abstract = {Despite the growing research on users’ perceptions of health AI, adolescents’ perspectives remain underexplored. This study explores adolescents’ perceived benefits and risks of health AI technologies in clinical and personal health settings. Employing Design Fiction, we conducted interviews with 16 adolescents (aged 13-17) using four fictional design scenarios that represent current and future health AI technologies as probes. Our findings revealed that with a positive yet cautious attitude, adolescents envision unique benefits and risks specific to their age group. While health AI technologies were seen as valuable learning resources, they also raised concerns about confidentiality with their parents. Additionally, we identified several factors, such as severity of health conditions and previous experience with AI, influencing their perceptions of trust and privacy in health AI. We explore how these insights can inform the future design of health AI technologies to support learning, engagement, and trust as adolescents navigate their healthcare journey.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1394-1},
  series = {{CHI},
}

@inproceedings{kraft_knowledge-enhanced_2024,
  title = {Knowledge-{Enhanced},
  author = {Kraft, Angelie and Soulier, Eloïse},
  year = {2024},
  doi = {10.1145/3630106.3658981},
  url = {https://doi.org/10.1145/3630106.3658981},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {1433--1445},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rio de Janeiro, Brazil},
  keywords = {bias, epistemology, fairness, feminism, knowledge enhancement, knowledge graphs, language models, natural language processing, representation},
  abstract = {The factual inaccuracies ("hallucinations") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of "objective" or "neutral" knowledge. Knowledge enhancement techniques commonly use Wikidata and Wikipedia as their sources for knowledge, due to their large scales, public accessibility, and assumed trustworthiness. In this work, they serve as a case study for the influence of the social setting and the identity of knowers on epistemic processes. Indeed, the communities behind Wikidata and Wikipedia are known to be male-dominated and many instances of hostile behavior have been reported in the past decade. In effect, the contents of these knowledge bases are highly biased. It is therefore doubtful that these knowledge bases would contribute to bias reduction. In fact, our empirical evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate that knowledge enhancement may not live up to the hopes of increased objectivity. In our study, the average probability for stereotypical associations was preserved on two out of three metrics and performance-related gender gaps on knowledge-driven task were also preserved. We build on these results and critical literature to argue that the label of "knowledge" and the commonly held beliefs about it can obscure the harm that is still done to marginalized groups. Knowledge enhancement is at risk of perpetuating epistemic injustice, and AI engineers’ understanding of knowledge as objective per se conceals this injustice. Finally, to get closer to trustworthy language models, we need to rethink knowledge in AI and aim for an agenda of diversification and scrutiny from outgroup members.},
  address = {New York, NY, USA},
  series = {{FAccT},
  isbn = {979-8-4007-0450-5},
}

@inproceedings{you_navigating_2025,
  title = {Navigating the {Testing},
  author = {You, Hanmo and Wang, Zan and Lin, Bin and Chen, Junjie},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00106},
  url = {https://doi.org/10.1109/icse55347.2025.00106},
  booktitle = {Proceedings of the {IEEE},
  pages = {2726--2738},
  publisher = {IEEE Press},
  keywords = {deep learning, interview study, software evolution, testing},
  abstract = {Deep Learning (DL) systems have been widely adopted across various industrial domains such as autonomous driving and intelligent healthcare. As with traditional software, DL systems also need to constantly evolve to meet ever-changing user requirements. However, ensuring the quality of these continuously evolving systems presents significant challenges, especially in the context of testing. Understanding how industry developers address these challenges and what extra obstacles they are facing could provide valuable insights for further safeguarding the quality of DL systems. To reach this goal, we conducted semi-structured interviews with 22 DL developers from diverse domains and backgrounds. More specifically, our study focuses on exploring the challenges developers encounter in testing evolving DL systems, the practical solutions they employ, and their expectations for extra support. Our results highlight the difficulties in testing evolving DL systems (e.g., regression faults, online-offline differences, and test data collection) and identify the best practices for DL developers to address these challenges. Additionally, we pinpoint potential future research directions to enhance testing effectiveness in evolving DL systems.},
  address = {Ottawa, Ontario, Canada},
  series = {{ICSE},
  isbn = {979-8-3315-0569-1},
}

@inproceedings{qiu_unearthing_2024,
  title = {Unearthing {Semantic},
  author = {Qiu, Yiming and Kon, Patrick Tser Jern and Beckett, Ryan and Chen, Ang},
  year = {2024},
  doi = {10.1145/3694715.3695974},
  url = {https://doi.org/10.1145/3694715.3695974},
  booktitle = {Proceedings of the {ACM},
  pages = {574--589},
  publisher = {Association for Computing Machinery},
  note = {event-place: Austin, TX, USA},
  keywords = {cloud management, configuration mining, infrastructure as code, program analysis, source: ACM},
  abstract = {Cloud infrastructures are increasingly managed by Infrastructure-as-Code (IaC) frameworks (e.g., Terraform). IaC frameworks enable cloud users to configure their resources in a declarative manner, without having to directly work with low-level cloud API calls. However, with today's IaC tooling, IaC programs that pass the compilation phase may still incur errors at deployment time, resulting in significant disruption. We observe that this stems from a fundamental semantic gap between IaC-level programs and cloud-level requirements—even a syntactically-correct IaC program may violate cloud-level expectations. To bridge this gap, we develop Zodiac, a tool that can unearth IaC-level semantic checks on cloud-level requirements. It provides an automated pipeline to mine these checks from online IaC repositories and validate them using deployment-based testing. We have applied Zodiac to Terraform resources offered by Microsoft Azure—a leading IaC framework and a leading cloud vendor—where it found 500+ semantic checks where violation would produce deployment failures. With these checks, we have identified 200+ buggy Terraform projects and helped fix errors within official Azure provider usage examples.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1251-7},
}

@inproceedings{nayak_experimental_2024,
  title = {Experimental {Security},
  author = {Nayak, Asmit and Khandelwal, Rishabh and Fernandes, Earlence and Fawaz, Kassem},
  year = {2024},
  doi = {10.1145/3589334.3645683},
  url = {https://doi.org/10.1145/3589334.3645683},
  booktitle = {Proceedings of the {ACM},
  pages = {1283--1294},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {browser extensions, browser vulnerabilities, chrome web store, data privacy, sensitive data access},
  abstract = {Browser extensions offer a variety of valuable features and functionalities. They also pose a significant security risk if not properly designed or reviewed. Prior works have shown that browser extensions can access and manipulate data fields, including sensitive data such as passwords, credit card numbers, and Social Security numbers. In this paper, we present an empirical study of the security risks posed by browser extensions. Specifically, we first build a proof-of-concept extension that can steal sensitive user information. We find that the extension passes the Chrome Webstore review process. We then perform a measurement study on the top 10K website login pages to check if the extension access to password fields via JS. We find that none of the password fields are actively protected, and can be accessed using JS. Moreover, we found that 1K websites store passwords in plaintext in their page source, including popular websites like Google.com and Cloudflare.com. We also analyzed over 160K Chrome Web Store extensions for malicious behavior, finding that 28K have permission to access sensitive fields and 190 store password fields in variables. To analyze the behavioral workflow of the potentially malicious extensions, we propose an LLM-driven framework, Extension Reviewer. Finally, we discuss two countermeasures to address these risks: a bolt-on JavaScript package for immediate adoption by website developers allowing them to protect sensitive input fields, and a browser-level solution that alerts users when an extension accesses sensitive input fields. Our research highlights the urgent need for improved security measures to protect sensitive user information online.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{tang_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Tang, Xing and Yang, Chaohua and Fu, Yuwen and Ao, Dongyang and Li, Shiwei and Lyu, Fuyuan and Liu, Dugang and He, Xiuqiang},
  year = {2025},
  doi = {10.1145/3711896.3737261},
  url = {https://doi.org/10.1145/3711896.3737261},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4891--4900},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {click-through rate prediction, cross-domain, lifelong user behavior},
  abstract = {Lifelong behavior modeling for single-domain has been widely investigated in industry click-through (CTR) prediction. However, some domains do not always have rich historical behaviors in online platforms, so cross-domain lifelong behavior modeling is overlooked. This paper proposes a novel retrieval augmented lifelong cross-domain net (RAL-CDNet) to address the challenges in cross-domain lifelong behavior modeling. There are three components in RAL-CDNet, i.e., cross-domain retrieval unit, cross-domain alignment unit, and cross-net. As the general search unit in the previous study, a cross-domain retrieval unit features a retrieval augmented paradigm that utilizes a pre-trained language model to learn the intrinsic textual information of user behaviors and generates the sequential behaviors from the source domain based on sequential behaviors in the target domain. The retrieval augmented behaviors can achieve consistency and capture accurate hidden interest for target domain CTR prediction. Furthermore, we propose the cross-domain alignment unit to align the embeddings across domains by adding a semantic-guided contrastive loss and auxiliary task loss in the source domain. This allows the embeddings to be consistent across domains and have enough source information to capture the cross-domain relation. Finally, the cross-net utilizes two-level attention techniques to enhance the final prediction in the target domain. We conduct extensive experiments on both a public dataset and an industrial dataset from the WeChat advertising platform to demonstrate the effectiveness of RAL-CDNet in terms of offline and online metrics.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{zhong_screenaudit_2025,
  title = {{ScreenAudit},
  author = {Zhong, Mingyuan and Chen, Ruolin and Chen, Xia and Fogarty, James and Wobbrock, Jacob O.},
  year = {2025},
  doi = {10.1145/3706598.3713797},
  url = {https://doi.org/10.1145/3706598.3713797},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {accessibility audit., large language models, Mobile accessibility},
  abstract = {Many mobile apps are inaccessible, thereby excluding people from their potential benefits. Existing rule-based accessibility checkers aim to mitigate these failures by identifying errors early during development but are constrained in the types of errors they can detect. We present ScreenAudit, an LLM-powered system designed to traverse mobile app screens, extract metadata and transcripts, and identify screen reader accessibility errors overlooked by existing checkers. We recruited six accessibility experts including one screen reader user to evaluate ScreenAudit’s reports across 14 unique app screens. Our findings indicate that ScreenAudit achieves an average coverage of 69.2\%, compared to only 31.3\% with a widely-used accessibility checker. Expert feedback indicated that ScreenAudit delivered higher-quality feedback and addressed more aspects of screen reader accessibility compared to existing checkers, and that ScreenAudit would benefit app developers in real-world settings.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{nandi_enhancing_2025,
  title = {Enhancing {Customer},
  author = {Nandi, Subhadip and Agrawal, Neeraj and Singh, Anshika and Bhatt, Priyanka},
  year = {2025},
  doi = {10.1145/3703323.3703723},
  url = {https://doi.org/10.1145/3703323.3703723},
  booktitle = {Proceedings of the 8th {International},
  pages = {220--228},
  publisher = {Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Customer service chatbots are conversational systems aimed at addressing customer queries, often by directing them to automated workflows. A crucial aspect of this process is the classification of the customer’s intent. Presently, most intent classification models for customer care utilise only customer query for intent prediction. This may result in low-accuracy models, which cannot handle ambiguous queries. An ambiguous query like “I didn’t receive my package” could indicate a delayed order, or an order that was delivered but the customer failed to receive it. Resolution of each of these scenarios requires the execution of very different sequence of steps. Utilizing additional information, such as the customer’s order delivery status, in the right manner can help identify the intent for such ambiguous queries. In this paper, we have introduced a context-aware NLU model that incorporates both, the customer query and contextual information from the customer’s order status for predicting customer intent. A novel selective attention module is used to extract relevant context features. We have also proposed a multi-task learning paradigm for the effective utilization of different label types available in our training data. Our suggested method, Multi-Task Learning Contextual NLU with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8\% increase in top 2 accuracy score over the baseline model which only uses user queries, and a 3.5\% improvement over existing state-of-the-art models that combine query and context. We have deployed our model to production for Walmart’s customer care domain. Accurate intent prediction through MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby significantly reducing escalations to human agents, leading to almost a million dollars in yearly savings for the company.},
  address = {New York, NY, USA},
  series = {{CODS},
  isbn = {979-8-4007-1124-4},
}

@inproceedings{platis_lion_2024,
  title = {The {Lion},
  author = {Platis, Dimitrios and Erlenhov, Linda and Neto, Francisco Gomes de Oliveira},
  year = {2024},
  doi = {10.1145/3663529.3663782},
  url = {https://doi.org/10.1145/3663529.3663782},
  booktitle = {Companion {Proceedings},
  pages = {482--486},
  publisher = {Association for Computing Machinery},
  note = {event-place: Porto de Galinhas, Brazil},
  keywords = {Bots in Software Engineering, DevBots, Empirical Software Engineering, Software Development Bots, source: ACM},
  abstract = {The vast majority of state-of-the-art and practice have, so far, focused on understanding and developing individual bots that support software development (DevBots), while the interactions and collaborations between those DevBots introduce intriguing challenges and synergies that can both disrupt and enhance development cycles. In this vision paper we propose a taxonomy for DevBot roles in an ecosystem, based on how they interact. Much like biology, DevBots ecosystems rely on a balance between the creation, usage and maintenance of DevBots, particularly, on how they depend on one another. Further we contribute with reflections on how these interactions affect multi-bot projects.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0658-5},
  series = {{FSE},
}

@inproceedings{crossley_exploratory_2025,
  title = {Exploratory {Assessment},
  author = {Crossley, Scott and Morris, Wesley and Choi, Joon Suh and Holmes, Langdon},
  year = {2025},
  doi = {10.1145/3698205.3729548},
  url = {https://doi.org/10.1145/3698205.3729548},
  booktitle = {Proceedings of the {Twelfth},
  pages = {2--12},
  publisher = {Association for Computing Machinery},
  note = {event-place: Palermo, Italy},
  keywords = {intelligent texts, natural language processing, reading assessment, source: ACM},
  abstract = {This study explores users' learning gains and experiences from reading within three versions of the same text. The versions included 1) a traditional digital text, 2) a productive text that required participants to produce knowledge about what they read, but without any feedback on the knowledge produced, and 3) an interactive text that required participants to produce knowledge and provided AI feedback and interaction. Learning gains were assessed in a randomized control trial where crowd-sourced users were assigned to one of the three versions and were examined based on the quality of constructed responses and summaries provided as well as through differences from a pre-test and a post-test. User experiences were investigated using survey results. Results indicated that users were generally satisfied with interacting with all versions the text, except the summary portion of the interactive text. Conversely, results indicated that users of the interactive text consistently wrote better summaries than in the productive condition, and they revised summaries to a greater degree and to a greater success. Lastly, results showed that knowledge gains occurred in all reading conditions and that readers in the interactive condition who spent more time reading the text showed stronger test scores overall.},
  address = {New York, NY, USA},
  series = {L@{S},
  isbn = {979-8-4007-1291-3},
}

@inproceedings{meng_bridging_2025,
  title = {Bridging the {Gap},
  author = {Meng, Chuan and Tonolini, Francesco and Mo, Fengran and Aletras, Nikolaos and Yilmaz, Emine and Kazai, Gabriella},
  year = {2025},
  doi = {10.1145/3726302.3729915},
  url = {https://doi.org/10.1145/3726302.3729915},
  booktitle = {Proceedings of the 48th {International},
  pages = {64--74},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {conversational search, proactive search, query prediction, source: ACM},
  abstract = {Proactive search in conversations (PSC) aims to reduce user effort in formulating explicit queries by proactively retrieving useful relevant information given conversational context. Previous work in PSC either directly uses this context as input to off-the-shelf ad-hoc retrievers or further fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on short and concise queries, while the PSC input is longer and noisier. This input mismatch between ad-hoc search and PSC limits retrieval quality. While fine-tuning on PSC data helps, its benefits remain constrained by this input gap. In this work, we propose Conv2Query, a novel conversation-to-query framework that adapts ad-hoc retrievers to PSC by bridging the input gap between ad-hoc search and PSC. Conv2Query maps conversational context into ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data. Extensive experiments on two PSC datasets show that Conv2Query significantly improves ad-hoc retrievers' performance, both when used directly and after fine-tuning on PSC.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{grimm_authoring_2024,
  title = {Authoring {Educational},
  author = {Grimm, Valentin and Rubart, Jessica},
  year = {2024},
  doi = {10.1145/3648188.3675124},
  url = {https://doi.org/10.1145/3648188.3675124},
  booktitle = {Proceedings of the 35th {ACM},
  pages = {88--97},
  publisher = {Association for Computing Machinery},
  note = {event-place: Poznan, Poland},
  keywords = {Authoring, GPT, Hypercomics, Large Language Models, Storytelling},
  abstract = {Interactive stories can be an effective approach for teaching purposes. One shortcoming is the effort necessary to author and create these stories, especially complex storylines with choices for the readers. Based on recent advances in Natural Language Processing (NLP), new opportunities arise for assistance systems in the context of interactive stories. In our work, we present an authoring approach and prototypical tool for the creation of visual comic-strip like interactive stories, a type of hypercomics, that integrate an Artificial Intelligence (AI) assistance. Such comics are already used in our Gekonnt hanDeln web platform. The AI assistance provides suggestions for the overall story outline as well as how to design and write individual story frames. We provide a detailed description about the approach and its prototypical implementation. Furthermore, we present a study evaluating the prototype with student groups and how the prototype evolved in an iterative style based on the students’ feedback.},
  address = {New York, NY, USA},
  series = {{HT},
  isbn = {979-8-4007-0595-3},
}

@inproceedings{mittal_wavepulse_2025,
  title = {{WavePulse},
  author = {Mittal, Govind and Gupta, Sarthak and Wagle, Shruti and Chopra, Chirag and DeMattee, Anthony J. and Memon, Nasir and Ahamad, Mustaque and Hegde, Chinmay},
  year = {2025},
  doi = {10.1145/3696410.3714810},
  url = {https://doi.org/10.1145/3696410.3714810},
  booktitle = {Proceedings of the {ACM},
  pages = {3731--3750},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {large language models, radio livestreams, web content analytics, source: ACM},
  abstract = {Radio remains a pervasive medium for mass information dissemination, with AM/FM stations reaching more Americans than either smartphone-based social networking or live television. Increasingly, radio broadcasts are also streamed online and accessed over the Internet. We present WavePulse, a framework that records, documents, and analyzes radio content in real-time. While our framework is generally applicable, we showcase the efficacy of WavePulse in a collaborative project with a team of political scientists focusing on the 2024 Presidential Election. We use WavePulse to monitor livestreams of 396 news radio stations over a period of three months, processing close to 500,000 hours of audio streams. These streams were converted into time-stamped, diarized transcripts and analyzed to answer key political science questions at both the national and state levels. Our analysis revealed how local issues interacted with national trends, providing insights into information flow. Our results demonstrate WavePulse's efficacy in capturing and analyzing content from radio livestreams sourced from the Web. Code and dataset can be accessed at https://wave-pulse.io},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{fischer_evaluation_2025,
  title = {Evaluation of a {Node},
  author = {Fischer, David Vincent and Haug, Jim and Schoppel, Paul and Abke, Jörg and Becker, Matthias and Hagel, Georg},
  year = {2025},
  doi = {10.1145/3723010.3723021},
  url = {https://doi.org/10.1145/3723010.3723021},
  booktitle = {Proceedings of the 6th {European},
  pages = {20--29},
  publisher = {Association for Computing Machinery},
  keywords = {AI in Education, ASAG, Automatic Short Answer Grading, Large Language Models, Natural Language Processing, Short Answer Scoring, Software Engineering Education},
  abstract = {NodeGrade tries to provide a suitable solution for the problem of time-intensive short answer grading. This research focuses simultaneously on performance, functionality and user experience, which is underlined by a triangulated approach. The evaluation results show comparable performance of NodeGrade on public datasets, even outperforming GPT-4 on the SemEval 2013 Task 7. Matching of NodeGrade’s output with multiple human expert raters reveals some weaknesses regarding cases at the lower and upper boundary. In terms of user experience, the interviewed and observed students recognized both positive facets, like better learning support and helpful feedback, and negative sides, including technical limitations and lack of transparency. Overall, NodeGrade promises high potential for further practical use and testing in the field of software engineering education and automatic short answer grading.},
  address = {New York, NY, USA},
  series = {{ECSEE},
  isbn = {979-8-4007-1282-1},
}

@inproceedings{kambhamettu_explainable_2024,
  title = {Explainable {Notes},
  author = {Kambhamettu, Hita and Metaxa, Danaë and Johnson, Kevin and Head, Andrew},
  year = {2024},
  doi = {10.1145/3613904.3642573},
  url = {https://doi.org/10.1145/3613904.3642573},
  booktitle = {Proceedings of the 2024 {CHI},
  publisher = {Association for Computing Machinery},
  note = {event-place: Honolulu, HI, USA},
  keywords = {attention, augmented medical texts, intelligent reading and writing, lines of reasoning, patient-provider communication, phrase-level understanding, progress notes},
  abstract = {Medical progress notes have recently become available to patients at an unprecedented scale. Progress notes offer patients insight into their care that they cannot find elsewhere. That said, reading a note requires patients to contend with the language, unspoken assumptions, and clutter common to clinical documentation. As the health system reinvents many of its interfaces to incorporate AI assistance, this paper examines what intelligent interfaces could do to help patients read their progress notes. In a qualitative study, we examine the needs of patients as they read a progress note. We then formulate a vision for the explainable note, an augmented progress note that provides support for directing attention, phrase-level understanding, and tracing lines of reasoning. This vision manifests in a set of patient-inspired opportunities for advancing intelligent interfaces for writing and reading progress notes.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-0330-0},
}

@inproceedings{zhang_cse-sfp_2025,
  title = {{CSE},
  author = {Zhang, Bowen and Song, Zixin and Li, Chunping},
  year = {2025},
  doi = {10.1145/3726302.3729938},
  url = {https://doi.org/10.1145/3726302.3729938},
  booktitle = {Proceedings of the 48th {International},
  pages = {1402--1412},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {contrastive learning, large language models, sentence representation, text embedding, text retrieval, unsupervised learning},
  abstract = {As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models. Our code and checkpoints are available at https://github.com/ZBWpro/CSE-SFP.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{dow_ar-based_2025,
  title = {{AR},
  author = {Dow, Travis and Pratishtha, Pratishtha and Alabood, Lorans and Jaswal, Vikram K. and Krishnamurthy, Diwakar},
  year = {2025},
  doi = {10.1145/3715336.3735807},
  url = {https://doi.org/10.1145/3715336.3735807},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {423--440},
  publisher = {Association for Computing Machinery},
  keywords = {Accessibility, Assistive Technology, Augmented Reality, Mixed Reality, Nonspeaking Autistic People},
  abstract = {Many nonspeaking autistic individuals rely on Communication and Regulation Partners (CRPs) to develop spelling-based communication using physical letterboards, but this support is often geographically inaccessible. We developed a remote presence system using Augmented Reality (AR) to enable immersive, collaborative spelling instruction. The system features holographic letterboards and fully embodied avatars with real-time head and hand tracking, allowing remote interaction between students and CRPs. In a study with 18 nonspeaking autistic participants, 15 (83\%) successfully completed avatar-supported sessions. Interaction was higher, and participants reported a preference for the avatar condition over voice-only support. These findings demonstrate the feasibility of avatar-based AR telepresence for remote communication training. The system provides a demonstration of AR-supported interaction designed with nonspeaking autistic individuals—an underrepresented group in HCI—and offers design insights for inclusive telepresence technologies that address geographic and accessibility barriers.},
  address = {New York, NY, USA},
  series = {{DIS},
  isbn = {979-8-4007-1485-6},
}

@inproceedings{wilhelm_how_2025,
  title = {How {Managers},
  author = {Wilhelm, Lance T and Ding, Xiaohan and Knutsen, Kirk McInnis and Carik, Buse and Rho, Eugenia H},
  year = {2025},
  doi = {10.1145/3719160.3736639},
  url = {https://doi.org/10.1145/3719160.3736639},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {adaptive systems, communication training, computational social science, conversational system, human-AI interaction, large language models, leadership, management, role-play, system design, workplace communication},
  abstract = {Effective workplace communication is essential for managerial success, yet many managers lack access to tailored and sustained training. Although AI-assisted communication systems may offer scalable training solutions, little is known about how managers envision the role of AI in helping them improve their communication skills. To investigate this, we designed a conversational role-play system, CommCoach, as a functional probe to understand how managers anticipate using AI to practice their communication skills. Through semi-structured interviews, participants emphasized the value of adaptive, low-risk simulations for practicing difficult workplace conversations. They also highlighted opportunities, including human-AI teaming, transparent and context-aware feedback, and greater control over AI-generated personas. AI-assisted communication training should balance personalization, structured learning objectives, and adaptability to different user styles and contexts. However, achieving this requires carefully navigating tensions between adaptive and consistent AI feedback, realism and potential bias, and the open-ended nature of AI conversations versus structured workplace discourse.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{lee_sensible_2025,
  title = {Sensible {Agent},
  author = {Lee, Geonsun and Xia, Min and Numan, Nels and Qian, Xun and Li, David and Chen, Yanhe and Kulshrestha, Achin and Chatterjee, Ishan and Zhang, Yinda and Manocha, Dinesh and Kim, David and Du, Ruofei},
  year = {2025},
  doi = {10.1145/3746059.3747748},
  url = {https://doi.org/10.1145/3746059.3747748},
  booktitle = {Proceedings of the 38th {Annual},
  publisher = {Association for Computing Machinery},
  keywords = {Adaptive Interfaces, Augmented Reality, Context-Awareness, Human-Agent Interaction, Large Multimodal Models, Multimodal Interaction, Proactive Agents, Unobtrusive Interaction},
  abstract = {Proactive AR agents promise context-aware assistance, but their interactions often rely on explicit voice prompts or responses, which can be disruptive or socially awkward. We introduce Sensible Agent, a framework designed for unobtrusive interaction with these proactive agents. Sensible Agent dynamically adapts both “what” assistance to offer and, crucially, “how” to deliver it, based on real-time multimodal context sensing. Informed by an expert workshop (n=12) and a data annotation study (n=40), the framework leverages egocentric cameras, multimodal sensing, and Large Multimodal Models (LMMs) to infer context and suggest appropriate actions delivered via minimally intrusive interaction modes. We demonstrate our prototype on an XR headset through a user study (n=10) in both AR and VR scenarios. Results indicate that Sensible Agent significantly reduces perceived interaction effort compared to voice-prompted baseline, while maintaining high usability and achieving higher preference.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-2037-6},
}

@inproceedings{wang_collaboration_2025,
  title = {Collaboration and {Controversy},
  author = {Wang, Bing and Zhao, Bingrui and Li, Ximing and Li, Changchun and Gao, Wanfu and Wang, Shengsheng},
  year = {2025},
  doi = {10.1145/3726302.3729928},
  url = {https://doi.org/10.1145/3726302.3729928},
  booktitle = {Proceedings of the 48th {International},
  pages = {468--478},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {adversarial training, language model, mixture-of-expert, rumor detection, social media, text generation},
  abstract = {Over the past decade, social media platforms have been key in spreading rumors, leading to significant negative impacts. To counter this, the community has developed various Rumor Detection (RD) algorithms to automatically identify them using user comments as evidence. However, these RD methods often fail in the early stages of rumor propagation when only limited user comments are available, leading the community to focus on a more challenging topic named Rumor Early Detection (RED). Typically, existing RED methods learn from limited semantics in early comments. However, our preliminary experiment reveals that the RED models always perform best when the number of training and test comments is consistent and extensive. This inspires us to address the RED issue by generating more human-like comments to support this hypothesis. To implement this idea, we tune a comment generator by simulating expert collaboration and controversy and propose a new RED framework named CAMERED. Specifically, we integrate a mixture-of-expert structure into a generative language model and present a novel routing network for expert collaboration. Additionally, we synthesize a knowledgeable dataset and design an adversarial learning strategy to align the style of generated comments with real-world comments. We further integrate generated and original comments with a mutual controversy fusion module. Experimental results show that CAMERED outperforms state-of-the-art RED baseline models and generation methods, demonstrating its effectiveness.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{venkatraman_you_2024,
  title = {You {Shall},
  author = {Venkatraman, Nithiya and Aiyer, Anand and Prakash, Yash and Ashok, Vikas},
  year = {2024},
  doi = {10.1145/3648188.3675151},
  url = {https://doi.org/10.1145/3648188.3675151},
  booktitle = {Proceedings of the 35th {ACM},
  pages = {230--238},
  publisher = {Association for Computing Machinery},
  note = {event-place: Poznan, Poland},
  keywords = {accessibility online forums, blind, discussion forums, language usage, linguistic analysis, screen readers, Social media},
  abstract = {Discussion forums are one of the favored platforms for knowledge sharing. Given their popularity, copious research exists on understanding linguistic and behavioral characteristics of forum conversations. However, prior investigations have mainly focused on general forums designed primarily for sighted users, and as such the applicability of their findings to the dedicated accessibility discussion forums frequented by blind individuals remains unanswered. To bridge this knowledge gap, and facilitate the development of better-informed assistive technologies for blind people, we investigated language use and identified the key semantic and cognitive characteristics of online accessibility forums. To aid our investigation, we collected a dataset of 1000 accessibility forum threads and a baseline of 1000 general forum threads. These threads were carefully curated to ensure similarity of topics discussed. We found the language in accessibility forum conversations to be more task-oriented and less abstract, with significantly higher number of descriptive action words than in general forum conversations. Results also showed an emphasis on sharing first-hand personal experiences in accessibility forums, relative to the general forums.},
  address = {New York, NY, USA},
  series = {{HT},
  isbn = {979-8-4007-0595-3},
}

@inproceedings{qiu_simplifying_2023,
  title = {Simplifying {Cloud},
  author = {Qiu, Yiming and Kon, Patrick Tser Jern and Xing, Jiarong and Huang, Yibo and Liu, Hongyi and Wang, Xinyu and Huang, Peng and Chowdhury, Mosharaf and Chen, Ang},
  year = {2023},
  doi = {10.1145/3626111.3628206},
  url = {https://doi.org/10.1145/3626111.3628206},
  booktitle = {Proceedings of the 22nd {ACM},
  pages = {95--101},
  publisher = {Association for Computing Machinery},
  note = {event-place: Cambridge, MA, USA},
  keywords = {cloud management, Infrastructure as code},
  abstract = {Cloud computing has transformed the IT industry, but managing cloud infrastructures remains a difficult task. We make a case for putting today's management practices, known as "Infrastructure-as-Code," on a firmer ground via a principled design. We call this end goal Cloudless Computing: it aims to simplify cloud infrastructure management tasks by supporting them "as-a-service," analogous to serverless computing that relieves users of the burden of managing server instances. By assisting tenants with these tasks, cloud resources will be presented to their users more readily without the undue burden of complex control. We describe the research problems by examining the typical lifecycle of today's cloud infrastructure management, and identify places where a cloudless approach will advance the state of the art.},
  address = {New York, NY, USA},
  series = {{HotNets},
  isbn = {979-8-4007-0415-4},
}

@inproceedings{bai_kgquiz_2024,
  title = {{KGQuiz},
  author = {Bai, Yuyang and Feng, Shangbin and Balachandran, Vidhisha and Tan, Zhaoxuan and Lou, Shiqi and He, Tianxing and Tsvetkov, Yulia},
  year = {2024},
  doi = {10.1145/3589334.3645623},
  url = {https://doi.org/10.1145/3589334.3645623},
  booktitle = {Proceedings of the {ACM},
  pages = {2226--2237},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {knowledge probing, large language models, source: ACM},
  abstract = {Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive tasks and knowledge domains. Extensive experiments demonstrate that LLMs achieve impressive performance in straightforward knowledge QA tasks, while settings and contexts requiring more complex reasoning or employing domain-specific facts still present significant challenges. We envision KGQuiz as a testbed to analyze such nuanced variations in performance across domains and task formats, and ultimately to understand, evaluate, and improve LLMs' knowledge abilities across a wide spectrum of knowledge domains and tasks.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{zou_pscon_2025,
  title = {{PSCon},
  author = {Zou, Jie and Aliannejadi, Mohammad and Kanoulas, Evangelos and Han, Shuxi and Ma, Heli and Wang, Zheng and Yang, Yang and Shen, Heng Tao},
  year = {2025},
  doi = {10.1145/3726302.3730278},
  url = {https://doi.org/10.1145/3726302.3730278},
  booktitle = {Proceedings of the 48th {International},
  pages = {3659--3669},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {conversational product search, dataset, product search},
  abstract = {Conversational Product Search ( CPS ) systems interact with users via natural language to offer personalized and context-aware product lists. However, most existing research on CPS is limited to simulated conversations, due to the lack of a real CPS dataset driven by human-like language. Moreover, existing conversational datasets for e-commerce are constructed for a particular market or a particular language and thus can not support cross-market and multi-lingual usage. In this paper, we propose a CPS data collection protocol and create a new CPS dataset, called PSCon, which assists product search through conversations with human-like language. The dataset is collected by a coached human-human data collection protocol and is available for dual markets and two languages. By formulating the task of CPS, the dataset allows for comprehensive and in-depth research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Moreover, we present a concise analysis of the dataset and propose a benchmark model on the proposed CPS dataset. Our proposed dataset and model will be helpful for facilitating future research on CPS.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{yao_rtlrewriter_2025,
  title = {{RTLRewriter},
  author = {Yao, Xufeng and Wang, Yiwen and Li, Xing and Lian, Yingzhao and Chen, Ran and Chen, Lei and Yuan, Mingxuan and Xu, Hong and Yu, Bei},
  year = {2025},
  doi = {10.1145/3676536.3676775},
  url = {https://doi.org/10.1145/3676536.3676775},
  booktitle = {Proceedings of the 43rd {IEEE},
  publisher = {Association for Computing Machinery},
  note = {event-place: Newark Liberty International Airport Marriott, New York, NY, USA},
  keywords = {source: ACM},
  abstract = {Register Transfer Level (RTL) code optimization is crucial for enhancing the efficiency and performance of digital circuits during early synthesis stages. Currently, optimization relies heavily on manual efforts by skilled engineers, often requiring multiple iterations based on synthesis feedback. In contrast, existing compiler-based methods fall short in addressing complex designs. This paper introduces RTLRewriter, an innovative framework that leverages large models to optimize RTL code. A circuit partition pipeline is utilized for fast synthesis and efficient rewriting. A multi-modal program analysis is proposed to incorporate vital visual diagram information as optimization cues. A specialized search engine is designed to identify useful optimization guides, algorithms, and code snippets that enhance the model's ability to generate optimized RTL. Additionally, we introduce a Cost-aware Monte Carlo Tree Search (C-MCTS) algorithm for efficient rewriting, managing diverse retrieved contents and steering the rewriting results. Furthermore, a fast verification pipeline is proposed to reduce verification cost. To cater to the needs of both industry and academia, we propose two benchmarking suites: the long Rewriter benchmark, targeting complex scenarios with extensive circuit partitioning, optimization trade-offs, and verification challenges, and the short Rewriter benchmark, designed for a wider range of scenarios and patterns. Our comparative analysis with established compilers such as Yosys and E-graph demonstrates significant improvements, highlighting the benefits of integrating large models into the early stages of circuit design. We provide our benchmarks at https://github.com/yaoxufeng/RTLRewriter-Bench.},
  address = {New York, NY, USA},
  series = {{ICCAD},
  isbn = {979-8-4007-1077-3},
}

@inproceedings{sun_harnessing_2024,
  title = {Harnessing {Multi},
  author = {Sun, Hongda and Liu, Yuxuan and Wu, Chengwei and Yan, Haiyu and Tai, Cheng and Gao, Xin and Shang, Shuo and Yan, Rui},
  year = {2024},
  doi = {10.1145/3589334.3645670},
  url = {https://doi.org/10.1145/3589334.3645670},
  booktitle = {Proceedings of the {ACM},
  pages = {4372--4382},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {prompt optimization, question answering, role-playing llms},
  abstract = {Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) Theretrieve-then-read paradigm retrieves pertinent documents from an external corpus; and (2) thegenerate-then-read paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. Furthermore, we introduce a novel prompt optimization algorithm to refine role-playing prompts and steer LLMs to produce higher-quality evidence and answers. Extensive experimental results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that LLMQA achieves the best performance in terms of both answer accuracy and evidence quality, showcasing its potential for advancing ODQA research and applications.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{uchkempirov_survey_2025,
  title = {Survey of {Graph},
  author = {Uchkempirov, Murataly and Tajeddini, Omid and Maliha Proma, Mayesha and Kulkarni, Parag},
  year = {2025},
  doi = {10.1145/3711542.3711572},
  url = {https://doi.org/10.1145/3711542.3711572},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {380--387},
  publisher = {Association for Computing Machinery},
  keywords = {Automatic text summarization, Graph theory, Graph-based summarization, Human resources management},
  abstract = {This study provides a comprehensive overview of graph-based algorithms and techniques for automatic text summarization, with a particular emphasis on their application in human resource management activities such as CV summarization, motivational letter summarization, and cover letter summarization. Furthermore, the paper discusses the advantages and limitations of graph-based algorithms and techniques in natural language processing based on 22 highly visible and open access articles, while also suggesting potential avenues for future research in this domain. The review highlights the importance of graph theory for automatic text summarization where it can significantly improve the quality and precision of derived information. This survey offers critical insights for researchers and practitioners in the area who seek innovative human resources text analytics approaches and techniques that helps to accelerate decision-making processes by providing informative and concise summaries of large documents.},
  address = {New York, NY, USA},
  series = {{NLPIR},
  isbn = {979-8-4007-1738-3},
}

@inproceedings{yan_when_2025,
  title = {When {Graph},
  author = {Yan, Hao and Li, Chaozhuo and Yin, Jun and Yu, Zhigang and Han, Weihao and Li, Mingzheng and Zeng, Zhengxin and Sun, Hao and Wang, Senzhang},
  year = {2025},
  doi = {10.1145/3711896.3737404},
  url = {https://doi.org/10.1145/3711896.3737404},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5842--5853},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {graph data mining, graph neural networks, multimodal attributed graphs, multimodal large language models},
  abstract = {Multimodal Attributed Graphs (MAGs) are ubiquitous in real-world applications, encompassing extensive knowledge through multimodal attributes attached to nodes (e.g., texts and images) and topological structure representing node interactions. Despite its potential to advance diverse research fields like social networks and e-commerce, MAG representation learning (MAGRL) remains underexplored due to the lack of standardized datasets and evaluation frameworks. In this paper, we first propose MAGB, a comprehensive MAG benchmark dataset, featuring curated graphs from various domains with both textual and visual attributes. Based on the MAGB dataset, we further systematically evaluate two mainstream MAGRL paradigms: GNN-as-Predictor, which integrates multimodal attributes via Graph Neural Networks (GNNs), and VLM-as-Predictor, which harnesses Vision Language Models (VLMs) for zero-shot reasoning. Extensive experiments on MAGB reveal the following critical insights: (i) Modality significances fluctuate drastically with specific domain characteristics. (ii) Multimodal embeddings can elevate the performance ceiling of GNNs. However, intrinsic biases among modalities may impede effective training, particularly in low-data scenarios. (iii) VLMs are highly effective at generating multimodal embeddings that alleviate the imbalance between textual and visual attributes. These discoveries, which illuminate the synergy between multimodal attributes and graph topologies, contribute to reliable benchmarks, paving the way for future research.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{foukas_future_2025,
  title = {The future of the industrial {AI},
  author = {Foukas, Xenofon and Radunovic, Bozidar},
  year = {2025},
  doi = {10.1145/3708468.3711887},
  url = {https://doi.org/10.1145/3708468.3711887},
  booktitle = {Proceedings of the 26th {International},
  pages = {61--66},
  publisher = {Association for Computing Machinery},
  note = {event-place: La Quinta, CA, USA},
  keywords = {source: ACM},
  abstract = {Ensuring reliable and high-bandwidth wireless connectivity and local processing at the edge are crucial enablers for emerging industrial AI applications. In this work, we argue that the recent trends in cellular networking make the technology the ideal connectivity solution for these applications, due to its virtualization and support for open APIs. We foresee the emergence of a converged industrial AI edge encompassing both compute and connectivity, in which application developers leverage the API to implement advanced functionalities. We demonstrate the usefulness of this approach through a case study evaluated on an enterprise-grade 5G testbed deployed in our lab.},
  address = {New York, NY, USA},
  series = {{HotMobile},
  isbn = {979-8-4007-1403-0},
}

@inproceedings{cui_nlctables_2025,
  title = {{nlcTables},
  author = {Cui, Lingxi and Li, Huan and Chen, Ke and Shou, Lidan and Chen, Gang},
  year = {2025},
  doi = {10.1145/3726302.3730296},
  url = {https://doi.org/10.1145/3726302.3730296},
  booktitle = {Proceedings of the 48th {International},
  pages = {3638--3647},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {joinable tables, semantic search, table search, unionable tables},
  abstract = {With the growing abundance of repositories containing tabular data, discovering relevant tables for in-depth analysis remains a challenging task. Existing table discovery methods primarily retrieve desired tables based on a query table or several vague keywords, leaving users to manually filter large result sets. To address this limitation, we propose a new task: NL-conditional table discovery (nlcTD), where users combine a query table with natural language (NL) requirements to refine search results. To advance research in this area, we present nlcTables, a comprehensive benchmark dataset comprising 627 diverse queries spanning NL-only, union, join, and fuzzy conditions, 22,080 candidate tables, and 21,200 relevance annotations. Our evaluation of six state-of-the-art table discovery methods on nlcTables reveals substantial performance gaps, highlighting the need for advanced techniques to tackle this challenging nlcTD scenario. The dataset, construction framework, and baseline implementations are publicly available at https://github.com/SuDIS-ZJU/nlcTables to foster future research.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{yin_panns_2025,
  title = {{PANNS},
  author = {Yin, Xizhe and Gao, Chao and Zhao, Zhijia and Gupta, Rajiv},
  year = {2025},
  doi = {10.1145/3710848.3710867},
  url = {https://doi.org/10.1145/3710848.3710867},
  booktitle = {Proceedings of the 30th {ACM},
  pages = {369--381},
  publisher = {Association for Computing Machinery},
  note = {event-place: Las Vegas, NV, USA},
  keywords = {graph processing, nearest neighbor search, vector search},
  abstract = {Approximate Nearest-Neighbor Search (ANNS) has become the standard querying method in vector databases, especially with the recent surge in large-scale, high-dimensional data driven by LLM-based applications. Recently, graph-based ANNS has shown improved throughput by constructing a graph from the dataset, with edges representing the distances between data points, and using best-first or beam search algorithms for query evaluation.This work highlights that key aspects of the design space for both graph construction and search in graph-based ANNS remain under-explored. Specifically, the construction phase often neglects the potential temporal correlation between input queries and their results, while the search phase lacks a thorough exploration of beam search parameterization. To address these gaps, we present PANNS, a system that embeds temporal information in the proximity graph construction to capture query-result correlations. Moreover, it introduces a fully parameterized beam search algorithm, enabling extensive performance optimization. PANNS achieves up to a 3.7× improvement in query throughput over state-of-the-art graph-based ANNS methods, while maintaining equivalent recall levels. Furthermore, it reduces graph size by up to 30\% without degrading query quality.},
  address = {New York, NY, USA},
  series = {{PPoPP},
  isbn = {979-8-4007-1443-6},
}

@inproceedings{vu_towards_2025,
  title = {Towards {Generating},
  author = {Vu, Anh Duc and Kehrer, Timo},
  year = {2025},
  doi = {10.1109/SCW63240.2024.00256},
  url = {https://doi.org/10.1109/scw63240.2024.00256},
  booktitle = {Proceedings of the {SC},
  pages = {2048--2055},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {To increase the dependability and portability of scientific data analysis workflows (DAWs), recent work has proposed contract-driven design of DAWs, providing verifiable expectations and obligations to ensure that tasks run in a proper environment and produce correct results. However, the specification of suitable contracts is still left to the discretion of DAW developers, imposing labor-intensive manual work which likely hampers the widespread adoption of contracts in scientific practice. We report about work-in-progress of developing a pipeline empowered by Large Language Models for automatically generating code contracts from logical workflow descriptions. We instantiate this pipeline within the workflow system Nextflow, and evaluate its contract generation capabilities in an experiment using real-world Nextflow modules. Our findings indicate that we generate a substantial amount of contracts serving as starting point for DAW developers. Our approach demonstrates potential in assisting domain scientists with contract-driven design of DAWs, laying the groundwork for its future adoption.},
  address = {Atlanta, GA, USA},
  series = {{SC},
  isbn = {979-8-3503-5554-3},
}

@inproceedings{aung_demand_2025,
  title = {Demand {Forecasting},
  author = {Aung, Nway Nway and Mao, Yuejingxian and Wijaya, Chandra Suwandi and Beck, Aryel and Koji, Miura and Yosuke, Tajika},
  year = {2025},
  doi = {10.1145/3701100.3701158},
  url = {https://doi.org/10.1145/3701100.3701158},
  booktitle = {Proceedings of the 2024 3rd {International},
  pages = {270--280},
  publisher = {Association for Computing Machinery},
  keywords = {B2B Sector, Economic Indicators, High-Frequency Indicator, Textual Trends Insight, Time-Series Forecasting},
  abstract = {Demand forecasting in the B2B sector is challenging due to complex supply chains and variable influences such as economic conditions and market trends. Traditional forecasting techniques rely on stable but often delayed data sources like government and industry reports, which struggle to provide timely and accurate predictions during volatile market conditions. This study introduces an enhanced forecasting methodology integrating high-frequency indicators from social media platforms and the stock market. Our novel approach consists of two components: the High-Frequency Textual Event Disruption Analyzer (HFT-EDA), which leverages advanced natural language processing techniques to analyze online trends and news data for immediate market insights, and the market health analyzer (MHA), which statistically combines hard, soft, and high-frequency economic indicators for a comprehensive market assessment. In this study, our methodology significantly improved monthly forecasts, surpassing existing models by up to 8.5\% for specific products, including electronic devices used in home and industrial appliances. This capability is particularly crucial during crises such as the COVID-19 lockdown, where rapid market shifts significantly impact demand dynamics. Our dual approach provides precise and actionable demand forecasts, enabling businesses to adapt quickly to market changes and stay attuned to immediate market fluctuations in the dynamic B2B landscape.},
  address = {New York, NY, USA},
  series = {{ADMIT},
  isbn = {979-8-4007-1812-0},
}

@inproceedings{gao_preference-guided_2024,
  title = {Preference-{Guided},
  author = {Gao, Xinyu and Xiong, Yun and Wang, Deze and Guan, Zhenhan and Shi, Zejian and Wang, Haofen and Li, Shanshan},
  year = {2024},
  doi = {10.1145/3691620.3694987},
  url = {https://doi.org/10.1145/3691620.3694987},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {65--77},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {deep reinforcement learning, preference-guided refactorer, retrieval-augmented code generation},
  abstract = {Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28\% on EM, 13\% on BLEU, and 6.8\% on CodeBLEU.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{zhao_enhancing_2024,
  title = {Enhancing {Automated},
  author = {Zhao, Jiuang and Yang, Donghao and Zhang, Li and Lian, Xiaoli and Yang, Zitian and Liu, Fang},
  year = {2024},
  doi = {10.1145/3691620.3695537},
  url = {https://doi.org/10.1145/3691620.3695537},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {1706--1718},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {automated program repair, design rationale, developer discussion, issue logs, source: ACM},
  abstract = {Automatic Program Repair (APR) endeavors to autonomously rectify issues within specific projects, which generally encompasses three categories of tasks: bug resolution, new feature development, and feature enhancement. Despite extensive research proposing various methodologies, their efficacy in addressing real issues remains unsatisfactory. It's worth noting that, typically, engineers have design rationales (DR) on solution— planed solutions and a set of underlying reasons—before they start patching code. In open-source projects, these DRs are frequently captured in issue logs through project management tools like Jira. This raises a compelling question: How can we leverage DR scattered across the issue logs to efficiently enhance APR?To investigate this premise, we introduce DRCodePilot, an approach designed to augment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt instruction. Furthermore, given GPT-4's constraints in fully grasping the broader project context and occasional shortcomings in generating precise identifiers, we have devised a feedback-based self-reflective framework, in which we prompt GPT-4 to reconsider and refine its outputs by referencing a provided patch and suggested identifiers. We have established a benchmark comprising 938 issue-patch pairs sourced from two open-source repositories hosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot achieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is utilized directly. Additionally, the CodeBLEU scores also exhibit promising enhancements. Moreover, our findings reveal that the standalone application of DR can yield promising increase in the full-match ratio across CodeLlama, GPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot initiative heralds a novel human-in-the-loop avenue for advancing the field of APR.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{huang_dca-bench_2025,
  title = {{DCA},
  author = {Huang, Benhao and Yu, Yingzhuo and Huang, Jin and Zhang, Xingjian and Ma, Jiaqi W.},
  year = {2025},
  doi = {10.1145/3711896.3737422},
  url = {https://doi.org/10.1145/3711896.3737422},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {5482--5492},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {automatic evaluation, dataset curation, large language models},
  abstract = {The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as incomplete documentation, inaccurate labels, ethical concerns, and outdated information, remain common in widely used datasets. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, therefore requiring identification and verification by dataset users or maintainers-a process that is both time-consuming and prone to human mistakes. With the surging ability of large language models (LLM), it's promising to streamline the discovery of hidden dataset issues with LLM agents. To achieve this, one significant challenge is enabling LLM agents to detect issues in the wild rather than simply fixing known ones. In this work, we establish a benchmark to measure LLM agent's ability to tackle this challenge. We carefully curate 221 real-world test cases from eight popular dataset platforms and propose an automatic evaluation framework using GPT-4o. Our proposed framework shows strong empirical alignment with expert evaluations, validated through extensive comparisons with human annotations. Without any hints, most competitive Curator agent can only reveal 30\% of the data quality issues in the proposed dataset, highlighting the complexity of this task and indicating that applying LLM agents to real-world dataset curation still requires further in-depth exploration and innovation. The data and code is available at https://github.com/TRAIS-Lab/dca-bench.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{longwell_triple_2024,
  title = {Triple {Augmented},
  author = {Longwell, Jack and Ali Akbar Alavi, Mahdiyar and Zarrinkalam, Fattane and Ensan, Faezeh},
  year = {2024},
  doi = {10.1145/3673791.3698426},
  url = {https://doi.org/10.1145/3673791.3698426},
  booktitle = {Proceedings of the 2024 {Annual},
  pages = {269--273},
  publisher = {Association for Computing Machinery},
  note = {event-place: Tokyo, Japan},
  keywords = {generative language models, knowledge graph question answering, sparql, source: ACM},
  abstract = {Knowledge Graph Question Answering (KGQA) leverages structured Knowledge Graphs (KG) to respond to Natural Language Questions (NLQ). This paper explores integrating Generative Language Models (GLMs) augmented with knowledge graph triple retrievers into the KGQA framework to generate accurate SPARQL queries from NLQs. Specifically, we evaluate the effectiveness of integrating triple retriever models with the SPARQL-generating capabilities of GLMs by investigating: (1) the standalone capabilities of GLMs independent of retriever performance, (2) the impact of incorporating a base retriever (BM25), and (3) a comparative analysis with state-of-the-art KGQA methods. Our experiments demonstrate that by incorporating a triple retrieval module, GLMs can generate accurate SPARQL queries and outperform current end-to-end KGQA methods, particularly when paired with an optimal retriever.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0724-7},
  series = {{SIGIR},
}

@inproceedings{feng_citygpt_2025,
  title = {{CityGPT},
  author = {Feng, Jie and Liu, Tianhui and Du, Yuwei and Guo, Siqi and Lin, Yuming and Li, Yong},
  year = {2025},
  doi = {10.1145/3711896.3736878},
  url = {https://doi.org/10.1145/3711896.3736878},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {591--602},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {large language models, spatial cognition, urban system, source: ACM},
  abstract = {Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose CityGPT, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale 'world model' into the model. Firstly, we construct a diverse instruction tuning dataset, CityInstruction, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of CityInstruction and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (SWFT) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark CityEval for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with CityInstruction by SWFT method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using CityEval. Our work highlights the potential for integrating spatial knowledge into LLMs, thereby expanding their spatial cognition abilities and applicability to the real-world physical environments. The dataset, benchmark, and source code are open-sourced and can be accessed through https://github.com/tsinghua-fib-lab/CityGPT.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{sanca_efficient_2024,
  title = {Efficient {Data},
  author = {Sanca, Viktor and Ailamaki, Anastasia},
  year = {2024},
  doi = {10.1145/3662010.3663448},
  url = {https://doi.org/10.1145/3662010.3663448},
  booktitle = {Proceedings of the 20th {International},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {Access Methods, Index Structures, Query Optimization, Vector Databases, Vector-Relational Databases},
  abstract = {The rapid growth of machine learning capabilities and the adoption of data processing methods using vector embeddings sparked a great interest in creating systems for vector data management. While the predominant approach of vector data management is to use specialized index structures for fast search over the entirety of the vector embeddings, once combined with other (meta)data, the search queries can also become selective on relational attributes - typical for analytical queries. As using vector indexes differs from traditional relational data access, we revisit and analyze alternative access paths for efficient mixed vector-relational search.We first evaluate the accurate but exhaustive scan-based search and propose hardware optimizations and alternative dense vector-based formulation and batching to offset the cost. We outline the complex access-path design space, primarily driven by relational selectivity, and the decisions to consider when selecting an exhaustive scan-based search against an approximate index-based approach. Since the vector index primarily avoids expensive computation across the entire dataset, contrary to the common relational knowledge, it is better to scan at lower relational selectivity and probe at higher, with a cross-point between the two approaches dictated by data dimensionality, vector index parameters, and the number of concurrent search queries.},
  address = {New York, NY, USA},
  series = {{DaMoN},
  isbn = {979-8-4007-0667-7},
}

@inproceedings{xiao_c-pack_2024,
  title = {C-{Pack},
  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas and Lian, Defu and Nie, Jian-Yun},
  year = {2024},
  doi = {10.1145/3626772.3657878},
  url = {https://doi.org/10.1145/3626772.3657878},
  booktitle = {Proceedings of the 47th {International},
  pages = {641--649},
  publisher = {Association for Computing Machinery},
  note = {event-place: Washington DC, USA},
  keywords = {benchmark, pre-trained models, text embeddings, training data},
  abstract = {We introduce C-Pack, a package of resources that significantly advances the field of general text embeddings for Chinese. C-Pack includes three critical resources. 1) C-MTP is a massive training dataset for text embedding, which is based on the curation of vast unlabeled corpora and the integration of high-quality labeled corpora. 2) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 3) BGE is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by more than +10\% upon the time of the release. We also integrate and optimize the entire suite of training methods for BGE. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models also achieve state-of-the-art performance on the MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. Both Chinese and English datasets are the largest public release of training data for text embeddings. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-0431-4},
}

@inproceedings{khanal_fathomgpt_2024,
  title = {{FathomGPT},
  author = {Khanal, Nabin and Yu, Chun Meng and Chiu, Jui-Cheng and Chaudhary, Anav and Zhang, Ziyue and Katija, Kakani and Forbes, Angus G.},
  year = {2024},
  doi = {10.1145/3654777.3676462},
  url = {https://doi.org/10.1145/3654777.3676462},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Natural Language Interfaces, Ocean Science, Scientific Databases},
  abstract = {We introduce FathomGPT, an open source system for the interactive investigation of ocean science data via a natural language interface. FathomGPT was developed in close collaboration with marine scientists to enable researchers to explore and analyze the FathomNet image database. FathomGPT provides a custom information retrieval pipeline that leverages OpenAI’s large language models to enable: the creation of complex queries to retrieve images, taxonomic information, and scientific measurements; mapping common names and morphological features to scientific names; generating interactive charts on demand; and searching by image or specified patterns within an image. In designing FathomGPT, particular emphasis was placed on enhancing the user’s experience by facilitating free-form exploration and optimizing response times. We present an architectural overview and implementation details of FathomGPT, along with a series of ablation studies that demonstrate the effectiveness of our approach to name resolution, fine tuning, and prompt modification. We also present usage scenarios of interactive data exploration sessions and document feedback from ocean scientists and machine learning experts.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{shrestha_espn_2024,
  title = {{ESPN},
  author = {Shrestha, Susav and Reddy, Narasimha and Li, Zongwang},
  year = {2024},
  doi = {10.1145/3652024.3665515},
  url = {https://doi.org/10.1145/3652024.3665515},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {95--107},
  publisher = {Association for Computing Machinery},
  note = {event-place: Copenhagen, Denmark},
  keywords = {Approximate Nearest Neighbor Search, GPUDirect Storage, Memory \&amp, Multi-vector, Neural Information Retrieval, Software Prefetching, Storage System, source: ACM},
  abstract = {Recent advances in large language models have demonstrated remarkable effectiveness in information retrieval (IR) tasks. While many neural IR systems encode queries and documents into single-vector representations, multi-vector models elevate the retrieval quality by producing multi-vector representations and facilitating similarity searches at the granularity of individual tokens. However, these models significantly amplify memory requirements for retrieval indices by an order of magnitude. This escalation in index size renders the scalability of multi-vector IR models progressively challenging due to their substantial memory demands. We introduce Embedding from Storage Pipelined Network (ESPN) where we offload the entire re-ranking embedding tables to SSDs and reduce the memory requirements by 5−16×. We design a flexible software prefetcher applicable to any hierarchical clustering based search, achieving hit rates exceeding 90\%. ESPN improves SSD based retrieval up to 6.4× and end-to-end throughput by 68\% to maintain near-memory levels of query latency even for large query batch sizes. The code is available at https://github.com/susavlsh10/ESPN-v1.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-0615-8},
  series = {{ISMM},
}

@inproceedings{rashik_ai-enabled_2025,
  title = {{AI},
  author = {Rashik, Mashrur and Sweth, Shilpa and Agrawal, Nishtha and Kochar, Saiyyam and Smith, Kara M and Rajabiyazdi, Fateme and Setlur, Vidya and Mahyar, Narges and Sarvghad, Ali},
  year = {2025},
  doi = {10.1145/3706598.3714280},
  url = {https://doi.org/10.1145/3706598.3714280},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {context, Conversational implicature, Gricean maxims, healthcare., journaling, source: ACM},
  abstract = {Journaling plays a crucial role in managing chronic conditions by allowing patients to document symptoms and medication intake, providing essential data for long-term care. While valuable, traditional journaling methods often rely on static, self-directed entries, lacking interactive feedback and real-time guidance. This gap can result in incomplete or imprecise information, limiting its usefulness for effective treatment. To address this gap, we introduce Patrika, an AI-enabled prototype designed specifically for people with Parkinson’s disease (PwPD). The system incorporates cooperative conversation principles, clinical interview simulations, and personalization to create a more effective and user-friendly journaling experience. Through two user studies with PwPD and iterative refinement of Patrika, we demonstrate conversational journaling’s significant potential in patient engagement and collecting clinically valuable information. Our results showed that generating probing questions Patrika turned journaling into a bi-directional interaction. Additionally, we offer insights for designing journaling systems for healthcare and future directions for promoting sustained journaling.},
  address = {New York, NY, USA},
  isbn = {979-8-4007-1394-1},
  series = {{CHI},
}

@inproceedings{gomes_nudge_2024,
  title = {Nudge {Evidence},
  author = {Gomes, Vinicius and Cunha, José Adson and Lima, Kássio and Moura, Hermano Perrelli},
  year = {2024},
  doi = {10.1145/3702038.3702073},
  url = {https://doi.org/10.1145/3702038.3702073},
  booktitle = {Proceedings of the {XXIII},
  publisher = {Association for Computing Machinery},
  keywords = {Evidence Briefing, Knowledge Transfer, Nudge, Privacy, Security, source: ACM},
  abstract = {A nudge is a concept from Behavioral Economics and Psychology that refers to any small change or intervention designed to influence people's behavior predictably, without restricting their options or significantly altering their incentives. The research follows the Design Science Research methodology, introducing Nudge Evidence Briefing (NEB) to facilitate the understanding, access, application of academic findings on nudges for non-academic professionals, considering the gap between academic research on nudges and their practical application. Leveraging insights from the Evidence-Based Medicine framework, NEBs distill key findings from primary research into concise, accessible documents. Through a systematic review of the literature on nudge integration into software privacy and security, 12 primary studies were selected and the data extracted from them was formatted into NEBs. Participants, specialists and non-specialists, were invited to evaluate the NEB through online questionnaires. Feedback highlighted the clarity and structured format of the NEB, with particular praise for its ability to communicate complex scientific evidence in an accessible way. Overall, the NEB demonstrates significant promise in making nudge-related research more accessible and feasible. Ongoing refinements based on participant feedback will be crucial to realizing its full potential, contributing to the advancement of Human-Computer Interaction (HCI) and the practical application of nudges in professional environments. Future work will focus on evaluating the practical applicability of the NEB with non-academic professionals, exploring more reliable alternatives for generating NEBs through LLM, and developing a comprehensive repository of NEBs.},
  address = {New York, NY, USA},
  series = {{IHC},
  isbn = {979-8-4007-1224-1},
}

@inproceedings{mikriukov_ai_2025,
  title = {{AI},
  author = {Mikriukov, Andrei and Senokosov, Artsiom and Succi, Giancarlo and Tormasov, Alexander and Plaksin, Yaroslav and Trofimova, Ekaterina and Sitnikov, Vladimir},
  year = {2025},
  doi = {10.1145/3747912.3747962},
  url = {https://doi.org/10.1145/3747912.3747962},
  booktitle = {Proceedings of the 2025 {International},
  pages = {25--30},
  publisher = {Association for Computing Machinery},
  keywords = {Artificial Intelligence, Data Extraction, Large Language Models, Reproducibility, Screening, Systematic Literature Review},
  abstract = {Systematic literature reviews (SLRs) are becoming increasingly time-consuming due to the rapid growth of scientific publications. Modern artificial intelligence—based tools, especially large language models (LLM), make it possible to automate individual review stages, from screening to data synthesis. The article examines more than 20 sources and suggests a classification of such solutions according to four parameters. Comparative metrics (F1, recall) are given, and key limitations are discussed: hallucinations, reproducibility, and domain adaptation. The work is aimed at researchers who introduce AI into the practice of evidence-based analysis.},
  address = {New York, NY, USA},
  series = {{SECA},
  isbn = {979-8-4007-1513-6},
}

@inproceedings{abbas_pitch_2025,
  title = {{PITCH},
  author = {Abbas, Adnan and Wohn, Caleb and Hu, Donghan and Rho, Eugenia H and Lee, Sang Won},
  year = {2025},
  doi = {10.1145/3719160.3736634},
  url = {https://doi.org/10.1145/3719160.3736634},
  booktitle = {Proceedings of the 7th {ACM},
  publisher = {Association for Computing Machinery},
  keywords = {behavior change, conversational agents, field studies, planning, self-reflection},
  abstract = {Effective planning and reflection are essential for knowledge workers’ productivity and well-being, yet many struggle with them. While conversational agents (CAs) have shown promise, existing approaches rely on repetitive check-in without variance. We designed PITCH, a CA that checks in twice daily for morning planning and evening reflection while considering the morning conversation. A two-week field study with 12 graduate students demonstrated that engagement with PITCH increased their perceived well-being over time. We also evaluated a rotation strategy, which cycles through diverse topics every day, hypothesizing that rotation would mitigate wear-out effects and offer new perspectives. The results revealed that the specificity of a randomly chosen goal was perceived as being out of context and authoritarian, with most preferring the non-rotation version for consistency and flexibility. These findings highlight the potential of CAs to support knowledge workers and offer design considerations for varying conversations to provide topical diversity.},
  address = {New York, NY, USA},
  series = {{CUI},
  isbn = {979-8-4007-1527-3},
}

@inproceedings{ko_subgraph-aware_2025,
  title = {Subgraph-{Aware},
  author = {Ko, Youmin and Yang, Hyemin and Kim, Taeuk and Kim, Hyunjoon},
  year = {2025},
  doi = {10.1145/3696410.3714946},
  url = {https://doi.org/10.1145/3696410.3714946},
  booktitle = {Proceedings of the {ACM},
  pages = {72--85},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {contrastive learning, hard negative sampling, knowledge graph completion},
  abstract = {Fine-tuning pre-trained language models (PLMs) has recently shown a potential to improve knowledge graph completion (KGC). However, most PLM-based methods focus solely on encoding textual information, neglecting the long-tailed nature of knowledge graphs and their various topological structures, e.g., subgraphs, shortest paths, and degrees. We claim that this is a major obstacle to achieving higher accuracy of PLMs for KGC. To this end, we propose a Subgraph-Aware Training framework for KGC (SATKGC) with two ideas: (i) subgraph-aware mini-batching to encourage hard negative sampling and to mitigate an imbalance in the frequency of entity occurrences during training, and (ii) new contrastive learning to focus more on harder in-batch negative triples and harder positive triples in terms of the structural properties of the knowledge graph. To the best of our knowledge, this is the first study to comprehensively incorporate the structural inductive bias of the knowledge graph into fine-tuning PLMs. Extensive experiments on three KGC benchmarks demonstrate the superiority of SATKGC. Our code is available.https://github.com/meaningful96/SATKGC},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1274-6},
}

@inproceedings{lee_peerspective_2025,
  title = {Peerspective: {A},
  author = {Lee, Kwangyoung and Jung, Yeohyun and Jung, Gyuwon and Lu, Xi and Hong, Hwajung},
  year = {2025},
  doi = {10.1145/3706598.3713404},
  url = {https://doi.org/10.1145/3706598.3713404},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Blind Spots, Peerspective, Personal Informatics, Reciprocal Tracking, Self-Understanding, Social Features in Tracking},
  abstract = {Personal informatics helps individuals understand themselves, but it often struggles to capture non-conscious behaviors such as stress responses, habitual actions, and communication styles. Incorporating social aspects into PI systems offers new perspectives on self-understanding, yet prior research has largely focused on unidirectional approaches that center benefits on the primary tracker. To address this gap, we introduce the Peerspective study, which explores reciprocal tracking—a bidirectional practice where two participants observe and provide feedback to each other, fostering mutual self-understanding and collaboration. In a week-long study with eight peer dyads, we explored how reciprocal observation and feedback influence self-awareness and interpersonal relationships. Our findings reveal that reciprocal tracking not only helps participants uncover blind spots and expand their self-concepts but also enhances empathy, deepens communication, and promotes sustained engagement. We discuss key facilitators and challenges of integrating reciprocity into personal informatics systems and offer design considerations for supporting collaborative tracking in everyday contexts.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{flokas_towards_2025,
  title = {Towards a {Framework},
  author = {Flokas, Lampros and Cao, Jeffery and Xu, Yujian and Wu, Eugene and Chu, Xu and Yu, Cong},
  year = {2025},
  doi = {10.1145/3735654.3735941},
  url = {https://doi.org/10.1145/3735654.3735941},
  booktitle = {Proceedings of the {Workshop},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {source: ACM},
  abstract = {Thanks to the in context learning abilities of LLMs, building a text classifier without access to labeled data is easier than ever. However, for more complex tasks than simple classification, the difficulties remain. One such task is hierarchical segmentation where a model needs to break down an input document in an hierarchy of segments each annotated with a label from a taxonomy of classes. The long input documents, the large input class taxonomies as well as the increased number of outputs make the problem challenging to solve with a single LLM call. While hierarchical segmentation is amenable to splitting in smaller more manageable tasks, there is a huge design space of such approaches making the process tedious and time consuming. To reduce the amount of ad hoc exploration and implementation, we propose the first framework for hierarchical text segmentation using LLMs. The key idea behind our framework is that hierarchical segmentation can be viewed as a join between document and taxonomy. Inspired by join operator design, we propose two highly configurable hierarchical segmentation algorithms based on index and merge sort joins. Our experiments highlight the existence of trade offs across algorithms and their configurations, indicating that machine learning engineers may benefit from quickly exploring the design space using our framework.},
  address = {New York, NY, USA},
  series = {{DEEM},
  isbn = {979-8-4007-1924-0},
}

@inproceedings{mozafari_wikihint_2025,
  title = {{WikiHint},
  author = {Mozafari, Jamshid and Gerhold, Florian and Jatowt, Adam},
  year = {2025},
  doi = {10.1145/3726302.3730284},
  url = {https://doi.org/10.1145/3726302.3730284},
  booktitle = {Proceedings of the 48th {International},
  pages = {3821--3831},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {hint dataset, hint evaluation, hint generation, hint ranking},
  abstract = {The use of Large Language Models (LLMs) has increased significantly with users frequently asking questions to chatbots. In the time when information is readily accessible, it is crucial to stimulate and preserve human cognitive abilities and maintain strong reasoning skills. This paper addresses such challenges by promoting the use of hints as an alternative or a supplement to direct answers. We first introduce a manually constructed hint dataset, WikiHint, which is based on Wikipedia and includes 5,000 hints created for 1,000 questions. We then finetune open-source LLMs for hint generation in answer-aware and answer-agnostic contexts. We assess the effectiveness of the hints with human participants who answer questions with and without the aid of hints. Additionally, we introduce a lightweight evaluation method, HintRank, to evaluate and rank hints in both answer-aware and answer-agnostic settings. Our findings show that (a) the dataset helps generate more effective hints (b) including answer information along with questions generally improves the quality of generated hints, and (c) encoder-based models perform better than decoder-based models in hint ranking.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{wang_exploiting_2025,
  title = {Exploiting {Student},
  author = {Wang, Weiyan and Jin, Yilun and Zhang, Yiming and Wei, Victor Junqiu and Tian, Han and Chen, Li and Xue, Jinbao and Tao, Yangyu and Wang, Di and Chen, Kai},
  year = {2025},
  doi = {10.1145/3711896.3736949},
  url = {https://doi.org/10.1145/3711896.3736949},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {3055--3066},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {distillation, efficient inference, information retrieval, text mining},
  abstract = {BERT-like models have been widely adopted in text mining and web search due to their high accuracy. However, large BERT-like models suffer from inefficient online inference on GPUs for two main reasons. First, their high accuracy relies on large model depth, which linearly increases sequential computation on GPUs. Second, stochastic and dynamic online workloads lead to extra costs due to batching and padding. To address the problem, we present Student Parallelism for efficient GPU inference of BERT-like models under real-world online workloads. At its core, Student Parallelism adopts stacking distillation and boosting ensemble, distilling the original deep model into a group of shallow but virtually stacked student models running in parallel. This enables Student Parallelism to achieve a low model depth (e.g., two layers), and thus low inference latency while maintaining accuracy. In addition, we design adaptive student pruning to adjust the number of students according to the dynamic online workloads. For example, during workload bursts, it can temporarily decrease the number of students with minimal accuracy loss to improve system throughput. Extensive experiments on real-world datasets and workloads show that Student Parallelism achieves up to 4.1× lower latency while maintaining accuracy and up to 22.27× higher throughput during workload bursts.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{pavlenko_vertically_2024,
  title = {Vertically {Autoscaling},
  author = {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodríguez, Jesús and Saur, Karla},
  year = {2024},
  doi = {10.1145/3626246.3653378},
  url = {https://doi.org/10.1145/3626246.3653378},
  booktitle = {Companion of the 2024 {International},
  pages = {241--254},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago AA, Chile},
  keywords = {containers, kubernetes, resource optimization, vertical auto-scaling},
  abstract = {Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-0422-2},
}

@inproceedings{may_sap_2025,
  title = {{SAP},
  author = {May, Norman and Böhm, Alexander and Ritter, Daniel and Renkes, Frank and Andrei, Mihnea and Lehner, Wolfgang},
  year = {2025},
  doi = {10.1145/3722212.3724452},
  url = {https://doi.org/10.1145/3722212.3724452},
  booktitle = {Companion of the 2025 {International},
  pages = {580--592},
  publisher = {Association for Computing Machinery},
  note = {event-place: Berlin, Germany},
  keywords = {cloud architecture patterns, cloud data management, data lake, multi-model, multi-tenancy, SAP HANA cloud},
  abstract = {Cloud Computing environments are a fantastic foundation for unprecedented opportunities in the context of data management. Scalability and availability can be achieved by consequently disaggregating compute and storage and leveraging different data centers around the globe. Although conceptually infinitely available resources promise an easy game for database systems, leveraging those opportunities on the one side and providing enterprise-scale data management solutions for a wide range of extremely challenging business applications on the other side is by far not trivial. Within this paper we share the fundamental principles of SAP HANA Cloud—the data management backbone for all SAP applications—by providing insights into design criteria and technological centerpieces of SAP HANA Cloud as a result of a stringent evolutionary shift from a purely on-premise, in-memory database engine to an on-Cloud, memory-storage hierarchy-aware data management platform. We will motivate the transition and the decision of certain developments by sharing key characteristics of some key applications run by SAP, which require enterprise qualities like availability, cloud-capabilities like resource elasticity, while managing overall costs and the flexibility to adapt the underlying data management system to their specific needs. The overall goal of the paper is to provide insights into the ”story of SAP HANA Cloud” by sharing challenges and lessons learned within the evolution towards a composable cloud-based data platform, thus strongly arguing for a ”one size fits all” solution, if done right.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-1564-8},
}

@inproceedings{wang_bingo_2025,
  title = {Bingo: {Radix},
  author = {Wang, Pinhuan and Huan, Chengying and Wang, Zhibin and Tian, Chen and Ji, Yuede and Liu, Hang},
  year = {2025},
  doi = {10.1145/3689031.3717456},
  url = {https://doi.org/10.1145/3689031.3717456},
  booktitle = {Proceedings of the {Twentieth},
  pages = {605--620},
  publisher = {Association for Computing Machinery},
  note = {event-place: Rotterdam, Netherlands},
  keywords = {GPUs, Monte Carlo Sampling, Random Walk},
  abstract = {Random walks are a primary means for extracting information from large-scale graphs. While most real-world graphs are inherently dynamic, state-of-the-art random walk engines failed to efficiently support such a critical use case. This paper takes the initiative to build a general random walk engine for dynamically changing graphs with two key principles: (i) This system should support both low-latency streaming updates and high-throughput batched updates. (ii) This system should achieve fast sampling speed while maintaining acceptable space consumption to support dynamic graph updates. Upholding both standards, we introduce Bingo, a GPU-based random walk engine for dynamically changing graphs. First, we propose a novel radix-based bias factorization algorithm to support constant time sampling complexity while supporting fast streaming updates. Second, we present a group-adaption design to reduce space consumption dramatically. Third, we incorporate GPU-aware designs to support high-throughput batched graph updates on massively parallel platforms. Together, Bingo outperforms existing efforts across various applications, settings, and datasets, achieving up to a 271.11x speedup compared to the state-of-the-art efforts.},
  address = {New York, NY, USA},
  series = {{EuroSys},
  isbn = {979-8-4007-1196-1},
}

@inproceedings{chen_emotion-aware_2025,
  title = {Emotion-aware {Design},
  author = {Chen, Xingtong and Wang, Xia and Fang, Cong and Fang, Le and Gong, Wei and Liu, Chengzhong and Wang, Stephen Jia},
  year = {2025},
  doi = {10.1145/3706598.3713571},
  url = {https://doi.org/10.1145/3706598.3713571},
  booktitle = {Proceedings of the 2025 {CHI},
  publisher = {Association for Computing Machinery},
  keywords = {Emotion, Human-centred Design, Human-vehicle Interaction},
  abstract = {The integration of emotion-aware systems in vehicles is accelerated by new technologies, including advancements in AI and ubiquitous sensing technologies. As the automotive industry shifts from technology-centred, feature-driven approaches to human-centred design, this research focuses on how to effectively incorporate emotion features into user-centred design to enhance effective human-vehicle interaction in practices. By conducting an interview study with 31 industrial design practitioners, supplemented by insights from engineers and AI experts involved in the early-stage design and development of novel in-vehicle user interfaces and systems, we examined current practices, and sampled their challenges, attitudes and expectations related to emotion-aware systems. Our findings provide critical insights to the design space of emotion-aware systems from both user and AI perspectives, inform efforts to support design practices in this evolving area, and identify opportunities for future innovation in emotion-aware in-vehicle design. Based on our findings, we propose adaptations to design practices and recommendations for further research.},
  address = {New York, NY, USA},
  series = {{CHI},
  isbn = {979-8-4007-1394-1},
}

@inproceedings{liu_tsconnect_2025,
  title = {{TSConnect},
  author = {Liu, Qianyu and Li, Xinran and Du, Xiaocong and Li, Quan},
  year = {2025},
  doi = {10.1145/3708359.3712108},
  url = {https://doi.org/10.1145/3708359.3712108},
  booktitle = {Proceedings of the 30th {International},
  pages = {1335--1353},
  publisher = {Association for Computing Machinery},
  keywords = {bias-aware design, communication gap, curse of knowledge, MOOC platform, student-instrutor communication},
  abstract = {Knowledge dissemination in educational settings is profoundly influenced by the curse of knowledge, a cognitive bias that causes experts to underestimate the challenges faced by learners due to their own in-depth understanding of the subject. This bias can hinder effective knowledge transfer and pedagogical effectiveness, and may be exacerbated by inadequate instructor-student communication. To encourage more effective feedback and promote empathy, we introduce TSConnect, a bias-aware, adaptable interactive MOOC (Massive Open Online Course) learning system, informed by a need-finding survey involving 129 students and 6 instructors. TSConnect integrates instructors, students, and Artificial Intelligence (AI) into a cohesive platform, facilitating diverse and targeted communication channels while addressing previously overlooked information needs. A notable feature is its dynamic knowledge graph, which enhances learning support and fosters a more interconnected educational experience. We conducted a between-subjects user study with 30 students comparing TSConnect to a baseline system. Results indicate that TSConnect significantly encourages students to provide more feedback to instructors. Additionally, interviews with 4 instructors reveal insights into how they interpret and respond to this feedback, potentially leading to improvements in teaching strategies and the development of broader pedagogical skills.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-1306-4},
}

@inproceedings{kalirai_toward_2024,
  title = {Toward {Faceted},
  author = {Kalirai, Manveer and Williams, Alex C. and Kuzminykh, Anastasia},
  year = {2024},
  doi = {10.1145/3640543.3645201},
  url = {https://doi.org/10.1145/3640543.3645201},
  booktitle = {Proceedings of the 29th {International},
  pages = {640--649},
  publisher = {Association for Computing Machinery},
  note = {event-place: Greenville, SC, USA},
  keywords = {conversational recommendation, discoverability, Intelligent personal assistants, skills, voice interface},
  abstract = {Research continuously shows that, despite the wide range of skills developed for Intelligent Personal Assistants (IPAs), users tend to engage with only a small number of them. One reason for this discrepancy is the issue of skill discoverability, which is commonly addressed through conversational recommendations. Current recommendation strategies, however, are limited due to information asymmetry, lack of interactivity, and an underdeveloped understanding of appropriate grouping of available skills. In this paper, we explore opportunities for interactive faceted skill recommendations using voice interfaces. Through an open card sort user study and semi-structured interviews, we identify and describe five facets driving users’ natural grouping of IPA skills (Thematic, Procedural, Cross-system, Environmental, and Recipient), and demonstrate the need for simultaneous support of these facets. We then discuss the implications of these findings for advancing the discoverability of IPA skills through the design of interactive conversational recommendations.},
  address = {New York, NY, USA},
  series = {{IUI},
  isbn = {979-8-4007-0508-3},
}

@inproceedings{de_la_rua_martinez_hopsworks_2024,
  title = {The {Hopsworks},
  author = {de la Rúa Martínez, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronström, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim},
  year = {2024},
  doi = {10.1145/3626246.3653389},
  url = {https://doi.org/10.1145/3626246.3653389},
  booktitle = {Companion of the 2024 {International},
  pages = {135--147},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago AA, Chile},
  keywords = {arrow flight, duckdb, feature store, mlops, rondb},
  abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference. In this paper, we present the Hopsworks feature store for machine learning as a highly available platform for managing feature data with API support for columnar, row-oriented, and similarity search query workloads. We introduce and address challenges solved by the feature stores related to feature reuse, how to organize data transformations, and how to ensure correct and consistent data between feature engineering, model training, and model inference. We present the engineering challenges in building high-performance query services for a feature store and show how Hopsworks outperforms existing cloud feature stores for training and online inference query workloads.},
  address = {New York, NY, USA},
  series = {{SIGMOD},
  isbn = {979-8-4007-0422-2},
}

@inproceedings{liu_compeer_2024,
  title = {{ComPeer},
  author = {Liu, Tianjian and Zhao, Hongzheng and Liu, Yuheng and Wang, Xingbo and Peng, Zhenhui},
  year = {2024},
  doi = {10.1145/3654777.3676430},
  url = {https://doi.org/10.1145/3654777.3676430},
  booktitle = {Proceedings of the 37th {Annual},
  publisher = {Association for Computing Machinery},
  note = {event-place: Pittsburgh, PA, USA},
  keywords = {Generative conversational agents, human-AI interaction, peer support, proactivity},
  abstract = {Conversational Agents (CAs) acting as peer supporters have been widely studied and demonstrated beneficial for people’s mental health. However, previous peer support CAs either are user-initiated or follow predefined rules to initiate the conversations, which may discourage users to engage and build relationships with the CAs for long-term benefits. In this paper, we develop ComPeer , a generative CA that can proactively offer adaptive peer support to users. ComPeer leverages large language models to detect and reflect significant events in the dialogue, enabling it to strategically plan the timing and content of proactive care. In addition, ComPeer incorporates peer support strategies, conversation history, and its persona into the generative messages. Our one-week between-subjects study (N=24) demonstrates ComPeer ’s strength in providing peer support over time and boosting users’ engagement compared to a baseline user-initiated CA. We report users’ interaction patterns with ComPeer and discuss implications for designing proactive generative agents to promote people’s well-being.},
  address = {New York, NY, USA},
  series = {{UIST},
  isbn = {979-8-4007-0628-8},
}

@inproceedings{dong_reasoning_2025,
  title = {Reasoning and {Retrieval},
  author = {Dong, Haoyu and Hu, Yue and Cao, Yanan},
  year = {2025},
  doi = {10.1145/3726302.3730071},
  url = {https://doi.org/10.1145/3726302.3730071},
  booktitle = {Proceedings of the 48th {International},
  pages = {1382--1391},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {information retrieval, symbolic reasoning, table generation},
  abstract = {We introduce TabFormer, a framework that normalizes diverse semi-structured tables into relational data via large language models to facilitate various table retrieval and reasoning tasks. Our approach employs a chain-of-thought methodology, transforming one or multiple tables through a sequence of soft operations. Compared to existing operators that are sensitive and brittle to human-induced artifacts in real-world tables, soft operators are designed with greater flexibility to accommodate diverse formatting variations. To address the lack of ground-truth labels for table transformation, we propose a reinforced fine-tuning strategy that sequentially and jointly optimizes table transformation and symbolic reasoning within a single LLM call. This process is guided by two novel reward functions without requiring human annotations on table transformation: (1) relational normalization quality and (2) symbolic reasoning accuracy. TabFormer is a scalable, one-time framework that leverages LLMs to transform one ore multiple tables in a single inference step, thereby enhancing both retrieval and reasoning for a wide range of tabular data tasks. Experimental evaluations on datasets such as WTQ, HiTab, MultiHiertt, and TabFact demonstrate significant gains in accuracy-improvements ranging from 4\% to 17\%-when applied to state-of-the-art methods, including GPT-4-based E5, Chain-of-Table, TableLlama, and others.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{dumitru_evaluating_2025,
  title = {Evaluating {List},
  author = {Dumitru, Alexandru and V, Venktesh and Jatowt, Adam and Anand, Avishek},
  year = {2025},
  doi = {10.1145/3731120.3744606},
  url = {https://doi.org/10.1145/3731120.3744606},
  booktitle = {Proceedings of the 2025 {International},
  pages = {369--379},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {retrieval, temporal question answering, temporal understanding},
  abstract = {Large Language Models (LLMs) have demonstrated immense advances in a wide range of natural language tasks. However, these models are susceptible to hallucinations and errors on particularly temporal understanding tasks involving multiple entities in answers. In such tasks, they fail to associate entities with accurate time intervals, generate a complete list of entities in answers or reason about events associated with specific temporal bounds. Existing works do not extensively evaluate the abilities of the model to perform implicit and explicit temporal understanding in a list answer construction setup. To bridge this gap, we propose the Time referenced List based Question Answering or TLQA benchmark that requires structured answers in list format aligned with corresponding time periods. Our TLQA benchmark, requires both list construction and temporal understanding simultaneously, which to the best of our knowledge has not been explored in prior benchmarks. We investigate the temporal understanding and list construction capabilities of state-of-the-art generative models on TLQA in closed-book and open-domain settings. Our findings reveal significant shortcomings in current models, particularly their inability to provide complete answers and temporally align facts in a closed-book setup and the need to improve retrieval in open-domain setup, providing clear future directions for research on TLQA. The benchmark and code can be publicly accessed at https://github.com/elixir-research-group/TLQA.},
  address = {New York, NY, USA},
  series = {{ICTIR},
  isbn = {979-8-4007-1861-8},
}

@inproceedings{li_tex2py-45k_2025,
  title = {{Tex2Py},
  author = {Li, Lei and Duan, Manni and Wang, Yongheng},
  year = {2025},
  doi = {10.1145/3709026.3709044},
  url = {https://doi.org/10.1145/3709026.3709044},
  booktitle = {Proceedings of the 2024 8th {International},
  pages = {197--203},
  publisher = {Association for Computing Machinery},
  keywords = {Bidirectional conversion, Code LLM, LaTeX, Parallel corpus, Python},
  abstract = {LaTeX and Python, integral to the scientific research workflow, each offers specialized capabilities. Bidirectional LaTeX-Python conversion models could augment the efficacy and precision of scientific research. In this paper, we release the Tex2Py-45K dataset1, containing 45,668 LaTeX-Python pairs. Based on the Tex2Py-45K dataset, we propose SMML format and Hybrid training mode, which trained effective LaTeX-Python bidirectional conversion models. At last, we analyze failure cases of the LaTeX-Python bidirectional conversion, providing direction for future research.},
  address = {New York, NY, USA},
  series = {{CSAI},
  isbn = {979-8-4007-1818-2},
}

@inproceedings{lin_knowledge-injected_2024,
  title = {A {Knowledge},
  author = {Lin, Xin and Su, Tianhuang and Huang, Zhenya and Xue, Shangzi and Liu, Haifeng and Chen, Enhong},
  year = {2024},
  doi = {10.1145/3589334.3645406},
  url = {https://doi.org/10.1145/3589334.3645406},
  booktitle = {Proceedings of the {ACM},
  pages = {1986--1997},
  publisher = {Association for Computing Machinery},
  note = {event-place: Singapore, Singapore},
  keywords = {curriculum learning, knowledge-injected pretraining, question answering},
  abstract = {Knowledge-based question answering (KBQA) is a key task in natural language processing research, and also an approach to access the web data and knowledge, which requires exploiting knowledge graphs (KGs) for reasoning. In the literature, one promising solution for KBQA is to incorporate the pretrained language model (LM) with KGs by generating KG-centered pretraining corpus, which has shown its superiority. However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning. To this end, in this paper, we propose a general K nowledge-I njected C urriculum P retraining framework (KICP) to achieve comprehensive KG learning and exploitation for KBQA tasks, which is composed of knowledge injection (KI), knowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KI module first injects knowledge into the LM by generating KG-centered pretraining corpus, and generalizes the process into three key steps that could work with different implementations for flexible application. Next, the KA module learns knowledge from the generated corpus with LM equipped with an adapter as well as keeps its original natural language understanding ability to reduce the negative impacts of the difference between the generated and natural corpus. Last, to enable the LM with complex reasoning, the CR module follows human reasoning patterns to construct three corpora with increasing difficulties of reasoning, and further trains the LM from easy to hard in a curriculum manner to promote model learning. We provide an implementation of the general framework, and evaluate the proposed KICP on four real-word datasets. The results demonstrate that our framework can achieve higher performances, and have good generalization ability to other QA tasks.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-0171-9},
}

@inproceedings{yamakawa_direct_2024,
  title = {Direct {Backpropagation},
  author = {Yamakawa, Yuto and Kimura, Masaomi},
  year = {2024},
  doi = {10.1145/3654522.3654555},
  url = {https://doi.org/10.1145/3654522.3654555},
  booktitle = {Proceedings of the 2024 9th {International},
  pages = {340--345},
  publisher = {Association for Computing Machinery},
  note = {event-place: Ho Chi Minh City, Vietnam},
  keywords = {artificial neural networks, black-box backpropagation, database, Retrieval Augmented Language Model},
  abstract = {The Retrieval Augmented Language Model, which is a model that combines a language model and a database, has various advantages from having knowledge in the form of a database and has been actively studied, especially in the Question Answering task. Since the Retrieval Augmented Language Model cannot define the derivative in the database retrieval process, it generally performs backpropagation using a path that does not go through the database, which makes learning of the model inefficient. To realize backpropagation using a path through the database, we propose a method that can perform backpropagation by adding a derivative value to the database data. Before applying our method to the Retrieval Augmented Language Model, we conducted experiments on a simple recursive task to confirm its effectiveness. The results showed that the model that learned using direct paths through the database was more efficient than the model that learned using indirect paths.},
  address = {New York, NY, USA},
  series = {{ICIIT},
  isbn = {979-8-4007-1671-3},
}

@inproceedings{verma_matryoshka_2025,
  title = {Matryoshka {Model},
  author = {Verma, Chetan and Timmaraju, Aditya Srinivas and Hsieh, Cho-Jui and Damle, Suyash and Bui, Ngot and Zhang, Yang and Chen, Wen and Liu, Xin and Jain, Prateek and Dhillon, Inderjit},
  year = {2025},
  doi = {10.1145/3711896.3737245},
  url = {https://doi.org/10.1145/3711896.3737245},
  booktitle = {Proceedings of the 31st {ACM},
  pages = {4935--4944},
  publisher = {Association for Computing Machinery},
  note = {event-place: Toronto ON, Canada},
  keywords = {elastic inference, matryoshka representations, online distillation},
  abstract = {Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20\% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24\% on SAT Math and over 10\% on the LAMBADA benchmark.},
  address = {New York, NY, USA},
  series = {{KDD},
  isbn = {979-8-4007-1454-2},
}

@inproceedings{sakhovskiy_bali_2025,
  title = {{BALI},
  author = {Sakhovskiy, Andrey and Tutubalina, Elena},
  year = {2025},
  doi = {10.1145/3726302.3729901},
  url = {https://doi.org/10.1145/3726302.3729901},
  booktitle = {Proceedings of the 48th {International},
  pages = {1152--1164},
  publisher = {Association for Computing Machinery},
  note = {event-place: Padua, Italy},
  keywords = {biomedical knowledge graph, biomedical language model, biomedical natural language processing, contrastive learning, natural language processing, representation learning},
  abstract = {In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Ali gnment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.},
  address = {New York, NY, USA},
  series = {{SIGIR},
  isbn = {979-8-4007-1592-1},
}

@inproceedings{argyrou_prompt2fashion_2024,
  title = {{Prompt2Fashion},
  author = {Argyrou, Georgia and Dimitriou, Angeliki and Lymperaiou, Maria and Filandrianos, Giorgos and Stamou, Giorgos},
  year = {2024},
  doi = {10.1145/3688671.3690604},
  url = {https://doi.org/10.1145/3688671.3690604},
  booktitle = {Proceedings of the 13th {Hellenic},
  publisher = {Association for Computing Machinery},
  keywords = {fashion synthesis, human evaluation, image dataset},
  abstract = {Despite the rapid evolution and increasing efficacy of language and vision generative models, there remains a lack of comprehensive datasets that bridge the gap between personalized fashion needs and AI-driven design, limiting the potential for truly inclusive and customized fashion solutions. In this work, we leverage generative models to automatically construct a fashion image dataset tailored to various occasions, styles, and body types as instructed by users. We use different Large Language Models (LLMs) and prompting strategies to offer personalized outfits of high aesthetic quality, detail, and relevance to both expert and non-expert users’ requirements, as demonstrated by qualitative analysis. Up until now the evaluation of the generated outfits has been conducted by non-expert human subjects. Despite the provided fine-grained insights on the quality and relevance of generation, we extend the discussion on the importance of expert knowledge for the evaluation of artistic AI-generated datasets such as this one. Our dataset is publicly available on GitHub at https://github.com/georgiarg/Prompt2Fashion.},
  address = {New York, NY, USA},
  series = {{SETN},
  isbn = {979-8-4007-0982-1},
}

@inproceedings{wang_hierarchical_2025,
  title = {Hierarchical {Prompt},
  author = {Wang, Zhe and Wang, Haozhu and Qi, Yanjun},
  year = {2025},
  doi = {10.1145/3701716.3715233},
  url = {https://doi.org/10.1145/3701716.3715233},
  booktitle = {Companion {Proceedings},
  pages = {520--529},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sydney NSW, Australia},
  keywords = {decision transformer, few-shot learning, reinforcement learning},
  abstract = {Decision transformers recast reinforcement learning as a conditional sequence generation problem, offering a simple but effective alternative to traditional value or policy-based methods. A recent key development in this area is the integration of prompting in decision transformers to facilitate few-shot policy generalization. However, current methods mainly use static prompt segments to guide rollouts, limiting their ability to provide context-specific guidance. Addressing this, we introduce a hierarchical prompting approach enabled by retrieval augmentation. Our method learns two layers of soft tokens as guiding prompts: (1) global tokens encapsulating task-level information about trajectories, and (2) adaptive tokens that deliver focused, timestep-specific instructions. The adaptive tokens are dynamically retrieved from a curated set of demonstration segments, ensuring context-aware guidance. Experiments across seven benchmark tasks in the MuJoCo and MetaWorld environments demonstrate the proposed approach consistently outperforms all baseline methods, suggesting that hierarchical prompting for decision transformers is an effective strategy to enable few-shot policy generalization.},
  address = {New York, NY, USA},
  series = {{WWW},
  isbn = {979-8-4007-1331-6},
}

@inproceedings{han_fair-co2_2025,
  title = {Fair-{CO2},
  author = {Han, Leo and Kakadia, Jash and Lee, Benjamin C. and Gupta, Udit},
  year = {2025},
  doi = {10.1145/3695053.3731023},
  url = {https://doi.org/10.1145/3695053.3731023},
  booktitle = {Proceedings of the 52nd {Annual},
  pages = {646--663},
  publisher = {Association for Computing Machinery},
  keywords = {carbon accounting, cloud computing, sustainable computing},
  abstract = {Fair-CO2 is a system for fairly attributing operational and embodied carbon in cloud data centers to user workloads. It leverages the Shapley value, a game theory solution for fair shared cost attribution with theoretical fairness guarantees. We propose the standard Shapley value solution as a ground truth for attribution in cloud data centers, addressing two key gaps in existing carbon attribution methods that lead to unfair attributions: the effect of dynamic demand on embodied carbon, and interference effects in colocated scenarios. However, the computational cost of the Shapley value solution scales exponentially with the number of workloads and becomes intractable for large systems. Fair-CO2 addresses the scalability challenges of the Shapley value while preserving its fairness benefits via two core components: demand-aware embodied carbon attribution and interference-aware resource cost attribution. Using Monte Carlo simulations of workload schedules and colocation scenarios, we show that Fair-CO2 can approximate the ground truth Shapley attribution solution at scale. We also show how users, once provided a fair way of estimating their workload carbon footprint, can dynamically optimize workload deployment for carbon savings.},
  address = {New York, NY, USA},
  series = {{ISCA},
  isbn = {979-8-4007-1261-6},
}

@inproceedings{kim_lapis_2024,
  title = {{LAPIS},
  author = {Kim, Heedou and Kim, Dain and Lee, Jiwoo and Yoon, Chanwoong and Choi, Donghee and Gim, Mogan and Kang, Jaewoo},
  year = {2024},
  doi = {10.1145/3627673.3680044},
  url = {https://doi.org/10.1145/3627673.3680044},
  booktitle = {Proceedings of the 33rd {ACM},
  pages = {4637--4644},
  publisher = {Association for Computing Machinery},
  note = {event-place: Boise, ID, USA},
  keywords = {crime investigation, large language models, police investigation, source: ACM},
  abstract = {Crime situations are race against time. An AI-assisted criminal investigation system, providing prompt but precise legal counsel is in need for police officers. We introduce LAPIS (Language Model Augmented Police Investigation System), an automated system that assists police officers to perform rational and legal investigative actions. We constructed a finetuning dataset and retrieval knowledgebase specialized in crime investigation legal reasoning task. We extended the dataset's quality by incorporating manual curation efforts done by a group of domain experts. We then finetuned the pretrained weights of a smaller Korean language model to the newly constructed dataset and integrated it with the crime investigation knowledgebase retrieval approach. Experimental results show LAPIS' potential in providing reliable legal guidance for police officers, even better than the proprietary GPT-4 model. Qualitative analysis on the rationales generated by LAPIS demonstrate the model's reasoning ability to leverage the premises and derive legally correct conclusions.},
  address = {New York, NY, USA},
  series = {{CIKM},
  isbn = {979-8-4007-0436-9},
}

@inproceedings{grafberger_towards_2024,
  title = {Towards {Interactively},
  author = {Grafberger, Stefan and Groth, Paul and Schelter, Sebastian},
  year = {2024},
  doi = {10.1145/3650203.3663327},
  url = {https://doi.org/10.1145/3650203.3663327},
  booktitle = {Proceedings of the {Eighth},
  pages = {7--11},
  publisher = {Association for Computing Machinery},
  note = {event-place: Santiago, AA, Chile},
  keywords = {source: ACM},
  abstract = {Data scientists develop ML pipelines in an iterative manner: they repeatedly screen a pipeline for potential issues, debug it, and then revise and improve its code according to their findings. However, this manual process is tedious and error-prone. Therefore, we propose to support data scientists during this development cycle with automatically derived interactive suggestions for pipeline improvements. We discuss our vision to generate these suggestions with so-called shadow pipelines, hidden variants of the original pipeline that modify it to auto-detect potential issues, try out modifications for improvements, and suggest and explain these modifications to the user. We envision to apply incremental view maintenance-based optimisations to ensure low-latency computation and maintenance of the shadow pipelines. We conduct preliminary experiments to showcase the feasibility of our envisioned approach and the potential benefits of our proposed optimisations.},
  address = {New York, NY, USA},
  series = {{DEEM},
  isbn = {979-8-4007-0611-0},
}

@inproceedings{wampfler_platform_2025,
  title = {A {Platform},
  author = {Wampfler, Rafael and Yang, Chen and Elste, Dillon and Kovacevic, Nikola and Witzig, Philine and Gross, Markus},
  year = {2025},
  doi = {10.1145/3721238.3730762},
  url = {https://doi.org/10.1145/3721238.3730762},
  booktitle = {Proceedings of the {Special},
  publisher = {Association for Computing Machinery},
  keywords = {Character Consistency, Conversational AI, Digital Characters, Embodied Conversational Agents, Interactive Storytelling, Large Language Models, Memory Systems, Multimodal Interaction, Personality Modeling, Speech Synthesis, Speech‑Driven Animation},
  abstract = {From movie characters to modern science fiction — bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.},
  address = {New York, NY, USA},
  series = {{SIGGRAPH},
  isbn = {979-8-4007-1540-2},
}

@inproceedings{jang_accelerating_2025,
  title = {Accelerating {Retrieval},
  author = {Jang, Je-Woo and Oh, Junyong and Kong, Youngbae and Hong, Jae-Youn and Cho, Sung-Hyuk and Lee, Jeongyeol and Yang, Hoeseok and Yang, Joon-Sung},
  year = {2025},
  doi = {10.1145/3725843.3756020},
  url = {https://doi.org/10.1145/3725843.3756020},
  booktitle = {Proceedings of the 58th {IEEE},
  pages = {246--262},
  publisher = {Association for Computing Machinery},
  keywords = {High Bandwidth Memory, Processing in Memory, Processing near memory, Retrieval augmented language model, Vector Search},
  abstract = {Retrieval-Augmented Language Models (RALMs) integrate a language model with an external database to generate high-quality outputs utilizing up-to-date information. However, both components of a RALM system, the language model and the retriever, suffer from distinct memory-bound bottlenecks. In particular, the attention mechanism of the language model heavily relies on General Matrix-Vector Multiplication (GEMV) operations using unique K/V matrices per request, complicating batch parallelization and exacerbating memory bandwidth constraints. Conversely, the retriever encounters performance bottlenecks due to frequent LUT lookups and intensive sorting operations, characterized by low arithmetic intensity and limited data reuse, making GPU acceleration challenging. To address these distinctive characteristics, this paper proposes MNM, a hardware architecture integrating Processing In Memory (PIM) within the HBM core die and Processing Near Memory (PNM) on the HBM logic die. The PIM module leverages the high internal bandwidth of HBM to accelerate GEMV operations in the language model, while the PNM module optimizes retrieval-specific tasks. Furthermore, this work introduces a novel RALM scheduling strategy combining selective batching and early generation to exploit the performance improvements achieved by the MNM architecture. By strategically overlapping retrieval and generation phases, the proposed scheduling scheme reduces idle cycles in a batched RALM system. Experimental results demonstrate that the proposed techniques achieve up to 29.2 × performance speedup compared to a conventional GPU-based RALM system. In addition, the proposed PIM/PNM-integrated approach saves up to 71.5\% of energy consumption, highlighting its applicability for memory-bound RALM workloads.},
  address = {New York, NY, USA},
  series = {{MICRO},
  isbn = {979-8-4007-1573-0},
}

@inproceedings{tran_memoriqa_2024,
  title = {{MemoriQA},
  author = {Tran, Quang-Linh and Nguyen, Binh and Jones, Gareth J. F. and Gurrin, Cathal},
  year = {2024},
  doi = {10.1145/3643479.3662050},
  url = {https://doi.org/10.1145/3643479.3662050},
  booktitle = {Proceedings of the 1st {ACM},
  pages = {7--12},
  publisher = {Association for Computing Machinery},
  note = {event-place: Phuket, Thailand},
  keywords = {Lifelog Dataset, Personal Lifelog Archive, Question Answering},
  abstract = {Lifelogging can be referred to as the process of passively collecting data on an individual's daily life. Lifelog data provides a large amount of information which can be used to understand the lifelogger's lifestyle and preferences. This data can also support the lifeloggers in saving their memories and important moments. Question-answering (QA) is a common task in natural language processing (NLP) and can be extended to multi-modal such as the visual question-answering task. QA for lifelog data can be described as the task of answering questions about a lifelogger's past using lifelog data, which can significantly help lifeloggers understand their life by asking questions about their lifelog. QA for lifelogs can also provide useful insights into lifelogger's life for those exploring their lifelog. This paper presents the MemoriQA lifelog dataset designed to explore the question-answering task for lifelogs. This dataset provides 61-day lifelog images and other lifelog data such as internet activity, health metrics, music listening history and GPS. A comprehensive annotation process is performed to create the description as well as question-answer pairs. We propose some methods to address the QA in lifelog problem in this paper.},
  address = {New York, NY, USA},
  series = {{AIQAM},
  isbn = {979-8-4007-0547-2},
}

@inproceedings{ferracani_personalized_2024,
  title = {Personalized {Generative},
  author = {Ferracani, Andrea and Bertini, Marco and Pala, Pietro and Nannotti, Gabriele and Principi, Filippo and Becchi, Giuseppe},
  year = {2024},
  doi = {10.1145/3689094.3689465},
  url = {https://doi.org/10.1145/3689094.3689465},
  booktitle = {Proceedings of the 6th {Workshop},
  pages = {28--32},
  publisher = {Association for Computing Machinery},
  note = {event-place: Melbourne VIC, Australia},
  keywords = {ai, cultural heritage, cultural tourism, gpt-4, image generation, personalization, stable diffusion, storytelling},
  abstract = {This paper presents a mobile application that exploits interactive narrative storytelling through GPT-4 and a custom image generative pipeline to improve cultural tourism experiences. The application helps tourists visiting cities to program and personalize cultural city tours creating stories with the user as the protagonist. The app guides the users to choose Point-Of-Interests (POIs) and narrative genres of the narrative while the image generation pipeline provides them with visual and coherent representations of their actions in the story contributing to a more immersive and personalized experience. Technical challenges include producing coherent stories and real-time and quality images, maintaining visual composition and person identity, including multiple concepts, through prompt engineering. We validate the effectiveness of the application and the image generative pipeline through users studies which evaluate the educational potential of our approach.},
  address = {New York, NY, USA},
  series = {{SUMAC},
  isbn = {979-8-4007-1205-0},
}

@inproceedings{zhang_copilot---loop_2024,
  title = {Copilot-in-the-{Loop},
  author = {Zhang, Beiqi and Liang, Peng and Feng, Qiong and Fu, Yujia and Li, Zengyang},
  year = {2024},
  doi = {10.1145/3691620.3695290},
  url = {https://doi.org/10.1145/3691620.3695290},
  booktitle = {Proceedings of the 39th {IEEE},
  pages = {2230--2234},
  publisher = {Association for Computing Machinery},
  note = {event-place: Sacramento, CA, USA},
  keywords = {code quality, code refactoring, code smell, GitHub copilot, source: ACM},
  abstract = {As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released in September 2023, functions as an interactive tool aimed at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot Chat's ability to fix the code smells. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot Chat in fixing these code smells employing different prompts. The results show that 8 out of 10 types of code smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1\%, showing promise in fixing Python code smells generated by Copilot itself. In addition, the effectiveness of Copilot Chat in fixing these smells can be improved by providing more detailed prompts.},
  address = {New York, NY, USA},
  series = {{ASE},
  isbn = {979-8-4007-1248-7},
}

@inproceedings{gsteiger_caribou_2024,
  title = {Caribou: {Fine},
  author = {Gsteiger, Viktor Urban and Long, Pin Hong (Daniel) and Sun, Yiran (Jerry) and Javanrood, Parshan and Shahrad, Mohammad},
  year = {2024},
  doi = {10.1145/3694715.3695954},
  url = {https://doi.org/10.1145/3694715.3695954},
  booktitle = {Proceedings of the {ACM},
  pages = {403--420},
  publisher = {Association for Computing Machinery},
  note = {event-place: Austin, TX, USA},
  keywords = {carbon-aware scheduling, cloud computing, geospatial shifting, serverless computing, sustainability, source: ACM},
  abstract = {Sustainability in computing is critical as environmental concerns rise. The cloud industry's carbon footprint is significant and rapidly growing. We show that dynamic geospatial shifting of cloud workloads to regions with lower carbon emission energy sources, particularly for more portable cloud workloads such as serverless applications, has a high potential to lower operational carbon emissions. To make the case, we build a comprehensive framework called Caribou that offloads serverless workflows across geo-distributed regions. Caribou requires no change in the application logic, nor on the provider side. It dynamically determines the best deployment plans, automatically (re-) deploys functions to appropriate regions, and redirects traffic to new endpoints. In reducing operational carbon through fine-grained, function-level offloading, Caribou does not undermine standard metrics such as performance and cost. We show how this approach can reduce the carbon footprint by an average of 22.9\% to 66.6\% across the North American continent. We demonstrate how a detailed specification of location constraints (e.g., to ensure compliance of one stage) can allow emission reductions for workflows (e.g., by offloading other stages). By showcasing the feasibility of carbon-aware geospatial application deployment, Caribou aims to push the boundaries of system techniques available to curtail cloud carbon emissions and provide a framework for future research.},
  address = {New York, NY, USA},
  series = {{SOSP},
  isbn = {979-8-4007-1251-7},
}

@inproceedings{legtchenko_storage_2025,
  title = {Storage {Class},
  author = {Legtchenko, Sergey and Stefanovici, Ioan and Black, Richard and Rowstron, Antony and Liu, Junyi and Costa, Paolo and Canakci, Burcu and Narayanan, Dushyanth and Wu, Xingbo},
  year = {2025},
  doi = {10.1145/3713082.3730381},
  url = {https://doi.org/10.1145/3713082.3730381},
  booktitle = {Proceedings of the 2025 {Workshop},
  pages = {111--118},
  publisher = {Association for Computing Machinery},
  note = {event-place: Banff, AB, Canada},
  keywords = {AI Inference, AI Infrastructure, Managed-Retention Memory, Memory},
  abstract = {AI clusters today are one of the major uses of High Bandwidth Memory (HBM). However, HBM is suboptimal for AI workloads for several reasons. Analysis shows HBM is overprovisioned on write performance, but underprovisioned on density and read bandwidth, and also has significant energy per bit overheads. It is also expensive, with lower yield than DRAM due to manufacturing complexity. We propose a new memory class: Managed-Retention Memory (MRM), which is more optimized to store key data structures for AI inference workloads. We believe that MRM may finally provide a path to viability for technologies that were originally proposed to support Storage Class Memory (SCM). These technologies traditionally offered long-term persistence (10+ years) but provided poor IO performance and/or endurance. MRM makes different trade-offs, and by understanding the workload IO patterns, MRM foregoes long-term data retention and write performance for better potential performance on the metrics important for these workloads.},
  address = {New York, NY, USA},
  series = {{HotOS},
  isbn = {979-8-4007-1475-7},
}

@inproceedings{sui_pre-warming_2024,
  title = {Pre-{Warming},
  author = {Sui, Yifan and Yu, Hanfei and Hu, Yitao and Li, Jianxun and Wang, Hao},
  year = {2024},
  doi = {10.1145/3698038.3698509},
  url = {https://doi.org/10.1145/3698038.3698509},
  booktitle = {Proceedings of the 2024 {ACM},
  pages = {178--195},
  publisher = {Association for Computing Machinery},
  note = {event-place: Redmond, WA, USA},
  keywords = {Cloud Computing, Cold-Start, Machine Learning, Serverless Computing},
  abstract = {Serverless computing has rapidly prospered as a new cloud computing paradigm with agile scalability, pay-as-you-go pricing, and ease-to-use features for Machine Learning (ML) inference tasks. Users package their ML code into lightweight serverless functions and execute them using containers. Unfortunately, a notorious problem, called cold-starts, hinders serverless computing from providing low-latency function executions. To mitigate cold-starts, pre-warming, which keeps containers warm predictively, has been widely accepted by academia and industry. However, pre-warming fails to eliminate the unique latency incurred by loading ML artifacts. We observed that for ML inference functions, the loading of libraries and models takes significantly more time than container warming. Consequently, pre-warming alone is not enough to mitigate the ML inference function's cold-starts.This paper introduces InstaInfer, an opportunistic preloading technique to achieve instant inference by eliminating the latency associated with loading ML artifacts, thereby achieving minimal time cost in function execution. InstaInfer fully utilizes the memory of warmed containers to preload the function's libraries and model, striking a balance between maximum acceleration and resource wastage. We design InstaInfer to be transparent to providers and compatible with existing pre-warming solutions. Experiments on OpenWhisk with real-world workloads show that InstaInfer reduces up to 93\% loading latency and achieves up to 8× speedup compared to state-of-the-art pre-warming solutions.},
  address = {New York, NY, USA},
  series = {{SoCC},
  isbn = {979-8-4007-1286-9},
}

@article{lu_hsg-rag_2025,
  title = {{HSG},
  author = {Lu, Zhouyang and Xu, Hailin and Chen, Anrui and Tang, Siyuan and Zhang, Junyi and Feng, Yifei and Pan, Wentao and Huang, Jiangli},
  year = {2025},
  doi = {10.1145/3731680},
  url = {https://doi.org/10.1145/3731680},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {embedded system, Knowledge base, large language model, retrieval augmented generation, source: ACM},
  abstract = {Customization is a fundamental aspect of embedded system development, requiring developers to acquire extensive domain-specific knowledge from technical documents. However, the sheer volume of these documents, coupled with the intricate relationships between their contents, makes it challenging to efficiently retrieve the necessary information. This challenge highlights the need for a structured approach to organize domain knowledge, with explicit representation of interrelationships. Moreover, while advanced large language models (LLMs) show promise in aiding embedded system development, they often lack the specialized knowledge needed to address domain-specific queries effectively. In this article, we present HSG-RAG, a knowledge base construction and retrieval method tailored for embedded system development that leverages knowledge graphs to represent the hierarchical structure within technical documentation. Unlike prior retrieval-augmented generation (RAG) or GraphRAG-based approaches, which build the index either rely on semantic similarity or keyword co-occurrence, HSG-RAG captures the inherited dependency and hierarchical relationships in the documents, which benefits the retrieval in both performance and efficiency. We also introduce a benchmark for evaluating the effectiveness of RAG systems in solving real-world challenges in embedded systems, particularly for multi-hop question answering. Experimental results show that HSG-RAG outperforms both RAG and GraphRAG, generating more specific and concise responses.},
  issn = {1084-4309},
  month = {oct},
}

@article{xiao_eda-copilot_2025,
  title = {{EDA},
  author = {Xiao, Zhe and He, Xu and Wu, Haoying and Yu, Bei and Guo, Yang},
  year = {2025},
  doi = {10.1145/3715326},
  url = {https://doi.org/10.1145/3715326},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {electronic design automation, Large language models, retrieval-augmented generation, RTL-to-GDSII, source: ACM},
  abstract = {With the rise of Large Language Models (LLMs), researchers have become increasingly interested in their applications in EDA flows, particularly in specific subdomains such as serving as knowledge assistants and generating RTL code. In this study, we present a Retrieval-Augmented Generation (RAG) framework tailored to EDA task processing, named EDA-Adaptive RAG. This framework addresses the implicit semantics of EDA data and facilitates efficient knowledge acquisition through classification and enhanced retrieval, significantly enhancing LLMs ability to acquire EDA knowledge. Furthermore, we aim to integrate RAG into the design process as an EDA assistant application. Using RTL code generation as a case study, we demonstrate that the performance of RTL code generation can be enhanced through highly relevant retrievals provided by our RAG. The experimental analysis involves EDA Q\&amp;A tasks and RTL code generation evaluation. It is shown that our method outperforms the latest works in terms of both answer stability and code quality.},
  issn = {1084-4309},
  month = {oct},
}

@article{xu_muralagent_2025,
  title = {{MuralAgent},
  author = {Xu, Zishan and Zhang, Xiaofeng and Yang, Yuqing and Chen, Wei and Liu, Jueting and Xu, Tingting and Wang, Zehua and El Saddik, Abdulmotaleb},
  year = {2025},
  doi = {10.1145/3743679},
  url = {https://doi.org/10.1145/3743679},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  volume = {21},
  number = {9},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {GPT-4V(ision), Mural, Outpainting, Retrieval-Augmented Generation, Stable Diffusion, source: ACM},
  abstract = {In the context of the digital age, utilizing cutting-edge technology for the digitization and creative expansion of ancient murals is crucial, aimed at preserving and passing on cultural heritage. Existing image outpainting techniques suffer from a lack of semantic guidance. This article introduces MuralAgent, a multimodal model based on Retrieval-Augmented Generation (RAG) technology. It precisely extracts key information from mural images and integrates it with a constructed ancient texts knowledge base to ensure the cultural and semantic consistency of the expanded images. Moreover, fine-tuning the Stable Diffusion model ensures the fidelity of the generated image styles. Specifically, this study involves constructing an ancient texts knowledge base for accurate matching, designing specific prompts for GPT-4V(ision) to extract key information, and innovatively expanding artworks through Stable Diffusion, providing a novel way for the public to reinterpret ancient murals.},
  issn = {1551-6857},
  month = {sep},
}

@article{arazzi_rag-ioe_2025,
  title = {{RAG},
  author = {Arazzi, Marco and Marconi Sciarroni, Monica and Nocera, Antonino and Storti, Emanuele},
  year = {2025},
  doi = {10.1145/3762669},
  url = {https://doi.org/10.1145/3762669},
  journal = {ACM Trans. Internet Things},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Context-aware, Industry 5.0, IoE, IoT, Knowledge Graph, Large Language Model, Retrieval-Augmented Generation, source: ACM},
  abstract = {Human-centric design, intelligence, and seamless interconnectivity are key pillars of the Industry 5.0. A critical challenge in these scenarios is the efficient retrieval of relevant, context-aware information for workers within Internet of Everything (IoE) networks. Traditional information retrieval techniques struggle with the heterogeneous, dynamic data generated in industrial settings. To address this, we define a context-aware data model for IoE scenarios, on top of which we propose RAG-IoE, a novel Retrieval-Augmented Generation (RAG) solution to enable adaptive, scalable, and context-based information retrieval from both structured and unstructured data sources. Our approach organizes IoE data within a semantic framework, integrating hybrid retrieval methods. It combines structured search on a Knowledge Graph with unstructured data retrieval using embeddings stored in a vector database, followed by LLM-driven reasoning to refine results. This architecture enhances decision-making, reduces cognitive overload, and ensures precise guidance for industrial operators. We validate the efficiency and effectiveness of RAG-IoE using a novel dataset through both a user study and quantitative analysis, demonstrating its potential to optimize human-machine collaboration in Industry 5.0 environments.},
  annote = {Just Accepted},
  month = {aug},
}

@article{lyu_crud-rag_2025,
  title = {{CRUD},
  author = {Lyu, Yuanjie and Li, Zhiyu and Niu, Simin and Xiong, Feiyu and Tang, Bo and Wang, Wenjin and Wu, Hao and Liu, Huanyong and Xu, Tong and Chen, Enhong},
  year = {2025},
  doi = {10.1145/3701228},
  url = {https://doi.org/10.1145/3701228},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Evaluation, Large Language Models, Retrieval-Augmented Generation, source: ACM},
  abstract = {Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate “hallucinated” content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types—create, read, update, and delete (CRUD). “Create” refers to scenarios requiring the generation of original, varied content. “Read” involves responding to intricate questions in knowledge-intensive situations. “Update” focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. “Delete” pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: .},
  issn = {1046-8188},
  month = {jan},
}

@article{zhao_chat2data_2024,
  title = {{Chat2Data},
  author = {Zhao, Xinyang and Zhou, Xuanhe and Li, Guoliang},
  year = {2024},
  doi = {10.14778/3685800.3685905},
  url = {https://doi.org/10.14778/3685800.3685905},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {12},
  pages = {4481--4484},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Traditional data analysis methods require users to write programming codes or issue SQL queries to analyze the data, which are inconvenient for ordinary users. Large language models (LLMs) can alleviate these limitations by enabling users to interact with the data with natural language (NL), e.g., result retrieval and summarization for unstructured data and transforming the NL text to SQL queries or codes for structured data. However, existing LLMs have three limitations: hallucination (due to lacking domain knowledge for vertical domains), high cost for LLM reasoning, and low accuracy for complicated tasks. To address these problems, we propose a prototype, Chat2Data, to interactively analyze the data with natural language. Chat2Data adopts a three-layer method, where the first layer uses Retrieval-Augmented Generation (RAG) to embed domain knowledge in order to address the hallucination problem, the second layer utilizes vector databases to reduce the number of interactions with LLMs so as to improve the performance, and the third layer designs a pipeline agent to decompose a complex task to multiple subtasks and use multiple round reasoning to generate the results in order to improve the accuracy of LLMs. We demonstrate Chat2Data with two real scenarios, unstructured data retrieval and summarization, and natural language-based structured data analysis. The online demo is available at http://vdemo.dbmind.cn.},
  issn = {2150-8097},
  month = {aug},
}

@article{wei_modelgen_2025,
  title = {{ModelGen},
  author = {Wei, Yangbo and Huang, Li and Feng, Qi and Chen, Zhanfei and Yan, Jinlong and Lin, Ting-Jung and Huang, Zhen and Ren, Kun and Xing, Wei and He, Lei},
  year = {2025},
  doi = {10.1145/3736165},
  url = {https://doi.org/10.1145/3736165},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI agent, bayesian optimization, device model, electronic design automation, large language models, Parameter extraction, source: ACM},
  abstract = {Device models require large numbers of parameters to characterize complex physical effects. Although the latest advancements in machine learning and automated tools have drastically improved efficiency over the classic methods, they still demand a considerable amount of human intervention in the loop to gain accuracy. This drastically limits further automation. Inspired by the success of Multimodal Large Language Models (MLLMs) in addressing tasks across diverse fields, we propose ModelGen, the first in-depth study to leverage MLLMs with RAG (Retrieval-Augmented Generation) to significantly reduce human effort in parameter extraction for compact model. Our contributions include (1) Automated Agentic Workflow Construction that learns to build and refine extraction workflows through iterative optimization, (2) MLLM Judge, a visual scoring mechanism that evaluates fitting quality using actual device characteristic plots rather than simple numerical metrics, and (3) Model-specific RAG for providing relevant domain knowledge during the extraction process. Experimental results demonstrate that ModelGen achieves a 26.8\%–33.1\% improvement in pass@1,3,5 compared to base LLM methods. The system completes complex model extractions for BSIMs and ASM-HEMT in hours (up to 168× faster) rather than days or weeks, making parameter extraction more accessible to non-experts while maintaining professional engineer-level accuracy.},
  issn = {1084-4309},
  month = {oct},
}

@article{ke_large_2025,
  title = {Large {Language},
  author = {Ke, Wenjun and Zheng, Yifan and Li, Yining and Xu, Hengyuan and Nie, Dong and Wang, Peng and He, Yao},
  year = {2025},
  doi = {10.1145/3768156},
  url = {https://doi.org/10.1145/3768156},
  journal = {ACM Trans. Inf. Syst.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Document Intelligence, Large Language Models, Long Context, RAG, source: ACM},
  abstract = {The rapid proliferation of documents has made document intelligence increasingly critical across various industries. In recent years, large language models (LLMs) have dramatically transformed the field of document intelligence, allowing for more advanced and accurate document processing solutions. Despite these advancements, most existing surveys have failed to focus on these breakthroughs, instead concentrating on traditional methods and earlier machine learning techniques. This survey seeks to fill that gap by offering an in-depth analysis of approximately 300 papers published between 2021 and mid-2025, thus providing a comprehensive overview of the impact of LLMs in document intelligence. The key topics explored include retrieval-augmented generation (RAG), long context processing, and fine-tuning LLMs for document comprehension. Furthermore, the survey highlights essential datasets, practical applications, current challenges, and future research directions, offering critical insights for both researchers and industry practitioners looking to advance the field.},
  annote = {Just Accepted},
  issn = {1046-8188},
  month = {sep},
}

@article{ouyang_dynagraph_2025,
  title = {{DynaGraph},
  author = {Ouyang, Jinhui and Zhu, Yijie and Deng, Hanhui and He, Jialyu and Wu, Di},
  year = {2025},
  doi = {10.1145/3749542},
  url = {https://doi.org/10.1145/3749542},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {9},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI Agent System, Spatial-Temporal Intelligence, Transportation Traffic Forecasting, source: ACM},
  abstract = {The precise prediction of multi-scale traffic is a ubiquitous challenge in the urbanization process for car owners, road administrators, and governments. In the case of complex road networks, current and past traffic information from both upstream and downstream roads are crucial since various road networks have different semantic information about traffic. Rationalizing the utilization of semantic information can realize short-term, long-term, and unseen road traffic prediction. As the demands of multi-scale traffic analysis increase, on-demand interactions and visualizations are expected to be available for transportation participants. We have designed a multi-scale traffic generation system, namely DynaGraph, using a multi-agent framework to process multi-scale traffic data, conduct multi-scale traffic analysis, and present multi-scale visualization results. DynaGraph consists of three essential AI agents: 1) a text-to-demand agent with deep thinking ability to interact with users and extract prediction tasks through texts or voice; 2) a traffic prediction agent that leverages multi-scale traffic data to generate temporal features and similarity, and fuse them with limited spatial features and similarity, to achieve accurate prediction of three tasks; and 3) a suggestion and visualization agent that uses the prediction results to generate suggestions and visualizations, providing users with a comprehensive understanding of traffic conditions. Our DynaGraph as a generic system focuses on addressing concerns about traffic prediction from transportation participants, and conducted extensive experiments on five real-world road datasets to demonstrate its competitive prediction accuracy, scalability, and superior interactive performance.},
  month = {sep},
}

@article{yu_cxxcrafter_2025,
  title = {{CXXCrafter},
  author = {Yu, Zhengmin and Zhang, Yuan and Wen, Ming and Nie, Yinan and Zhang, Wenhui and Yang, Min},
  year = {2025},
  doi = {10.1145/3729386},
  url = {https://doi.org/10.1145/3729386},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Agent, Software Building, source: ACM},
  abstract = {Project building is pivotal to support various program analysis tasks, such as generating intermediate representation code for static analysis and preparing binary code for vulnerability reproduction. However, automating the building process for C/C++ projects is a highly complex endeavor, involving tremendous technical challenges, such as intricate dependency management, diverse build systems, varied toolchains, and multifaceted error handling mechanisms. Consequently, building C/C++ projects often proves to be difficult in practice, hindering the progress of downstream applications. Unfortunately, research on facilitating the building of C/C++ projects remains to be inadequate. The emergence of Large Language Models (LLMs) offers promising solutions to automated software building. Trained on extensive corpora, LLMs can help unify diverse build systems through their comprehension capabilities and address complex errors by leveraging tacit knowledge storage. Moreover, LLM-based agents can be systematically designed to dynamically interact with the environment, effectively managing dynamic building issues. Motivated by these opportunities, we first conduct an empirical study to systematically analyze the current challenges in the C/C++ project building process. Particularly, we observe that most popular C/C++ projects encounter an average of five errors when relying solely on the default build systems. Based on our study, we develop an automated build system called CXXCrafter to specifically address the above-mentioned challenges, such as dependency resolution. Our evaluation on open-source software demonstrates that CXXCrafter achieves a success rate of 78\% in project building. Specifically, among the Top100 dataset, 72 projects are built successfully by both CXXCrafter and manual efforts, 3 by CXXCrafter only, and 14 manually only. Despite the slightly lower performance,CXXCrafter can save tremendous manual efforts and can also be easily applied to a wider range of applications automatically.},
  month = {jun},
}

@article{xia_demystifying_2025,
  title = {Demystifying {LLM},
  author = {Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  year = {2025},
  doi = {10.1145/3715754},
  url = {https://doi.org/10.1145/3715754},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI Software Engineer, Automated Program Repair, Autonomous Programming, Large Language Model, source: ACM},
  abstract = {Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless – an agentless approach to automatically resolve software development issues. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00\%, 96 correct fixes) and low cost (\$0.70) compared with all existing open-source software agents at the time of paper submission! Agentless also achieves more than 50\% solve rate when using Claude 3.5 Sonnet on the new SWE-bench Verified benchmark. In fact, Agentless has already been adopted by OpenAI as the go-to approach to showcase the real-world coding performance of both GPT-4o and the new o1 models; more recently, Agentless has also been used by DeepSeek to evaluate their newest DeepSeek V3 and R1 models. Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patches or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-𝑆 by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the currently overlooked potential of a simplistic, cost-effective technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction. We have open-sourced Agentless at: https://github.com/OpenAutoCoder/Agentless},
  month = {jun},
}

@article{lambiase_motivations_2024,
  title = {Motivations, {Challenges},
  author = {Lambiase, Stefano and Catolino, Gemma and Palomba, Fabio and Ferrucci, Filomena},
  year = {2024},
  doi = {10.1145/3704806},
  url = {https://doi.org/10.1145/3704806},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Bot, chatbot, literature review, software engineering, source: ACM},
  abstract = {Bots are software systems designed to support users by automating specific processes, tasks, or activities. When these systems implement a conversational component to interact with users, they are also known as conversational agents or chatbots. Bots—particularly in their conversation-oriented version and AI-powered—have seen increased adoption over time for software development and engineering purposes. Despite their exciting potential, which has been further enhanced by the advent of Generative AI and Large Language Models, bots still face challenges in terms of development and integration into the development cycle, as practitioners report that bots can add difficulties rather than provide improvements. In this work, we aim to provide a taxonomy for characterizing bots, as well as a series of challenges for their adoption in software engineering, accompanied by potential mitigation strategies. To achieve our objectives, we conducted a multivocal literature review, examining both research and practitioner literature. Through such an approach, we hope to contribute to both researchers and practitioners by providing (i) a series of future research directions to pursue, (ii) a list of strategies to adopt for improving the use of bots for software engineering purposes, and (iii) fostering technology and knowledge transfer from the research field to practice—one of the primary goals of multivocal literature reviews.},
  issn = {0360-0300},
  month = {dec},
}

@article{qayyum_llm-assisted_2025,
  title = {{LLM},
  author = {Qayyum, Khushboo and Jha, Chandan Kumar and Ahmadi-Pour, Sallar and Hassan, Muhammad and Drechsler, Rolf},
  year = {2025},
  doi = {10.1145/3733237},
  url = {https://doi.org/10.1145/3733237},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {bug fixing, bug identification, hardware description language, Large language model, source: ACM},
  abstract = {As technology continues to advance, it becomes increasingly integrated into daily life facilitating complex tasks across a range of environments. While some applications such as smartphones and smartwatches are less critical, others like healthcare devices and autonomous vehicles demand bug-free performance to prevent financial loss or harm. Traditionally, simulation-based testing and formal verification played a major role in ensuring a bug-free device. However, the simulation of bigger systems is limited to a definite number of scenarios on the Design under Verification\&nbsp;(DUV). Hence, it is unable to explore all possible inputs that can occur. Formal verification, on the other hand, offers a higher level of assurance through mathematical proofs but is both time-consuming and suffers from scalability issues, especially as designs grow in complexity. Recently, Large Language Models\&nbsp;(LLMs) have shown promise in tasks previously limited to human expertise. Their natural language processing capabilities can assist in handling extensive specifications and source code, particularly in debugging hardware descriptions and analyzing security and functionality. The utilization of Retrieval Augmented Generation\&nbsp;(RAG) has further enhanced LLMs by incorporating large specification or source code bases, thereby improving their bug-identification and correction capabilities. While recent advancements in LLMs, particularly with RAG, have yielded promising results in bug identification and correction for a small class of hardware bugs, significant gaps remain in their full potential for systematically addressing a wide range of hardware bugs. For instance, existing LLM methodologies struggle to detect bugs involving incorrect constant values, i.e., the use of wrong constants in source code. This limitation underscores the need for further exploration in utilizing LLMs to fully optimize the verification process. To bridge this gap, we propose a 3-phased 4-stage LLM-assisted systematic bug closure methodology that focuses on functional bugs in Verilog HDL rather than structural or syntactic issues. Our approach extracts functional properties of the DUV and systematically breaks down complex expressions into smaller sub-expressions to facilitate bug detection and correction. By employing RAG, the LLM is guided using the functional specifications and source code to identify and correct bugs. If the initial guidance through RAG is insufficient, our methodology initiates an iterative bug closure process. This includes incorporating more extensive information from the specifications, fetching additional lines of code for bug localization, and breaking down complex Verilog HDL expressions. In our comprehensive evaluation, we assess the LLM’s capabilities using 9 different categories of bugs. As benchmarks, we use 5 OpenTitan Intellectual Property\&nbsp;(IP) cores to demonstrate the scalability and effectiveness of our bug closure methodology where ≈ 60\% of the bugs were corrected. Specifically, we evaluate OpenAI’s GPT-4 in its ability to identify and correct functional bugs in Verilog HDL code.},
  issn = {1084-4309},
  month = {oct},
}

@article{liu_how_2024,
  title = {How {Can},
  author = {Liu, Jiangfeng and Ma, Xueliang and Wang, Lanyu and Pei, Lei},
  year = {2024},
  doi = {10.1145/3690391},
  url = {https://doi.org/10.1145/3690391},
  journal = {J. Comput. Cult. Herit.},
  volume = {17},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AIGC, Ancient Book Revitalization, ChatGPT, Computational Humanities, Generative AI, Intelligent Information Processing of Ancient Texts, source: ACM},
  abstract = {Generative AI changes the paradigm of natural language processing research, sets off a new trend of research in computational humanities and computational social sciences, and provides unique perspectives on digital intelligence-enabled ancient book revitalization and intelligent applications. The article explores the role of multimodal large models in image processing and OCR of ancient books. We discuss and exemplify how to use Large Language Models for intelligent information processing of ancient texts and explore combining prompt engineering, retrieval augmented generation (RAG), supervised fine-tuning, LangChain, and other techniques to improve performance in ancient text mining and applications. This article also looks forward to the broad prospect of intelligent agent technology combined with the Large Language Model in the innovative application of ancient book revitalization. The research focuses on digitizing ancient books, intelligent processing of ancient texts, and intelligent application of ancient book revitalization. It demonstrates the feasibility, advancement, and creativity of the application of generative AI and its derivative technologies in the field of computational humanities, especially in the field of ancient book preservation, to provide intelligent solutions for the dissemination of traditional thought and culture, from the perspective of the whole process of the technology of digital humanities and computational humanities research. The article also gives examples of the intelligent application of AI in the restoration of ancient books and the annotation of ancient texts. Although Large Language Models demonstrate transformative potential in advancing the field of ancient text research toward intelligent analysis, there remain certain limitations. This article points out their shortcomings in areas such as knowledge completion for ancient texts, understanding emotions and cultural nuances, as well as ethical and accountability issues. It emphasizes the need for a more balanced perspective on the role that generative AI plays in the exploration and utilization of cultural heritage.},
  issn = {1556-4673},
  month = {dec},
}

@article{choube_gloss_2025,
  title = {{GLOSS},
  author = {Choube, Akshat and Le, Ha and Li, Jiachen and Ji, Kaixin and Swain, Vedant Das and Mishra, Varun},
  year = {2025},
  doi = {10.1145/3749474},
  url = {https://doi.org/10.1145/3749474},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {9},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {digital health \&amp, large language models, mobile sensing, sensemaking, wearables, wellbeing, source: ACM},
  abstract = {The ubiquitous presence of smartphones and wearables has enabled researchers to build prediction and detection models for various health and behavior outcomes using passive sensing data from these devices. Achieving a high-level, holistic understanding of an individual's behavior and context, however, remains a significant challenge. Due to the nature of the passive sensing data, sensemaking — the process of interpreting and extracting insights - requires both domain knowledge and technical expertise, creating barriers for different stakeholders. Existing systems designed to support sensemaking are not open-ended or cannot perform complex data triangulation. In this paper, we present a novel sensemaking system, Group of LLMs for Open-ended Sensemaking (GLOSS), for open-ended sensemaking capable of performing complex multimodal triangulation to derive insights. We demonstrate that GLOSS significantly outperforms commonly used Retrieval-Augmented Generation (RAG) technique, achieving 87.93\% accuracy and 66.19\% consistency compared to RAG's 29.31\% accuracy and 52.85\% consistency. Furthermore, we showcase the promise of GLOSS using four use cases inspired by prior and ongoing work in UbiComp and HCI communities. Finally, we discuss the potential of GLOSS, the broader implications, and the limitations of our work.},
  month = {sep},
}

@article{hadadi_llm_2025,
  title = {{LLM},
  author = {Hadadi, Fatemeh and Xu, Qinghua and Bianculli, Domenico and Briand, Lionel},
  year = {2025},
  doi = {10.1145/3771283},
  url = {https://doi.org/10.1145/3771283},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {anomaly detection, data efficiency, ensemble learning, large language models, unstable logs, source: ACM},
  abstract = {Most log-based anomaly detectors assume logs are stable, though in reality they are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models — decision tree, k-nearest neighbors, and a feedforward neural network — with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points (pp) in F1 score while using much less labeled data (62.87 pp reduction). When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 pp increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications, except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog’s key components: cache, RAG, and ensemble learning.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {oct},
}

@article{omar_dialogue_2025,
  title = {Dialogue {Benchmark},
  author = {Omar, Reham and Mangukiya, Omij and Mansour, Essam},
  year = {2025},
  doi = {10.1145/3709681},
  url = {https://doi.org/10.1145/3709681},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {assertion-based validation, benchmarking, conversational question answering, cost-effecive inference, graph serialization, knowledge graphs (kgs), large language models (llms), retrieval-augumented generation (rag), source: ACM},
  abstract = {Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. Chatty-Gen decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that Chatty-Gen significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-4o, Gemini 1.5, Llama 3, and Mistral.},
  month = {feb},
}

@article{wang_unique_2025,
  title = {Unique {Security},
  author = {Wang, Shang and Zhu, Tianqing and Liu, Bo and Ding, Ming and Ye, Dayong and Zhou, Wanlei and Yu, Philip},
  year = {2025},
  doi = {10.1145/3764113},
  url = {https://doi.org/10.1145/3764113},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {agent, Large language models, model robustness, security and privacy risks, source: ACM},
  abstract = {With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable advancements in natural language processing. These models are trained on vast datasets to exhibit powerful language understanding and generation capabilities across various applications, including chatbots and agents. However, LLMs have revealed a variety of privacy and security issues throughout their life cycle, drawing significant academic and industrial attention. Moreover, the risks faced by LLMs differ significantly from those encountered by traditional language models. Given that current surveys lack a clear taxonomy of unique threat models across diverse scenarios, we emphasize the unique privacy and security threats associated with four specific scenarios: pre-training, fine-tuning, deployment, and LLM-based agents. Addressing the characteristics of each risk, this survey outlines and analyzes potential countermeasures. Research on attack and defense situations can offer feasible research directions, enabling more areas to benefit from LLMs.},
  issn = {0360-0300},
  month = {oct},
}

@article{torre_effect_2021,
  title = {The {Effect},
  author = {Torre, Ilaria and Carrigan, Emma and Domijan, Katarina and McDonnell, Rachel and Harte, Naomi},
  year = {2021},
  doi = {10.1145/3469232},
  url = {https://doi.org/10.1145/3469232},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {28},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {artificial agent, Multimodal emotional expression, smiling, social influence, source: ACM},
  abstract = {Emotional expressivity is essential for human interactions, informing both perception and decision-making. Here, we examine whether creating an audio-visual emotional channel mismatch influences decision-making in a cooperative task with a virtual character. We created a virtual character that was either congruent in its emotional expression (smiling in the face and voice) or incongruent (smiling in only one channel). People (N = 98) evaluated the character in terms of valence and arousal in an online study; then, visitors in a museum played the “lunar survival task” with the character over three experiments (N = 597, 78, 101, respectively). Exploratory results suggest that multi-modal expressions are perceived, and reacted upon, differently than unimodal expressions, supporting previous theories of audio-visual integration.},
  issn = {1073-0516},
  month = {nov},
}

@article{wang_netconfeval_2024,
  title = {{NetConfEval},
  author = {Wang, Changjie and Scazzariello, Mariano and Farshin, Alireza and Ferlin, Simone and Kostić, Dejan and Chiesa, Marco},
  year = {2024},
  doi = {10.1145/3656296},
  url = {https://doi.org/10.1145/3656296},
  journal = {Proc. ACM Netw.},
  volume = {2},
  number = {CoNEXT2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {benchmark, code generation, function calling, large language models (llms), network configuration, network synthesizer, p4, rag, routing algorithms, source: ACM},
  abstract = {This paper explores opportunities to utilize Large Language Models (LLMs) to make network configuration human-friendly, simplifying the configuration of network devices \&amp; development of routing algorithms and minimizing errors. We design a set of benchmarks (NetConfEval) to examine the effectiveness of different models in facilitating and automating network configuration. More specifically, we focus on the scenarios where LLMs translate high-level policies, requirements, and descriptions (i.e., specified in natural language) into low-level network configurations \&amp; Python code. NetConfEval considers four tasks that could potentially facilitate network configuration, such as (i) generating high-level requirements into a formal specification format, (ii) generating API/function calls from high-level requirements, (iii) developing routing algorithms based on high-level descriptions, and (iv) generating low-level configuration for existing and new protocols based on input documentation. Learning from the results of our study, we propose a set of principles to design LLM-based systems to configure networks. Finally, we present two GPT-4-based prototypes to (i) automatically configure P4-enabled devices from a set of high-level requirements and (ii) integrate LLMs into existing network synthesizers.},
  month = {jun},
}

@article{balaka_pneuma_2025,
  title = {Pneuma: {Leveraging},
  author = {Balaka, Muhammad Imam Luthfi and Alexander, David and Wang, Qiming and Gong, Yue and Krisnadhi, Adila and Castro Fernandez, Raul},
  year = {2025},
  doi = {10.1145/3725337},
  url = {https://doi.org/10.1145/3725337},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Finding relevant tables among databases, lakes, and repositories is the first step in extracting value from data. Such a task remains difficult because assessing whether a table is relevant to a problem does not always depend only on its content but also on the context, which is usually tribal knowledge known to the individual or team. While tools like data catalogs and academic data discovery systems target this problem, they rely on keyword search or more complex interfaces, limiting non-technical users' ability to find relevant data. The advent of large language models (LLMs) offers a unique opportunity for users to ask questions directly in natural language, making dataset discovery more intuitive, accessible, and efficient.In this paper, we introduce Pneuma, a retrieval-augmented generation (RAG) system designed to efficiently and effectively discover tabular data. Pneuma leverages large language models (LLMs) for both table representation and table retrieval. For table representation, Pneuma preserves schema and row-level information to ensure comprehensive data understanding. For table retrieval, Pneuma augments LLMs with traditional information retrieval techniques, such as full-text and vector search, harnessing the strengths of both to improve retrieval performance. To evaluate Pneuma, we generate comprehensive benchmarks that simulate table discovery workload on six real-world datasets including enterprise data, scientific databases, warehousing data, and open data. Our results demonstrate that Pneuma outperforms widely used table search systems (such as full-text search and state-of-the-art RAG systems) in accuracy and resource efficiency.},
  month = {jun},
}

@article{chen_automatic_2025,
  title = {Automatic {Database},
  author = {Chen, Sibei and Fan, Ju and Wu, Bin and Tang, Nan and Deng, Chao and Wang, Pengyi and Li, Ye and Tan, Jian and Li, Feifei and Zhou, Jingren and Du, Xiaoyong},
  year = {2025},
  doi = {10.1145/3709663},
  url = {https://doi.org/10.1145/3709663},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {database configuration debugging, retrieval-augmented generation, source: ACM},
  abstract = {Database management system (DBMS) configuration debugging, e.g., diagnosing poorly configured DBMS knobs and generating troubleshooting recommendations, is crucial in optimizing DBMS performance. However, the configuration debugging process is tedious and, sometimes challenging, even for seasoned database administrators (DBAs) with sufficient experience in DBMS configurations and good understandings of the DBMS internals (e.g., MySQL or Oracle). To address this difficulty, we propose Andromeda, a framework that utilizes large language models (LLMs) to enable automatic DBMS configuration debugging. Andromeda serves as a natural surrogate of DBAs to answer a wide range of natural language (NL) questions on DBMS configuration issues, and to generate diagnostic suggestions to fix these issues. Nevertheless, directly prompting LLMs with these professional questions may result in overly generic and often unsatisfying answers. To this end, we propose a retrieval-augmented generation (RAG) strategy that effectively provides matched domain-specific contexts for the question from multiple sources. They come from related historical questions, troubleshooting manuals and DBMS telemetries, which significantly improve the performance of configuration debugging. To support the RAG strategy, we develop a document retrieval mechanism addressing heterogeneous documents and design an effective method for telemetry analysis. Extensive experiments on real-world DBMS configuration debugging datasets show that Andromeda significantly outperforms existing solutions.},
  month = {feb},
}

@article{zhang_llm_2025,
  title = {{LLM},
  author = {Zhang, Ziyao and Wang, Chong and Wang, Yanlin and Shi, Ensheng and Ma, Yuchi and Zhong, Wanjun and Chen, Jiachi and Mao, Mingzhi and Zheng, Zibin},
  year = {2025},
  doi = {10.1145/3728894},
  url = {https://doi.org/10.1145/3728894},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {ISSTA},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Hallucination, Large Language Models, Repository-Level Code Generation, source: ACM, source: Google Scholar},
  abstract = {Code generation aims to automatically generate code from input requirements, significantly enhancing development efficiency. Recent large language models (LLMs) based approaches have shown promising results and revolutionized code generation task. Despite the promising performance, LLMs often generate contents with hallucinations, especially for the code generation scenario requiring the handling of complex contextual dependencies in practical development process. Although previous study has analyzed hallucinations in LLM-powered code generation, the study is limited to standalone function generation. In this paper, we conduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within more practical and complex development contexts in repository-level generation scenario. First, we manually examine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-generated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across different models. We then analyze causes of hallucinations and identify four potential factors contributing to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent effectiveness in all studied LLMs.},
  month = {jun},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_llm_2024,
  title = {{LLM},
  author = {Li, Guoliang and Zhou, Xuanhe and Zhao, Xinyang},
  year = {2024},
  doi = {10.14778/3685800.3685838},
  url = {https://doi.org/10.14778/3685800.3685838},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {12},
  pages = {4213--4216},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Machine learning techniques have been verified to be effective in optimizing data management systems and are widely researched in recent years. However, traditional small-sized ML models often struggle to generalize to new scenarios, and have limited context understanding ability (e.g., inputting discrete features only). The emergence of LLMs offers a promising solution to these challenges. LLMs have been trained over a vast number of scenarios and tasks and acquire human-competitive capabilities like context understanding and summarization, which can be highly beneficial for data management tasks (e.g., natural language based data analytics). In this tutorial, we present how to utilize LLMs to optimize data management systems and review new techniques for addressing these technical challenges, including hallucination of LLMs, high cost of interacting with LLMs, and low accuracy for processing complicated tasks. First, we discuss retrieval augmented generation (RAG) techniques to address the hallucination problem. Second, we present vector database techniques to improve the latency. Third, we present LLM agent techniques for processing complicated tasks by generating multi-round pipelines. We also showcase some real-world data management scenarios that can be well optimized by LLMs, including query rewrite, database diagnosis and data analytics. Finally, we summarize some open research challenges.},
  issn = {2150-8097},
  month = {aug},
}

@article{chi_reaccept_2025,
  title = {{REACCEPT},
  author = {Chi, Jianlei and Wang, Xiaotian and Huang, Yuhan and Yu, Lechen and Cui, Di and Sun, Jianguo and Sun, Jun},
  year = {2025},
  doi = {10.1145/3728930},
  url = {https://doi.org/10.1145/3728930},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {ISSTA},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Dynamic Validation, Large Language Model, Product-Test Co-evolution, Test Generation, source: ACM},
  abstract = {Synchronizing production and test code, known as PT co-evolution, is critical for software quality. Given the significant manual effort involved, researchers have tried automating PT co-evolution using predefined heuristics and machine learning models. However, existing solutions are still incomplete. Most approaches only detect and flag obsolete test cases, leaving developers to manually update them. Meanwhile, existing solutions may suffer from low accuracy, especially when applied to real-world software projects. In this paper, we propose ReAccept, a novel approach leveraging large language models (LLMs), retrievalaugmented generation (RAG), and dynamic validation to fully automate PT co-evolution with high accuracy. ReAccept employs an experience-guided approach to generate prompt templates for the identification and subsequent update processes. After updating a test case, ReAccept performs dynamic validation by checking syntax, verifying semantics, and assessing test coverage. If the validation fails, ReAccept leverages the error messages to iteratively refine the patch. To evaluate ReAccept's effectiveness, we conducted extensive experiments with a dataset of 537 Java projects and compared ReAccept's performance with several stateof-the-art methods. The evaluation results show that ReAccept achieved an update accuracy of 60.16\% on the correctly identified obsolete test code, surpassing the state-of-the-art technique CEPROT by 90\%. These findings demonstrate that ReAccept can effectively maintain test code, improve overall software quality, and significantly reduce maintenance effort.},
  month = {jun},
}

@article{chamotra_sage_2025,
  title = {{SAGE},
  author = {Chamotra, Saurabh and Barbhuiya, Ferdous},
  year = {2025},
  doi = {10.1145/3769011},
  url = {https://doi.org/10.1145/3769011},
  journal = {ACM Trans. Internet Things},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Finite State Machines (FSMs), GraphRAG, Honeypots, IoT Security, Large Language Models (LLM), LLM-Powered IoT Honeypots, RAGs, source: ACM},
  abstract = {Increasing security threats in Internet of Things (IoT) ecosystems necessitate advanced deception mechanisms that can engage adversaries and generate realistic interactions. Traditional IoT honeypots suffer from limited protocol emulation, static response mechanisms, and susceptibility to fingerprinting, making them ineffective against sophisticated attacks. To address these challenges, this paper introduces SAGE (State-Aware Graph-Enhanced Honeypot), an adaptive IoT honeypot that integrates Finite-State Machine (FSM)-driven protocol emulation with GraphRAG-enhanced retrieval. Unlike conventional honeypots, SAGE dynamically models stateful IoT protocols, ensuring structured and realistic request-response interactions. For undefined requests, GraphRAG retrieval selects relevant protocol knowledge from knowledge graphs and integrates it with FSM-derived contextual information, enabling the Large Language Model (LLM) to generate coherent and deception-resilient responses. Additionally, a fact-checking engine and feedback mechanism refine responses, mitigating hallucinations and ensuring protocol fidelity. A key feature of SAGE is its ability to create an IoT honeypot with limited protocol knowledge while progressively evolving its emulation depth by dynamically expanding its state-response mappings based on observed adversarial interactions. Experimental evaluation demonstrates that SAGE enhances deception robustness, improves adversary engagement, and mitigates honeypot fingerprinting risks. By combining FSM-based protocol modeling, GraphRAG-enhanced retrieval, and adaptive LLM-generated responses, SAGE establishes a scalable, intelligence-driven security framework that significantly advances IoT honeypot technology.},
  annote = {Just Accepted},
  month = {sep},
}

@article{xu_hlsrewriter_2025,
  title = {{HLSRewriter},
  author = {Xu, Kangwei and Zhang, Grace Li and Yin, Xunzhao and Zhuo, Cheng and Schlichtmann, Ulf and Li, Bing},
  year = {2025},
  doi = {10.1145/3749986},
  url = {https://doi.org/10.1145/3749986},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {electronic design automation, high-level synthesis, Large language models, source: ACM},
  abstract = {In High-Level Synthesis (HLS), refactoring a standard C/C++ code into its HLS-compatible version (HLS-C) still requires significant human effort. While various program scripts have been introduced to automate this process, the resulting code still contains many HLS-incompatible issues that need to be manually refactored and optimized by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated code refactoring and optimization in HLS. However, due to the limited training of LLMs, considering hardware and software simultaneously, hallucinations may occur when using LLMs for HLS, leading to synthesis failures. To address these challenges, we introduce HLSRewriter, an LLM-aided code refactoring and optimization framework that takes regular C/C++ code as input and automatically generates its corresponding optimized HLS-C code for hardware synthesis with minimal human intervention. To mitigate LLM hallucinations, a step-wise reasoning process is employed to analyze and detect HLS-incompatible errors. Afterwards, a repair library containing reference templates is efficiently created by scanning the HLS tool manual, followed by cooperation with a Retrieval-Augmented Generation (RAG) paradigm to guide the LLMs toward correct refactoring. In addition, a pipeline-aware decomposition strategy is introduced to progressively break down complex loop structures into smaller tasks with a balanced trade-off between latency and area, thereby enabling efficient pipelining and parallel execution. To further improve hardware efficiency, a bit width adjuster module is incorporated into this framework to optimize the precision of floating-point variables. Moreover, LLM-aided HLS optimization strategies are introduced to add/tune hardware directives in HLS-C code, thereby enhancing the performance of the final synthesized hardware. Experimental results demonstrate that the proposed LLM-aided framework can achieve higher refactoring pass rates and superior hardware performance in 24 real-world tasks compared with traditional approaches and the direct application of LLMs for code refactoring and optimization. The codes are open-sourced at this link: https://github.com/code-source1/catapult.},
  annote = {Just Accepted},
  issn = {1084-4309},
  month = {jul},
}

@article{chen_standing_2025,
  title = {Standing on the {Shoulders},
  author = {Chen, Mengzhuo and Liu, Zhe and Chen, Chunyang and Wang, Junjie and Wu, Boyu and Hu, Jun and Wang, Qing},
  year = {2025},
  doi = {10.1145/3715755},
  url = {https://doi.org/10.1145/3715755},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {GUI testing, Large language model, Mobile App, source: ACM},
  abstract = {In software development, similar apps often encounter similar bugs due to shared functionalities and implementation methods. However, current automated GUI testing methods mainly focus on generating test scripts to cover more pages by analyzing the internal structure of the app, without targeted exploration of paths that may trigger bugs, resulting in low efficiency in bug discovery. Considering that a large number of bug reports on open source platforms can provide external knowledge for testing, this paper proposes BugHunter, a novel bug-aware automated GUI testing approach that generates exploration paths guided by bug reports from similar apps, utilizing a combination of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). Instead of focusing solely on coverage, BugHunter dynamically adapts the testing process to target bug paths, thereby increasing bug detection efficiency. BugHunter first builds a high-quality bug knowledge base from historical bug reports. Then it retrieves relevant reports from this large bug knowledge base using a two-stage retrieval process, and generates test paths based on similar apps’ bug reports. BugHunter also introduces a local and global path-planning mechanism to handle differences in functionality and UI design across apps, and the ambiguous behavior or missing steps in the online bug reports. We evaluate BugHunter on 121 bugs across 71 apps and compare its performance against 16 state-of-the-art baselines. BugHunter achieves 60\% improvement in bug detection over the best baseline, with comparable or higher coverage against the baselines. Furthermore, BugHunter successfully detects 49 new crash bugs in real-world apps from Google Play, with 33 bugs fixed, 9 confirmed, and 7 pending feedback.},
  month = {jun},
}

@article{xie_opensearch-sql_2025,
  title = {{OpenSearch},
  author = {Xie, Xiangjin and Xu, Guangwei and Zhao, Lingyan and Guo, Ruijie},
  year = {2025},
  doi = {10.1145/3725331},
  url = {https://doi.org/10.1145/3725331},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucinations. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Furthermore, we introduce SQL-Like (an intermediate language), optimize the structured Chain-of-Thought (CoT) based on SQL-Like, and develop a dynamic few-shot strategy via self-taught Query-CoT-SQL. In terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3\% on the BIRD development set, 72.28\% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36\%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency.},
  month = {jun},
}

@article{tan_prompt-based_2025,
  title = {Prompt-based {Code},
  author = {Tan, Hanzhuo and Luo, Qi and Jiang, Ling and Zhan, Zizheng and Li, Jing and Zhang, Haotian and Zhang, Yuqun},
  year = {2025},
  doi = {10.1145/3725812},
  url = {https://doi.org/10.1145/3725812},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Code Completion, Multi-Retriever, Prompting, source: ACM},
  abstract = {Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92\% on the public benchmark CCEval, 3.19\% in HumanEval-Infilling, 2.80\% on our collected open-source benchmark suite, and 4.48\% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5\% improvement over the fine-tuned model.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {mar},
}

@article{ma_swe-gpt_2025,
  title = {{SWE},
  author = {Ma, Yingwei and Cao, Rongyu and Cao, Yongchang and Zhang, Yue and Chen, Jue and Liu, Yibo and Liu, Yuchen and Li, Binhua and Huang, Fei and Li, Yongbin},
  year = {2025},
  doi = {10.1145/3728981},
  url = {https://doi.org/10.1145/3728981},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {ISSTA},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Automated Program Repair, Automatic Software Engineering (ASE), Fault Localization, Large Language Models (LLMs), Software Engineering Agents, source: ACM},
  abstract = {Large language models (LLMs) have demonstrated remarkable performance in code generation, significantly enhancing the coding efficiency of developers. Recent advancements in LLM-based agents have led to significant progress in end-to-end automatic software engineering (ASE), particularly in software maintenance (e.g., fixing software issues) and evolution (e.g., adding new features). Despite these encouraging advances, current research faces two major challenges. First, state-of-the-art performance primarily depends on closed-source models like GPT-4, which significantly limits the technology’s accessibility, and potential for customization in diverse software engineering tasks. This dependence also raises concerns about data privacy, particularly when handling sensitive codebases. Second, these models are predominantly trained on static code data, lacking a deep understanding of the dynamic interactions, iterative problem-solving processes, and evolutionary characteristics inherent in software development. Consequently, they may face challenges in navigating complex project structures and generating contextually relevant solutions, which can affect their practical utility in real-world scenarios. To address these challenges, our study adopts a software engineering perspective. We recognize that real-world software maintenance and evolution processes encompass not only static code data but also developers’ thought processes, utilization of external tools, and the interaction between different functional personnel. Our objective is to develop an open-source large language model specifically optimized for software improvement, aiming to match the performance of closed-source alternatives while offering greater accessibility and customization potential. Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma SWE-GPT 7B and Lingma SWE-GPT 72B. By learning from and simulating real-world code submission activities, Lingma SWE-GPT systematically incorporates the dynamic interactions and iterative problem-solving inherent in software development process—such as repository understanding, fault localization, and patch generation—thereby achieving a more comprehensive understanding of software improvement processes. We conducted experimental evaluations using SWE-bench-Verified benchmark (comprising 500 real GitHub issues), recently proposed by OpenAI. The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30.20\% of the GitHub issues, marking a significant improvement in automatic issue resolution (22.76\% relative improvement compared to Llama 3.1 405B), approaching the performance of closed-source models (31.80\% issues of GPT-4o resolved). Notably, Lingma SWE-GPT 7B resolves 18.20\% of the issues, surpassing the 17.20\% resolution rate of Llama 3.1 70B, highlighting the potential for applying smaller models to ASE tasks.},
  month = {jun},
}

@article{he_large_2025,
  title = {Large {Language},
  author = {He, Zhuolun and Pu, Yuan and Wu, Haoyuan and Qiu, Tairu and Yu, Bei},
  year = {2025},
  doi = {10.1145/3736167},
  url = {https://doi.org/10.1145/3736167},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Large Language Model, source: ACM},
  abstract = {In this article, we explore the burgeoning intersection of large language models (LLMs) and electronic design automation (EDA). We critically assess whether LLMs represent a transformative future for EDA or merely a fleeting mirage. By organizing existing research into four critical domains of EDA—code generation, verification and debugging, knowledge representation and retrieval, and optimization/modeling—we provide a comprehensive overview of the current state-of-the-art. The survey concludes with a 5-level roadmap to guide the progressive integration and advancement of LLMs in EDA. Ultimately, this article aims to provide a comprehensive, evidence-based perspective on the role of LLMs in shaping the future of EDA.},
  issn = {1084-4309},
  month = {oct},
}

@article{grunde-mclaughlin_designing_2025,
  title = {Designing {LLM},
  author = {Grunde-McLaughlin, Madeleine and Lam, Michelle S. and Krishna, Ranjay and Weld, Daniel S. and Heer, Jeffrey},
  year = {2025},
  doi = {10.1145/3716134},
  url = {https://doi.org/10.1145/3716134},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {32},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {case studies, Crowdsourcing workflows, design space, large language model chains, source: ACM},
  abstract = {LLM chains enable complex tasks by decomposing work into a sequence of subtasks. Similarly, the more established techniques of crowdsourcing workflows decompose complex tasks into smaller tasks for human crowdworkers. Chains address LLM errors analogously to the way crowdsourcing workflows address human error. To characterize opportunities for LLM chaining, we survey 107 papers across the crowdsourcing and chaining literature to construct a design space for chain development. The design space covers a designer’s objectives and the tactics used to build workflows. We then surface strategies that mediate how workflows use tactics to achieve objectives. To explore how techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing workflows to implement LLM chains across three case studies: creating a taxonomy, shortening text, and writing a short story. From the design space and our case studies, we identify takeaways for effective chain design and raise implications for future research and development.},
  issn = {1073-0516},
  month = {jun},
}

@article{li_autosilicon_2025,
  title = {{AutoSilicon},
  author = {Li, Cangyuan and Chen, Chujie and Pan, Yudong and Xu, Wenjun and Liu, Yiqi and Chang, Kaiyan and Wang, Yujie and Wang, Mengdi and Li, Huawei and Han, Yinhe and Wang, Ying},
  year = {2025},
  doi = {10.1145/3737286},
  url = {https://doi.org/10.1145/3737286},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {agent, LLM, verilog, Verilog code generation, source: ACM},
  abstract = {Hardware description language (HDL) code designing is a critical component of the chip design process, requiring substantial engineering and time resources. Recent advancements in large language models (LLMs), such as GPT series, have shown promise in automating HDL code generation. However, current LLM-based approaches face significant challenges in meeting real-world hardware design requirements, particularly in handling complex designs and ensuring code correctness. Our evaluations reveal that the functional correctness rate of LLM-generated HDL code significantly decreases as design complexity increases. In this article, we propose the AutoSilicon framework, which aims to scale up the hardware design capability of LLMs. AutoSilicon incorporates an agent system, which (1) allows for the decomposition of large-scale, complex code design tasks into smaller, simpler tasks; (2) provides a compilation and simulation environment that enables LLMs to compile and test each piece of code it generates; and (3) introduces a series of optimization strategies. Experimental results demonstrate that AutoSilicon can scale hardware designs to projects with code equivalent to over 10,000 tokens. In terms of design quality, it further improves the syntax correctness rate and functional correctness rate compared with approaches that do not employ any extensions. For example, compared to directly generating HDL code using GPT-4-turbo, AutoSilicon enhances the syntax correctness rate by an average of 35.8\% and improves functional correctness by an average of 35.6\%.},
  issn = {1084-4309},
  month = {oct},
}

@article{zhu_large_2025,
  title = {Large {Language},
  author = {Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Liu, Zheng and Dou, Zhicheng and Wen, Ji-Rong},
  year = {2025},
  doi = {10.1145/3748304},
  url = {https://doi.org/10.1145/3748304},
  journal = {ACM Trans. Inf. Syst.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Fine-tuning, Information Retrieval, Large Language Models, Prompting, Query Rewriter, Reader, Reranking, source: ACM},
  abstract = {As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs) has revolutionized natural language processing due to their remarkable language understanding, generation, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, readers, and search agents.},
  annote = {Just Accepted},
  issn = {1046-8188},
  month = {sep},
}

@article{hoey_social_2025,
  title = {Social organization as the collective management of uncertainty},
  author = {Hoey, Jesse},
  year = {2025},
  doi = {10.1177/26339137251324131},
  url = {https://doi.org/10.1177/26339137251324131},
  journal = {Collective Intelligence},
  volume = {4},
  number = {1},
  note = {Place: USA
Publisher: Sage Publications, Inc.},
  keywords = {Bayesian learning, Complementarity, cultural structure, duality, free energy, source: ACM},
  abstract = {This paper explores the notion of complementarity in the modeling of social systems. Complementary variables in physics exhibit a duality, and similar dualities are found in the analysis of social, political, cultural, and neurophysiological structures. In this paper, I show that the management of uncertainty in a hierarchical model exhibits certain dualities that closely parallel those observed in social systems. I argue that, due to the necessity of coordination within collectives, agents will mirror each other to a large extent, and will develop heuristics and tactics that are aligned with how each agent is managing uncertainty dualities. I connect sociological literature on social structures with neuroscientific, economic, and psychological developments, and show a simplified derivation of these connections from free energy principles. I then explore how these connections can be used to gain insights into observable socio-cultural processes.},
  month = {mar},
}

@article{su_automated_2025,
  title = {Automated {Soap},
  author = {Su, Yanqi and Xing, Zhenchang and Wang, Chong and Chen, Chunyang and Xu, Sherry (Xiwei) and Lu, Qinghua and Zhu, Liming},
  year = {2025},
  doi = {10.1145/3715752},
  url = {https://doi.org/10.1145/3715752},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Knowledge Graph, Large Language Models, Soap Opera Testing, source: ACM},
  abstract = {Exploratory testing (ET) harnesses tester's knowledge, creativity, and experience to create varying tests that uncover unexpected bugs from the end-user's perspective. Although ET has proven effective in system-level testing of interactive systems, the need for manual execution has hindered large-scale adoption. In this work, we explore the feasibility, challenges and road ahead of automated scenario-based ET (a.k.a soap opera testing). We conduct a formative study, identifying key insights for effective manual soap opera testing and challenges in automating the process. We then develop a multi-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to automate soap opera testing. The system consists of three multi-modal agents, Planner, Player, and Detector that collaborate to execute tests and identify potential bugs. Experimental results demonstrate the potential of automated soap opera testing, but there remains a significant gap compared to manual execution, especially under-explored scenario boundaries and incorrectly identified bugs. Based on the observation, we envision road ahead for the future of automated soap opera testing, focusing on three key aspects: the synergy of neural and symbolic approaches, human-AI co-learning, and the integration of soap opera testing with broader software engineering practices. These insights aim to guide and inspire the future research.},
  month = {jun},
}

@article{yuan_improving_2025,
  title = {Improving {Workplace},
  author = {Yuan, Aijia and Garcia Colato, Edlin and Pescosolido, Bernice and Song, Hyunju and Samtani, Sagar},
  year = {2025},
  doi = {10.1145/3701041},
  url = {https://doi.org/10.1145/3701041},
  journal = {ACM Trans. Manage. Inf. Syst.},
  volume = {16},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {chatbots, conversational agents, Large language models, mental health, well-being, workplace, source: ACM},
  abstract = {The global rise in mental disorders, particularly in workplaces, necessitated innovative and scalable solutions for delivering therapy. Large Language Model (LLM)-based mental health chatbots have rapidly emerged as a promising tool for overcoming the time, cost, and accessibility constraints often associated with traditional mental health therapy. However, LLM-based mental health chatbots are in their nascency, with significant opportunities to enhance their capabilities to operate within organizational contexts. To this end, this research seeks to examine the role and development of LLMs in mental health chatbots over the past half-decade. Through our review, we identified over 50 mental health-related chatbots, including 22 LLM-based models targeting general mental health, depression, anxiety, stress, and suicide ideation. These chatbots are primarily used for emotional support and guidance but often lack capabilities specifically designed for workplace mental health, where such issues are increasingly prevalent. The review covers their development, applications, evaluation, ethical concerns, integration with traditional services, LLM-as-a-Service, and various other business implications in organizational settings. We provide a research illustration of how LLM-based approaches could overcome the identified limitations and also offer a system that could help facilitate systematic evaluation of LLM-based mental health chatbots. We offer suggestions for future research tailored to workplace mental health needs.},
  issn = {2158-656X},
  month = {feb},
}

@article{xue_demonstration_2024,
  title = {Demonstration of {DB},
  author = {Xue, Siqiao and Qi, Danrui and Jiang, Caigao and Cheng, Fangyin and Chen, Keting and Zhang, Zhiping and Zhang, Hongyang and Wei, Ganglin and Zhao, Wang and Zhou, Fan and Yi, Hong and Liu, Shaodong and Yang, Hongjun and Chen, Faqiang},
  year = {2024},
  doi = {10.14778/3685800.3685876},
  url = {https://doi.org/10.14778/3685800.3685876},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {12},
  pages = {4365--4368},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility. DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert. Its system design supports deployment across local, distributed, and cloud environments. Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL). The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs. Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily. The code of DB-GPT is available at Github.},
  issn = {2150-8097},
  month = {aug},
}

@article{yao_hdldebugger_2025,
  title = {{HDLdebugger},
  author = {Yao, Xufeng and Li, Haoyang and Chan, Tsz Ho and Xiao, Wenyi and Yuan, Mingxuan and Huang, Yu and Chen, Lei and Yu, Bei},
  year = {2025},
  doi = {10.1145/3735638},
  url = {https://doi.org/10.1145/3735638},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Code Debugging, Large Language Model, Retrieval Augmented Generation, source: ACM},
  abstract = {In the domain of chip design, hardware description languages (HDLs) play a pivotal role. However, due to the inherent complexity of HDLs and the scarcity of high-quality debugging resources, HDL bug fixing remains a challenging and time-consuming task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of large language models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Industry, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.},
  issn = {1084-4309},
  month = {oct},
}

@article{xu_large_2025-1,
  title = {Large {Language},
  author = {Xu, Hanxiang and Wang, Shenao and Li, Ningke and Wang, Kailong and Zhao, Yanjie and Chen, Kai and Yu, Ting and Liu, Yang and Wang, Haoyu},
  year = {2025},
  doi = {10.1145/3769676},
  url = {https://doi.org/10.1145/3769676},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Cybersecurity, Large language model, Software security, source: ACM},
  abstract = {The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in a variety of application domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity\&nbsp;(LLM4Security). By comprehensively collecting over 40K relevant papers and systematically analyzing 185 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain.Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to an expanding range of cybersecurity tasks, including vulnerability detection, malware analysis, and network intrusion detection. Second, we analyze application trends of different LLM architectures (such as encoder-only, encoder-decoder, and decoder-only) across security domains. Third, we identify increasingly sophisticated techniques for adapting LLMs to cybersecurity, such as advanced fine-tuning, prompt engineering, and external augmentation strategies. A significant emerging trend is the use of LLM-based autonomous agents, which represent a paradigm shift from single-task execution to orchestrating complex, multi-step security workflows. Furthermore, we find that the datasets used for training and evaluating LLMs are often limited, highlighting the need for more comprehensive datasets and the use of LLMs for data augmentation. Finally, we discuss the main challenges and opportunities for future research, including the need for more interpretable models, addressing the inherent security risks of LLMs, and their potential for proactive defense.Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research. We believe that the insights and findings presented in this survey will contribute to the growing body of knowledge on the application of LLMs in cybersecurity and provide valuable guidance for researchers and practitioners working in this field.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {sep},
}

@article{sun_script-strategy_2025,
  title = {Script-{Strategy},
  author = {Sun, Xin and de Wit, Jan and Li, Zhuying and Pei, Jiahuan and El Ali, Abdallah and Bosch, Jos A.},
  year = {2025},
  doi = {10.1145/3757655},
  url = {https://doi.org/10.1145/3757655},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although advances in large language models (LLMs) offer potential for more flexible interactions, their lack of controllability and explanability poses challenges in high-stakes contexts like psychotherapy. To address this, we conducted two studies in this work to explore how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. In Study 1 (N=43), an online experiment with a within-subjects design, we compared rule-based, pure LLM, and LLMs aligned with expert-crafted scripts via fine-tuning and prompting. Results showed that aligned LLMs significantly outperformed the other types of chatbots in empathy, dialogue relevance, and adherence to therapeutic principles. Building on findings, we proposed ”Script-Strategy Aligned Generation (SSAG)”, a more flexible alignment approach that reduces reliance on fully scripted content while maintaining LLMs' therapeutic adherence and controllability. In a 10-day field Study 2 (N=21), SSAG achieved comparable therapeutic effectiveness to full-scripted LLMs while requiring less than 40\% of expert-crafted dialogue content. Beyond these results, this work advances LLM applications in psychotherapy by providing a controllable and scalable solution, reducing reliance on expert effort. By enabling domain experts to align LLMs through high-level strategies rather than full scripts, SSAG supports more efficient co-development and expands access to a broader context of psychotherapy.},
  month = {oct},
}

@article{yang_autoverus_2025,
  title = {{AutoVerus},
  author = {Yang, Chenyuan and Li, Xuheng and Misu, Md Rakib Hossain and Yao, Jianan and Cui, Weidong and Gong, Yeyun and Hawblitzel, Chris and Lahiri, Shuvendu and Lorch, Jacob R. and Lu, Shuai and Yang, Fan and Zhou, Ziqiao and Lu, Shan},
  year = {2025},
  doi = {10.1145/3763174},
  url = {https://doi.org/10.1145/3763174},
  journal = {Proc. ACM Program. Lang.},
  volume = {9},
  number = {OOPSLA2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Generative AI has shown its value for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses LLMs to automatically generate correctness proof for Rust code. AutoVerus is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AutoVerus consists of a network of agents that are crafted and orchestrated to mimic human experts' three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AutoVerus and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90\% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.},
  month = {oct},
}

@article{wang_survey_2025,
  title = {Survey on {Factuality},
  author = {Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Guo, Qipeng and Hu, Xiangkun and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Hu, Xuming and Qi, Zehan and Gao, Wenyang and Wang, Yidong and Yang, Linyi and Wang, Jindong and Xie, Xing and Zhang, Zheng and Zhang, Yue},
  year = {2025},
  doi = {10.1145/3742420},
  url = {https://doi.org/10.1145/3742420},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Factuality, knowledge, large language models, retrieval augmentation, source: ACM},
  abstract = {This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the “factuality issue” as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs. We consistently maintain and update the related open-source materials at .},
  issn = {0360-0300},
  month = {sep},
}

@article{dick_introduction_2025,
  title = {Introduction to {Special},
  author = {Dick, Robert Paul and Pearce, Hammond and Shang, Li and Yang, Fan},
  year = {2025},
  doi = {10.1145/3746636},
  url = {https://doi.org/10.1145/3746636},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {artificial intelligence, Design automation, hardware, large language models, machine learning, software, source: ACM},
  abstract = {Large Language Models are having a substantial impact on electronic design automation in areas ranging from hardware architecture to verification and optimization. The special issue provides a snapshot of work on this topic. This introduction describes and provides context for the research area, describes the organization of the special issue, and provides terse summaries of each of its papers.},
  issn = {1084-4309},
  month = {oct},
}

@article{pernici_sustainable_2025,
  title = {Sustainable quality in data preparation},
  author = {Pernici, Barbara and Cappiello, Cinzia and Bono, Carlo Alberto and Sancricca, Camilla and Catarci, Tiziana and Angelini, Marco and Filosa, Matteo and Palmonari, Matteo and De Paoli, Flavio and Bergamaschi, Sonia and Simonini, Giovanni and Mozzillo, Angelo and Zecchini, Luca},
  year = {2025},
  doi = {10.1145/3769120},
  url = {https://doi.org/10.1145/3769120},
  journal = {J. Data and Information Quality},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Data preparation, Data quality, Sustainability, source: ACM},
  abstract = {Data preparation is crucial for achieving good data management following the four foundational FAIR principles — Findability, Accessibility, Interoperability, and Reusability. Processing datasets to achieve high data (and metadata) quality is mandatory in modern applications. However, the data preparation activities that are needed to reach such levels may easily become unsustainable due to, for example, resource intensity or scalability challenges. Moreover, some preparation efforts may become unnecessary if they result in negligible improvements or duplicate actions. This paper examines the sustainability aspects of data preparation through the lens of a circular economy. Within the data landscape, this perspective encourages practices that minimize waste, extend the data life cycle, and maximize reuse in alignment with the FAIR principles. We explore these practices and their impact on selecting and configuring effective data preparation strategies to design sustainable, high-quality pipelines. To this end, we propose an evaluation model that integrates data quality metrics with sustainability parameters for human and computational tasks. Finally, we apply the model in a comparative analysis of key data preparation methods, demonstrating its effectiveness in assessing sustainability and quality trade-offs.},
  annote = {Just Accepted},
  issn = {1936-1955},
  month = {oct},
}

@article{chen_llm_2025,
  title = {{LLM},
  author = {Chen, Daihang and Liu, Yonghui and Zhou, Mingyi and Zhao, Yanjie and Wang, Haoyu and Wang, Shuai and Chen, Xiao and Bissyandé, Tegawendé F. and Klein, Jacques and Li, Li},
  year = {2025},
  doi = {10.1145/3708528},
  url = {https://doi.org/10.1145/3708528},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {34},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {LLM, Mobile, On-device model, Security, source: ACM},
  abstract = {When mobile meets LLMs, mobile app users deserve to have more intelligent usage experiences. For this to happen, we argue that there is a strong need to apply LLMs for the mobile ecosystem. We therefore provide a research roadmap for guiding our fellow researchers to achieve that as a whole. In this roadmap, we sum up six directions that we believe are urgently required for research to enable native intelligence in mobile devices. In each direction, we further summarize the current research progress and the gaps that still need to be filled by our fellow researchers.},
  issn = {1049-331X},
  month = {may},
}

@article{sheng_llms_2025,
  title = {{LLMs},
  author = {Sheng, Ze and Chen, Zhicheng and Gu, Shuning and Huang, Heqing and Gu, Guofei and Huang, Jeff},
  year = {2025},
  doi = {10.1145/3769082},
  url = {https://doi.org/10.1145/3769082},
  journal = {ACM Comput. Surv.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Cybersecurity, Large Language Models, Vulnerability Detection, source: ACM},
  abstract = {Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection. Traditional methods, including static and dynamic analysis, face limitations in efficiency, false-positive rates, and scalability with modern software complexity. Through code structure analysis, pattern identification, and repair suggestion generation, LLMs demonstrate a novel approach to vulnerability mitigation. This survey examines LLMs in vulnerability detection, analyzing problem formulation, model selection, application methodologies, datasets, and evaluation metrics. We investigate current research challenges, emphasizing cross-language detection, multimodal integration, and repository-level analysis. Based on our findings, we propose solutions addressing dataset scalability, model interpretability, and low-resource scenarios. Our contributions include: (1) a systematic analysis of LLM applications in vulnerability detection; (2) a unified framework examining patterns and variations across studies; and (3) identification of key challenges and research directions. This work advances the understanding of LLM-based vulnerability detection. The latest findings are maintained at https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection},
  annote = {Just Accepted},
  issn = {0360-0300},
  month = {sep},
}

@article{yang_patch_2025,
  title = {Patch {Generation},
  author = {Yang, Yaopeng and Li, Chuanyi and Han, Zhifeng and Li, Rui and Xu, Kui and Li, Qingyuan and Zhong, Wenkang and Shen, Zongwen and Fei, Zhiwei and Ge, Jidong and Luo, Bin},
  year = {2025},
  doi = {10.1145/3764584},
  url = {https://doi.org/10.1145/3764584},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {APR-specific information, Automated Program Repair, Large Language Model, source: ACM},
  abstract = {Automated Program Repair (APR) is a crucial task in software development and maintenance, aiming to patch software bugs automatically without human intervention. The rise of Large Language Models (LLMs) has significantly advanced APR. However, as researchers delve deeper into APR as a downstream task for LLMs, a key challenge remains: how to effectively integrate APR-specific information to complement LLMs capabilities. This survey revisits 124 existing APR studies, most from 2021 to 2024, from two perspectives: Utilizing LLMs and APR-specific Information. First, we define the concept of APR-specific information. Then, we summarize techniques for utilizing LLMs in patch generation in four dimensions. Next, we explore critical factors influencing the effectiveness of generating patches by LLMs, such as prompting context and fine-tuning configurations. After that, we focus on the evaluation of generated patches, including benchmarks, reasoning cost scaling, etc. Furthermore, we distill all APR-specific information (e.g., bug-fix pairs and error messages), highlighting their unique importance and features. Finally, we comprehensively outline challenges, limitations, and potential future research directions in APR in the LLM era. To conclude, by adopting the two perspectives, we aim to provide valuable insights into mining and leveraging APR-specific information in utilizing LLMs process for the APR community.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {aug},
}

@article{cao_lego-graphrag_2025,
  title = {{LEGO},
  author = {Cao, Yukun and Gao, Zengyi and Li, Zhiyang and Xie, Xike and Zhou, S. Kevin and Xu, Jianliang},
  year = {2025},
  doi = {10.14778/3748191.3748194},
  url = {https://doi.org/10.14778/3748191.3748194},
  journal = {Proc. VLDB Endow.},
  volume = {18},
  number = {10},
  pages = {3269--3283},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {GraphRAG integrates (knowledge) graphs with large language models (LLMs) to improve reasoning accuracy and contextual relevance. Despite its promising applications and strong relevance to multiple research communities, such as databases and natural language processing, GraphRAG currently lacks modular workflow analysis, systematic solution frameworks, and insightful empirical studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2) systematic classification of existing techniques and implemented GraphRAG instances, and 3) creation of new GraphRAG instances. Our framework facilitates comprehensive empirical studies of GraphRAG on large-scale real-world graphs and diverse query sets, revealing insights into balancing reasoning quality, runtime efficiency, and token or GPU cost, that are essential for building advanced GraphRAG systems.},
  issn = {2150-8097},
  month = {sep},
}

@article{levin_chatdbg_2025,
  title = {{ChatDBG},
  author = {Levin, Kyla H. and van Kempen, Nicolas and Berger, Emery D. and Freund, Stephen N.},
  year = {2025},
  doi = {10.1145/3729355},
  url = {https://doi.org/10.1145/3729355},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Artificial Intelligence, Debugging, Software Engineering, source: ACM},
  abstract = {Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67\% of the time; one additional follow-up query increased the success rate to 85\%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.},
  month = {jun},
}

@article{li_matching_2025,
  title = {From {Matching},
  author = {Li, Xiaoxi and Jin, Jiajie and Zhou, Yujia and Zhang, Yuyao and Zhang, Peitian and Zhu, Yutao and Dou, Zhicheng},
  year = {2025},
  doi = {10.1145/3722552},
  url = {https://doi.org/10.1145/3722552},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Generative Document Retrieval, Generative Information Retrieval, Reliable Response Generation, source: ACM},
  abstract = {Information Retrieval (IR) systems are crucial tools for users to access information, which have long been dominated by traditional methods relying on similarity matching. With the advancement of pre-trained language models, Generative Information Retrieval (GenIR) emerges as a novel paradigm, attracting increasing attention. Based on the form of information provided to users, current research in GenIR can be categorized into two aspects: (1) Generative Retrieval (GR) leverages the generative model’s parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. (2) Reliable Response Generation employs language models to directly generate information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching while offering flexibility, efficiency, and creativity to meet practical needs. This article aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training and structure, document identifier, incremental learning, and so on, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, and so on. We also review the evaluation, challenges, and future developments in GenIR systems. This review aims to offer a comprehensive reference for researchers, encouraging further development in the GenIR field (Github Repository: ).},
  issn = {1046-8188},
  month = {may},
}

@article{zhou_large_2025,
  title = {Large {Language},
  author = {Zhou, Xin and Cao, Sicong and Sun, Xiaobing and Lo, David},
  year = {2025},
  doi = {10.1145/3708522},
  url = {https://doi.org/10.1145/3708522},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {34},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {large language models, Literature review, vulnerability detection, vulnerability repair, source: ACM},
  abstract = {The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, encompassing 43 papers published across 25 distinct venues, along with 15 high-quality preprint papers, bringing the total to 58 papers. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of limitations of existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.},
  issn = {1049-331X},
  month = {may},
}

@article{assi_llm-cure_2025,
  title = {{LLM},
  author = {Assi, Maram and Hassan, Safwat and Zou, Ying},
  year = {2025},
  doi = {10.1145/3744644},
  url = {https://doi.org/10.1145/3744644},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {The exponential growth of the mobile app market underscores the importance of constant innovation and rapid response to user demands. As user satisfaction is paramount to the success of a mobile application (app), developers typically rely on user reviews, which represent user feedback that includes ratings and comments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in manual analysis, necessitating automated approaches. Existing automated approaches either analyze only the target app's reviews, neglecting the comparison of similar features to competitors or fail to provide suggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM)-based Competitive User Review Analysis for Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically generate suggestions for mobile app feature improvements. More specifically, LLM-Cure identifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a user review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint and proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the state-of-the-art approaches in assigning features to reviews by up to 13\% in F1-score, up to 16\% in recall and up to 11\% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving user complaints. We verify the suggestions using the release notes that reflect the changes of features in the target mobile app. LLM-Cure achieves a promising average of 73\% of the implementation of the provided suggestions, demonstrating its potential for competitive feature enhancement.},
  month = {jun},
  annote = {Just Accepted},
  issn = {1049-331X},
}

@article{yu_cashift_2025,
  title = {{CAShift},
  author = {Yu, Jiongchi and Xie, Xiaofei and Hu, Qiang and Zhang, Bowen and Zhao, Ziming and Lin, Yun and Ma, Lei and Feng, Ruitao and Liauw, Frank},
  year = {2025},
  doi = {10.1145/3729346},
  url = {https://doi.org/10.1145/3729346},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {FSE},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Anomaly Detection, Cloud Native Systems, Intrusion Detection, Log Analysis, Normality Shift, Software Vulnerabilities, source: ACM},
  abstract = {With the rapid advancement of cloud-native computing, securing cloud environments has become an important task. Log-based Anomaly Detection (LAD) is the most representative technique used in different systems for attack detection and safety guarantee, where multiple LAD methods and relevant datasets have been proposed. However, even though some of these datasets are specifically prepared for cloud systems, they only cover limited cloud behaviors and lack information from a whole-system perspective. Another critical issue to consider is normality shift, which implies that the test distribution could differ from the training distribution and highly affect the performance of LAD. Unfortunately, existing works only focus on simple shift types such as chronological changes, while other cloud-specific shift types are ignored, e.g., different deployed cloud architectures. Therefore, a dataset that captures diverse cloud system behaviors and various types of normality shifts is essential. To fill this gap, we construct a dataset CAShift to evaluate the performance of LAD in cloud, which considers different roles of software in cloud systems, supports three real-world normality shift types (application shift, version shift, and cloud architecture shift), and features 20 different attack scenarios in various cloud system components. Based on CAShift, we conduct a comprehensive empirical study to investigate the effectiveness of existing LAD methods in normality shift scenarios. Additionally, to explore the feasibility of shift adaptation, we further investigate three continuous learning approaches, which are the most common methods to mitigate the impact of distribution shift. Results demonstrated that 1) all LAD methods suffer from normality shift where the performance drops up to 34\%, and 2) existing continuous learning methods are promising to address shift drawbacks, but the ratio of data used for model retraining and the selection of algorithms highly affect the shift adaptation, with an increase in the F1-Score of up to 27\%. Based on our findings, we offer valuable implications for future research in designing more robust LAD models and methods for LAD shift adaptation.},
  month = {jun},
}

@article{li_urban_2025,
  title = {Urban {Computing},
  author = {Li, Zhonghang and Xia, Lianghao and Ren, Xubin and Tang, Jiabin and Chen, Tianyi and Xu, Yong and Huang, Chao},
  year = {2025},
  doi = {10.1145/3768163},
  url = {https://doi.org/10.1145/3768163},
  journal = {ACM Trans. Intell. Syst. Technol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Large Language Models (LLMs), Spatio-Temporal Data Mining, Transportation, Urban Computing, source: ACM},
  abstract = {Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs’ functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing.},
  annote = {Just Accepted},
  issn = {2157-6904},
  month = {sep},
}

@article{xie_chatts_2025,
  title = {{ChatTS},
  author = {Xie, Zhe and Li, Zeyan and He, Xiao and Xu, Longlong and Wen, Xidao and Zhang, Tieying and Chen, Jianjun and Shi, Rui and Pei, Dan},
  year = {2025},
  doi = {10.14778/3742728.3742735},
  url = {https://doi.org/10.14778/3742728.3742735},
  journal = {Proc. VLDB Endow.},
  volume = {18},
  number = {8},
  pages = {2385--2398},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Understanding time series is crucial for its application in real-world scenarios. Recently, large language models (LLMs) have been increasingly applied to time series tasks, leveraging their strong language capabilities to enhance various applications. However, research on multimodal LLMs (MLLMs) for time series understanding and reasoning remains limited, primarily due to the scarcity of high-quality datasets that align time series with textual information. This paper introduces ChatTS, a novel MLLM designed for time series analysis. ChatTS treats time series as a modality, similar to how vision MLLMs process images, enabling it to perform both understanding and reasoning with time series. To address the scarcity of training data, we propose an attribute-based method for generating synthetic time series and Time Series Evol-Instruct to generates diverse Q\&amp;As for enhanced reasoning capabilities. To the best of our knowledge, ChatTS is the first MLLM that takes multivariate time series as input for understanding and reasoning, which is fine-tuned exclusively on synthetic datasets. We evaluate its performance using benchmark datasets with real-world data, including six alignment tasks and four reasoning tasks. Our results show that ChatTS significantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0\% improvement in alignment tasks and a 25.8\% improvement in reasoning tasks. We have open-sourced the source code, model checkpoint and datasets at https://github.com/NetManAIOps/ChatTS.},
  issn = {2150-8097},
  month = {sep},
}

@article{arakawa_prism-qamp_2024,
  title = {{PrISM},
  author = {Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank},
  year = {2024},
  doi = {10.1145/3699759},
  url = {https://doi.org/10.1145/3699759},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {8},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {context-aware, large language models, procedure tracking, question answering, task assistance, source: ACM},
  abstract = {Voice assistants capable of answering user queries during various physical tasks have shown promise in guiding users through complex procedures. However, users often find it challenging to articulate their queries precisely, especially when unfamiliar with the specific terminologies required for machine-oriented tasks. We introduce PrISM-Q\&amp;A, a novel question-answering (Q\&amp;A) interaction termed step-aware Q\&amp;A, which enhances the functionality of voice assistants on smartwatches by incorporating Human Activity Recognition (HAR) and providing the system with user context. It continuously monitors user behavior during procedural tasks via audio and motion sensors on the watch and estimates which step the user is performing. When a question is posed, this contextual information is supplied to Large Language Models (LLMs) as part of the context used to generate a response, even in the case of inherently vague questions like "What should I do next with this?" Our studies confirmed that users preferred the convenience of our approach compared to existing voice assistants. Our real-time assistant represents the first Q\&amp;A system that provides contextually situated support during tasks without camera use, paving the way for the ubiquitous, intelligent assistant.},
  month = {nov},
}

@article{wang_graph_2025,
  title = {Graph {Machine},
  author = {Wang, Shijie and Huang, Jiani and Chen, Zhikai and Song, Yu and Tang, Wenzhuo and Mao, Haitao and Fan, Wenqi and Liu, Hui and Liu, Xiaorui and Yin, Dawei and Li, Qing},
  year = {2025},
  doi = {10.1145/3732786},
  url = {https://doi.org/10.1145/3732786},
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {16},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {and Representation Learning, Graph Machine Learning, Large Language Models (LLMs), Pre-training and Fine-tuning, Prompting, source: ACM},
  abstract = {Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graphs. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications, such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML’s generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations, such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph Heterophily and Out-of-Distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.},
  issn = {2157-6904},
  month = {aug},
}

@article{vanbrabant_echo_2025,
  title = {{ECHO},
  author = {Vanbrabant, Sebe and Eerlings, Gilles and Rovelo Ruiz, Gustavo Alberto and Vanacken, Davy},
  year = {2025},
  doi = {10.1145/3734191},
  url = {https://doi.org/10.1145/3734191},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Artificial Intelligence, Explainability, Explainable AI, Human-AI Interaction, Intelligibility, Interpretability, Large Language Models, Machine Learning, source: ACM},
  abstract = {This paper introduces ECHO, an LLM-powered system framework to explore and interrogate the internals of AI models through tool-augmented language models. While traditional XAI methods typically offer a small and technical set of explanation types, ECHO advances the accessibility and usability of AI explanations through a conversational approach, combining LLMs with a collection of tools and a human-in-the-loop process. We identify various explanation types from the literature, for which we create a set of predefined tools for tabular data. Using a modular architecture, ECHO integrates these predefined tools with dynamically generated tools to interact with AI models, facilitating tailored explanations for a large variety of user queries. This paper details ECHO’s design, implementation, and use cases, demonstrating its capabilities in the context of a movie recommender, healthcare decision tree and neural network for educational classification.},
  month = {jun},
}

@article{kreikemeyer_using_2025,
  title = {Using ({Not},
  author = {Kreikemeyer, Justin Noah and Jankowski, Miłosz and Wilsdorf, Pia and Uhrmacher, Adelinde M.},
  year = {2025},
  doi = {10.1145/3733719},
  url = {https://doi.org/10.1145/3733719},
  journal = {ACM Trans. Model. Comput. Simul.},
  volume = {35},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {constrained decoding, knowledge extraction, language model, natural language processing, Simulation model generation, source: ACM},
  abstract = {Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-efficient, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to 84.5\% of cases. In addition, our small-scale user study demonstrates the model’s practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.},
  issn = {1049-3301},
  month = {sep},
}

@article{sun_when_2025,
  title = {When {Traditional},
  author = {Sun, Yuling and Yue, Wenjing and Jin, Xiaofu and Ma, Shuai and Ma, Xiaojuan and Wang, Xiaoling},
  year = {2025},
  doi = {10.1145/3757705},
  url = {https://doi.org/10.1145/3757705},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {adoption, artificial intelligence, clinical decision-making, clinician, qualitative, traditional Chinese medicine, traditional medicine, usability, source: ACM},
  abstract = {Traditional Medicine (TM) is the oldest healthcare form and has been increasingly adopted as the primary or complementary medical therapy in the world. However, TM's practical development remains highly challenging. While artificial intelligence (AI) has become powerful in advancing modern medicine, limited attention has been paid to its potential and usage in TM. This study addresses this gap through a probe-based interview study with 16 TM clinicians, examining their experiences, perceptions, and expectations of AI-empowered clinical support systems. Our findings reveal that despite numerous AI-CDS systems, their practical usage in TM settings was still limited. We identify a series of practical challenges when integrating AI-CDS into TM clinical scenarios, largely due to TM's unique features and the significant data work challenges these features present. We end by critically discussing the potential issues that may arise when integrating AI into practical TM scenarios, and proposing a series of practical recommendations for future studies.},
  month = {oct},
}

@article{sakib_genai_2025,
  title = {A {GenAI},
  author = {Sakib, Syed N and Naha, Kallol and Rubaiat, Sajratul Y and Jamil, Hasan M},
  year = {2025},
  doi = {10.1145/3770753},
  url = {https://doi.org/10.1145/3770753},
  journal = {J. Data and Information Quality},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Computational Biology, Data Integration, Deep Web Database, FAIR, Information Retrieval, Knowledge Representation, Large Language Model, Natural Language Processing, Scientific Inquiries, source: ACM},
  abstract = {Life sciences research increasingly requires identifying, accessing, and effectively processing data from an ever-evolving array of information sources on the Linked Open Data (LOD) network. This dynamic landscape places a significant burden on researchers, as the quality of query responses depends heavily on the selection and semantic integration of data sources –processes that are often labor-intensive, error-prone, and costly. While the adoption of FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has aimed to address these challenges, barriers to efficient and accurate scientific data processing persist. In this paper, we introduce FAIRBridge, an experimental natural language-based query processing system designed to empower scientists to discover, access, and query biological databases, even when they are not FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query intents, map them to relevant databases described in scientific literature, and generate executable queries via intelligent resource access plans. The system also includes robust tools for mitigating low-quality query processing, ensuring high fidelity and responsiveness in the information delivered. FAIRBridge’s autonomous query processing framework enables users to explore alternative data sources, make informed choices at every step, and leverage community-driven crowd curation when needed. By providing a user-friendly, automated hypothesis-testing platform in natural English, FAIRBridge significantly enhances the integration and processing of scientific data, offering researchers a powerful new tool for advancing their inquiries.},
  annote = {Just Accepted},
  issn = {1936-1955},
  month = {oct},
}

@article{zhang_beyond_2025,
  title = {Beyond {Texts},
  author = {Zhang, Haoyao and Qin, Zhida and Liang, Xufeng and Guo, Jing and Li, Shuang and Huang, Tianyu and Lui, John C.S.},
  year = {2025},
  doi = {10.1145/3771276},
  url = {https://doi.org/10.1145/3771276},
  journal = {ACM Trans. Inf. Syst.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Conversational recommendation, Recommender systems, source: ACM},
  abstract = {Conversational recommender systems (CRSs) interact with users through natural language to provide recommendations and generate responses. Due to limited information in conversation, existing works utilize KGs or reviews to improve CRS. Despite achievements, they overlook co-occurrence relations which have shown effectiveness in collaborative filtering systems. In this work, we first propose a novel framework named CoCRS, aiming to incorporate Co-occurrences into the review-based Conversation Recommendation Systems. In CoCRS, we mine co-occurrences from two aspects: (1) item and entity, (2) user and item. For the first one, we extract entities from redundant review texts by KG and construct a relation-aware item-entity heterogeneous graph. In the second aspect, we analyze review sentiments and construct a sentiment-aware user-item bipartite graph. We encode two graphs to obtain user and entity embeddings. Since users in CRS are anonymous, we generate a virtual similar user representation to match reviews with users. Besides, we capture time-aware preference representation from two time dimensions. Finally, we generate word-level user representation with word-oriented KG and model user preference by integrating the above representations. Extensive experiments demonstrate that CoCRS outperforms baselines and the cold-start experiment highlights its robustness. The LLM experiment illustrates the significant role of co-occurrence relationships in LLM-based CRS. Our code are available at https://github.com/Qin-lab-code/CoCRS.},
  annote = {Just Accepted},
  issn = {1046-8188},
  month = {oct},
}

@article{blinn_statically_2024,
  title = {Statically {Contextualizing},
  author = {Blinn, Andrew and Li, Xiang and Kim, June Hyung and Omar, Cyrus},
  year = {2024},
  doi = {10.1145/3689728},
  url = {https://doi.org/10.1145/3689728},
  journal = {Proc. ACM Program. Lang.},
  volume = {8},
  number = {OOPSLA2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Large Language Models, Program Repair, Program Synthesis, source: ACM},
  abstract = {Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate code context, particularly when working with definitions that are neither in the training data nor near the cursor. This paper demonstrates that tighter integration with the type and binding structure of the programming language in use, as exposed by its language server, can help address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server is able to identify the type and typing context of the hole that the programmer is filling, with Hazel's total syntax and type error correction ensuring that a meaningful program sketch is available whenever the developer requests a completion. This allows the system to prompt the LLM with codebase-wide contextual information that is not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server, which provides error localization and error messages. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications with accompanying unit tests that have been written from scratch to avoid data contamination, and that can easily be ported to new languages because they do not have large external library dependencies. These applications serve as challenge problems due to their extensive reliance on application-specific data structures. Through an ablation study, we examine the impact of contextualization with type definitions, function headers, and errors messages, individually and in combination. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel, a low-resource language, we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource mainstream languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.},
  month = {oct},
}

@article{reddy_lhs_2025,
  title = {{LHS},
  author = {Reddy, E Bhawani Eswar and Bhattacharyya, Sutirtha and Sarmah, Ankur and Nongpoh, Fedrick and Maddala, Karthik and Karfa, Chandan},
  year = {2025},
  doi = {10.1145/3734523},
  url = {https://doi.org/10.1145/3734523},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {CNN, high-level synthesis, LLM, source: ACM},
  abstract = {Deep learning tasks, especially those involving complex convolution neural networks (CNNs), are computationally intensive and pose significant challenges when implemented on hardware. Accelerating these tasks is critical for improving performance. High-level Synthesis (HLS) has the potential to automate the efficient hardware accelerator designs directly from high-level C/C++ specification of trained machine learning (ML) models. Traditional HLS tools cannot synthesize certain high-level constructs, which require manual intervention. Many source code optimizations and the selection of pragmas for HLS optimizations are crucial for generating efficient hardware accelerators with HLS. However, both of these tasks are mostly manual efforts. Recently, Large Language Models (LLMs) have shown remarkable capabilities in various generative tasks. In this work, we explore the application of LLMs to remove these manual efforts in adapting HLS for ML accelerator designs. Our framework called LLM-assisted HLS, i.e., LHS, uses LLMs to automate the resolution of synthesis issues, ensuring compatibility with HLS tools. Furthermore, our framework automates the source code modification and optimization selection through pragma insertion steps, which are crucial for optimizing the synthesized design. Our experimental results with LHS demonstrate a significant improvement in latency for deep learning tasks with underlying complex CNN models without much area overhead. Our LHS allows us to achieve up to 2690 × latency improvement. Promisingly, LHS performs better than the state-of-the-art ML accelerator design tool hls4ml in 4 out of 6 cases in the context of latency improvement at the expense of area overhead (i.e., performance to hardware gain). This work highlights the potential of LLMs to assist and accelerate the HLS process, thereby creating more efficient hardware implementation for deep learning models.},
  issn = {1084-4309},
  month = {oct},
}

@article{wang_large_2025,
  title = {Large {Language},
  author = {Wang, Shenao and Zhao, Yanjie and Hou, Xinyi and Wang, Haoyu},
  year = {2025},
  doi = {10.1145/3708531},
  url = {https://doi.org/10.1145/3708531},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {34},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Large Language Models, LLM Supply Chain, source: ACM},
  abstract = {The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, introducing unprecedented capabilities in natural language processing and multimodal content generation. However, the increasing complexity and scale of these models have given rise to a multifaceted supply chain that presents unique challenges across infrastructure, foundation models, and downstream applications. This article provides the first comprehensive research agenda of the LLM supply chain, offering a structured approach to identify critical challenges and opportunities through the dual lenses of software engineering (SE) and security and privacy (S\&amp;P). We begin by establishing a clear definition of the LLM supply chain, encompassing its components and dependencies. We then analyze each layer of the supply chain, presenting a vision for robust and secure LLM development, reviewing the current state of practices and technologies, and identifying key challenges and research opportunities. This work aims to bridge the existing research gap in systematically understanding the multifaceted issues within the LLM supply chain, offering valuable insights to guide future efforts in this rapidly evolving domain.},
  issn = {1049-331X},
  month = {may},
}

@article{zheng_towards_2025,
  title = {Towards {Lifelong},
  author = {Zheng, Junhao and Qiu, Shengjie and Shi, Chengming and Ma, Qianli},
  year = {2025},
  doi = {10.1145/3716629},
  url = {https://doi.org/10.1145/3716629},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {8},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {catastrophic forgetting, continual learning, incremental learning, large language models, Lifelong learning, source: ACM},
  abstract = {As the applications of large language models (LLMs) expand across diverse fields, their ability to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods with static datasets are inadequate for coping with the dynamic nature of real-world information. Lifelong learning, or continual learning, addresses this by enabling LLMs to learn continuously and adapt over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. Our survey explores the landscape of lifelong learning, categorizing strategies into two groups based on how new knowledge is integrated: Internal Knowledge, where LLMs absorb new knowledge into their parameters through full or partial training, and External Knowledge, which incorporates new knowledge as external resources such as Wikipedia or APIs without updating model parameters. The key contributions of our survey include: (1) introducing a novel taxonomy to categorize the extensive literature of lifelong learning into 12 scenarios; (2) identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups; (3) highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era. Resources are available at .},
  issn = {0360-0300},
  month = {mar},
}

@article{su_scene-aware_2023,
  title = {Scene-{Aware},
  author = {Su, Zejia and Fan, Qingnan and Chen, Xuelin and Van Kaick, Oliver and Huang, Hui and Hu, Ruizhen},
  year = {2023},
  doi = {10.1145/3618338},
  url = {https://doi.org/10.1145/3618338},
  journal = {ACM Trans. Graph.},
  volume = {42},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {activity program generation, dynamic scene graph, language guidance, source: ACM},
  abstract = {We address the problem of scene-aware activity program generation, which requires decomposing a given activity task into instructions that can be sequentially performed within a target scene to complete the activity. While existing methods have shown the ability to generate rational or executable programs, generating programs with both high rationality and executability still remains a challenge. Hence, we propose a novel method where the key idea is to explicitly combine the language rationality of a powerful language model with dynamic perception of the target scene where instructions are executed, to generate programs with high rationality and executability. Our method iteratively generates instructions for the activity program. Specifically, a two-branch feature encoder operates on a language-based and graph-based representation of the current generation progress to extract language features and scene graph features, respectively. These features are then used by a predictor to generate the next instruction in the program. Subsequently, another module performs the predicted action and updates the scene for perception in the next iteration. Extensive evaluations are conducted on the VirtualHome-Env dataset, showing the advantages of our method over previous work. Key algorithmic designs are validated through ablation studies, and results on other types of inputs are also presented to show the generalizability of our method.},
  issn = {0730-0301},
  month = {dec},
}

@article{guo_omnigirl_2025,
  title = {{OmniGIRL},
  author = {Guo, Lianghong and Tao, Wei and Jiang, Runhan and Wang, Yanlin and Chen, Jiachi and Liu, Xilin and Ma, Yuchi and Mao, Mingzhi and Zhang, Hongyu and Zheng, Zibin},
  year = {2025},
  doi = {10.1145/3728871},
  url = {https://doi.org/10.1145/3728871},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {ISSTA},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Benchmark, Github Issue Resolution, Large Language Models, source: ACM},
  abstract = {The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6\% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5\% of the issues with image information. Finally, we analyze the reasons behind current LLMs’ failure on OmniGIRL, providing insights for future improvements.},
  month = {jun},
}

@article{das_security_2025,
  title = {Security and {Privacy},
  author = {Das, Badhan Chandra and Amini, M. Hadi and Wu, Yanzhao},
  year = {2025},
  doi = {10.1145/3712001},
  url = {https://doi.org/10.1145/3712001},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {attack and defense mechanisms, Large language models, source: ACM},
  abstract = {Large language models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Today, LLMs have become quite popular tools in natural language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and personally identifiable information leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks against LLMs, and review potential defense mechanisms. Additionally, the survey outlines existing research gaps and highlights future research directions.},
  issn = {0360-0300},
  month = {feb},
}

@article{zhou_conformal_2025,
  title = {Conformal {Prediction},
  author = {Zhou, Xiaofan and Chen, Baiting and Gui, Yu and Cheng, Lu},
  year = {2025},
  doi = {10.1145/3736575},
  url = {https://doi.org/10.1145/3736575},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Computer vision, Natural language processing, Time series, source: ACM},
  abstract = {Conformal prediction (CP), a distribution-free uncertainty quantification (UQ) framework, reliably provides valid predictive inference for black-box models. CP constructs prediction sets or intervals that contain the true output with a specified probability. However, modern data science’s diverse modalities, along with increasing data and model complexity, challenge traditional CP methods. These developments have spurred novel approaches to address evolving scenarios. This survey reviews the foundational concepts of CP and recent advancements from a data-centric perspective, including applications to structured, unstructured, and dynamic data. We also discuss the challenges and opportunities CP faces in large-scale data and models.},
  issn = {0360-0300},
  month = {sep},
}

@article{kaniwa_chitchatguide_2024,
  title = {{ChitChatGuide},
  author = {Kaniwa, Yuka and Kuribayashi, Masaki and Kayukawa, Seita and Sato, Daisuke and Takagi, Hironobu and Asakawa, Chieko and Morishima, Shigeo},
  year = {2024},
  doi = {10.1145/3676492},
  url = {https://doi.org/10.1145/3676492},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {MHCI},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {To enable people with visual impairments (PVI) to explore shopping malls, it is important to provide information for selecting destinations and obtaining information based on the individual's interests. We achieved this through conversational interaction by integrating a large language model (LLM) with a navigation system. ChitChatGuide allows users to plan a tour through contextual conversations, receive personalized descriptions of surroundings based on transit time, and make inquiries during navigation. We conducted a study in a shopping mall with 11 PVI, and the results reveal that the system allowed them to explore the facility with increased enjoyment. The LLM-based conversational interaction, by understanding vague and context-based questions, enabled the participants to explore unfamiliar environments effectively. The personalized and in-situ information generated by the LLM was both useful and enjoyable. Considering the limitations we identified, we discuss the criteria for integrating LLMs into navigation systems to enhance the exploration experiences of PVI.},
  month = {sep},
}

@article{kharkwal_university_2021,
  title = {University {Operations},
  author = {Kharkwal, Himanshu and Olson, Dakota and Huang, Jiali and Mohan, Abhiraj and Mani, Ankur and Srivastava, Jaideep},
  year = {2021},
  doi = {10.1145/3460125},
  url = {https://doi.org/10.1145/3460125},
  journal = {ACM Trans. Manage. Inf. Syst.},
  volume = {12},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Agent based modeling, bipartite networks, COVID-19, decision analysis, simulation, source: ACM},
  abstract = {Modeling infection spread during pandemics is not new, with models using past data to tune simulation parameters for predictions. These help in understanding of the healthcare burden posed by a pandemic and responding accordingly. However, the problem of how college/university campuses should function during a pandemic is new for the following reasons: (i) social contact in colleges are structured and can be engineered for chosen objectives; (ii) the last pandemic to cause such societal disruption was more than 100 years ago, when higher education was not a critical part of society; (iii) not much was known about causes of pandemics, and hence effective ways of safe operations were not known; and (iv) today with distance learning, remote operation of an academic institution is possible. As one of the first to address this problem, our approach is unique in presenting a flexible simulation system, containing a suite of model libraries, one for each major component. The system integrates agent-based modeling and the stochastic network approach, and models the interactions among individual entities (e.g., students, instructors, classrooms, residences) in great detail. For each decision to be made, the system can be used to predict the impact of various choices, and thus enables the administrator to make informed decisions. Although current approaches are good for infection modeling, they lack accuracy in social contact modeling. Our agent-based modeling approach, combined with ideas from Network Science, presents a novel approach to contact modeling. A detailed case study of the University of Minnesota’s Sunrise Plan is presented. For each decision made, its impact was assessed, and results were used to get a measure of confidence. We believe that this flexible tool can be a valuable asset for various kinds of organizations to assess their infection risks in pandemic-time operations, including middle and high schools, factories, warehouses, and small/medium-sized businesses.},
  issn = {2158-656X},
  month = {sep},
}

@article{li_graph_2024,
  title = {Graph and {Sequential},
  author = {Li, Zihao and Yang, Chao and Chen, Yakun and Wang, Xianzhi and Chen, Hongxu and Xu, Guandong and Yao, Lina and Sheng, Michael},
  year = {2024},
  doi = {10.1145/3696413},
  url = {https://doi.org/10.1145/3696413},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {graph neural networks, Recommendation survey, sequential neural networks, session-based recommendation, source: ACM},
  abstract = {Recent years have witnessed the remarkable success of recommendation systems (RSs) in alleviating the information overload problem. As a new paradigm of RSs, session-based recommendation (SR) specializes in users’ short-term preferences and aims at providing a more dynamic and timely recommendation based on ongoing interactions. This survey presents a comprehensive overview of the recent works on SR. First, we clarify the key definitions within SR and compare the characteristics of SR against other recommendation tasks. Then, we summarize the existing methods in two categories: sequential neural network based methods and graph neural network (GNN) based methods. The relevant frameworks and technical details are further introduced. Finally, we discuss the challenges of SR and new research directions in this area.},
  issn = {0360-0300},
  month = {nov},
}

@article{stalnaker_developer_2025,
  title = {Developer {Perspectives},
  author = {Stalnaker, Trevor and Wintersgill, Nathan and Chaparro, Oscar and Heymann, Laura A. and Di Penta, Massimiliano and German, Daniel M and Poshyvanyk, Denys},
  year = {2025},
  doi = {10.1145/3743133},
  url = {https://doi.org/10.1145/3743133},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {generative ai, large language models, machine learning, open-source software, qualitative research, source: ACM},
  abstract = {Despite the utility that Generative AI (GenAI) tools provide for tasks such as writing code, the use of these tools raises important legal questions and potential risks, particularly those associated with copyright law. As lawmakers and regulators respond to these questions, the views of users can offer relevant perspectives. In this paper, we provide: (1) a survey of 574 developers on the licensing and copyright aspects of GenAI for coding, as well as follow-up interviews; (2) a snapshot of developers’ views at a time when GenAI and perceptions of it were rapidly evolving; and (3) an analysis of developers’ perspectives, yielding insights and recommendations that can inform future regulatory decisions in this evolving field. Our results show the benefits developers derive from GenAI, how they view the use of AI-generated code as similar to using other existing code, the varied opinions they have on who should own or be compensated for such code, that they are concerned about data leakage via GenAI, and other findings, providing organizations and policymakers with valuable insights into how the technology is being used and the concerns that stakeholders believe warrant attention.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {jun},
}

@article{qiao_multi-view_2025,
  title = {Multi-view {Intent},
  author = {Qiao, Shutong and Zhou, Wei and Wen, Junhao and Gao, Chen and Luo, Qun and Chen, Peixuan and Li, Yong},
  year = {2025},
  doi = {10.1145/3719344},
  url = {https://doi.org/10.1145/3719344},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Data Augmentation, Large Language Models, Recommender System, Session-based Recommendation, source: ACM},
  abstract = {Session-based recommendation (SBR) methods often rely on user behavior data, which can struggle with the sparsity of session data, limiting performance. Researchers have identified that beyond behavioral signals, rich semantic information in item descriptions is crucial for capturing hidden user intent. While Large Language Models (LLMs) offer new ways to leverage this semantic data, the challenges of session anonymity, short-sequence nature, and high LLM training costs have hindered the development of a lightweight, efficient LLM framework for SBR.To address the above challenges, we propose an LLM-enhanced SBR framework that integrates semantic and behavioral signals from multiple views. This two-stage framework leverages the strengths of both LLMs and traditional SBR models while minimizing training costs. In the first stage, we use multi-view prompts to infer latent user intentions at the session semantic level, supported by an intent localization module to alleviate LLM hallucinations. In the second stage, we align and unify these semantic inferences with behavioral representations, effectively merging insights from both large and small models. Extensive experiments on two real datasets demonstrate that the LLM4SBR framework can effectively improve model performance. We release our codes along with the baselines at .},
  issn = {1046-8188},
  month = {may},
}

@article{gomes_lopes_exploring_2025,
  title = {Exploring {Large},
  author = {Gomes Lopes, Samuel and Zhu, Shien and Alonso, Gustavo},
  year = {2025},
  doi = {10.1145/3742430},
  url = {https://doi.org/10.1145/3742430},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {electronic design automation (EDA), hardware description language (HDL), Large language models (LLMs), source: ACM},
  abstract = {Designing and verifying hardware circuits using a Hardware Description Language (HDL) is an essential but time-consuming part of hardware design. Generating the desired correct circuit and testbench code usually requires a significant engineering effort. Recently, Large Language Models (LLMs) have claimed to have strong code generation capabilities to reduce such engineering costs. Existing work has provided quantitative evaluations using LLMs for single-module, simple circuit generation. However, it is still unclear whether modern LLMs are useful in production workflows, e.g., generating correct hierarchical circuits with testbenches. And if they are capable, what are the best prompt engineering practices for hardware design? In this article, we evaluate LLMs for HDL generation by exploring a 3-dimensional design space: commercial and open-source language models, single-module and hierarchical circuits, and prompting methods with varying complexity. We propose a 3-step design space exploration methodology to answer the two aforementioned questions. First, we explore the best prompt engineering practices across generating simple, middle, and hard single-module circuits with testbenches on CodeLLama-34B. We also define two fine-grained checklists to evaluate the circuit and testbench quality from a user’s perspective. Second, we benchmark 11 LLMs with prompt adaptation on 4 single-module circuits that CodeLLama-34B has trouble with to further find models that may be useful in a production workflow. Third, we apply the learned prompt practices on four top-level models to generate simple 2 to 4-module and more complex multi-module hierarchical circuits and testbenches. As a result, we find that some of the latest LLMs can generate correct simple hierarchical circuits and testbenches with given proper prompts, but still struggle with complex hierarchical circuits. We further provide useful guidelines from an end-user’s perspective on leveraging LLMs for hardware design.},
  issn = {1084-4309},
  month = {oct},
}

@article{raza_improving_2023,
  title = {Improving {Clinical},
  author = {Raza, Shaina and Ding, Chen},
  year = {2023},
  doi = {10.1109/TCBB.2023.3318209},
  url = {https://doi.org/10.1109/tcbb.2023.3318209},
  journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
  volume = {21},
  number = {5},
  pages = {1180--1190},
  note = {Place: Washington, DC, USA
Publisher: IEEE Computer Society Press},
  keywords = {source: ACM},
  abstract = {Clinical decision-making is complex and time-intensive. To help in this effort, clinical recommender systems (RS) have been designed to facilitate healthcare practitioners with personalized advice. However, designing an effective clinical RS poses challenges due to the multifaceted nature of clinical data and the demand for tailored recommendations. In this article, we introduce a 2-Stage Recommendation framework for clinical decision-making, which leverages a publicly accessible dataset of electronic health records. In the first stage, a deep neural network-based model is employed to extract a set of candidate items, such as diagnoses, medications, and prescriptions, from a patient's electronic health records. Subsequently, the second stage utilizes a deep learning model to rank and pinpoint the most relevant items for healthcare providers. Both retriever and ranker are based on pre-trained transformer models that are stacked together as a pipeline. To validate our model, we compared its performance against several baseline models using different evaluation metrics. The results reveal that our proposed model attains a performance gain of approximately 12.3\% macro-average F1 compared to the second best performing baseline. Qualitative analysis across various dimensions also confirms the model's high performance. Furthermore, we discuss challenges like data availability, privacy concerns, and shed light on future exploration in this domain.},
  issn = {1545-5963},
  month = {sep},
}

@article{pereira_inacia_2025,
  title = {{INACIA},
  author = {Pereira, Jayr and Assumpcao, Andre and Trecenti, Julio and Airosa, Luiz and Lente, Caio and Cléto, Jhonatan and Dobins, Guilherme and Nogueira, Rodrigo and Mitchell, Luis and Lotufo, Roberto},
  year = {2025},
  doi = {10.1145/3652951},
  url = {https://doi.org/10.1145/3652951},
  journal = {Digit. Gov.: Res. Pract.},
  volume = {6},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI in Legal Systems, Artificial Intelligence-Assisted Legal Instruction, Brazilian Federal Court of Accounts (TCU), Large Language Models (LLMs), source: ACM},
  abstract = {This article introduces Instrução Assistida com Inteligência Artificial (INACIA), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts. The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA’s potential in extracting relevant information from case documents, evaluating its legal plausibility, and formulating propositions for judicial decision-making. Utilizing a validation dataset alongside LLMs, our evaluation methodology, to the best of our knowledge, presents a novel approach to assessing system performance, correlating highly with human judgment. These results underscore INACIA’s potential in complex legal task handling while also acknowledging the current limitations. This study discusses possible improvements and the broader implications of applying Artificial Intelligence (AI) in legal contexts, suggesting that INACIA represents a significant step toward integrating AI in legal systems globally, albeit with cautious optimism grounded in the empirical findings.},
  month = {feb},
}

@article{bayram_healthcare-focused_2025,
  title = {Healthcare-{Focused},
  author = {Bayram, M. Ali and Diri, Banu and Yildirim, Savas},
  year = {2025},
  doi = {10.1145/3772000},
  url = {https://doi.org/10.1145/3772000},
  journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {catastrophic forgetting, healthcare AI, Low-Rank Adaptation, model fine-tuning, patient-doctor interactions, Turkish Medical LLM, source: ACM},
  abstract = {The development of a Turkish-specific Large Language Model (LLM) for healthcare presents a unique opportunity to enhance AI’s accessibility and relevance for Turkish-speaking medical practitioners and patients. This study introduces a specialized Turkish Medical LLM fine-tuned on over 167,732 real patient-doctor question-answer pairs sourced from a trusted medical platform and capturing authentic linguistics in Turkish medical language. Utilizing models like LLAMA 3, the fine-tuning process was supported by Low-Rank Adaptation (LoRA) and involved innovative methods to mitigate catastrophic forgetting, including spherical linear interpolation (Slerp) merging. Evaluation of the model’s performance through similarity scores, GPT-3.5 assessments, and expert reviews indicates significant improvement in the model’s ability to generate medically accurate responses. This Turkish Medical LLM demonstrates potential to support medical decision-making and patient interaction in Turkish healthcare settings, offering an essential resource for enhancing AI inclusivity across languages.},
  annote = {Just Accepted},
  issn = {2375-4699},
  month = {oct},
}

@article{jones_shred_2022,
  title = {{SHRED},
  author = {Jones, R. Kenny and Habib, Aalia and Ritchie, Daniel},
  year = {2022},
  doi = {10.1145/3550454.3555440},
  url = {https://doi.org/10.1145/3550454.3555440},
  journal = {ACM Trans. Graph.},
  volume = {41},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {fine-grained components, shape analysis, shape segmentation, source: ACM},
  abstract = {We present SHRED, a method for 3D SHape REgion Decomposition. SHRED takes a 3D point cloud as input and uses learned local operations to produce a segmentation that approximates fine-grained part instances. We endow SHRED with three decomposition operations: splitting regions, fixing the boundaries between regions, and merging regions together. Modules are trained independently and locally, allowing SHRED to generate high-quality segmentations for categories not seen during training. We train and evaluate SHRED with fine-grained segmentations from PartNet; using its merge-threshold hyperparameter, we show that SHRED produces segmentations that better respect ground-truth annotations compared with baseline methods, at any desired decomposition granularity. Finally, we demonstrate that SHRED is useful for downstream applications, out-performing all baselines on zero-shot fine-grained part instance segmentation and few-shot finegrained semantic segmentation when combined with methods that learn to label shape regions.},
  issn = {0730-0301},
  month = {nov},
}

@article{cuevas_collecting_2025,
  title = {Collecting {Qualitative},
  author = {Cuevas, Alejandro and Scurrell, Jennifer V. and Brown, Eva M. and Entenmann, Jason and Daepp, Madeleine I. G.},
  year = {2025},
  doi = {10.1145/3710947},
  url = {https://doi.org/10.1145/3710947},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Chatbots have shown promise as tools to scale qualitative data collection. Recent advances in Large Language Models (LLMs) could accelerate this process by allowing researchers to easily deploy sophisticated interviewing chatbots. We test this assumption by conducting a large-scale user study (n=399) evaluating 3 different chatbots, two of which are LLM-based and a baseline which employs hard-coded questions. We evaluate the results with respect to participant engagement and experience, established metrics of chatbot quality grounded in theories of effective communication, and a novel scale evaluating ”richness” or the extent to which responses capture the complexity and specificity of the social context under study. We find that, while the chatbots were able to elicit high-quality responses based on established evaluation metrics, the responses rarely capture participants' specific motives or personalized examples, and thus perform poorly with respect to richness. We further find low inter-rater reliability between LLMs and humans in the assessment of both quality and richness metrics. Our study offers a cautionary tale for scaling and evaluating qualitative research with LLMs.},
  month = {may},
}

@article{sicari_open-ethical_2024,
  title = {Open-{Ethical},
  author = {Sicari, Sabrina and Cevallos M., Jesus F. and Rizzardi, Alessandra and Coen-Porisini, Alberto},
  year = {2024},
  doi = {10.1145/3703454},
  url = {https://doi.org/10.1145/3703454},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {human-centric AI, large-language models, Neural language models, open-source, source: ACM},
  abstract = {This survey summarises the most recent methods for building and assessing helpful, honest, and harmless neural language models, considering small, medium, and large-size models. Pointers to open-source resources that help to align pre-trained models are given, including methods that use parameter-efficient techniques, specialized prompting frameworks, adapter modules, case-specific knowledge injection, and adversarially robust training techniques. Special care is given to evidencing recent progress on value alignment, commonsense reasoning, factuality enhancement, and abstract reasoning of language models. Most reviewed works in this survey publicly shared their code and related data and were accepted in world-leading Machine Learning venues. This work aims at helping researchers and practitioners accelerate their entrance into the field of human-centric neural language models, which might be a cornerstone of the contemporary and near-future industrial and societal revolution.},
  issn = {0360-0300},
  month = {dec},
}

@article{ren_conversations_2021,
  title = {Conversations with {Search},
  author = {Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Kanoulas, Evangelos and Monz, Christof and De Rijke, Maarten},
  year = {2021},
  doi = {10.1145/3432726},
  url = {https://doi.org/10.1145/3432726},
  journal = {ACM Trans. Inf. Syst.},
  volume = {39},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Conversational modeling, dataset, neural model, search engine, source: ACM},
  abstract = {In this article, we address the problem of answering complex information needs by conducting conversations with search engines, in the sense that users can express their queries in natural language and directly receive the information they need from a short system response in a conversational manner. Recently, there have been some attempts towards a similar goal, e.g., studies on Conversational Agents (CAs) and Conversational Search (CS). However, they either do not address complex information needs in search scenarios or they are limited to the development of conceptual frameworks and/or laboratory-based user studies.We pursue two goals in this article: (1)the creation of a suitable dataset, the Search as a Conversation (SaaC) dataset, for the development of pipelines for conversations with search engines, and(2)the development of a state-of-the-art pipeline for conversations with search engines, Conversations with Search Engines (CaSE), using this dataset. SaaC is built based on a multi-turn conversational search dataset, where we further employ workers from a crowdsourcing platform to summarize each relevant passage into a short, conversational response. CaSE enhances the state-of-the-art by introducing a supporting token identification module and a prior-aware pointer generator, which enables us to generate more accurate responses.We carry out experiments to show that CaSE is able to outperform strong baselines. We also conduct extensive analyses on the SaaC dataset to show where there is room for further improvement beyond CaSE. Finally, we release the SaaC dataset and the code for CaSE and all models used for comparison to facilitate future research on this topic.},
  issn = {1046-8188},
  month = {aug},
}

@article{formosa_generative_2025,
  title = {Generative {AI},
  author = {Formosa, Paul and Kashyap, Bhanuraj and Sahebi, Siavosh},
  year = {2025},
  doi = {10.1145/3674844},
  url = {https://doi.org/10.1145/3674844},
  journal = {Digit. Gov.: Res. Pract.},
  volume = {6},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {citizenship, deliberative democracy, Generative AI, Large Language Models (LLM), social media, source: ACM},
  abstract = {Generative Artificial Intelligence (AI) technologies have the potential to be socially and politically transformative. In this article, we focus on exploring the potential impacts that Generative AI could have on the functioning of our democracies and the nature of citizenship. We do so by drawing on accounts of deliberative democracy and the deliberative virtues associated with it, as well as the reciprocal impacts that social media and Generative AI will have on each other and the broader information landscape. Drawing on this background theory, we outline some of the key positive and negative impacts that Generative AI is likely to have on democratic citizenship. The political significance of these impacts suggests the need for further regulation.},
  month = {jun},
}

@article{pinckney_revisiting_2025,
  title = {Revisiting {VerilogEval},
  author = {Pinckney, Nathaniel and Batten, Christopher and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
  year = {2025},
  doi = {10.1145/3718088},
  url = {https://doi.org/10.1145/3718088},
  journal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {30},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {benchmarks, Large language models, RTL code generation, source: ACM},
  abstract = {The application of large language models (LLMs) to digital hardware code generation is an emerging field, with most LLMs primarily trained on natural language and software code. Hardware code like Verilog constitutes a small portion of training data, and few hardware benchmarks exist. The open-source VerilogEval benchmark, released in November 2023, provided a consistent evaluation framework for LLMs on code completion tasks. Since then, both commercial and open models have seen significant development.In this work, we evaluate new commercial and open models since VerilogEval’s original release—including GPT-4o, GPT-4 Turbo, Llama3.1 (8B/70B/405B), Llama3 70B, Mistral Large, DeepSeek Coder (33B and 6.7B), CodeGemma 7B, and RTL-Coder—against an improved VerilogEval benchmark suite. We find measurable improvements in state-of-the-art models: GPT-4o achieves a 63\% pass rate on specification-to-RTL tasks. The recently released and open Llama3.1 405B achieves a 58\% pass rate, almost matching GPT-4o, while the smaller domain-specific RTL-Coder 6.7B models achieve an impressive 34\% pass rate.Additionally, we enhance VerilogEval’s infrastructure by automatically classifying failures, introducing in-context learning support, and extending the tasks to specification-to-RTL translation. We find that prompt engineering remains crucial for achieving good pass rates and varies widely with model and task. A benchmark infrastructure that allows for prompt engineering and failure analysis is essential for continued model development and deployment.},
  issn = {1084-4309},
  month = {oct},
}

@article{esterle_loosening_2022,
  title = {Loosening {Control},
  author = {Esterle, Lukas and King, David W.},
  year = {2022},
  doi = {10.1145/3502725},
  url = {https://doi.org/10.1145/3502725},
  journal = {ACM Trans. Auton. Adapt. Syst.},
  volume = {16},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {autonomous systems, decentralized control, distributed control, fog computing, hybrid control, mobile pervasive systems, online multi-object k-assignment, Self-organisation, source: ACM},
  abstract = {Large pervasive systems, deployed in dynamic environments, require flexible control mechanisms to meet the demands of chaotic state changes while accomplishing system goals. As centralized control approaches may falter in environments where centralized communication and knowledge may be impossible to implement, researchers have proposed decentralized control methods that leverage agent-driven, self-organizing behaviors, to achieve reliable, flexible systems. This article presents and compares the performance of three decentralized control approaches in the online multi-object k-assignment problem. In this domain, a set of sensors is tasked to detect and track an unknown and changing set of targets. Results show that a proposed hybrid approach that incorporates supervisory devices within the population while allowing semi-autonomous operations in non-supervisory devices produces a flexible and reliable system capable of both high detection and coverage rates.},
  issn = {1556-4665},
  month = {mar},
}

@article{bonifati_versatile_2025,
  title = {Versatile {Property},
  author = {Bonifati, Angela},
  year = {2025},
  doi = {10.14778/3750601.3760517},
  url = {https://doi.org/10.14778/3750601.3760517},
  journal = {Proc. VLDB Endow.},
  volume = {18},
  number = {12},
  pages = {5516--5526},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Property graphs are key components of modern graph database systems as well as graph analytical systems. They support highly expressive data models consisting of multi-labeled nodes and edges, along with properties represented as key/value pairs. Property graphs serve as versatile data integration paradigms, enabling data in any format to be seamlessly transformed into this model. Moreover, they are at the core of an active standardization effort led by ISO/IEC, which aims to establish standardized declarative graph query languages such as GQL and SQL/PGQ. In addition to these standards for data manipulation languages, other languages have emerged for property graph schemas and constraints as part of future data definition languages.In this paper, we introduce a new declarative paradigm for expressing property graph transformations, supporting both graph data integration and data cleaning tasks. We discuss the properties of these transformations, along with algorithmic issues and considerations for efficiency and scalability. Furthermore, we showcase the utility of property graph transformations for causal analysis and elaborate on a research agenda aimed at designing analytical extensions of graph languages to support property graph transformations for advanced analytical workloads on heterogeneous data.},
  issn = {2150-8097},
  month = {sep},
}

@article{zhou_d-bot_2024,
  title = {D-{Bot},
  author = {Zhou, Xuanhe and Li, Guoliang and Sun, Zhaoyan and Liu, Zhiyuan and Chen, Weize and Wu, Jianming and Liu, Jiesi and Feng, Ruohang and Zeng, Guoyang},
  year = {2024},
  doi = {10.14778/3675034.3675043},
  url = {https://doi.org/10.14778/3675034.3675043},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {10},
  pages = {2514--2527},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Database administrators (DBAs) play an important role in managing database systems. However, it is hard and tedious for DBAs to manage vast database instances and give timely response (waiting for hours is intolerable in many online cases). In addition, existing empirical methods only support limited diagnosis scenarios, which are also labor-intensive to update the diagnosis rules for database version updates. Recently large language models (LLMs) have shown great potential in various fields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can automatically acquire knowledge from diagnosis documents, and generate reasonable and well-founded diagnosis report (i.e., identifying the root causes and solutions) within acceptable time (e.g., under 10 minutes compared to hours by a DBA). The techniques in D-Bot include (i) offline knowledge extraction from documents, (ii) automatic prompt generation (e.g., knowledge matching, tool retrieval), (iii) root cause analysis using tree search algorithm, and (iv) collaborative mechanism for complex anomalies with multiple root causes. We verify D-Bot on real benchmarks (including 539 anomalies of six typical applications), and the results show D-Bot can effectively identify root causes of unseen anomalies and significantly outperforms traditional methods and vanilla models like GPT-4.},
  issn = {2150-8097},
  month = {jun},
}

@article{wang_knowledge_2024,
  title = {Knowledge {Editing},
  author = {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
  year = {2024},
  doi = {10.1145/3698590},
  url = {https://doi.org/10.1145/3698590},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {fine-tuning, knowledge update, large language models, Model editing, source: ACM},
  abstract = {Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME), also known as Knowledge Editing or Model Editing, has attracted increasing attention, which aims at precisely modifying the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim at providing a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
  issn = {0360-0300},
  month = {nov},
}

@article{madden_databases_2024,
  title = {Databases {Unbound},
  author = {Madden, Samuel and Cafarella, Michael and Franklin, Michael and Kraska, Tim},
  year = {2024},
  doi = {10.14778/3685800.3685916},
  url = {https://doi.org/10.14778/3685800.3685916},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {12},
  pages = {4546--4554},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Over the past five decades, the relational database model has proven to be a scaleable and adaptable model for querying a variety of structured data, with use cases in analytics, transactions, graphs, streaming and more. However, most of the world's data is unstructured. Thus, despite their success, the reality is that the vast majority of the world's data has remained beyond the reach of relational systems.The rise of deep learning and generative AI offers an opportunity to change this. These models provide a stunning capability to extract semantic understanding from almost any type of document, including text, images, and video, which can extend the reach of databases to all the world's data. In this paper we explore how these new technologies will transform the way we build database management software, creating new that systems that can ingest, store, process, and query all data. Building such systems presents many opportunities and challenges. In this paper we focus on three: scalability, correctness, and reliability, and argue that the declarative programming paradigm that has served relational systems so well offers a path forward in the new world of AI data systems as well. To illustrate this, we describe several examples of such declarative AI systems we have built in document and video processing, and provide a set of research challenges and opportunities to guide research in this exciting area going forward.And lovely apparitions, -dim at first,Then radiant, as the mind arising brightFrom the embrace of beauty (whence the formsOf which these are the phantoms) casts on themThe gathered rays which are reality-Shall visit us the progeny immortalOf Painting, Sculpture, and rapt Poesy,And arts, though unimagined, yet to be;Prometheus Unbound, Percy Bysshe Shelley},
  month = {aug},
  issn = {2150-8097},
}

@article{sun_gaussdb-vector_2025,
  title = {{GaussDB},
  author = {Sun, Ji and Li, Guoliang and Pan, James and Wang, Jiang and Xie, Yongqing and Liu, Ruicheng and Nie, Wen},
  year = {2025},
  doi = {10.14778/3750601.3750619},
  url = {https://doi.org/10.14778/3750601.3750619},
  journal = {Proc. VLDB Endow.},
  volume = {18},
  number = {12},
  pages = {4951--4963},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Vector databases are widely used as a fundamental tool for addressing the weaknesses of large language model (LLM) applications, specifically hallucinations and the high cost of inference. However, existing vector databases either cater to niche applications with low-latency in-memory search, or offer sophisticated data management capabilities but at the cost of low performance.To address these limitations, we propose GaussDB-Vector, a high-performance, real-time persistent vector database that excels in low-latency scalable search, real-time inserts and deletes, high availability, large-scale distributed search, and hybrid scalar-vector filtered search capabilities. These features are primarily achieved through an innovative storage architecture designed for a graph-based vector index, optimized for I/O operations and adaptable across various dataset sizes and dimensions, complemented by novel buffering strategies to further reduce I/O burdens. GaussDB-Vector supports product quantization, parallel search, and hardware acceleration via SIMD, GPUs, and NPUs in order to further accelerate queries. Experimental results show that GaussDB-Vector outperforms competitive baselines by a factor of 1 to 5 times.},
  issn = {2150-8097},
  month = {sep},
}

@article{thulke_task-oriented_2023,
  title = {Task-{Oriented},
  author = {Thulke, David and Daheim, Nico and Dugast, Christian and Ney, Hermann},
  year = {2023},
  doi = {10.1109/TASLP.2023.3267832},
  url = {https://doi.org/10.1109/taslp.2023.3267832},
  journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  volume = {32},
  pages = {733--741},
  note = {Publisher: IEEE Press},
  keywords = {source: ACM},
  abstract = {This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel model that allows for increasing the factuality of the generated responses. In addition to summarizing our previous contributions, in this work, we also report on a few small improvements and reconsider the automatic evaluation metrics for the generation task which have shown a low correlation to human judgments.},
  issn = {2329-9290},
  month = {apr},
}

@article{happe_can_2025,
  title = {Can {LLMs},
  author = {Happe, Andreas and Cito, Jürgen},
  year = {2025},
  doi = {10.1145/3766895},
  url = {https://doi.org/10.1145/3766895},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Traditional enterprise penetration-testing, while critical for validating defenses and uncovering vulnerabilities, is often limited by high operational costs and the scarcity of human expertise. This paper investigates the feasibility and effectiveness of using Large Language Model (LLM)-driven autonomous systems to address these challenges in real-world Active Directory (AD) enterprise networks.We introduce a novel prototype, cochise, designed to employ LLMs to autonomously perform Assumed Breach penetration-testing against enterprise networks. Our system represents the first demonstration of a fully autonomous, LLM-driven framework capable of compromising accounts within a real-life Microsoft Active Directory testbed, the Game of Active Directory (GOAD). The evaluation deliberately utilizes GOAD to capture the intricate interactions and sometimes nondeterministic outcomes of live network penetration-testing, moving beyond the limitations of synthetic benchmarks.We perform our empirical evaluation using five LLMs, comparing reasoning to non-reasoning models as well as including open-weight models. Through comprehensive quantitative and qualitative analysis, incorporating insights from cybersecurity experts, we demonstrate that autonomous LLMs can effectively conduct Assumed Breach simulations. Key findings highlight their ability to dynamically adapt attack strategies, perform inter-context attacks (e.g., web application audits, social engineering, and unstructured data analysis for credentials), and generate scenario-specific attack parameters like realistic password candidates. The prototype also exhibits robust self-correction mechanisms, automatically installing missing tools and rectifying invalid command generations.Critically, we find that the associated costs are competitive with, and often significantly lower than, those incurred by professional human penetration testers, suggesting a path toward democratizing access to essential security testing for organizations with budgetary constraints. However, our research also illuminates existing limitations, including instances of LLM “going down rabbit holes”, challenges in comprehensive information transfer between planning and execution modules, and critical safety concerns that necessitate human oversight. Our findings lay foundational groundwork for future software engineering research into LLM-driven cybersecurity automation, emphasizing that the prototype's underlying LLM-driven architecture and techniques are domain-agnostic and hold promise for improving autonomous LLM usage in broader software engineering domains. The source code, traces, and analyzed logs are open-sourced to foster collective cybersecurity and future research.},
  month = {sep},
  annote = {Just Accepted},
  issn = {1049-331X},
}

@article{li_generative_2024,
  title = {Generative {AI},
  author = {Li, Jialong and Zhang, Mingyue and Li, Nianyu and Weyns, Danny and Jin, Zhi and Tei, Kenji},
  year = {2024},
  doi = {10.1145/3686803},
  url = {https://doi.org/10.1145/3686803},
  journal = {ACM Trans. Auton. Adapt. Syst.},
  volume = {19},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {diffusion model, Generative AI, Large Language Model, MAPE, Self-Adaptive Systems, survey, source: ACM},
  abstract = {Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this article aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI’s within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.†},
  issn = {1556-4665},
  month = {sep},
}

@article{gruetzemacher_deep_2022,
  title = {Deep {Transfer},
  author = {Gruetzemacher, Ross and Paradice, David},
  year = {2022},
  doi = {10.1145/3505245},
  url = {https://doi.org/10.1145/3505245},
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {10s},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {artificial intelligence, deep learning, language models, Natural language processing, text mining, transfer learning, source: ACM},
  abstract = {AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving transformer language models (TLMs) offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizing text mining in top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher quality text analytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.},
  issn = {0360-0300},
  month = {sep},
}

@article{xu_tapping_2025,
  title = {Tapping the {Potential},
  author = {Xu, Lanling and Zhang, Junjie and Li, Bingqian and Wang, Jinpeng and Chen, Sheng and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2025},
  doi = {10.1145/3726871},
  url = {https://doi.org/10.1145/3726871},
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {19},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Empirical Study, Large Language Models, Recommender Systems, source: ACM},
  abstract = {Recently, Large Language Models (LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompt engineering. We propose a general framework for leveraging LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task descriptions, user interest modeling, candidate items construction, and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on recommendation performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research.},
  issn = {1556-4681},
  month = {jun},
}

@article{duan_pgtuner_2025,
  title = {{PGTuner},
  author = {Duan, Hao and Song, Yitong and Yao, Bin and Liang, Anqi},
  year = {2025},
  doi = {10.1145/3749179},
  url = {https://doi.org/10.1145/3749179},
  journal = {Proc. ACM Manag. Data},
  volume = {3},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {approximate nearest neighbor search, configuration tuning, model transfer, proximity graph, source: ACM},
  abstract = {Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key areas. Proximity graphs (PGs) are the leading method for ANNS, offering the best balance between query efficiency and accuracy. However, their performance heavily depends on various construction and query parameters, which are difficult to optimize due to their complex interdependencies. Given that users often prioritize specific accuracy levels, efficiently identifying the optimal PG configurations to meet these targets is essential. Although some studies have explored automatic configuration tuning for PGs, they are limited by inefficiencies and suboptimal results. These issues stem from the need to construct numerous PGs for searching and re-tuning from scratch whenever the dataset changes, as well as the failure to capture the complex dependencies between configurations, query performance, and tuning objectives. To address these challenges, we propose PGTuner, an efficient framework for automatic PG configuration tuning leveraging pre-training knowledge and model transfer techniques. PGTuner improves efficiency through a pre-trained query performance prediction (QPP) model, eliminating the need to build multiple PGs. It also features a deep reinforcement learning-based parameter configuration recommendation (PCR) model to recommend optimal configurations for specific datasets and accuracy targets. Additionally, PGTuner incorporates out-of-distribution detection and deep active learning for efficient tuning in dynamic scenarios and transferring to new datasets. Extensive experiments demonstrate that PGTuner can stably achieve the top-level tuning effect across different datasets while significantly improving tuning efficiency by up to 14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner are available online at https://github.com/hao-duan/PGTuner.},
  month = {sep},
}

@article{lang_leveraging_2025,
  title = {Leveraging {LLMs},
  author = {Lang, Jan-Hendrik and Schreck, Thomas},
  year = {2025},
  doi = {10.1145/3748263},
  url = {https://doi.org/10.1145/3748263},
  journal = {Digital Threats},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Digital Forensics, LLM, Malware, Memory Forensics, Volatility3, source: ACM},
  abstract = {Memory forensics plays an important role in modern digital investigations in terms of detecting stealthy, fileless malware, and advanced persistent threats. Moreover, large language models (LLMs) have shown promise in different cybersecurity tasks. In this paper, we integrate intelligence based on LLM into memory forensic workflows and evaluate multiple LLMs, including OpenAI GPT4o, OpenAI o1, Gemini 2.0 Flash, Gemini 2.0 Flash-Thinking, Grok 3, and Grok 3 with thinking mode enabled. We collect memory dumps encompassing a variety of attack scenarios such as process injection (using msfvenom), a PowerShell Empire-based attack, and real-world malware such as Quasar RAT, MassLogger, DarkCloud, LockBit, and LockiBot. Our evaluation includes accuracy, precision, recall and F1 score metrics and statistical analyses (ANOVA and correlation tests). The findings show that the reasoning-based (’thinking’) LLM models outperform standard models. OpenAI o1 and Gemini Flash-Thinking excel at decoding base64 obfuscated payloads, while Grok3 leads in detecting network anomalies. All LLM-based approaches suffer from high false-positive (FP) rates, reflected in low precision (often \&lt;20\%). This tendency appears to stem from the precautionary principle in AI safety orientation, leading to models erring on the side of caution and occasionally hallucinating plausible threats when faced with ambiguous or incomplete evidence. The LockBit indicator of compromise (IoC) could not be detected with the LLM because the IoCs lie beyond the Volatility3 modules used. Due to this reason and the limited size of the context window from the LLM, it is essential to select appropriate data. Despite limitations, the study demonstrates the practical viability of integrating LLM-driven intelligence into a forensic system. The study lays the foundation for hybrid forensic systems combining symbolic analysis, domain-specific heuristics, and LLM-driven intelligence.},
  annote = {Just Accepted},
  month = {jul},
}

@article{hrckova_autonomation_2025,
  title = {Autonomation, {Not},
  author = {Hrckova, Andrea and Moro, Robert and Srba, Ivan and Simko, Jakub and Bielikova, Maria},
  year = {2025},
  doi = {10.1145/3764592},
  url = {https://doi.org/10.1145/3764592},
  journal = {ACM J. Responsib. Comput.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {disinformation, fact-checkers, human-centered artificial intelligence, human-information interaction, misinformation, source: ACM},
  abstract = {To mitigate the negative effects of false information more effectively, the development of Artificial Intelligence\&nbsp;(AI)\&nbsp;systems to assist fact-checkers is needed. Nevertheless, the lack of focus on the needs of these stakeholders results in their limited acceptance and skepticism toward automating the whole fact-checking process. In this study, we conducted semi-structured in-depth interviews with Central European fact-checkers. Their activities and problems were analyzed using iterative content analysis. The most significant problems were validated with a survey of European fact-checkers, in which we collected 24 responses from 20 countries, i.e., 62\% of active European signatories of the International Fact-Checking Network (IFCN). Our contributions include an in-depth examination of the variability of fact-checking work in non-English-speaking regions, which still remained largely uncovered. By aligning them with the knowledge from prior studies, we created conceptual models that help to understand the fact-checking processes. In addition, we mapped our findings on the fact-checkers’ activities and needs to the relevant tasks for AI research, while providing a discussion on three AI tasks that were not covered by previous similar studies. The new opportunities identified for AI researchers and developers have implications for the focus of AI research in this domain.},
  annote = {Just Accepted},
  month = {sep},
}

@article{bai_kinet_2023,
  title = {{KINet},
  author = {Bai, Jiaqi and Yang, Ze and Yang, Jian and Guo, Hongcheng and Li, Zhoujun},
  year = {2023},
  doi = {10.1109/TASLP.2023.3240654},
  url = {https://doi.org/10.1109/taslp.2023.3240654},
  journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  volume = {31},
  pages = {1213--1222},
  note = {Publisher: IEEE Press},
  keywords = {source: ACM},
  abstract = {Knowledge-grounded conversation has led to great progress in producing informative dialog responses by leveraging external knowledge. This work focuses on two affiliated knowledge grounded conversation tasks: \&lt;italic\&gt;Knowledge Selection\&lt;/italic\&gt; and \&lt;italic\&gt;Response Generation\&lt;/italic\&gt;. Previous work followed the paradigm of selecting the most optimal knowledge piece to guide the conversation towards generating the proper response. However, some knowledge pieces, which are not recognized as optimal, may still benefit response generation. How to effectively leverage these relevant knowledge pieces for response generation still remain a tricky issue. To address this problem, we propose \&lt;sc\&gt;\&lt;bold\&gt;KINet\&lt;/bold\&gt;\&lt;/sc\&gt;, a \&lt;bold\&gt;K\&lt;/bold\&gt;nowledge \&lt;bold\&gt;I\&lt;/bold\&gt;ncorporation \&lt;bold\&gt;Net\&lt;/bold\&gt;work, which deals with the problem by boosting both the knowledge selection and the response generation. The proposed model contains a negative enhanced knowledge approximator which improves knowledge selection by enhancing the dense representation of knowledge pieces, and a curriculum knowledge sampler which improves generated responses by incorporating more relevant knowledge pieces in an easy-to-hard manner. We conduct the experiment on two datasets of knowledge-grounded conversations, the results show that the proposed model significantly outperforms state-of-the-art methods in terms of both automatic and human evaluations.},
  issn = {2329-9290},
  month = {jan},
}

@article{ling_domain_2025,
  title = {Domain {Specialization},
  author = {Ling, Chen and Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Li, Yun and Cui, Hejie and Zhang, Xuchao and Zhao, Tianjiao and Panalkar, Amit and Mehta, Dhagash and Pasquali, Stefano and Cheng, Wei and Wang, Haoyu and Liu, Yanchi and Chen, Zhengzhang and Chen, Haifeng and White, Chris and Gu, Quanquan and Pei, Jian and Yang, Carl and Zhao, Liang},
  year = {2025},
  doi = {10.1145/3764579},
  url = {https://doi.org/10.1145/3764579},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {domain specialization, Large language models, natural language processing, source: ACM},
  abstract = {Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to making large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to summarize better and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.},
  issn = {0360-0300},
  month = {oct},
}

@article{fu_should_2025,
  title = {Should {ChatGPT},
  author = {Fu, Yue and Chen, Yixin and Gomes Da Costa Lai, Zelia and Hiniker, Alexis},
  year = {2025},
  doi = {10.1145/3757424},
  url = {https://doi.org/10.1145/3757424},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {ai-mediated relationship, breakup, generative ai, interview, mental health, recovery, source: ACM},
  abstract = {Relationships are essential to our happiness and wellbeing, yet their dissolution-the final stage of a relationship's lifecycle-is among the most stressful events individuals can experience, often leading to profound and lasting impacts. With the breakup process increasingly facilitated by technology, such as computer-mediated communication, and the likely future influence of generative AI tools, we conducted a semi-structured interview study with 21 participants. We aim to understand: 1) the current role of technology in the breakup process, 2) the needs and support individuals seek during this time, and 3) how GenAI might address or undermine these needs. Our findings show that people have distinct needs at various stages of breakups. While currently technology plays an important role, it falls short in supporting users' unmet needs. Participants envision that GenAI could: 1) aid in prompting self-reflection, providing neutral second opinions, and assisting with planning leading up to a breakup; 2) serve as a communication mediator, supporting wording and tone to facilitate emotional expression during breakup conversations; and 3) support personal growth and offer companionship after a breakup. However, our findings also reveal participants' various concerns about involving GenAI in this process. Based on our results, we discuss the potential opportunities, harms, and design implications of GenAI tools in facilitating people's relationship dissolution.},
  month = {oct},
}

@article{ecormier-nocca_authoring_2021,
  title = {Authoring consistent landscapes with flora and fauna},
  author = {Ecormier-Nocca, Pierre and Cordonnier, Guillaume and Carrez, Philippe and Moigne, Anne-Marie and Memari, Pooran and Benes, Bedrich and Cani, Marie-Paule},
  year = {2021},
  doi = {10.1145/3450626.3459952},
  url = {https://doi.org/10.1145/3450626.3459952},
  journal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {ecosystems, natural phenomena, source: ACM},
  abstract = {We present a novel method for authoring landscapes with flora and fauna while considering their mutual interactions. Our algorithm outputs a steady-state ecosystem in the form of density maps for each species, their daily circuits, and a modified terrain with eroded trails from a terrain, climatic conditions, and species with related biological information. We introduce the Resource Access Graph, a new data structure that encodes both interactions between food chain levels and animals traveling between resources over the terrain. A novel competition algorithm operating on this data progressively computes a steady-state solution up the food chain, from plants to carnivores. The user can explore the resulting landscape, where plants and animals are instantiated on the fly, and interactively edit it by over-painting the maps. Our results show that our system enables the authoring of consistent landscapes where the impact of wildlife is visible through animated animals, clearings in the vegetation, and eroded trails. We provide quantitative validation with existing ecosystems and a user-study with expert paleontologist end-users, showing that our system enables them to author and compare different ecosystems illustrating climate changes over the same terrain while enabling relevant visual immersion into consistent landscapes.},
  issn = {0730-0301},
  month = {jul},
}

@article{zhang_recommendation_2025,
  title = {Recommendation as {Instruction},
  author = {Zhang, Junjie and Xie, Ruobing and Hou, Yupeng and Zhao, Xin and Lin, Leyu and Wen, Ji-Rong},
  year = {2025},
  doi = {10.1145/3708882},
  url = {https://doi.org/10.1145/3708882},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Instruction Tuning, Large Language Models, Recommender Systems, source: ACM},
  abstract = {In the past few decades, recommender systems have attracted much attention in both research and industry communities. Existing recommendation models mainly learn the underlying user preference from historical behavior data (typically in the forms of item IDs), and then estimate the user–item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we develop a different recommendation paradigm, considering recommendation as instruction following by LLMs. The key idea is that the needs of a user can be expressed in natural language descriptions (called instructions), so that LLMs can understand and further execute the instruction for fulfilling the recommendation. For this purpose, we instruction tune the 3B Flan-T5-XL, to better adapt LLMs to recommender systems. We first design a general instruction format for describing the preference, intention, and task form of a user in natural language. Then we manually design 39 instruction templates and automatically generate large amounts of user-personalized instruction data with varying types of preferences and intentions. To demonstrate the effectiveness of our approach, we instantiate the instructions into several widely studied recommendation (or search) tasks, and conduct extensive experiments with real-world datasets. Experiment results show that our approach can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks. Our approach sheds light on developing user-friendly recommender systems, in which users can freely communicate with the system and obtain accurate recommendations via natural language instructions.},
  issn = {1046-8188},
  month = {jul},
}

@article{wong_decllm_2025,
  title = {{DecLLM},
  author = {Wong, Wai Kin and Wu, Daoyuan and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
  year = {2025},
  doi = {10.1145/3728958},
  url = {https://doi.org/10.1145/3728958},
  journal = {Proc. ACM Softw. Eng.},
  volume = {2},
  number = {ISSTA},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Large Language Model, Recompilable Decompilation, Reverse Engineering, source: ACM},
  abstract = {Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path. In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70\% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30\% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.},
  month = {jun},
}

@article{chung_is_2025,
  title = {Is {Long},
  author = {Chung, Yeounoh and Kakkar, Gaurav T. and Gan, Yu and Milne, Brenton and Özcan, Fatma},
  year = {2025},
  doi = {10.14778/3742728.3742761},
  url = {https://doi.org/10.14778/3742728.3742761},
  journal = {Proc. VLDB Endow.},
  volume = {18},
  number = {8},
  pages = {2735--2747},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.In this work, we explore the performance and the latency tradeoffs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (gemini-1.5-pro). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's Gemini-pro-1.5 achieves strong performance across multiple benchmark datasets without fine-tuning or expensive self-consistency based techniques.},
  issn = {2150-8097},
  month = {sep},
}

@article{xi_efficient_2025,
  title = {Efficient and {Deployable},
  author = {Xi, Yunjia and Liu, Weiwen and Lin, Jianghao and Weng, Muyan and Cai, Xiaoling and Zhu, Hong and Zhu, Jieming and Chen, Bo and Tang, Ruiming and Yu, Yong and Zhang, Weinan},
  year = {2025},
  doi = {10.1145/3725894},
  url = {https://doi.org/10.1145/3725894},
  journal = {ACM Trans. Recomm. Syst.},
  volume = {4},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {knowledge augmentation, large language models, Recommender systems, source: ACM},
  abstract = {Recommender system plays a pervasive role in today’s online services, yet its closed-loop nature, i.e., training and deploying within a specific closed domain, constrains its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating advanced reasoning capabilities. However, previous attempts to directly implement LLMs as recommenders fall short in meeting the demanding requirements of industrial recommender systems, particularly in terms of online inference latency and offline resource efficiency. In this work, we propose an Open-World Recommendation Framework with Efficient and Deployable Knowledge Infusion from Large Language Models, dubbed REKI, to acquire two types of external knowledge about users and items from LLMs. Specifically, we introduce factorization prompting to elicit accurate knowledge reasoning on user preferences and items. With factorization prompting, we develop individual knowledge extraction and collective knowledge extraction tailored for different scales of recommendation scenarios, effectively reducing offline resource consumption. Subsequently, the generated user and item knowledge undergoes efficient transformation and condensation into augmented vectors through a hybridized expert-integrated network, ensuring its compatibility with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any conventional recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments demonstrate that REKI significantly outperforms the state-of-the-art baselines and is compatible with a diverse array of recommendation algorithms and tasks. Now, REKI has been deployed to Huawei’s news and music recommendation platforms and gained a 7\% and 1.99\% improvement during the online A/B test.},
  month = {jul},
}

@article{yang_requirements-based_2025,
  title = {Requirements-{Based},
  author = {Yang, Zhenzhen and Huang, Rubing and Cui, Chenhui and Niu, Nan and Towey, Dave},
  year = {2025},
  doi = {10.1145/3771727},
  url = {https://doi.org/10.1145/3771727},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {software requirements, Software testing, survey, test generation, source: ACM},
  abstract = {As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirements types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {oct},
}

@article{liu_improving_2025-1,
  title = {Improving {Emotional},
  author = {Liu, Yiren and Li, Yerong and Mayfield, Ryan and Huang, Yun},
  year = {2025},
  doi = {10.1145/3711012},
  url = {https://doi.org/10.1145/3711012},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Emotional support is a crucial aspect of communication between community members and police dispatchers during incident reporting. However, there is a lack of understanding about how emotional support is delivered through text-based systems, especially in various non-emergency contexts. In this study, we analyzed two years of chat logs comprising 57,114 messages across 8,239 incidents from 130 higher education institutions. Our empirical findings revealed significant variations in emotional support provided by dispatchers, influenced by the type of incident, service time, and a noticeable decline in support over time across multiple organizations. To improve the consistency and quality of emotional support, we developed and implemented a fine-tuned Large Language Model (LLM), named dispatcherLLM, designed to suggest replies through simulating human dispatchers' languages with appropriate emotional support. We evaluated dispatcherLLM by comparing its generated responses to those of human dispatchers and other off-the-shelf models using real chat messages. Additionally, we conducted a human evaluation to assess the perceived effectiveness of the support provided by dispatcherLLM. This study not only contributes new empirical understandings of emotional support in text-based dispatch systems but also demonstrates the significant potential of generative AI in improving service delivery.},
  month = {may},
}

@article{iannuzzi_improving_2025,
  title = {Improving {Accessibility},
  author = {Iannuzzi, Nicola and Manca, Marco and Paterno', Fabio and Santoro, Carmen},
  year = {2025},
  doi = {10.1145/3762815},
  url = {https://doi.org/10.1145/3762815},
  journal = {ACM J. Responsib. Comput.},
  volume = {2},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Digital accessibility is considered an important aspect to allow all people, including those with permanent or temporary disabilities, to access the continuously increasing number of digital services. This raises the need for tools able to provide support for monitoring the accessibility of many websites, to understand their level of accessibility, and identify the areas that need more interventions for their improvement. The primary objectives of this work are to describe the challenges that have been faced in extending the MAUVE++ tool to carry out web accessibility validation in the context of large-scale assessments and provide an up-to-date view of the accessibility landscape of public administration websites. We discuss how the tool was adapted to analyse more than four million web pages and associated PDF documents from Italian public administration websites, assessing their compliance with WCAG guidelines. The results of this large-scale analysis have been utilised to provide insights on the state of accessibility at the national level, through a dedicated public dashboard updated quarterly, and stimulate its improvement. This research identifies the most violated success criteria, thus highlighting areas that should require attention from web developers and designers. Finally, we highlight some broader as well as practical implications of this research, and to what extent it can stimulate a more responsible approach and attention to accessibility aspects.},
  month = {oct},
}

@article{kuang_natural_2025,
  title = {Natural {Language},
  author = {Kuang, Jiayi and Shen, Ying and Xie, Jingyou and Luo, Haohao and Xu, Zhe and Li, Ronghao and Li, Yinghui and Cheng, Xianfeng and Lin, Xika and Han, Yu},
  year = {2025},
  doi = {10.1145/3711680},
  url = {https://doi.org/10.1145/3711680},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {8},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {multimodal large language models, multimodal representation and reasoning, Visual question answering, source: ACM},
  abstract = {Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge. Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.},
  issn = {0360-0300},
  month = {mar},
}

@article{khanshan_evaluation_2024,
  title = {Evaluation of {Code},
  author = {Khanshan, Alireza and Van Gorp, Pieter and Markopoulos, Panos},
  year = {2024},
  doi = {10.1145/3661143},
  url = {https://doi.org/10.1145/3661143},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {EICS},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Behavior Simulation, Experience Sampling Method, Large Language Model, Prompt Engineering, source: ACM},
  abstract = {The Experience Sampling Method (ESM) is commonly used to understand behaviors, thoughts, and feelings in the wild by collecting self-reports. Sustaining sufficient response rates, especially in long-running studies remains challenging. To avoid low response rates and dropouts, experimenters rely on their experience, proposed methodologies from earlier studies, trial and error, or the scarcely available participant behavior data from previous ESM protocols. This approach often fails in finding the acceptable study parameters, resulting in redesigning the protocol and repeating the experiment. Research has shown the potential of machine learning to personalize ESM protocols such that ESM prompts are delivered at opportune moments, leading to higher response rates. The corresponding training process is hindered due to the scarcity of open data in the ESM domain, causing a cold start, which could be mitigated by simulating participant behavior. Such simulations provide training data and insights for the experimenters to update their study design choices. Creating this simulation requires behavioral science, psychology, and programming expertise. Large language models (LLMs) have emerged as facilitators for information inquiry and programming, albeit random and occasionally unreliable. We aspire to assess the readiness of LLMs in an ESM use case. We conducted research using GPT-3.5 turbo-16k to tackle an ESM simulation problem. We explored several prompt design alternatives to generate ESM simulation programs, evaluated the output code in terms of semantics and syntax, and interviewed ESM practitioners. We found that engineering LLM-enabled ESM simulations have the potential to facilitate data generation, but they perpetuate trust and reliability challenges.},
  month = {jun},
}

@article{sun_exploring_2024,
  title = {Exploring {Parent},
  author = {Sun, Yuling and Chen, Jiaju and Yao, Bingsheng and Liu, Jiali and Wang, Dakuo and Ma, Xiaojuan and Lu, Yuxuan and Xu, Ying and He, Liang},
  year = {2024},
  doi = {10.1145/3687035},
  url = {https://doi.org/10.1145/3687035},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CSCW2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Interactive storytelling is vital for preschooler development. While children's interactive partners have traditionally been their parents and teachers, recent advances in artificial intelligence (AI) have sparked a surge of AI-based storytelling and reading technologies. As these technologies become increasingly ubiquitous in preschoolers' lives, questions arise regarding how they function in practical storytelling and reading scenarios and, how parents, the most critical stakeholders, experience and perceive these technologies. This paper investigates these questions through a qualitative study with 17 parents of children aged 3-6. Our findings suggest that even though AI-based storytelling and reading technologies provide more immersive and engaging interaction, they still cannot meet parents' expectations due to a series of interactive and algorithmic challenges. We elaborate on these challenges and discuss the possible implications of future AI-based interactive storytelling technologies for preschoolers.},
  month = {nov},
}

@article{yang_socialmind_2025,
  title = {{SocialMind},
  author = {Yang, Bufang and Guo, Yunqi and Xu, Lilin and Yan, Zhenyu and Chen, Hongkai and Xing, Guoliang and Jiang, Xiaofan},
  year = {2025},
  doi = {10.1145/3712286},
  url = {https://doi.org/10.1145/3712286},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {9},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AR Glasses, Augmented Reality, Internet of Things, LLMs, Multi-modal Sensor Data, Proactive Assistive Systems, Social Interaction, source: ACM},
  abstract = {Social interactions are fundamental to human life. The recent emergence of large language models (LLMs)-based virtual assistants has demonstrated their potential to revolutionize human interactions and lifestyles. However, existing assistive systems mainly provide reactive services to individual users, rather than offering in-situ assistance during live social interactions with conversational partners. In this study, we introduce SocialMind, the first LLM-based proactive AR social assistive system that provides users with in-situ social assistance. SocialMind employs human-like perception leveraging multi-modal sensors to extract both verbal and nonverbal cues, social factors, and implicit personas, incorporating these social cues into LLM reasoning for social suggestion generation. Additionally, SocialMind employs a multi-tier collaborative generation strategy and proactive update mechanism to display social suggestions on Augmented Reality (AR) glasses, ensuring that suggestions are timely provided to users without disrupting the natural flow of conversation. Evaluations on three public datasets and a user study with 20 participants show that SocialMind achieves 38.3\% higher engagement compared to baselines, and 95\% of participants are willing to use SocialMind in their live social interactions.},
  month = {mar},
}

@article{ben_chaaben_utility_2025,
  title = {On the {Utility},
  author = {Ben Chaaben, Meriem and Burgueño, Lola and David, Istvan and Sahraoui, Houari},
  year = {2025},
  doi = {10.1145/3744920},
  url = {https://doi.org/10.1145/3744920},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {domain modeling, generative AI, language models, model-driven engineering, prompt learning, user study, source: ACM},
  abstract = {Model-driven engineering (MDE) simplifies software development through abstraction, yet challenges such as time constraints, incomplete domain understanding, and adherence to syntactic constraints hinder the design process. This paper presents a study to evaluate the usefulness of a novel approach utilizing large language models (LLMs) and few-shot prompt learning to assist in domain modeling. The aim of this approach is to overcome the need for extensive training of traditional AI-based completion algorithms on domain-specific datasets and to offer versatile support for various modeling activities, providing valuable recommendations to software modelers. To support this approach, we developed MAGDA, a user-friendly tool, through which we conduct a user study and assess the real-world applicability of our approach in the context of domain modeling, offering valuable insights into its usability and effectiveness.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {jun},
}

@article{xu_reimagining_2025,
  title = {Reimagining {Digital},
  author = {Xu, Chunchen and Ge, Xiao},
  year = {2025},
  doi = {10.1145/3757396},
  url = {https://doi.org/10.1145/3757396},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Digital environments today often lack affordances that cultivate a sense of place and embodied being and acting. We develop a theoretical framework for embodied place-making based on people's psychological tendency to balance structure (clarity, predictability, and certainty) with unstructuredness (openness and possibility). This framework provides a unified theoretical foundation for examining continuity and discontinuity in social psychological processes and behaviors across digital, physical, and natural environments. We illustrate this framework through an original story (NewsWood ) that reconceptualizes digital news reading as an experience akin to wandering through woods. We hope to inspire radical reimagining of how future digital environments can foster well-being across individuals, communities, societies, and the natural world within their interconnected relationships.},
  month = {oct},
}

@article{yang_drhouse_2024,
  title = {{DrHouse},
  author = {Yang, Bufang and Jiang, Siyang and Xu, Lilin and Liu, Kaiwei and Li, Hai and Xing, Guoliang and Chen, Hongkai and Jiang, Xiaofan and Yan, Zhenyu},
  year = {2024},
  doi = {10.1145/3699765},
  url = {https://doi.org/10.1145/3699765},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {8},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Diagnostic Reasoning Systems, Internet of Things, Knowledge Retrieval, LLMs, Proactive Conversational Systems, Sensor Data, Up-to-Date, source: ACM},
  abstract = {Large language models (LLMs) have the potential to transform digital healthcare, as evidenced by recent advances in LLM-based virtual doctors. However, current approaches rely on patient's subjective descriptions of symptoms, causing increased misdiagnosis. Recognizing the value of daily data from smart devices, we introduce a novel LLM-based multi-turn consultation virtual doctor system, DrHouse, which incorporates three significant contributions: 1) It utilizes sensor data from smart devices in the diagnosis process, enhancing accuracy and reliability. 2) DrHouse leverages continuously updating medical knowledge bases to ensure its model remains at diagnostic standard's forefront. 3) DrHouse introduces a novel diagnostic algorithm that concurrently evaluates potential diseases and their likelihood, facilitating more nuanced and informed medical assessments. Through multi-turn interactions, DrHouse determines the next steps, such as accessing daily data from smart devices or requesting in-lab tests, and progressively refines its diagnoses. Evaluations on three public datasets and our self-collected datasets show that DrHouse can achieve up to an 31.5\% increase in diagnosis accuracy over the state-of-the-art baselines. The results of a 32-participant user study show that 75\% medical experts and 91.7\% test subjects are willing to use DrHouse.},
  month = {nov},
}

@article{miao_towards_2025,
  title = {Towards {Efficient},
  author = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  year = {2025},
  doi = {10.1145/3754448},
  url = {https://doi.org/10.1145/3754448},
  journal = {ACM Comput. Surv.},
  volume = {58},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {algorithm, efficiency, inference, Large language model, serving, system, source: ACM},
  abstract = {In the rapidly evolving landscape of artificial intelligence (AI), generative large language models (LLMs) stand at the forefront, revolutionizing how we interact with our data. However, the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput. This survey addresses the imperative need for efficient LLM serving methodologies from a machine learning system (MLSys) research perspective, standing at the crux of advanced AI innovations and practical system optimizations. We provide in-depth analysis, covering a spectrum of solutions, ranging from cutting-edge algorithmic modifications to groundbreaking changes in system designs. The survey aims to provide a comprehensive understanding of the current state and future directions in efficient LLM serving, offering valuable insights for researchers and practitioners in overcoming the barriers of effective LLM deployment, thereby reshaping the future of AI.},
  issn = {0360-0300},
  month = {sep},
}

@article{karinshak_working_2023,
  title = {Working {With},
  author = {Karinshak, Elise and Liu, Sunny Xun and Park, Joon Sung and Hancock, Jeffrey T.},
  year = {2023},
  doi = {10.1145/3579592},
  url = {https://doi.org/10.1145/3579592},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {7},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {AI-mediated communication, large language models, message factors, natural language processing, persuasion, public health messaging},
  abstract = {Artificial Intelligence (AI) is a transformative force in communication and messaging strategy, with potential to disrupt traditional approaches. Large language models (LLMs), a form of AI, are capable of generating high-quality, humanlike text. We investigate the persuasive quality of AI-generated messages to understand how AI could impact public health messaging. Specifically, through a series of studies designed to characterize and evaluate generative AI in developing public health messages, we analyze COVID-19 pro-vaccination messages generated by GPT-3, a state-of-the-art instantiation of a large language model. Study 1 is a systematic evaluation of GPT-3's ability to generate pro-vaccination messages. Study 2 then observed peoples' perceptions of curated GPT-3-generated messages compared to human-authored messages released by the CDC (Centers for Disease Control and Prevention), finding that GPT-3 messages were perceived as more effective, stronger arguments, and evoked more positive attitudes than CDC messages. Finally, Study 3 assessed the role of source labels on perceived quality, finding that while participants preferred AI-generated messages, they expressed dispreference for messages that were labeled as AI-generated. The results suggest that, with human supervision, AI can be used to create effective public health messages, but that individuals prefer their public health messages to come from human institutions rather than AI sources. We propose best practices for assessing generative outputs of large language models in future social science research and ways health professionals can use AI systems to augment public health messaging.},
  month = {apr},
}

@article{zheng_understanding_2023,
  title = {Understanding {Safety},
  author = {Zheng, Qingxiao and Xu, Shengyang and Wang, Lingqing and Tang, Yiliu and Salvi, Rohan C. and Freeman, Guo and Huang, Yun},
  year = {2023},
  doi = {10.1145/3579630},
  url = {https://doi.org/10.1145/3579630},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {7},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {online harassment, safety design, safety risks, social virtual reality, source: ACM},
  abstract = {Understanding emerging safety risks in nuanced social VR spaces and how existing safety features are used is crucial for the future development of safe and inclusive 3D social worlds. Prior research on safety risks in social VR is mainly based on interview or survey data about social VR users' experiences and opinions, which lacks "in-situ observations" of how individuals react to these risks. Using two empirical studies, this paper seeks to understand safety risks and safety design in social VR. In Study 1, we investigated 212 YouTube videos and their transcripts that document social VR users' immediate experiences of safety risks as victims, attackers, or bystanders. We also analyzed spectators' reactions to these risks shown in comments to the videos. In Study 2, we summarized 13 safety features across various social VR platforms and mapped how each existing safety feature in social VR can mitigate the risks identified in Study 1. Based on the uniqueness of social VR interaction dynamics and users' multi-modal simulated reactions, we call for further re-thinking and re-approaching safety designs for future social VR environments and propose potential design implications for future safety protection mechanisms in social VR.},
  month = {apr},
}

@article{poon_computer-mediated_2023,
  title = {Computer-{Mediated},
  author = {Poon, Anthony and Luebke, Matthew and Loughman, Julia and Lee, Ann and Guerrero, Lourdes and Sterling, Madeline and Dell, Nicola},
  year = {2023},
  doi = {10.1145/3579472},
  url = {https://doi.org/10.1145/3579472},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {7},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Home care workers (HCWs) provide essential care in patients' homes but are often underappreciated and work in stressful and isolated environments with diverse and intersecting support needs. This paper describes a computer-mediated peer support program that centers around sharing circles: spaces for personal, narrative storytelling to encourage HCWs to collaboratively reflect on their home care experiences and build rapport and shared identity with their peers. We describe the design of this program and a 12-week deployment that we conducted to evaluate the program with 42 HCWs in New York City. Our findings show that participants engaged in multiple types of peer support including emotional validation, learning how to navigate the workplace and patient care, defining and enabling good home care praxis, and building understanding around purpose and identity as HCWs. We discuss how these findings inform the design of technology and use of holistic pedagogies, such as storytelling, to enable this support in computer-mediated peer support programs. Such programs can help researchers and practitioners interested in addressing diverse needs that occur in intersectional contexts, such as that of HCWs and other marginalized populations.},
  month = {apr},
}

@article{alvarado_towards_2022,
  title = {Towards {Tangible},
  author = {Alvarado, Oscar and Vanden Abeele, Vero and Geerts, David and Verbert, Katrien},
  year = {2022},
  doi = {10.1145/3555757},
  url = {https://doi.org/10.1145/3555757},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {6},
  number = {CSCW2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {algorithmic experience, recommender systems, social computing, tangible algorithms, tangible interfaces, source: ACM},
  abstract = {Artificial Intelligence (AI) supports many of our everyday activities and decisions. However, personalized algorithmic recommendations often produce adverse experiences due to a lack of awareness, control, or transparency. While research has directed solutions on graphical user interfaces (GUIs), there are no explorations of Tangible User Interfaces (TUIs) to improve the experience with such systems, despite the valid existing academic arguments in favor of this exploration. Therefore, centering on transparency and control, we analyzed how 18 users of movie recommender systems perceived four different TUIs using individual co-design sessions and post-interview questionnaires. Through thematic analysis, we identified seven design considerations while designing TUIs to interact with algorithmic movie recommender systems: (1) Distinctions between TUIs and GUIs; (2) TUIs replacing predominant interfaces; (3) Preference for single-device TUIs; (4) The relevance of granular control for TUIs; (5) Apparent transparency limitations of TUIs; (6) TUIs and algorithmic social computing; and (7) Overview of specific design choices, including advantages and disadvantages of soft, hard, rounded, cubic, and humanoid interfaces. These findings inspired Recffy: the first functional TUI designed to enhance awareness and control in personalized movie recommendations. Based on this study, we propose the concept of Tangible Algorithms: TUIs dedicated to enhancing the interaction of algorithmic systems and their profiling processes or decisions in a specific context. Furthermore, we describe the relevance of tangible algorithms and design guidelines to promote them in diverse AI contexts. Finally, we invite the HCI and CSCW community to continue exploring tangible algorithms to address the interaction with algorithmic systems, including the collaborative and social computing dynamics they can promote in diverse AI contexts.},
  month = {nov},
}

@article{lalapura_recurrent_2021,
  title = {Recurrent {Neural},
  author = {Lalapura, Varsha S. and Amudha, J. and Satheesh, Hariramn Selvamuruga},
  year = {2021},
  doi = {10.1145/3448974},
  url = {https://doi.org/10.1145/3448974},
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {artificial intelligence (AI), compression, edge intelligence (EI), low-rank, Recurrent neural networks (RNNs), resource constrained modeling, sequence modeling, sparsity, training, source: ACM},
  abstract = {Recurrent Neural Networks are ubiquitous and pervasive in many artificial intelligence applications such as speech recognition, predictive healthcare, creative art, and so on. Although they provide accurate superior solutions, they pose a massive challenge “training havoc.” Current expansion of IoT demands intelligent models to be deployed at the edge. This is precisely to handle increasing model sizes and complex network architectures. Design efforts to meet these for greater performance have had inverse effects on portability on edge devices with real-time constraints of memory, latency, and energy. This article provides a detailed insight into various compression techniques widely disseminated in the deep learning regime. They have become key in mapping powerful RNNs onto resource-constrained devices. While compression of RNNs is the main focus of the survey, it also highlights challenges encountered while training. The training procedure directly influences model performance and compression alongside. Recent advancements to overcome the training challenges with their strengths and drawbacks are discussed. In short, the survey covers the three-step process, namely, architecture selection, efficient training process, and suitable compression technique applicable to a resource-constrained environment. It is thus one of the comprehensive survey guides a developer can adapt for a time-series problem context and an RNN solution for the edge.},
  issn = {0360-0300},
  month = {may},
}

@article{sparrow_towards_2024,
  title = {Towards {Ethical},
  author = {Sparrow, Lucy A. and Galwey, Ren and Jovic, Dahlia and Hardwick, Taylor and Butt, Mahli-Ann},
  year = {2024},
  doi = {10.1145/3677109},
  url = {https://doi.org/10.1145/3677109},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CHI PLAY},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {AI is increasingly being used to moderate player behaviour in online multiplayer games, working to identify and respond to toxic and problematic conduct with greater efficiency and accuracy than existing automated systems. However, little work has explored the application of AI moderation in the gaming ecosystem, despite growing ethical concerns about AI applications in other domains. In this study, we conducted 2 expert workshops and interviewed 26 players and industry professionals on their understandings, perceptions and experiences with AI moderation in multiplayer games. Applying a metaphorical frame via template analysis, we outline four metaphors that capture participants' views on the roles of AI and automation in moderation: the Unreliable Police Force, the Unscrupulous Governor, the Uncaring Judge, and the Untiring Assistant. We discuss these roles as exacerbating a top-down, punitive online justice system and identify ethical concerns around transparency, fairness and inclusion, privacy, and human-AI collaboration. To address these concerns, we put forward a set of ethical design considerations and alternative roles for AI moderation in multiplayer games.},
  month = {oct},
}

@article{asperti_syllabification_2021,
  title = {Syllabification of the {Divine},
  author = {Asperti, Andrea and Bianco, Stefano Dal},
  year = {2021},
  doi = {10.1145/3459011},
  url = {https://doi.org/10.1145/3459011},
  journal = {J. Comput. Cult. Herit.},
  volume = {14},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Dante Alighieri, Divina Commedia, Divine comedy, hendecasyllable, syllabification, Synalephe, source: ACM},
  abstract = {We provide a syllabification algorithm for the Divine Comedy using techniques from probabilistic and constraint programming. We particularly focus on the synalephe, addressed in terms of the "propensity" of a word to take part in a synalephe with adjacent words. We jointly provide an online vocabulary containing, for each word, information about its syllabification, the location of the tonic accent, and the aforementioned synalephe propensity, on the left and right sides. The algorithm is intrinsically nondeterministic, producing different possible syllabifications for each verse, with different likelihoods; metric constraints relative to accents on the 10th, 4th, and 6th syllables are used to further reduce the solution space. The most likely syllabification is hence returned as output. We believe that this work could be a major milestone for a lot of different investigations. From the point of view of digital humanities it opens new perspectives on computer-assisted analysis of digital sources, comprising automated detection of anomalous and problematic cases, metric clustering of verses and their categorization, or more foundational investigations addressing, e.g., the phonetic roles of consonants and vowels. From the point of view of text processing and deep learning, information about syllabification and the location of accents opens a wide range of exciting perspectives, from the possibility of automatic learning syllabification of words and verses to the improvement of generative models, aware of metric issues, and more respectful of the expected musicality.},
  issn = {1556-4673},
  month = {jul},
}

@article{kokosza_scintilla_2024,
  title = {Scintilla: {Simulating},
  author = {Kokosza, Andrzej and Wrede, Helge and Gonzalez Esparza, Daniel and Makowski, Milosz and Liu, Daoming and Michels, Dominik L. and Pirk, Soren and Palubicki, Wojtek},
  year = {2024},
  doi = {10.1145/3658192},
  url = {https://doi.org/10.1145/3658192},
  journal = {ACM Trans. Graph.},
  volume = {43},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {branch litter, combustion, fire spotting, fire spread, fluid dynamics, grass, level of detail, numerical simulation, physics-based modeling, understory, vegetation, wildfires, source: ACM},
  abstract = {Wildfires are a complex physical phenomenon that involves the combustion of a variety of flammable materials ranging from fallen leaves and dried twigs to decomposing organic material and living flora. All these materials can potentially act as fuel with different properties that determine the progress and severity of a wildfire. In this paper, we propose a novel approach for simulating the dynamic interaction between the varying components of a wildfire, including processes of convection, combustion and heat transfer between vegetation, soil and atmosphere. We propose a novel representation of vegetation that includes detailed branch geometry, fuel moisture, and distribution of grass, fine fuel, and duff. Furthermore, we model the ignition, generation, and transport of fire by firebrands and embers. This allows simulating and rendering virtual 3D wildfires that realistically capture key aspects of the process, such as progressions from ground to crown fires, the impact of embers carried by wind, and the effects of fire barriers and other human intervention methods. We evaluate our approach through numerous experiments and based on comparisons to real-world wildfire data.},
  issn = {0730-0301},
  month = {jul},
}

@article{gupta_first_2024,
  title = {The {First},
  author = {Gupta, Ankur and Sawhney, Sahil and Kompella, Kashyap},
  year = {2024},
  doi = {10.1145/3665495},
  url = {https://doi.org/10.1145/3665495},
  journal = {ACM Comput. Surv.},
  volume = {56},
  number = {11},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {first principles for the metaverse, Metaverse, metaverse security, source: ACM},
  abstract = {The metaverse delivered through converged and amalgamated technologies holds promise. No wonder technology heavyweights, large corporates, research organizations and businesses cutting across industry verticals are racing to put in place a metaverse-first strategy. The bets on consumers rapidly migrating from traditional social networks and collaborative applications to more immersive digital experiences have been placed. However, the transition is not expected to be seamless. Privacy, safety and security concerns abound in the early versions of the metaverse. Increased regulatory oversight and diverse national laws threaten to derail the hype around the metaverse. It is increasingly clear that the final iteration of the metaverse will need to assuage the concerns of individual users while addressing complex legal and regulatory requirements. Thus, a multi-perspective approach needs to be adopted to help set the agenda for the evolution of the metaverse. This research paper examines the different aspects and challenges which the future metaverse will need to address. A set of “first principles” are formulated, which if implemented will lead to the development of an equitable, inclusive, safe and secure metaverse.},
  issn = {0360-0300},
  month = {jul},
}

@article{xu_can_2024,
  title = {Can {Large},
  author = {Xu, Zhenyu and Xu, Hailin and Lu, Zhouyang and Zhao, Yingying and Zhu, Rui and Wang, Yujiang and Dong, Mingzhi and Chang, Yuhu and Lv, Qin and Dick, Robert P. and Yang, Fan and Lu, Tun and Gu, Ning and Shang, Li},
  year = {2024},
  doi = {10.1145/3659600},
  url = {https://doi.org/10.1145/3659600},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {8},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Developing chatbots as personal companions has long been a goal of artificial intelligence researchers. Recent advances in Large Language Models (LLMs) have delivered a practical solution for endowing chatbots with anthropomorphic language capabilities. However, it takes more than LLMs to enable chatbots that can act as companions. Humans use their understanding of individual personalities to drive conversations. Chatbots also require this capability to enable human-like companionship. They should act based on personalized, real-time, and time-evolving knowledge of their users. We define such essential knowledge as the common ground between chatbots and their users, and we propose to build a common-ground-aware dialogue system from an LLM-based module, named OS-1, to enable chatbot companionship. Hosted by eyewear, OS-1 can sense the visual and audio signals the user receives and extract real-time contextual semantics. Those semantics are categorized and recorded to formulate historical contexts from which the user's profile is distilled and evolves over time, i.e., OS-1 gradually learns about its user. OS-1 combines knowledge from real-time semantics, historical contexts, and user-specific profiles to produce a common-ground-aware prompt input into the LLM module. The LLM's output is converted to audio, spoken to the wearer when appropriate. We conduct laboratory and in-field studies to assess OS-1's ability to build common ground between the chatbot and its user. The technical feasibility and capabilities of the system are also evaluated. Our results show that by utilizing personal context, OS-1 progressively develops a better understanding of its users. This enhances user satisfaction and potentially leads to various personal service scenarios, such as emotional support and assistance.},
  month = {may},
}

@article{kordyaka_its_2025,
  title = {“{It},
  author = {Kordyaka, Bastian},
  year = {2025},
  doi = {10.1145/3748622},
  url = {https://doi.org/10.1145/3748622},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {attribution, esports, normalization, perpetration, Toxic behavior, toxicity, source: ACM},
  abstract = {Toxicity is a widespread challenge in various esports titles that continues to grow despite implementing various countermeasures. Attribution theory, consisting of the dimensions of locus of causality, stability, and internal and external control, offers a promising way to understand toxicity better, as how players interpret events in the game influences their perceptions and, ultimately, their toxic behavior. Given that attribution to external factors can reinforce and normalize toxic behavior in esports, this study investigates the timely topic of associations between toxic behavior and player attributions. To do so, we conducted an online survey with 217 League of Legends players. Our stepwise regression analysis indicates that players’ perceptions of external causality play a role in normalizing toxic behavior, whereas the perception that such behaviors are temporary enhances their execution. Additionally, the age of participants showed a substantial negative relationship with toxic normalization and toxic perpetration, indicating that older participants were significantly less likely to accept or engage in toxic behavior. In summary, players do not perceive toxic behavior as their responsibility, which explains the challenge of deriving effective intervention strategies against toxicity.},
  month = {oct},
}

@article{parker-holder_automated_2022,
  title = {Automated {Reinforcement},
  author = {Parker-Holder, Jack and Rajan, Raghu and Song, Xingyou and Biedenkapp, André and Miao, Yingjie and Eimer, Theresa and Zhang, Baohe and Nguyen, Vu and Calandra, Roberto and Faust, Aleksandra and Hutter, Frank and Lindauer, Marius},
  year = {2022},
  doi = {10.1613/jair.1.13596},
  url = {https://doi.org/10.1613/jair.1.13596},
  journal = {J. Artif. Int. Res.},
  volume = {74},
  note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
  keywords = {source: ACM},
  abstract = {The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems and also limits its full potential. In many other areas of machine learning, AutoML has shown that it is possible to automate such design choices, and AutoML has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games, such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey, we seek to unify the field of AutoRL, provide a common taxonomy, discuss each area in detail and pose open problems of interest to researchers going forward.},
  issn = {1076-9757},
  month = {sep},
}

@article{cui_senselens_2021,
  title = {{SenseLens},
  author = {Cui, Hang and Abdelzaher, Tarek},
  year = {2021},
  doi = {10.1145/3485047},
  url = {https://doi.org/10.1145/3485047},
  journal = {ACM Trans. Sen. Netw.},
  volume = {18},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {active learning, maximum likelihood estimation, semi supervision, Social sensing, truth discovery, source: ACM},
  abstract = {This article narrows the gap between physical sensing systems that measure physical signals and social sensing systems that measure information signals by (i) defining a novel algorithm for extracting information signals (building on results from text embedding) and (ii) showing that it increases the accuracy of truth discovery—the separation of true information from false/manipulated one. The work is applied in the context of separating true and false facts on social media, such as Twitter and Reddit, where users post predominantly short microblogs. The new algorithm decides how to aggregate the signal across words in the microblog for purposes of clustering the miscroblogs in the latent information signal space, where it is easier to separate true and false posts. Although previous literature extensively studied the problem of short text embedding/representation, this article improves previous work in three important respects: (1) Our work constitutes unsupervised truth discovery, requiring no labeled input or prior training. (2) We propose a new distance metric for efficient short text similarity estimation, we call Semantic Subset Matching, that improves our ability to meaningfully cluster microblog posts in the latent information signal space. (3) We introduce an iterative framework that jointly improves miscroblog clustering and truth discovery. The evaluation shows that the approach improves the accuracy of truth-discovery by 6.3\%, 2.5\%, and 3.8\% (constituting a 38.9\%, 14.2\%, and 18.7\% reduction in error, respectively) in three real Twitter data traces.},
  issn = {1550-4859},
  month = {oct},
}

@article{bak-coleman_collective_2022,
  title = {Collective wisdom in polarized groups},
  author = {Bak-Coleman, Joseph B and Tokita, Christopher K and Morris, Dylan H and Rubenstein, Daniel I and Couzin, Iain D},
  year = {2022},
  doi = {10.1177/26339137221104788},
  url = {https://doi.org/10.1177/26339137221104788},
  journal = {Collective Intelligence},
  volume = {1},
  number = {1},
  note = {Place: USA
Publisher: Sage Publications, Inc.},
  keywords = {collective wisdom, polarization, wisdom of the crowds, source: ACM},
  abstract = {The potential for groups to outperform the cognitive capabilities of even highly skilled individuals, known as the “wisdom of the crowd”, is crucial to the functioning of democratic institutions. In recent years, increasing polarization has led to concern about its effects on the accuracy of electorates, juries, courts, and congress. While there is empirical evidence of collective wisdom in partisan crowds, a general theory has remained elusive. Central to the challenge is the difficulty of disentangling the effect of limited interaction between opposing groups (homophily) from their tendency to hold opposing viewpoints (partisanship). To overcome this challenge, we develop an agent-based model of collective wisdom parameterized by the experimentally-measured behaviour of participants across the political spectrum. In doing so, we reveal that differences across the political spectrum in how individuals express and respond to knowledge interact with the structure of the network to either promote or undermine wisdom. We verify these findings experimentally and construct a more general theoretical framework. Finally, we provide evidence that incidental, context-specific differences across the political spectrum likely determine the impact of polarization. Overall, our results show that whether polarized groups benefit from collective wisdom is generally predictable but highly context-specific.},
  month = {sep},
}

@article{di_foraging_2022,
  title = {A {Foraging},
  author = {Di, Kai and Zhou, Yifeng and Yan, Fuhan and Jiang, Jiuchuan and Yang, Shaofu and Jiang, Yichuan},
  year = {2022},
  doi = {10.1145/3514499},
  url = {https://doi.org/10.1145/3514499},
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {13},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {dynamic programming, heuristic, integer programming, Mobile robot foraging, path planning, robotics in adversarial environments, source: ACM},
  abstract = {As an essential problem in robotics, foraging means that robots collect objects from a given environment and return them to a specified location. On many occasions, robots are required to perform foraging tasks in adversarial environments, such as battlefield rescue, where potential adversaries may damage robots with a certain probability. The longer an individual robot moves through adversarial environments, the higher the probability of being damaged by adversaries. The robot system can gain utility only when the robot brings carried objects back to a predetermined home station. Such a risk of being damaged makes returning home at different locations potentially relevant to the expected utility produced by the robot. Thus, the individual robot faces a dilemma when it responds to the potential risks in adversarial environments: whether to return the carried resources home or continue foraging tasks. In this article, two fundamental environment settings are discussed, homogeneous cases and heterogeneous cases. The former is analyzed as having both the optimal substructure property and the non-aftereffect property. Then, we present a dynamic programming (DP) algorithm that can find an optimal solution with polynomial time complexity. For the latter, it is proven that finding an optimal solution is ( mathcal NP ) -hard. We then propose a heuristic algorithm: A division hierarchical path planning (DHPP) algorithm that is based on the idea of dividing the foraging routes generated initially into a certain number of subroutes to dilute risks. Finally, these algorithms are extensively evaluated in simulations, concluding that in adversarial environments, they can significantly improve the productivity of an individual robot before it is damaged.},
  issn = {2157-6904},
  month = {jun},
}

@article{alvarez_de_la_vega_understanding_2023,
  title = {Understanding {Platform},
  author = {Alvarez de la Vega, Juan Carlos and Cecchinato, Marta E. and Rooksby, John and Newbold, Joseph},
  year = {2023},
  doi = {10.1145/3579539},
  url = {https://doi.org/10.1145/3579539},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {7},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {gig economy, online freelancing, Upwork, work practices, work-life balance, source: ACM},
  abstract = {Online freelancing platforms, such as Upwork, hold great promise in enabling flexible work opportunities where freelancers can combine their work with other life responsibilities, hereafter work-life. However, prior research suggests that platform features and self-managing demands of freelance work can jeopardise this apparent flexibility. In this paper, we report findings from a qualitative study, combining a 14-diary and semi-structured interview with 15 Upwork freelancers. We explored online freelancers' work practices, challenges, and the impact of platform features on their everyday lives. Our qualitative data suggest that platform features and individual context shape online freelancers' work-life practices. Freelancers develop strategies to mitigate platforms' constraints and balance their individual preferences and responsibilities. Further, our findings illustrate how platform features challenge freelancers' availability expectations, work autonomy, and work detachment. This paper contributes an empirical understanding of the factors influencing online freelancers' work-life practices by drawing upon Wanda J. Orlikowski's Structuration Model of Technology. This theoretical lens renders the interplay of freelancers, platforms, and instituted norms of freelance work.},
  month = {apr},
}

@article{ba_keep_2024,
  title = {Keep {It},
  author = {Ba, Jinsheng and Rigger, Manuel},
  year = {2024},
  doi = {10.1145/3654991},
  url = {https://doi.org/10.1145/3654991},
  journal = {Proc. ACM Manag. Data},
  volume = {2},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {join, logic bug, source: ACM},
  abstract = {Query optimizers perform various optimizations, many of which have been proposed to optimize joins. It is pivotal that these optimizations are correct, meaning that they should be extensively tested. Besides manually written tests, automated testing approaches have gained broad adoption. Such approaches semi-randomly generate databases and queries. More importantly, they provide a so-called test oracle that can deduce whether the system's result is correct. Recently, researchers have proposed a novel testing approach called Transformed Query Synthesis (TQS) specifically designed to find logic bugs in join optimizations. TQS is a sophisticated approach that splits a given input table into several sub-tables and validates the results of the queries that join these sub-tables by retrieving the given table. We studied TQS's bug reports, and found that 14 of 15 unique bugs were reported by showing discrepancies in executing the same query with different query plans. Therefore, in this work, we propose a simple alternative approach to TQS. Our approach enforces different query plans for the same query and validates that the results are consistent. We refer to this approach as Differential Query Plan (DQP) testing. DQP can reproduce 14 of the 15 unique bugs found by TQS, and found 26 previously unknown and unique bugs. These results demonstrate that a simple approach with limited novelty can be as effective as a complex, conceptually appealing approach. Additionally, DQP is complementary to other testing approaches for finding logic bugs. 81\% of the logic bugs found by DQP cannot be found by NoREC and TLP, whereas DQP overlooked 86\% of the bugs found by NoREC and TLP. We hope that the practicality of our approach—we implemented in less than 100 lines of code per system—will lead to its wide adoption.},
  month = {may},
}

@article{ramos_forsense_2022,
  title = {{ForSense},
  author = {Ramos, Gonzalo and Rachatasumrit, Napol and Suh, Jina and Ng, Rachel and Meek, Christopher},
  year = {2022},
  doi = {10.1145/3532853},
  url = {https://doi.org/10.1145/3532853},
  journal = {ACM Trans. Interact. Intell. Syst.},
  volume = {12},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Human-AI collaboration, sensemaking, source: ACM},
  abstract = {Online research is a frequent and important activity people perform on the Internet, yet current support for this task is basic, fragmented and not well integrated into web browser experiences. Guided by sensemaking theory, we present ForSense, a browser extension for accelerating people’s online research experience. The two primary sources of novelty of ForSense\&nbsp; are the integration of multiple stages of online research and providing machine assistance to the user by leveraging recent advances in neural-driven machine reading. We use ForSense\&nbsp; as a design probe to explore (1) the benefits of integrating multiple stages of online research, (2) the opportunities to accelerate online research using current advances in machine reading, (3) the opportunities to support online research tasks in the presence of imprecise machine suggestions, and (4) insights about the behaviors people exhibit when performing online research, the pages they visit, and the artifacts they create. Through our design probe, we observe people performing online research tasks, and see that they benefit from ForSense’s integration and machine support for online research. From the information and insights we collected, we derive and share key recommendations for designing and supporting imprecise machine assistance for research tasks.},
  issn = {2160-6455},
  month = {nov},
}

@article{chakraborty_experiences_2024,
  title = {Experiences from {Running},
  author = {Chakraborty, Dipanjan and Chatterjee, Sayonee and Tripathi, Rajeshwari and Gupta, Akshay and Seth, Aaditeshwar, Seth},
  year = {2024},
  doi = {10.1145/3653685},
  url = {https://doi.org/10.1145/3653685},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {digital divide, ivr, participatory media, platform for women, source: ACM},
  abstract = {Patriarchal practices and other socio-cultural norms in rural North India often prevent women from participating in digital platforms. In this paper we present a voice-based participatory community media platform that runs over simple voice-based phone calls. The platform is meant for women users and is led by women and has been used by 12,222 women users over the course of 20 months. We conduct a qualitative study and present several innovations, such as creating a safe space for women, which have contributed to more participation of women on the platform. We present a framework which can serve as a guidance for researchers and practitioners who want to set up digital platforms for women users.},
  month = {apr},
}

@article{ibrahim_understanding_2024,
  title = {Understanding {Online},
  author = {Ibrahim, Seray and Ang, Jazz Rui Xia and Petsolari, Melina and Michelson, Rebecca and Dong, Yuzhen and Theofanopoulou, Nikki and Van Kleek, Max and Davis, Katie and Slovák, Petr},
  year = {2024},
  doi = {10.1145/3653690},
  url = {https://doi.org/10.1145/3653690},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {online support systems, parenting, parenting interventions, socio-technical design, source: ACM},
  abstract = {Early parenting is one of the strongest predictors of child well-being. Online social communities have shown promise in supporting parents across a range of contexts. However, we only have a limited understanding of how posters and commenters interact within a forum, or how well commenter responses can support complex parenting questions, such as attempts to change a child's behaviour or to apply new parenting approaches. We start addressing this gap by combining an empirical analysis of 1 year of parent posts from an exemplar online forum (Mumsnet) with literature on parenting interventions from psychology. In particular, we examine the types of question parents of 2-5 year olds seek help for around their children's behaviour, and the challenges with the support that they do (or do not) receive from the Mumsnet community. Combining empirical and theory-driven insights, we outline an 'information-to-application' gap that conceptually underpins the difficulties observed, and suggest plausible research directions that could address such design problems.},
  month = {apr},
}

@article{seeman_between_2024,
  title = {Between {Privacy},
  author = {Seeman, Jeremy and Susser, Daniel},
  year = {2024},
  doi = {10.1145/3626494},
  url = {https://doi.org/10.1145/3626494},
  journal = {ACM J. Responsib. Comput.},
  volume = {1},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {critical code studies, Differential privacy, science and technology studies, source: ACM},
  abstract = {Differential privacy (DP) aims to confer data processing systems with inherent privacy guarantees, offering strong protections for personal data. But DP’s approach to privacy carries with it certain assumptions about how mathematical abstractions will be translated into real-world systems, which—if left unexamined and unrealized in practice—could function to shield data collectors from liability and criticism, rather than substantively protect data subjects from privacy harms. This article investigates these assumptions and discusses their implications for using DP to govern data-driven systems. In Parts 1 and 2, we introduce DP as, on one hand, a mathematical framework and, on the other hand, a kind of real-world sociotechnical system, using a hypothetical case study to illustrate how the two can diverge. In Parts 3 and 4, we discuss the way DP frames privacy loss, data processing interventions, and data subject participation, arguing it could exacerbate existing problems in privacy regulation. In part 5, we conclude with a discussion of DP’s potential interactions with the endogeneity of privacy law, and we propose principles for best governing DP systems. In making such assumptions and their consequences explicit, we hope to help DP succeed at realizing its promise for better substantive privacy protections.},
  month = {mar},
}

@article{li_mmhsv_2023,
  title = {{mmHSV},
  author = {Li, Wanqing and He, Tongtong and Jing, Nan and Wang, Lin},
  year = {2023},
  doi = {10.1145/3614443},
  url = {https://doi.org/10.1145/3614443},
  journal = {ACM Trans. Internet Things},
  volume = {4},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {authentication, Handwritten, millimeter-wave radar, siamese network, signature verification, source: ACM},
  abstract = {Electronic signatures are widely used in financial business, telecommuting, and identity authentication. Offline electronic signatures are vulnerable to copy or replay attacks. Contact-based online electronic signatures are limited by indirect contact such as handwriting pads and may threaten the health of users. Consider combining hand shape features and writing process features to form electronic signatures, the article proposes an in-air handwritten signature verification system with millimeter-wave(mmWave) radar, namely mmHSV. First, the biometrics of the handwritten signature process are modeled, and phase-dependent biometrics and behavioral features are extracted from the mmWave radar mixture signal. Secondly, a handwritten feature recognition network based on few-sample learning is presented to fuse multi-dimensional features and determine user legitimacy. Finally, mmHSV is implemented and evaluated with commercial mmWave devices in different scenarios and attack mode conditions. Experimental results show that the mmHSV can achieve accurate, efficient, robust and scalable handwritten signature verification. Area Under Curve (AUC) is 98.96\%, False Acceptance Rate (FAR) is 5.1\% at the fixed threshold, AUC is 97.79\% for untrained users.},
  month = {nov},
}

@article{kapp_data-driven_2020,
  title = {Data-driven authoring of large-scale ecosystems},
  author = {Kapp, Konrad and Gain, James and Guérin, Eric and Galin, Eric and Peytavie, Adrien},
  year = {2020},
  doi = {10.1145/3414685.3417848},
  url = {https://doi.org/10.1145/3414685.3417848},
  journal = {ACM Trans. Graph.},
  volume = {39},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {ecosystem simulation, natural phenomena, source: ACM},
  abstract = {In computer graphics populating a large-scale natural scene with plants in a fashion that both reflects the complex interrelationships and diversity present in real ecosystems and is computationally efficient enough to support iterative authoring remains an open problem. Ecosystem simulations embody many of the botanical influences, such as sunlight, temperature, and moisture, but require hours to complete, while synthesis from statistical distributions tends not to capture fine-scale variety and complexity.Instead, we leverage real-world data and machine learning to derive a canopy height model (CHM) for unseen terrain provided by the user. Trees in the canopy layer are then fitted to the resulting CHM through a constrained iterative process that optimizes for a given distribution of species, and, finally, an understorey layer is synthesised using distributions derived from biome-specific undergrowth simulations. Such a hybrid data-driven approach has the advantage that it incorporates subtle biotic, abiotic, and disturbance factors implicitly encoded in the source data and evidences accepted biological behaviour, such as self-thinning, climatic adaptation, and gap dynamics.},
  issn = {0730-0301},
  month = {nov},
}

@article{reid_feeling_2022,
  title = {Feeling {Good},
  author = {Reid, Elizabeth and Mandryk, Regan L. and Beres, Nicole A. and Klarkowski, Madison and Frommel, Julian},
  year = {2022},
  doi = {10.1145/3549498},
  url = {https://doi.org/10.1145/3549498},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {6},
  number = {CHI PLAY},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Game developers, researchers, and players recognize the harm of toxic behaviour in online games-yet toxicity persists. Players' coping strategies are limited to tools that focus on punishing toxic players (e.g., muting, blocking, reporting), which are inadequate and often misused. To address the needs of players experiencing toxicity, we took inspiration from research in other online spaces that provide support tools for targets of harassment. We iteratively designed and evaluated in-game tools to support targets of toxicity. While we found that most players prefer tools that explicitly address toxicity and increase feelings of control, we also found that tools that solely provide social or emotional support also decrease stress, increase feelings of control, and increase positive affect. Our findings suggest that players may benefit from variety in toxicity support tools that both explicitly address toxicity in the moment and help players cope after it has occurred.},
  month = {oct},
}

@article{stojkovski_unless_2022,
  title = {“{Unless},
  author = {Stojkovski, Borce and Abu-Salma, Ruba and Triquet, Karen and Lenzini, Gabriele},
  year = {2022},
  doi = {10.1145/3480466},
  url = {https://doi.org/10.1145/3480466},
  journal = {Digital Threats},
  volume = {3},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {contact tracing, COVID-19, human factors, proximity tracing, user adoption, source: ACM},
  abstract = {Globally, countries have been developing contact tracing applications to control the spread of the coronavirus (COVID-19) disease. In this work, we present the findings of eight focus groups we conducted with participants living in France and Germany, to explore why they decided to adopt, or not adopt, a contact tracing application as well as understand how they perceived the benefits, drawbacks, and threat model of a contact tracing application.},
  month = {mar},
}

@article{wang_tdefsi_2020,
  title = {{TDEFSI},
  author = {Wang, Lijing and Chen, Jiangzhuo and Marathe, Madhav},
  year = {2020},
  doi = {10.1145/3380971},
  url = {https://doi.org/10.1145/3380971},
  journal = {ACM Trans. Spatial Algorithms Syst.},
  volume = {6},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {causal model, deep neural network, Epidemic forecasting, LSTM, physical consistency, synthetic information, source: ACM},
  abstract = {Influenza-like illness (ILI) places a heavy social and economic burden on our society. Traditionally, ILI surveillance data are updated weekly and provided at a spatially coarse resolution. Producing timely and reliable high-resolution spatiotemporal forecasts for ILI is crucial for local preparedness and optimal interventions. We present Theory-guided Deep Learning-based Epidemic Forecasting with Synthetic Information (TDEFSI),1 an epidemic forecasting framework that integrates the strengths of deep neural networks and high-resolution simulations of epidemic processes over networks. TDEFSI yields accurate high-resolution spatiotemporal forecasts using low-resolution time-series data.During the training phase, TDEFSI uses high-resolution simulations of epidemics that explicitly model spatial and social heterogeneity inherent in urban regions as one component of training data. We train a two-branch recurrent neural network model to take both within-season and between-season low-resolution observations as features and output high-resolution detailed forecasts. The resulting forecasts are not just driven by observed data but also capture the intricate social, demographic, and geographic attributes of specific urban regions and mathematical theories of disease propagation over networks.We focus on forecasting the incidence of ILI and evaluate TDEFSI’s performance using synthetic and real-world testing datasets at the state and county levels in the USA. The results show that, at the state level, our method achieves comparable/better performance than several state-of-the-art methods. At the county level, TDEFSI outperforms the other methods. The proposed method can be applied to other infectious diseases as well.},
  issn = {2374-0353},
  month = {apr},
}

@article{schott_terrain_2024,
  title = {Terrain {Amplification},
  author = {Schott, Hugo and Galin, Eric and Guérin, Eric and Paris, Axel and Peytavie, Adrien},
  year = {2024},
  doi = {10.1145/3658200},
  url = {https://doi.org/10.1145/3658200},
  journal = {ACM Trans. Graph.},
  volume = {43},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {erosion simulation, landscapes, source: ACM},
  abstract = {Modeling high-resolution terrains is a perennial challenge in the creation of virtual worlds. In this paper, we focus on the amplification of a low-resolution input terrain into a high-resolution, hydrologically consistent terrain featuring complex patterns by a multi-scale approach. Our framework combines the best of both worlds, relying on physics-inspired erosion models producing consistent erosion landmarks and introducing control at different scales, thus bridging the gap between physics-based erosion simulations and multi-scale procedural modeling. The method uses a fast and accurate approximation of different simulations, including thermal, stream power erosion and deposition performed at different scales to obtain a range of effects. Our approach provides landscape designers with tools for amplifying mountain ranges and valleys with consistent details.},
  issn = {0730-0301},
  month = {jul},
}

@article{li_blockchain_2025,
  title = {Blockchain in the {Digital},
  author = {Li, Dun and Han, Dezhi and Crespi, Noel and Minerva, Roberto and Raza, Syed Mohsan and Farahbakhsh, Reza and Liang, Wei and Zheng, Zibin},
  year = {2025},
  doi = {10.1145/3772366},
  url = {https://doi.org/10.1145/3772366},
  journal = {ACM Comput. Surv.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Blockchain, Digital twin, Industrial applications., Industry 4.0, Smart contract, source: ACM},
  abstract = {Digital twin (DT) technology integrates Internet of Things (IoT), communication networks, and sensor systems through high-fidelity modeling and multi-dimensional simulation, enabling dynamic mapping and real-time optimization of physical objects. However, DT development still faces several challenges, including cross-platform interoperability limitations, excessive latency in real-time scenarios, security vulnerabilities in distributed deployments, and the complexity of accurately modeling multi-modal systems. Blockchain (BC) enhances the security and functional scope of DTs across diverse applications. This survey begins by introducing the core principles of BC and DT, and then investigates the rationale and benefits behind their integration. From a data-centric perspective, we explore how Blockchain-empowered Digital Twins (BCDTs) enhance data storage, secure exchange, privacy protection, and system interoperability. The survey further explores the architecture of BCDT systems, covering network topology, functional modules, platform design, and representative prototypes, offering insights into real-world applications. In addition, we survey how BCDT supports the convergence of key Industry 4.0 technologies, including the Internet of Things, vehicle networks, unmanned aerial systems, artificial intelligence, federated learning, 5G mobile networks, and software-defined networking. Industrial-grade quality BCDT-supported applications are highlighted, providing a solid foundation for further research. Finally, we analyze the challenges faced by BCDT and offer some optimistic suggestions for further research in the field of BCDT.},
  annote = {Just Accepted},
  issn = {0360-0300},
  month = {oct},
}

@article{chang_designing_2025,
  title = {Designing {Authoritative},
  author = {Chang, Yuan-Chia and Rea, Daniel J. and Lee, Chi-Jung and Kanda, Takayuki and Chen, Bing-Yu},
  year = {2025},
  doi = {10.1145/3748262},
  url = {https://doi.org/10.1145/3748262},
  journal = {J. Hum.-Robot Interact.},
  volume = {15},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {authoritative presence, authority, compliance, Human-robot interaction, presence, source: ACM},
  abstract = {We define authoritative presence as letting people experience authority through human-made technology in sensory or non-sensory ways. Our goal is to design a robot that creates the impression of possessing capabilities worthy of respect as a source of authority, thereby enhancing compliance without attributing that authority to external sources, such as a specific person or organization. We hypothesized that strategies commonly used in the Wizard-of-Oz method could help manipulate authoritative presence as it strives to ensure that the robot is not perceived as having additional abilities beyond those introduced by the manipulations. Wizards typically need to maintain the robot’s functionality and abilities at an appropriate level to minimize unwanted influence on participants’ perceptions and interactions. By interviewing HRI researchers who have wizarded, we summarized their usual strategies and implemented the opposite behaviors in a robot to investigate if this would contribute to authoritative presence. Based on the findings, we designed four behaviors that include (1) let the robot have an open-ended conversation with people, (2) randomize the robot’s reaction delay timing, (3) let the robot move with inconsistent velocity, and (4) let the robot perceive people’s status without looking at them. To evaluate the impact of these behaviors, we conducted a video-based online experiment with 942 participants, using a between-subjects design. The experiment aimed to determine whether the behaviors conveying authoritative presence would make people perceive the robot as having more authority and increase their likelihood of complying with its requests. A mediation analysis indicated that despite a decrease in perceived authority, the imply authoritative presence condition had a positive effect on participant compliance. Our study formally introduces the concept of authoritative presence, providing a proof-of-concept for how robots can create authoritative presence through specific behaviors. This work lays the groundwork for future research on authority and robotics.},
  month = {sep},
}

@article{norihama_examining_2025,
  title = {Examining {Input},
  author = {Norihama, Shunpei and Geng, Shixian and Miyazaki, Kakeru and Sato, Arissa J. and Hirano, Mari and Hosio, Simo and Yatani, Koji},
  year = {2025},
  doi = {10.1145/3743723},
  url = {https://doi.org/10.1145/3743723},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {cathartic effect, digital micro-interventions, expressive writing, mental self-care, stress coping, source: ACM},
  abstract = {Expressive writing is an established approach for stress management. Recently, information technologies, such as smartphones, have also been explored for expressive writing. Although mobile interfaces have the potential to support various daily writing activities, interface designs for mobile expressive writing and their effects on stress relief still lack empirical understanding. We examined the interface design of mobile expressive writing by investigating the influence of input modalities and visual feedback designs on usability and perceived cathartic effects through field studies. While our studies confirmed the stress-relieving effects of mobile expressive writing, our results offer important insights into interface design. We found keyboard-based text entry more suited and preferred over voice messages for its privacy and reflective nature. Participants expressed different reasons for preferring different post-writing visual feedback depending on the cause and type of stress. This work advances interface design for mobile expressive writing and deepens understanding of its effects.},
  month = {sep},
}

@article{chen_bridging_2025,
  title = {Bridging {Cultural},
  author = {Chen, Max and Smith, Gillian},
  year = {2025},
  doi = {10.1145/3748627},
  url = {https://doi.org/10.1145/3748627},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {As the game industry expands, an increasing number of students and hobbyists are entering the game development field, eager to create their own titles. However, there is a lack of structured training programs that effectively prepare them for industry demands. Games convey meaningful messages through stories, assets, and mechanics, yet how early-stage developers make creative decisions while building production skills remains unclear. We explored skill development and cultural representation in game design during a 10-week professional development program at MassDigi with early-stage developers from the United Arab Emirates (UAE). Our findings reveal three observed strategies as themes through which developers incorporate their personal experiences and cultural backgrounds into game development to achieve cultural representation, as well as four themes that address developers' skill set improvement and personal growth during this process. We recommend structuring programs by considering these themes to enhance self-expressive game design and support skill development through a peer learning community.},
  month = {oct},
}

@article{franco_integrating_2025,
  title = {Integrating {Content},
  author = {Franco, Mirko and Gaggi, Ombretta and Palazzi, Claudio E.},
  year = {2025},
  doi = {10.1145/3700789},
  url = {https://doi.org/10.1145/3700789},
  journal = {ACM Trans. Web},
  volume = {19},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Content moderation, large language models, online social networks, source: ACM},
  abstract = {Online Social Networks (OSNs) rely on content moderation systems to ensure platform and user safety by preventing malicious activities such as the spread of harmful content. However, there is a growing consensus suggesting that such systems are unfair to historically marginalized individuals, fragile users, and minorities. Additionally, OSN policies are often hardcoded in artificial intelligence–based violation classifiers, making personalized content moderation challenging. In addition, there is a need for more communication between users and platform administrators, especially in the case of disagreement about a moderation decision. To address these issues, we propose integrating content moderation systems with Large Language Models (LLMs) to enhance support for personal content moderation and improve user–platform communication. We also evaluate the content moderation capabilities of GPT 3.5 and LLaMa 2, comparing them with commercial products, as well as discuss the limitations of our approach and the open research directions.},
  issn = {1559-1131},
  month = {may},
}

@article{keyes_reimagining_2020,
  title = {Reimagining ({Women},
  author = {Keyes, Os and Peil, Burren and Williams, Rua M. and Spiel, Katta},
  year = {2020},
  doi = {10.1145/3404218},
  url = {https://doi.org/10.1145/3404218},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {27},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {critical theory, embodiment, essentialism, Gender, health, marginalisation, speculative design, women’s health, source: ACM},
  abstract = {An ever-increasing body of work within HCI investigates questions of around “Women’s Health” with the aim to disrupt the status quo of defaulting to an implicit norm of cis-male bodies. This laudable and feminist project has the potential to drastically improve the inclusivity and availability of health care. To explore how this research attends to gender, embodiment and identity, we conducted a critical discourse analysis of 17 publications explicitly positioning themselves as works concerned with “Women’s Health”. We find essentialised articulations of embodiment and gender, though little discussion on the intersections of race, class, sexuality and cultural contexts. Through two speculative designs, we illustrate potential responses to our analysis: The Shadow Zine, a reflection of self and the Compass, a token for community care.1 Our work provides an opportunity to develop a broader frame of gender and health, one that centers (gendered) marginalised health by attending to the power structures of existing medical practices and norms.},
  issn = {1073-0516},
  month = {aug},
}

@article{stuhn_hidden_2024,
  title = {The {Hidden},
  author = {Stühn, Jakob and Hilgert, Jan-Niclas and Lambertz, Martin},
  year = {2024},
  doi = {10.1145/3688808},
  url = {https://doi.org/10.1145/3688808},
  journal = {Digital Threats},
  volume = {5},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Detection, Incident Response, Linux, Malware, Rootkits, source: ACM},
  abstract = {This article addresses the significant threat posed by rootkits as part of the diverse malware landscape of today. Rootkits enable an attacker to regain access to an already comprised system at root-level making their prompt identification and removal crucial. However, rootkits implement advanced stealth features, enabling them to evade detection by conventional measures during analysis. Consequently, analysts generally rely on customized tools to detect the presence of a rootkit. However, our research highlights significant deficits in the tools available for the detection of rootkits on the Linux operating system, which is frequently encountered in investigations of server environments. Recognizing the need for improved awareness and capabilities among investigators, we conducted an in-depth analysis of 21 distinct Linux rootkits allowing us to dive into their techniques and features. Furthermore, we critically assessed the effectiveness of standard detection tools, revealing their limitations. Based on these insights, we propose best practices for investigators to effectively identify and detect signs of rootkit infections. Additionally, we provide a repository of indicators of compromise we extracted during our analyses to facilitate the detection of the analyzed rootkits on compromised systems along with a utility to detect rootkits via hidden files on a live system.},
  month = {oct},
}

@article{cordonnier_forming_2023,
  title = {Forming {Terrains},
  author = {Cordonnier, Guillaume and Jouvet, Guillaume and Peytavie, Adrien and Braun, Jean and Cani, Marie-Paule and Benes, Bedrich and Galin, Eric and Guérin, Eric and Gain, James},
  year = {2023},
  doi = {10.1145/3592422},
  url = {https://doi.org/10.1145/3592422},
  journal = {ACM Trans. Graph.},
  volume = {42},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {erosion, glacial erosion, landscape, simulation of natural phenomena, terrain, source: ACM},
  abstract = {We introduce the first solution for simulating the formation and evolution of glaciers, together with their attendant erosive effects, for periods covering the combination of glacial and inter-glacial cycles. Our efficient solution includes both a fast yet accurate deep learning-based estimation of highorder ice flows and a new, multi-scale advection scheme enabling us to account for the distinct time scales at which glaciers reach equilibrium compared to eroding the terrain. We combine the resulting glacial erosion model with finer-scale erosive phenomena to account for the transport of debris flowing from cliffs. This enables us to model the formation of terrain shapes not previously adequately modeled in Computer Graphics, ranging from U-shaped and hanging valleys to fjords and glacial lakes.},
  issn = {0730-0301},
  month = {jul},
}

@article{shrestha_no_2025,
  title = {No {Country},
  author = {Shrestha, Grishma and Shrestha, Shristi and Mahmoud, Anas},
  year = {2025},
  doi = {10.1145/3736578},
  url = {https://doi.org/10.1145/3736578},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {closed testing, Google Play, indie developers, source: ACM},
  abstract = {In November 2023, Google Play introduced new closed testing requirements for apps submitted by developers operating with personal accounts, or indie app developers. These requirements mandate that at least 20 testers must remain opted-in (use the app) for at least 14 consecutive days before the app can be published on the Play Store. According to Google, these new requirements aim to ensure the quality and security of submitted apps. However, for individual developers operating without organizational support, adhering to such requirements can pose logistical challenges and lead to production delays. To understand these challenges, in this paper, we qualitatively analyze app developers’ discussions of Google Play’s new closed testing requirements on Reddit. Additionally, we report insights from a survey of 14 indie app developers who recently passed the requirements or are actively seeking compliance. Our results show that Google Play’s closed testing requirements for indie apps are commonly perceived as discriminatory, imposing logistical and bureaucratic barriers on small-scale creators in their quest to compete in the mobile app market. Our analysis also uncovers the strategies the Android developer community has adapted to navigate such requirements. Based on our findings, we propose several guidelines to help indie app developers integrate the testing requirements into their workflow. We further suggest design strategies to mitigate the impact of such requirements on innovation, fairness, and competition in the mobile app market.},
  annote = {Just Accepted},
  issn = {1049-331X},
  month = {may},
}

@article{liu_envisioning_2025,
  title = {Envisioning {AI},
  author = {Liu, Zhe and Dai, Jiamin and Conati, Cristina and McGrenere, Joanna},
  year = {2025},
  doi = {10.1145/3710909},
  url = {https://doi.org/10.1145/3710909},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Semi-structured interviews are a critical qualitative method in many areas, including CSCW and HCI, enabling researchers to uncover deep contextualized insights. Using this flexible method, interviewers must adapt to interviewees' responses while adhering to the protocol, necessitating strong active listening and real-time analytical skills. While recent studies have explored how AI can support researchers in qualitative analysis, to our knowledge, no research has investigated AI's role in supporting semi-structured interviews while they are underway. Taking one step toward filling this gap, we interviewed 16 researchers with a range of prior interviewing expertise. Our inductive thematic analysis reveals that interviewers expect real-time AI assistance to support research objectives and facilitate interpersonal communication, but also have concerns about its impact on long-term skill development. We discuss how semi-structured interviews differ from other problem-solving or creative human-AI collaboration contexts, highlighting the time constraints, multimodal collaboration, and the triangular dynamic among interviewers, interviewees, and AI. We also delve into how interviewers' levels of expertise affect their envisioned interviewer-AI collaboration. We then propose design challenges for future CSCW work on AI-driven assistants in interview contexts.},
  month = {may},
}

@article{liu_gsfl_2025,
  title = {{GSFL},
  author = {Liu, Qi and Wang, Zhilu and Zhou, Xiaokang and Zhang, Yonghong and Liu, Xiaodong and Lin, Haiyang},
  year = {2025},
  doi = {10.1145/3725221},
  url = {https://doi.org/10.1145/3725221},
  journal = {ACM Trans. Auton. Adapt. Syst.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Clustering, Federated learning, Non-IID, Split learning, source: ACM},
  abstract = {The advancement of mobile multimedia communications, 5G, and Internet of Things (IoT) has led to the widespread use of edge devices, including sensors, smartphones, and wearables. This has generated in a large amount of distributed data, leading to new prospects for deep learning. However, this data is confined within data silos and contains sensitive information, making it difficult to be processed in a centralized manner, particularly under stringent data privacy regulations. Federated learning (FL) offers a solution by enabling collaborative learning while ensuring privacy. Nonetheless, data and device heterogeneity complicate FL implementation. This research presents a specialized FL algorithm for heterogeneous edge computing. It integrates a lightweight grouping strategy for homogeneous devices, a scheduling algorithm within groups, and a Split Learning (SL) approach. These contributions enhance model accuracy and training speed, alleviate the burden on resource-constrained devices, and strengthen privacy. Experimental results demonstrate that the GSFL outperforms FedAvg and SplitFed by 6.53× and 1.18×. Under experimental conditions with (alpha=0.05) , representing a highly heterogeneous data distribution typical of extreme Non-IID scenarios, GSFL showed better accuracy compared to FedAvg by 10.64\%, HACCS by 4.53\%, and Cluster-HSFL by 1.16\%. GSFL effectively balances privacy protection and computational efficiency for real-world applications in mobile multimedia communications.},
  annote = {Just Accepted},
  issn = {1556-4665},
  month = {mar},
}

@article{cohen_this_2024,
  title = {This {Is},
  author = {Cohen, Ben and Hu, Ashley and Patino, Deisy and Coffman, Joel},
  year = {2024},
  doi = {10.1145/3675230},
  url = {https://doi.org/10.1145/3675230},
  journal = {ACM J. Responsib. Comput.},
  volume = {1},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {cloud computing, computer security, law, NIST Cybersecurity Framework, privacy, standards, student records, The Family Educational Rights and Privacy Act (FERPA), source: ACM},
  abstract = {Moving operations to the cloud has become a way of life for many educational institutions. Much of the information these institutions store in the cloud is protected by the Family Educational Rights and Privacy Act (FERPA), which was last amended in 2002, well before cloud computing became ubiquitous. The application of a 1974 law to 21st-century technology presents a plethora of legal and technical questions. In this article, we present an interdisciplinary analysis of these issues. We examine both existing statutes and case law and contemporary research into cloud security, focusing on the impact of the latter on the former. We find that FERPA excludes information that students and faculty often believe is protected and that lower-court decisions have created further ambiguity. We additionally find that given current technology, the statute is no longer sufficient to protect student data, and we present recommendations for revisions.},
  month = {aug},
}

@article{mostafa_modality_2024,
  title = {Modality {Deep},
  author = {Mostafa, Mohamed and Almogren, Ahmad S and Al-Qurishi, Muhammad and Alrubaian, Majed},
  year = {2024},
  doi = {10.1145/3700748},
  url = {https://doi.org/10.1145/3700748},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {deep learning, fake news detection, modality architectures, multimodal, Social computing, text classification, unimodal, source: ACM},
  abstract = {Fake news on social networks is a challenging problem due to the rapid dissemination and volume of information, as well as the ease of creating and sharing content anonymously. Fake news stories are problematic not only for the credibility of online journalism, but also due to their detrimental real-world consequences. The primary research objective of this study is to identify recent state-of-the-art deep learning methods used to detect fake news in social networks. This article presents a systematic literature review of deep learning-based fake news detection models in social networks. The methodology followed a rigorous approach, including predefined criteria for study selection of deep learning modalities. This study focuses on the types of deep learning modalities: unimodal (refers to the use of a single model for analysis or modeling purposes) and multimodal models (refers to the integration of multiple models). The results of this review reveal the strengths and weaknesses of modalities approaches, as well as the limitations of low-resource languages datasets. Furthermore, it provides insights into future directions for deep learning models and different fact-checking techniques. At the end of this study, we discuss the problem of fake news detection in the era of large language models in terms of advantages, drawbacks, and challenges.},
  issn = {0360-0300},
  month = {nov},
}

@article{mensonge_historically_2023,
  title = {Historically {Informed},
  author = {Mensonge, Kien},
  year = {2023},
  doi = {10.1145/3517144},
  url = {https://doi.org/10.1145/3517144},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {29},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Design fiction, history, research fiction, thought experiment, source: ACM},
  abstract = {As computing technology comes to dominate every aspect of social and political life, HCI must take greater account of History. The article considers four different historical periods impacted by division and denunciation: the European Witch Hunts, the Soviet Purges, the McCarthy Era, and the Chinese Cultural Revolution. Historians have identified patterns common to such periods including: the unity of accusation and action; condemnation as a show of virtue, and defense of the accused as collusion with enemies. These patterns are mapped to findings from social media research such as: impulsive shares are easy to make but difficult to retract; angry posts travel fastest and furthest; likes and retweets express group identity and solidarity. Anachronistic memes, tweets and selfies explore what previous eras might have looked like if contemporary technology had existed in the past. It is argued that such anachronistic fiction may be a useful method for exploring the potential impact of particular design choices.},
  issn = {1073-0516},
  month = {apr},
}

@article{nicol_revealing_2022,
  title = {Revealing {Cumulative},
  author = {Nicol, Emma and Briggs, Jo and Moncur, Wendy and Htait, Amal and Carey, Daniel Paul and Azzopardi, Leif and Schafer, Burkhard},
  year = {2022},
  doi = {10.1145/3555214},
  url = {https://doi.org/10.1145/3555214},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {6},
  number = {CSCW2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {cybersecurity, digital traces, human computer interaction, personal data, research design, source: ACM},
  abstract = {When pieces from an individual's personal information available online are connected over time and across multiple platforms, this more complete digital trace can give unintended insights into their life and opinions. In a data narrative interview study with 26 currently employed participants, we examined risks and harms to individuals and employers when others joined the dots between their online information. We discuss the themes of visibility and self-disclosure, unintentional information leakage and digital privacy literacies constructed from our analysis. We contribute insights not only into people's difficulties in recalling and conceptualising their digital traces but of subsequently envisioning how their online information may be combined, or (re)identified across their traces and address a current gap in research by showing that awareness is lacking around the potential for personal information to be correlated by and made coherent to/by others, posing risks to individuals, employers, and even the state. We touch on inequalities of privacy, freedom and legitimacy that exist for different groups with regard to what they make (or feel compelled to make) available online and we contribute to current methodological work on the use of sketching to support visual sense making in data narrative interviews. We conclude by discussing the need for interventions that support personal reflection on the potential visibility of combined digital traces to spotlight hidden vulnerabilities, and promote more proactive action about what is shared and not shared online.},
  month = {nov},
}

@article{brito_scpp_2020,
  title = {{SCPP},
  author = {Brito, Denise E. F. and Assunção, Renato M. and Souza, Roberto C. S. N. P. and JR., Wagner Meira},
  year = {2020},
  doi = {10.1145/3423405},
  url = {https://doi.org/10.1145/3423405},
  journal = {ACM Trans. Spatial Algorithms Syst.},
  volume = {7},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {point process-based spatial clustering, spatial behavior, Spatial patterns, source: ACM},
  abstract = {A collection of individuals is represented by point patterns. Each individual is a finite set of geographical locations representing their visiting pattern to places in a region. We present SCPP, an algorithm for clustering these individuals considering the spatial patterns of their visiting locations. We adopted a probabilistic framework based on the theory of point processes that allows us to derive a non-obvious distance metric between each individual point pattern and the underlying, unobserved continuous intensity function. This metric is the Kullback-Leibler divergence between the true data-generating point process distribution and the model-generating distribution. We also introduce a theoretically based framework for the cost function to be minimized, a functional T(P) taking as arguments the probability distributions underlying the unknown clusters. We present an extensive experimental analysis to show SCPP’s effectiveness using several synthetic datasets and spatial mobility patterns from geo-tagged social media.},
  issn = {2374-0353},
  month = {oct},
}

@article{nourani_importance_2022,
  title = {On the {Importance},
  author = {Nourani, Mahsan and Roy, Chiradeep and Block, Jeremy E. and Honeycutt, Donald R. and Rahman, Tahrima and Ragan, Eric D. and Gogate, Vibhav},
  year = {2022},
  doi = {10.1145/3531066},
  url = {https://doi.org/10.1145/3531066},
  journal = {ACM Trans. Interact. Intell. Syst.},
  volume = {12},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {cognitive biases, conceptual models, Explainable AI, HCI, user studies, source: ACM},
  abstract = {While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this article, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video activity recognition tool in cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. However, those who encountered weaknesses earlier made significantly fewer errors, since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Motivated by these findings and similar existing work, we formalize and present a conceptual model of user’s past experiences that examine the relations between user’s backgrounds, experiences, and human factors in XAI systems based on usage time. Our work presents strong findings and implications, aiming to raise the awareness of AI designers toward biases associated with user impressions and backgrounds.},
  issn = {2160-6455},
  month = {dec},
}

@article{pescetelli_benefits_2022,
  title = {Benefits of spontaneous confidence alignment between dyad members},
  author = {Pescetelli, Niccolò and Yeung, Nick},
  year = {2022},
  doi = {10.1177/26339137221126915},
  url = {https://doi.org/10.1177/26339137221126915},
  journal = {Collective Intelligence},
  volume = {1},
  number = {2},
  note = {Place: USA
Publisher: Sage Publications, Inc.},
  keywords = {confidence, confidence alignment, decision-making, dyads, metacognition, source: ACM},
  abstract = {In many domains, imitating others’ behaviour can help individuals to solve problems that would be too difficult or too complex for the individuals. In collective decision making tasks, people have been shown to use confidence as a means to communicate the uncertainty surrounding internal noisy estimates. Here, we show that confidence alignment, namely, shifting average confidence between dyad members towards each other, naturally emerges when interacting with others’ opinions. This alignment has a measurable impact on group performance as well as the accuracy of individual members following information exchange. It is suggested that confidence alignment arises among individuals from the necessity of minimising confidence variation arising from task-unrelated variables (trait confidence), while at the same time maximising variation arising from stimulus characteristics (state confidence).},
  month = {dec},
}

@article{uma_learning_2022,
  title = {Learning from {Disagreement},
  author = {Uma, Alexandra N. and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo},
  year = {2022},
  doi = {10.1613/jair.1.12752},
  url = {https://doi.org/10.1613/jair.1.12752},
  journal = {J. Artif. Int. Res.},
  volume = {72},
  pages = {1385--1470},
  note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
  keywords = {machine learning, natural language, uncertainty, vision, source: ACM},
  abstract = {Many tasks in Natural Language Processing (NLP) and Computer Vision (CV) offer evidence that humans disagree, from objective tasks such as part-of-speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises. While most learning in artificial intelligence (AI) still relies on the assumption that a single (gold) interpretation exists for each item, a growing body of research aims to develop learning methods that do not rely on this assumption. In this survey, we review the evidence for disagreements on NLP and CV tasks, focusing on tasks for which substantial datasets containing this information have been created. We discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement. We systematically compare these different approaches by training them with each of the available datasets, considering several ways to evaluate the resulting models. Finally, we discuss the results in depth, focusing on four key research questions, and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions. Our results suggest, first of all, that even if we abandon the assumption of a gold standard, it is still essential to reach a consensus on how to evaluate models. This is because the relative performance of the various training methods is critically affected by the chosen form of evaluation. Secondly, we observed a strong dataset effect. With substantial datasets, providing many judgments by high-quality coders for each item, training directly with soft labels achieved better results than training from aggregated or even gold labels. This result holds for both hard and soft evaluation. But when the above conditions do not hold, leveraging both gold and soft labels generally achieved the best results in the hard evaluation. All datasets and models employed in this paper are freely available as supplementary materials.},
  issn = {1076-9757},
  month = {jan},
}

@article{sengers_speculation_2021,
  title = {Speculation and the {Design},
  author = {Sengers, Phoebe and Williams, Kaiton and Khovanskaya, Vera},
  year = {2021},
  doi = {10.1145/3449195},
  url = {https://doi.org/10.1145/3449195},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {5},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {development, futures, history, jamaica, newfoundland and labrador, postcolonial computing, speculation, speculative design, source: ACM},
  abstract = {This paper examines the role of technoscientific speculation in large-scale development projects in postcolonial spaces, building on recent work in STS, design research, and postcolonial studies in and beyond CSCW. We analyze two historical cases of technology-infused development projects in the Canadian province of Newfoundland and Labrador and in Jamaica. We find that speculation in these contexts remixes the constructive stance toward speculation typical for normative technoscience with the critical, contesting orientation of speculative design. Conflicts between these stances are resolved by leveraging fantasy for pragmatic ends, grounding audacious fictions in imported realities, unmooring from conventional understandings of linear technological progress, and using even conservative futures to trouble colonial conventions.},
  month = {apr},
}

@article{chen_sdnshield_2021,
  title = {{SDNShield},
  author = {Chen, Kuan-Yin and Liu, Sen and Xu, Yang and Siddhrau, Ishant Kumar and Zhou, Siyu and Guo, Zehua and Chao, H. Jonathan},
  year = {2021},
  doi = {10.1109/TNET.2021.3105187},
  url = {https://doi.org/10.1109/tnet.2021.3105187},
  journal = {IEEE/ACM Trans. Netw.},
  volume = {30},
  number = {1},
  pages = {1--17},
  note = {Publisher: IEEE Press},
  keywords = {source: ACM},
  abstract = {Software-defined networking (SDN) is increasingly popular in today’s information technology industry, but existing SDN control plane is insufficiently scalable to support on-demand, high-frequency flow requests. Weaknesses along SDN control paths can be exploited by malicious third parties to launch distributed denial-of-service (DDoS) attacks against the SDN control plane. Recently proposed solutions only partially solve the problem, by protecting either the SDN network edges or the centralized controller. We propose SDNShield, a solution based on emerging network function virtualization (NFV) technologies, which enforces more comprehensive defense against potential DDoS attacks on SDN control plane. SDNShield incorporates a three-stage overload control scheme. The first stage statistically identifies legitimate flows with low complexity and performance overhead. The second stage further performs in-depth TCP handshake verification to ensure good flows are eventually served. The third stage intellectually salvages the misclassified legitimate flows that are falsely dropped from the first two stages. Prototype tests and real data-driven simulation results show that SDNShield can achieve high resilience against brute-force attacks, and maintain good flow-level service quality at the same time.},
  issn = {1063-6692},
  month = {aug},
}

@article{zhang_edge_2021,
  title = {Edge {Learning},
  author = {Zhang, Jie and Qu, Zhihao and Chen, Chenxi and Wang, Haozhao and Zhan, Yufeng and Ye, Baoliu and Guo, Song},
  year = {2021},
  doi = {10.1145/3464419},
  url = {https://doi.org/10.1145/3464419},
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {edge computing, Edge learning, federated learning, machine learning, security and privacy, source: ACM},
  abstract = {Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues.To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field.},
  issn = {0360-0300},
  month = {jul},
}

@article{contreras_integrating_2021,
  title = {Integrating {Collaboration},
  author = {Contreras, David and Salamó, Maria and Boratto, Ludovico},
  year = {2021},
  doi = {10.1145/3462759},
  url = {https://doi.org/10.1145/3462759},
  journal = {ACM Trans. Inf. Syst.},
  volume = {39},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {collaboration, Group recommendation, interactions, leadership, live-user evaluation, source: ACM},
  abstract = {Recent observational studies highlight the importance of considering the interactions between users in the group recommendation process, but to date their integration has been marginal. In this article, we propose a collaborative model based on the social interactions that take place in a web-based conversational group recommender system. The collaborative model allows the group recommender to implicitly infer the different roles within the group, namely, collaborative and leader user(s). Moreover, it serves as the basis of several novel collaboration-based consensus strategies that integrate both individual and social interactions in the group recommendation process. A live-user evaluation confirms that our approach accurately identifies the collaborative and leader users in a group and produces more effective recommendations.},
  issn = {1046-8188},
  month = {aug},
}

@article{kelly_emergent_2021,
  title = {Emergent {Tangled},
  author = {Kelly, Stephen and Smith, Robert J. and Heywood, Malcolm I. and Banzhaf, Wolfgang},
  year = {2021},
  doi = {10.1145/3468857},
  url = {https://doi.org/10.1145/3468857},
  journal = {ACM Trans. Evol. Learn. Optim.},
  volume = {1},
  number = {3},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Coevolution, modularity, partial observability, time series, source: ACM},
  abstract = {Modularity represents a recurring theme in the attempt to scale evolution to the design of complex systems. However, modularity rarely forms the central theme of an artificial approach to evolution. In this work, we report on progress with the recently proposed Tangled Program Graph (TPG) framework in which programs are modules. The combination of the TPG representation and its variation operators enable both teams of programs and graphs of teams of programs to appear in an emergent process. The original development of TPG was limited to tasks with, for the most part, complete information. This work details two recent approaches for scaling TPG to tasks that are dominated by partially observable sources of information using different formulations of indexed memory. One formulation emphasizes the incremental construction of memory, again as an emergent process, resulting in a distributed view of state. The second formulation assumes a single global instance of memory and develops it as a communication medium, thus a single global view of state. The resulting empirical evaluation demonstrates that TPG equipped with memory is able to solve multi-task recursive time-series forecasting problems and visual navigation tasks expressed in two levels of a commercial first-person shooter environment.},
  month = {aug},
}

@article{schlegel_general_2021,
  title = {General {Value},
  author = {Schlegel, Matthew and Jacobsen, Andrew and Abbas, Zaheer and Patterson, Andrew and White, Adam and White, Martha},
  year = {2021},
  doi = {10.1613/jair.1.12105},
  url = {https://doi.org/10.1613/jair.1.12105},
  journal = {J. Artif. Int. Res.},
  volume = {70},
  pages = {497--543},
  note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
  keywords = {source: ACM},
  abstract = {State construction is important for learning in partially observable environments. A general purpose strategy for state construction is to learn the state update using a Recurrent Neural Network (RNN), which updates the internal state using the current internal state and the most recent observation. This internal state provides a summary of the observed sequence, to facilitate accurate predictions and decision-making. At the same time, specifying and training RNNs is notoriously tricky, particularly as the common strategy to approximate gradients back in time, called truncated Back-prop Through Time (BPTT), can be sensitive to the truncation window. Further, domain-expertise—which can usually help constrain the function class and so improve trainability—can be difficult to incorporate into complex recurrent units used within RNNs. In this work, we explore how to use multi-step predictions to constrain the RNN and incorporate prior knowledge. In particular, we revisit the idea of using predictions to construct state and ask: does constraining (parts of) the state to consist of predictions about the future improve RNN trainability? We formulate a novel RNN architecture, called a General Value Function Network (GVFN), where each internal state component corresponds to a prediction about the future represented as a value function. We first provide an objective for optimizing GVFNs, and derive several algorithms to optimize this objective. We then show that GVFNs are more robust to the truncation level, in many cases only requiring one-step gradient updates.},
  issn = {1076-9757},
  month = {may},
}

@article{wu_influences_2025,
  title = {Influences of precollege out-of-school time computer science experiences on students’ career interest in computer science},
  author = {Wu, Rongxiu and Sunbury, Susan and Sadler, Philip and Sonnert, Gerhard},
  year = {2025},
  doi = {10.1145/3770069},
  url = {https://doi.org/10.1145/3770069},
  journal = {ACM Trans. Comput. Educ.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Career interest, computer science, high school, OST CS-related activities, source: ACM},
  abstract = {Background and context: Although out-of-school time (OST) computer science (CS) experiences during the high school years have been considered an efficacious means to cultivate students’ career interest in CS, there has been a paucity of rigorous research on the topic.Objective: Examine the effects of a wide variety of OST activities on students’ career interest in CS.Method: We carried out a retrospective cohort study, collecting data from a nationally representative sample of 6,044 U.S. first year university students. From 27 survey items about OST CS-related activities during high school, we first selected a list of top-ranking influential variables through machine learning. Then, a multinomial logistic regression was used to examine the relationship between these top-ranking variables and students’ career interests at the end of high school.Findings: The analysis showed that high school aged students’ participation in unstructured CS-related activities (e.g., talking about CS with family or friends); structured CS-related activities (e.g., CS-related summer camps or programs); along with the opportunities that students experienced during OST CS programs/activities (e.g., designing their own CS projects) boosted interest in a CS career vis-à-vis careers in other-STEM or non-STEM fields. It also showed that engaging in some activities (e.g., using social media) was associated with a decreased likelihood of intending a CS career, compared to a career in other-STEM or non-STEM fields. An interaction effect between having a prior career interest in CS and creating blogs/podcasts/video was also observed.Implications: First large-scale analysis of CS OST related activities on CS career interest.},
  annote = {Just Accepted},
  month = {sep},
}

@article{stephenson_legal_2025,
  title = {Legal {Evidence},
  author = {Stephenson, Sophie and Gupta, Naman and Polamarasetty, Akhil and Huang, Kyle and Youssef, David and Cowan, Kayleigh and Chatterjee, Rahul},
  year = {2025},
  doi = {10.1145/3757622},
  url = {https://doi.org/10.1145/3757622},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {7},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {at-risk users, digital safety, technology-facilitated abuse, source: ACM},
  abstract = {Abusers routinely use technology to spy on and harass their targets. This harmful behavior is known as technology-facilitated abuse, or tech abuse. Survivors of tech abuse may turn to the legal system for safety and security, and to do so, they need evidence of tech abuse. However, prior work indicates challenges to collecting evidence of tech abuse and using it in legal proceedings. Thus, in this work, we study legal evidence used by survivors of tech abuse in Wisconsin, USA. We report on qualitative interviews and focus groups with 19 legal support providers who work with survivors seeking protective orders, divorces, and criminal charges. Our findings surface current practices that survivors and legal support providers use to prepare and present evidence of tech abuse in Wisconsin and the challenges they face. For example, survivors struggle to collect evidence of covert monitoring and surveillance. When they can collect evidence, it is often difficult to connect that evidence to the abuser due to the anonymous nature of many forms of tech abuse. In court, evidence of tech abuse is frequently challenged and vulnerable to objections and counter-evidence. And at the end of a proceeding, it's not uncommon for a judge to determine that the tech abuse does not meet the statutes. Informed by these results, we encourage CSCW and HCI researchers to work towards designing and deploying sociotechnical solutions that support survivors' use of evidence, in careful collaboration with advocates, legal experts, and survivors.},
  month = {oct},
}

@article{varela-vaca_smart_2021,
  title = {Smart {Contract},
  author = {Varela-Vaca, Ángel Jesús and Quintero, Antonia M. Reina},
  year = {2021},
  doi = {10.1145/3423166},
  url = {https://doi.org/10.1145/3423166},
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {blockchain, multivocal literature mapping study, Smart contract language, systematic literature review, source: ACM},
  abstract = {Blockchain is a disruptive technology that has attracted the attention of the scientific community and companies, as proven by the exponential growth of publications on this topic in recent years. This growing interest is mainly due to the promise that the use of blockchain enables it to be verified, without including any trusted intermediaries, that the information received from the network is authentic and up-to-date. In this respect, blockchain is a distributed database that can be seen as a ledger that records all transactions that have ever been executed. In this context, smart contracts are pieces of software used to facilitate, verify, and enforce the negotiation of a transaction on a blockchain platform. These pieces of software are implemented by using programming languages, which are sometimes provided by the blockchain platforms themselves. This study aims to (1) identify and categorise the state-of-the-art related to smart contract languages, in terms of the existing languages and their main features, and (2) identify new research opportunities. The review has been conducted as a multivocal mapping study that follows the guidelines proposed by Garousi et\&nbsp;al. for conducting multivocal literature reviews, as well as the guidelines proposed by Kitchenham and Charters for conducting mapping studies. As a result of the implementation of the review protocol, 4,119 papers were gathered, and 109 of them were selected for extraction. The contributions of this article are twofold: (1) 101 different smart contract languages have been identified and classified according to a variety of criteria; (2) a discussion on the findings and their implications for future research have been outlined. As a conclusion, it could be stated that a rigorous and replicable overview of the state-of-the-art of smart contract languages has been provided that can benefit not only researchers but also practitioners in the field, thanks to its multivocal nature.},
  issn = {0360-0300},
  month = {jan},
}

@article{esterle_i_2020,
  title = {I {Think},
  author = {Esterle, Lukas and Brown, John N. A.},
  year = {2020},
  doi = {10.1145/3375403},
  url = {https://doi.org/10.1145/3375403},
  journal = {ACM Trans. Cyber-Phys. Syst.},
  volume = {4},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {artificial intelligence, autonomous systems, collaborative systems, networked self-awareness, Self-aware systems, source: ACM},
  abstract = {Cyber-physical systems operate in our real world, constantly interacting with the environment and collaborating with other systems. The increasing number of devices will make it infeasible to control each one individually. It will also be infeasible to prepare each of them for every imaginable rapidly unfolding situation. Therefore, we must increase the autonomy of future Cyber-physical Systems. Making these systems self-aware allows them to reason about their own capabilities and their immediate environment. In this article, we extend the idea of the self-awareness of individual systems toward networked self-awareness. This gives systems the ability to reason about how they are being affected by the actions and interactions of others within their perceived environment, as well as in the extended environment that is beyond their direct perception. We propose that different levels of networked self-awareness can develop over time in systems as they do in humans. Furthermore, we propose that this could have the same benefits for networks of systems that it has had for communities of humans, increasing performance and adaptability.},
  issn = {2378-962X},
  month = {jun},
}

@article{sahin_linspector_2020,
  title = {{LINSPECTOR},
  author = {Şahin, Gözde Gül and Vania, Clara and Kuznetsov, Ilia and Gurevych, Iryna},
  year = {2020},
  doi = {10.1162/coli_a_00376},
  url = {https://doi.org/10.1162/coli_a_00376},
  journal = {Comput. Linguist.},
  volume = {46},
  number = {2},
  pages = {335--385},
  note = {Place: Cambridge, MA, USA
Publisher: MIT Press},
  keywords = {source: ACM},
  abstract = {Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with .},
  issn = {0891-2017},
  month = {jun},
}

@article{oguine_how_2025,
  title = {How the {Internet},
  author = {Oguine, Ozioma C. and Park, Jinkyung Katie and Akter, Mamtaj and Olesk, Johanna and Alluhidan, Abdulmalik and Wisniewski, Pamela and Badillo-Urquiola, Karla},
  year = {2025},
  doi = {10.1145/3710995},
  url = {https://doi.org/10.1145/3710995},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {9},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {adolescent online safety, adverse childhood experiences, child welfare, children in need of services, juvenile justice, online support seeking, vulnerable youth, source: ACM},
  abstract = {Youth implicated in the child welfare and juvenile justice systems, as well as those with an incarcerated parent, are considered the most vulnerable Children in Need of Services (CHINS). We identified 1,160 of these at-risk youth (ages 13-17) who sought support via an online peer support platform to understand their adverse childhood experiences and explore how the internet played a role in providing an outlet for support, as well as potentially facilitating risks. We first analyzed posts from 1,160 youth who self-identified as CHINS while sharing about their adverse experiences. Then, we retrieved all 239,929 posts by these users to identify salient topics within their support-seeking posts: 1) Urges to self-harm due to social drama, 2) desire for social connection, 3) struggles with family, and 4) substance use and sexual risks. We found that the internet often helped facilitate these problems; for example, the desperation for social connection often led to meeting unsafe people online, causing additional trauma. Family members and other unsafe people used the internet to perpetrate cyberabuse, while CHINS themselves leveraged online channels to engage in illegal and risky behavior. Our study calls for tailored support systems that address the unique needs of CHINS to promote safe online spaces and foster resilience to break the cycle of adversity. Empowering CHINS requires amplifying their voices and acknowledging the challenges they face as a result of their adverse childhood experiences.},
  month = {may},
}

@article{tome_i_2024,
  title = {"{I},
  author = {Tomé, Filipe and Pires, Ana and Jiskrová, Anna and Saial, Ana and Campos, Pedro F.},
  year = {2024},
  doi = {10.1145/3677083},
  url = {https://doi.org/10.1145/3677083},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CHI PLAY},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {source: ACM},
  abstract = {Horror video games have often been criticized for their negative and unrealistic depiction of mental health, primarily through their portrayal of characters with mental illnesses. Such representations can perpetuate stigma and misconceptions about mental health issues. However, there has recently been a growing interest in accurate portrayals of these characters and how playing these video games could also elicit empathy and mental health awareness. In our study, we explored how a psychological horror game such as Silent Hill 2 could foster empathy and mental health awareness through individual interviews with 11 participants. Our findings revealed that participants gained a deeper understanding of mental health issues and empathy towards characters struggling with mental illnesses by interacting and controlling them. Participants had a greater attachment to the characters' inner struggles and their gameplay was experienced as cathartic. Moreover, our findings highlighted the potential of psychological horror to serve as a platform for character-driven narratives, showcasing how engagement with such games elicited empathy among participants. We contribute to an exploratory understanding that can assist game designers and writers in crafting narratives within the Psychological Horror genre that foster empathy and mental health awareness.},
  month = {oct},
}

@article{hu_openmae_2025,
  title = {{OpenMAE},
  author = {Hu, Chenzhi and Chen, Yatong and Kara, Denizhan and Liu, Shengzhong and Abdelzaher, Tarek and Wu, Fan and Chen, Guihai},
  year = {2025},
  doi = {10.1145/3729485},
  url = {https://doi.org/10.1145/3729485},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {9},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Self-Supervised Learning, Time-Series Signals, Vibration Sensing, source: ACM},
  abstract = {This paper introduces OpenMAE, a novel data enrichment framework utilizing open-world sensor data streams to facilitate efficient masked autoencoder (MAE) pretraining on vibration signals. Due to highly sparse event occurrences and inevitable distributional shifts from downstream tasks, directly concatenating large-scale open-domain data with limited in-domain data during pretraining leads to degraded downstream task performance. The problem is further complicated by missing knowledge of open-world sensor environments and associated physical event semantics. Against these challenges, OpenMAE makes the following contributions to vibration MAE pretraining with open-domain data: First, it automatically filters out uninformative samples based on the event activeness and information consistency without relying on human annotations; Second, to mind the gap between open-domain and in-domain distributions, OpenMAE develops a novel data mixing method, FreqCutMix, that combines two data types in the frequency domain as augmented pretraining samples, preserving both events-of-interest semantics from in-domain data and real-world diversity from open-domain data. The open-domain data scale in data mixing is dynamically increased as pretraining progresses to stabilize the model convergence. We download over 5 million open-world vibration samples from the Raspberry Shake datacenter1 and conduct extensive experiments with two applications (i.e., indoor activity and outdoor transportation analysis). The evaluation results show OpenMAE improves downstream task accuracies by up to 23\% and achieves enhanced generalizability into diverse downstream tasks, domain variations, and sensor-to-target distances.},
  month = {jun},
}

@article{russo_towards_2025,
  title = {Towards an {Ontology},
  author = {Russo, Mayra and Vidal, Maria-Esther},
  year = {2025},
  doi = {10.1613/jair.1.19388},
  url = {https://doi.org/10.1613/jair.1.19388},
  journal = {J. Artif. Int. Res.},
  volume = {83},
  note = {Place: El Segundo, CA, USA
Publisher: AI Access Foundation},
  keywords = {AI in Healthcare, Explainable AI, Trustworthy AI, source: ACM},
  abstract = {Machine learning (ML)-powered systems are capable of reproducing and often amplifying undesired biases embedded in society, emphasizing the importance of operating under practices that enable the study and understanding of the intrinsic characteristics of ML pipelines. This supports the emergence of documentation frameworks with the idea that “any remedy for bias starts with awareness of its existence.” However, a resource that can formally describe ML pipelines in terms of detected biases is still missing. To address this gap, we present the Doc-BiasO ontology, a resource that sets out to create an integrated vocabulary of biases defined in the Trustworthy AI literature and their measures, as well as to incorporate relevant domain terminology and relationships between them. Overseeing ontology engineering best practices, we reuse existing vocabularies on machine learning and AI to foster knowledge sharing and interoperability between the actors concerned with its research, development, regulation, and others. In addition, we demonstrate the potential of Doc-BiasO with an experiment on an existing benchmark and as part of a neuro-symbolic system. Overall, our main objective is to contribute towards clarifying existing terminology on bias research as it rapidly expands to all areas of AI and to improve the interpretation of bias in data and downstream impact through its documentation.},
  issn = {1076-9757},
  month = {aug},
}

@article{chander_toward_2025,
  title = {Toward {Trustworthy},
  author = {chander, Bhanu and John, Chinju and Warrier, Lekha and Gopalakrishnan, Kumaravelan},
  year = {2025},
  doi = {10.1145/3675392},
  url = {https://doi.org/10.1145/3675392},
  journal = {ACM Comput. Surv.},
  volume = {57},
  number = {6},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Artificial intelligence, data privacy, decision-making, deep learning, explainability, fairness, machine learning, robustness, trustworthy AI, source: ACM},
  abstract = {From the innovation, Artificial Intelligence (AI) materialized as one of the noticeable research areas in various technologies and has almost expanded into every aspect of modern human life. However, nowadays, the development of AI is unpredictable with the stated values of those developing them; hence, the risk of misbehaving AI increases continuously. Therefore, there are uncertainties about endorsing that the development and deploying of AI are favorable and not unfavorable to humankind. In addition, AI holds a black-box pattern, which results in a lack of understanding of how systems can work based on the raised concerns. From the above discussion, trustworthy AI is vital for the extensive adoption of AI in many applications, with strong attention to humankind and the need to focus on AI systems developing into the system outline at the time of system design. In this survey, we discuss compound materials on trustworthy AI and the present state-of-the-art of trustworthy AI technologies, revealing new perspectives, bridging knowledge gaps, and paving the way for potential advances of robustness, and explainability rules which play a proactive role in designing AI systems. Systems that are reliable and secure and mimic human behavior significantly impact the technological AI ecosystem. We provide various contemporary technologies to build explainability and robustness for AI-based solutions, so AI works in a safer and more trustworthy manner. Finally, we conclude our survey article with high-end opportunities, challenges, and future research directions for trustworthy AI to investigate in the future.},
  issn = {0360-0300},
  month = {feb},
}

@article{chen_semicmt_2024,
  title = {{SemiCMT},
  author = {Chen, Yatong and Hu, Chenzhi and Kimura, Tomoyoshi and Li, Qinya and Liu, Shengzhong and Wu, Fan and Chen, Guihai},
  year = {2024},
  doi = {10.1145/3699779},
  url = {https://doi.org/10.1145/3699779},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {8},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Contrastive Learning, Cross-Modal Knowledge Transfer, Multi-Modal Time-Series Signals, source: ACM},
  abstract = {This paper proposes a novel contrastive cross-modal knowledge transfer framework, SemiCMT, for multi-modal IoT sensing applications. It effectively transfers the feature extraction capability (also called knowledge) learned from a source modality (e.g., acoustic signals) with abundant unlabeled training data, to a target modality (e.g., seismic signals) that lacks enough training data, in a self-supervised manner with the help of only a small set of synchronized multi-modal pairs. The transferred model can be quickly finetuned to downstream target-modal tasks with only limited labels. The key design constitutes of three aspects: First, we factorize the latent embedding of each modality into shared and private components and perform knowledge transfer considering both the modality information commonality and gaps. Second, we enforce structural correlation constraints between the source modality and the target modality, to push the target modal embedding space symmetric to the source modal embedding space, with the anchoring of additional source-modal samples, which expands the existing modal-matching objective in current multi-modal contrastive frameworks. Finally, we conduct downstream task finetuning in the spherical space with a KNN classifier to better align with the structured modality embedding space. Extensive evaluations on five multimodal IoT datasets are performed to validate the effectiveness of SemiCMT in cross-modal knowledge transfer, including a new self-collected dataset using seismic and acoustic signals for office activity monitoring. SemiCMT consistently outperforms existing self-supervised and knowledge transfer approaches by up to 36.47\% in the finetuned target-modal classification tasks. The code and the self-collected dataset will be released at https://github.com/SJTU-RTEAS/SemiCMT.},
  month = {nov},
}

@article{alsoubai_profiling_2024,
  title = {Profiling the {Offline},
  author = {Alsoubai, Ashwaq and Razi, Afsaneh and Agha, Zainab and Ali, Shiza and Stringhini, Gianluca and De Choudhury, Munmun and Wisniewski, Pamela J.},
  year = {2024},
  doi = {10.1145/3637391},
  url = {https://doi.org/10.1145/3637391},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {human-computer interaction, mixture factor, youth online safety, youth risk profiles, source: ACM},
  abstract = {We conducted a study with 173 adolescents (ages 13-21), who self-reported their offline and online risk experiences and uploaded their Instagram data to our study website to flag private conversations as unsafe. Risk profiles were first created based on the survey data and then compared with the risk-flagged social media data. Five risk profiles emerged: Low Risks (51\% of the participants), Medium Risks (29\%), Increased Sexting (8\%), Increased Self-Harm (8\%), and High Risk Perpetration (4\%). Overall, the profiles correlated well with the social media data with the highest level of risk occurring in the three smallest profiles. Youth who experienced increased sexting and self-harm frequently reported engaging in unsafe sexual conversations. Meanwhile, high risk perpetration was characterized by increased violence, threats, and sales/promotion of illegal activities. A key insight from our study was that offline risk behavior sometimes manifested differently in online contexts (i.e., offline self-harm as risky online sexual interactions). Our findings highlight the need for targeted risk prevention strategies for youth online safety.},
  month = {apr},
}

@article{tao_multimodal_2024,
  title = {Multimodal {Consistency},
  author = {Tao, Zhulin and Zhao, Runze and Shi, Xin and Gao, Xingyu and Wang, Xi and Huang, Xianglin},
  year = {2024},
  doi = {10.1145/3699959},
  url = {https://doi.org/10.1145/3699959},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Fake news detection, Multi-modal learning, Social media, source: ACM},
  abstract = {Recent multi-modal fake news detection methods often use the consistency between textual and visual contents to determine the truth or fake of a news information. Higher levels of textual-visual consistency typically lead to a greater likelihood of classifying a news item as real. However, a critical observation reveals that creators of most fake news intentionally select images that align with the textual content, thereby enhancing the credibility of the news. Consequently, high consistency between textual and visual contents alone cannot guarantee the authenticity of the information. To address this problem, we introduce a novel approach termed Multimodal Consistency-based Suppression Factor to modulate the significance of textual-visual consistency in information assessment. When the textual-visual matching is high, this suppression factor reduces the influence of consistency during the judgment process. Moreover, we use Contrastive Language-Image Pre-training (CLIP) model to extract features and measure the consistency level between modalities to guide multimodal fusion. In addition, we also use a method of compressing and fusing modal information based on Variational Autoencoder (VAE) to reconstruct CLIP features, learning the shared representation of different modal information of CLIP. Finally, extensive experiments were conducted on three publicly datasets, Weibo, Twitter and Weibo21, and the results confirmed that our method outperformed the state-of-the-art methods in the field, and had 0.8\%, 2.6\% and 4.1\% effect improvement on the accuracy rate.},
  annote = {Just Accepted},
  issn = {1551-6857},
  month = {oct},
}

@article{robledo_yamamoto_we_2023,
  title = {“{We},
  author = {Robledo Yamamoto, Fujiko and Cho, Janghee and Voida, Amy and Voida, Stephen},
  year = {2023},
  doi = {10.1145/3589956},
  url = {https://doi.org/10.1145/3589956},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {30},
  number = {5},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {digital mental health, graduate students, Self-care, stress management, source: ACM},
  abstract = {Graduate students are facing a mental health crisis due to a combination of individual, community, and societal factors. Many existing stress management interventions engage with one factor at a time, typically focusing on providing a user with data about their stress state. We conducted co-design workshops with graduate students who work closely together to explore their strategies for managing stress and to learn about what types of technologies they envision to help address their stress. Using Ecological Systems Theory as an conceptual framework, our analysis of the designs and discussions from these workshops contributes an expanded design space for stress management—one that foregrounds the affordances and challenges of designing interventions that cut across ecological systems levels along with designs that approach stress management using a broader diversity of strategies: controlling, disconnecting, and normalizing stress. We argue that this expanded design space embraces a more holistic and human approach to designing stress management technologies.},
  issn = {1073-0516},
  month = {sep},
}

@article{ramokapane_what_2022,
  title = {What {Users},
  author = {Ramokapane, Kopo Marvin and Such, Jose and Rashid, Awais},
  year = {2022},
  doi = {10.1145/3546578},
  url = {https://doi.org/10.1145/3546578},
  journal = {ACM Trans. Priv. Secur.},
  volume = {26},
  number = {1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {cloud computing, cloud deletion, cloud deletion mechanisms, cloud storage, data deletion, Deletion, deletion preferences, participatory design, preferences, user studies, source: ACM},
  abstract = {Current cloud deletion mechanisms fall short in meeting users’ various deletion needs. They assume all data is deleted the same way—data is temporally removed (or hidden) from users’ cloud accounts before being completely deleted. This assumption neglects users’ desire to have data completely deleted instantly or their preference to have it recoverable for a more extended period. To date, these preferences have not been explored. To address this gap, we conducted a participatory study with four groups of active cloud users (five subjects per group). We examined their deletion preferences and the information they require to aid deletion. In particular, we explored how users want to delete cloud data and identify what information about cloud deletion they consider essential, the time it should be made available to them, and the communication channel that should be used. We show that cloud deletion preferences are complex and multi-dimensional, varying between subjects and groups. Information about deletion should be within reach when needed, for instance, be part of deletion controls. Based on these findings, we discuss the implications of our study in improving the current deletion mechanism to accommodate these preferences.},
  issn = {2471-2566},
  month = {nov},
}

@article{lin_detecting_2021,
  title = {On detecting cherry-picked generalizations},
  author = {Lin, Yin and Youngmann, Brit and Moskovitch, Yuval and Jagadish, H. V. and Milo, Tova},
  year = {2021},
  doi = {10.14778/3485450.3485457},
  url = {https://doi.org/10.14778/3485450.3485457},
  journal = {Proc. VLDB Endow.},
  volume = {15},
  number = {1},
  pages = {59--71},
  note = {Publisher: VLDB Endowment},
  keywords = {source: ACM},
  abstract = {Generalizing from detailed data to statements in a broader context is often critical for users to make sense of large data sets. Correspondingly, poorly constructed generalizations might convey misleading information even if the statements are technically supported by the data. For example, a cherry-picked level of aggregation could obscure substantial sub-groups that oppose the generalization. We present a framework for detecting and explaining cherry-picked generalizations by refining aggregate queries. We present a scoring method to indicate the appropriateness of the generalizations. We design efficient algorithms for score computation. For providing a better understanding of the resulting score, we also formulate practical explanation tasks to disclose significant counterexamples and provide better alternatives to the statement. We conduct experiments using real-world data sets and examples to show the effectiveness of our proposed evaluation metric and the efficiency of our algorithmic framework.},
  issn = {2150-8097},
  month = {sep},
}

@article{agadakos_large-scale_2020,
  title = {Large-scale {Debloating},
  author = {Agadakos, Ioannis and Demarinis, Nicholas and Jin, Di and Williams-King, Kent and Alfajardo, Jearson and Shteinfeld, Benjamin and Williams-King, David and Kemerlis, Vasileios P. and Portokalidis, Georgios},
  year = {2020},
  doi = {10.1145/3414997},
  url = {https://doi.org/10.1145/3414997},
  journal = {Digital Threats},
  volume = {1},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Code debloating, software security, static binary analysis, source: ACM},
  abstract = {Developers nowadays have access to an arsenal of toolkits and libraries for rapid application prototyping. However, when an application loads a library, the entirety of that library’s code is mapped into the process address space, even if only a single function is actually needed. The unused portion is bloat that can negatively impact software defenses by unnecessarily inflating their overhead or increasing the attack surface. In this article, we investigate whether debloating is possible and practical at the binary level. To this end, we present Nibbler: a system that identifies and erases unused functions within dynamic shared libraries. Nibbler works in tandem with defenses such as continuous code re-randomization and control-flow integrity, enhancing them without incurring additional run-time overhead. We developed and tested a prototype of Nibbler on x86-64 Linux; Nibbler reduces the size of shared libraries and the number of available functions, for real-world binaries and the SPEC CINT2006 suite, by up to 56\% and 82\%, respectively. We also demonstrate that Nibbler benefits defenses by showing that: (i) it improves the deployability of a continuous re-randomization system for binaries, namely, Shuffler, by increasing its efficiency by 20\%, and (ii) it improves certain fast but coarse and context-insensitive control-flow integrity schemes by reducing the number of gadgets reachable through indirect branch instructions by 75\% and 49\%, on average. Last, we apply Nibbler on ≈30K C/C++ binaries and ≈5K unique dynamic shared libraries (i.e., almost the complete set of the Debian sid distribution), as well as on nine official Docker images (with millions of downloads in Docker Hub), reporting entrancing findings regarding code bloat at large.},
  month = {dec},
}

@article{moitra_parsing_2021,
  title = {Parsing the '{Me},
  author = {Moitra, Aparna and Ahmed, Syed Ishtiaque and Chandra, Priyank},
  year = {2021},
  doi = {10.1145/3449185},
  url = {https://doi.org/10.1145/3449185},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {5},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {feminism, infrastructure, intersectionality, justice, lgbtq+, sexual harassment, source: ACM},
  abstract = {India's \#MeToo movement began in late-2018, and was largely a platform for some privileged women sharing their accounts of sexual harassment. Beyond issues of access to digital technology, our paper investigates why various sections of India's female and LGBTQ+ population chose not to engage with the \#MeToo movement. Focusing on experiences with sexual harassment, we conducted 44 qualitative interviews with middle-class working women, feminist and queer activists, academics, and other stakeholders working against gender-based violence, to understand their perspectives on \#MeToo. Our paper explores why some survivors bypass the legal infrastructure to speak out against sexual harassment using \#MeToo, while others choose not to participate despite having access to social media platforms. Using the lens of infrastructure, we outline the imbrication of social media movements with existing social norms and legal infrastructures. Further, we highlight how infrastructural politics are connected to patriarchy, colonialism, caste, class, and gender struggles.},
  month = {apr},
}

@article{feuston_conformity_2020,
  title = {Conformity of {Eating},
  author = {Feuston, Jessica L. and Taylor, Alex S. and Piper, Anne Marie},
  year = {2020},
  doi = {10.1145/3392845},
  url = {https://doi.org/10.1145/3392845},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {4},
  number = {CSCW1},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {conformity, content moderation, digital ethnography, eating disorders, mental illness, multiplicity, online communities, social media, source: ACM},
  abstract = {For individuals with mental illness, social media platforms are considered spaces for sharing and connection. However, not all expressions of mental illness are treated equally on these platforms. Different aggregates of human and technical control are used to report and ban content, accounts, and communities. Through two years of digital ethnography, including online observation and interviews, with people with eating disorders, we examine the experience of content moderation. We use a constructivist grounded theory approach to analysis that shows how practices of moderation across different platforms have particular consequences for members of marginalized groups, who are pressured to conform and compelled to resist. Above all, we argue that platform moderation is enmeshed with wider processes of conformity to specific versions of mental illness. Practices of moderation reassert certain bodies and experiences as 'normal' and valued, while rejecting others. At the same time, navigating and resisting these normative pressures further inscribes the marginal status of certain individuals. We discuss changes to the ways that platforms handle content related to eating disorders by drawing on the concept of multiplicity to inform design.},
  month = {may},
}

@article{sharma_blockchain_2020,
  title = {Blockchain {Technology},
  author = {Sharma, Pratima and Jindal, Rajni and Borah, Malaya Dutta},
  year = {2020},
  doi = {10.1145/3403954},
  url = {https://doi.org/10.1145/3403954},
  journal = {ACM Comput. Surv.},
  volume = {53},
  number = {4},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {Blockchain technology, cloud computing, cloud security, cloud storage, decentralization, source: ACM},
  abstract = {The demand for Blockchain innovation and the significance of its application has inspired ever-progressing exploration in various scientific and practical areas. Even though it is still in the initial testing stage, the blockchain is being viewed as a progressive solution to address present-day technology concerns, such as decentralization, identity, trust, character, ownership of data, and information-driven choices. Simultaneously, the world is facing an increase in the diversity and quantity of digital information produced by machines and users. While effectively looking for the ideal approach to storing and processing cloud data, the blockchain innovation provides significant inputs. This article reviews the application of blockchain technology for securing cloud storage.},
  issn = {0360-0300},
  month = {aug},
}

@article{heldens_landscape_2020,
  title = {The {Landscape},
  author = {Heldens, Stijn and Hijma, Pieter and Werkhoven, Ben Van and Maassen, Jason and Belloum, Adam S. Z. and Van Nieuwpoort, Rob V.},
  year = {2020},
  doi = {10.1145/3372390},
  url = {https://doi.org/10.1145/3372390},
  journal = {ACM Comput. Surv.},
  volume = {53},
  number = {2},
  note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
  keywords = {data-driven analysis, Exascale computing, extreme-scale computing, high-performance computing, literature review, source: ACM},
  abstract = {The next generation of supercomputers will break the exascale barrier. Soon we will have systems capable of at least one quintillion (billion billion) floating-point operations per second (1018 FLOPS). Tremendous amounts of work have been invested into identifying and overcoming the challenges of the exascale era. In this work, we present an overview of these efforts and provide insight into the important trends, developments, and exciting research opportunities in exascale computing. We use a three-stage approach in which we (1) discuss various exascale landmark studies, (2) use data-driven techniques to analyze the large collection of related literature, and (3) discuss eight research areas in depth based on influential articles. Overall, we observe that great advancements have been made in tackling the two primary exascale challenges: energy efficiency and fault tolerance. However, as we look forward, we still foresee two major concerns: the lack of suitable programming tools and the growing gap between processor performance and data bandwidth (i.e., memory, storage, networks). Although we will certainly reach exascale soon, without additional research, these issues could potentially limit the applicability of exascale computing.},
  issn = {0360-0300},
  month = {mar},
}

@inproceedings{lo_ai_2025,
  title = {{AI},
  author = {Lo, Frank Po Wen and Qiu, Jianing and Wang, Zeyu and Yu, Haibao and Chen, Yeming and Zhang, Gao and Lo, Benny Ping Lai},
  year = {2025},
  doi = {10.1109/CVPRW67362.2025.00402},
  url = {https://doi.org/10.1109/cvprw67362.2025.00402},
  booktitle = {{IEEE},
  pages = {4184 -- 4193},
  note = {Type: Conference paper},
  keywords = {agentic system, ai hiring, bias migration, Computational modeling, Computer vision, Conferences, Context modeling, Data mining, explainable ai, Knowledge based systems, Large language models, llm, rag, Recruitment, Resumes, Retrieval augmented generation, source: IEEE},
  annote = {Cited by: 2},
}

@article{ogdu_adaptive_2025,
  title = {An {Adaptive},
  author = {Ogdu, Çağatay Umut and Arslanoǧlu, Kübra and Karakose, Mehmet},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3613340},
  url = {https://doi.org/10.1109/access.2025.3613340},
  journal = {IEEE Access},
  volume = {13},
  pages = {167390 -- 167404},
  note = {Type: Article},
  keywords = {Accuracy, Clinical decision support system, Data integration, Databases, Decision support systems, Heuristic algorithms, large language models, Large language models, Medical diagnostic imaging, multi-agent system, Protocols, Real-time systems, Reliability, source: IEEE},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{van_ai-powered_2025,
  title = {{AI},
  author = {Van, Nguyen Nang Hung and Do, Phuc Hao and Hoang, Vannam and Nguyen, Truc Thi Kim and Pham, Minh Tuan},
  year = {2025},
  doi = {10.1109/TLT.2025.3604096},
  url = {https://doi.org/10.1109/tlt.2025.3604096},
  journal = {IEEE Transactions on Learning Technologies},
  volume = {18},
  pages = {856 -- 868},
  note = {Type: Article},
  keywords = {Accuracy, AI in education, Artificial intelligence, Artificial intelligence (AI), educational chatbots, Employee welfare, Engineering profession, Large language models, large language models (LLMs), Mathematical models, Measurement, natural language processing (NLP), Retrieval augmented generation, retrieval-augmented generation (RAG), Scalability, student confidence, student counseling, university admission, university admission counseling, Vectors, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{godwin_hybrid_2025,
  title = {A {Hybrid},
  author = {Godwin, Jerrick and Athuraliya, Banuka},
  year = {2025},
  doi = {10.1109/ECAI65401.2025.11095523},
  url = {https://doi.org/10.1109/ecai65401.2025.11095523},
  note = {Type: Conference paper},
  keywords = {Accuracy, Distance measurement, Fine-tuning, Indexing, Large language models, Large Language Models, Retrieval augmented generation, Retrieval Augmented Generation, Size measurement, Social networking (online), Surveys, Training, Tuning, source: IEEE},
  annote = {Cited by: 0},
}

@article{nawara_comprehensive_2025,
  title = {A {Comprehensive},
  author = {Nawara, Dina and Kashef, Rasha F.},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3599832},
  url = {https://doi.org/10.1109/access.2025.3599832},
  journal = {IEEE Access},
  volume = {13},
  pages = {145772 -- 145798},
  note = {Type: Article},
  keywords = {Accuracy, Adaptation models, Automobiles, Collaborative filtering, discriminative learning, Electronic commerce, evaluation metrics, Filtering, Large language models (LLMs), LLM-as-a-judge, Motion pictures, multi-modal paradigms, Real-time systems, recommendation systems, Recommender systems, retrieval-augmented generation (RAG), Surveys, source: IEEE},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{pan_survey_2025-1,
  title = {A {Survey},
  author = {Pan, Feng and Zhou, Qiyun and Guo, Weitong and Yang, Hongwu},
  year = {2025},
  doi = {10.1109/CSTE64638.2025.11092120},
  url = {https://doi.org/10.1109/cste64638.2025.11092120},
  pages = {803 -- 807},
  note = {Type: Conference paper},
  keywords = {Ecosystems, Education, Educational application, Ethics, Generative AI, Information retrieval, Intelligence education, Large language model, Large language models, Navigation, Retrieval augmented generation, Retrieval-augmented generation, Surveys, Vocational training, source: IEEE},
  annote = {Cited by: 0},
}

@article{shokrnezhad_autonomous_2025,
  title = {An {Autonomous},
  author = {Shokrnezhad, Masoud and Taleb, Tarik},
  year = {2025},
  doi = {10.1109/MCOM.001.2400526},
  url = {https://doi.org/10.1109/mcom.001.2400526},
  journal = {IEEE Communications Magazine},
  volume = {63},
  number = {8},
  pages = {78 -- 84},
  note = {Type: Article},
  keywords = {Adaptation models, Cognition, Complexity theory, Continuing education, Contrastive learning, Few shot learning, Generators, Large language models, Planning, Reinforcement learning, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{yao_adapting_2025,
  title = {Adapting {Large},
  author = {Yao, Jiasheng and Guo, Yanliang and Yu, Qing},
  year = {2025},
  doi = {10.1109/ICAACE65325.2025.11019949},
  url = {https://doi.org/10.1109/icaace65325.2025.11019949},
  pages = {2601 -- 2606},
  note = {Type: Conference paper},
  keywords = {Adaptation models, Benchmark testing, Cognition, Fine-tuning, Heuristic algorithms, Knowledge based systems, Large language models, Large Language models, Medical services, Real-time systems, Reliability, Retrieval Argumented Generation, Retrieval augmented generation, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{bhardwaj_ai-powered_2025,
  title = {{AI},
  author = {Bhardwaj, Shivam and Joshi, Rakesh Chandra and Tiwari, Vansh and Říha, Kamil and Sikora, Pavel and Dutta, Malay Kishore},
  year = {2025},
  doi = {10.1109/InCACCT65424.2025.11011436},
  url = {https://doi.org/10.1109/incacct65424.2025.11011436},
  pages = {616 -- 621},
  note = {Type: Conference paper},
  keywords = {Accuracy, Databases, Healthcare data management, Large language models, Large Language Models (LLMs), Pipelines, Protocols, Public health response, Public healthcare, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Standards, Surveillance, Vector database, Vectors, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{omoush_advancing_2025,
  title = {Advancing {Arabic},
  author = {Omoush, Ebtehal H. and Ghnemat, Rawan},
  year = {2025},
  doi = {10.1109/ICTCS65341.2025.10989446},
  url = {https://doi.org/10.1109/ictcs65341.2025.10989446},
  pages = {511 -- 516},
  note = {Type: Conference paper},
  keywords = {Accuracy, Arabic language, Biomedical equipment, fine-tuning technique, JAIS LLM, Large language models, Market research, Medical services, Question answering (information retrieval), Retrieval augmented generation, Retrieve augmented generation, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{hemmat_adaptive_2025,
  title = {Adaptive {Chunking},
  author = {Hemmat, Arshia and Vadaei, Kianoosh and Shirian, Melika and Heydari, Mohammad Hassan and Fatemi, Afsaneh},
  year = {2025},
  doi = {10.1109/CSICC65765.2025.10967455},
  url = {https://doi.org/10.1109/csicc65765.2025.10967455},
  note = {Type: Conference paper},
  keywords = {Academic Question Answering, Context modeling, Datasets Preparation, Education, Large language models, Measurement, Multilingual, Pipelines, Question answering (information retrieval), Retrieval augmented generation, Standards, Video QA, Visualization, source: IEEE},
  annote = {Cited by: 0},
}

@article{du_retrieval-augmented_2025,
  title = {A {Retrieval},
  author = {Du, Kelvin and Zhao, Yazhi and Mao, Rui and Xing, Frank Z. and Cambria, Erik},
  year = {2025},
  doi = {10.1109/MIS.2025.3544912},
  url = {https://doi.org/10.1109/mis.2025.3544912},
  journal = {IEEE Intelligent Systems},
  volume = {40},
  number = {2},
  pages = {15 -- 22},
  note = {Type: Article},
  keywords = {Benchmark testing, Generators, Intelligent systems, Large language models, Multi-agent systems, Oral communication, Problem-solving, Retrieval augmented generation, Sentiment analysis, Transformers, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{lee_novel_2025,
  title = {A {Novel},
  author = {Lee, Hyeoksoo and Yang, Minyeol and Jeong, Jongpil},
  year = {2025},
  doi = {10.23919/ICACT63878.2025.10936654},
  url = {https://doi.org/10.23919/icact63878.2025.10936654},
  booktitle = {International {Conference},
  pages = {395 -- 400},
  note = {Type: Conference paper},
  keywords = {Code Generation, Codes, Databases, Documentation, Electronics industry, Generative AI, Large language models, LLM (Large Language Model), Prompt, Protocols, RAG (Retrieval Augmented Generation), Retrieval augmented generation, Smart manufacturing, Standards, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{kong_retrieval-augmented_2024,
  title = {A {Retrieval},
  author = {Kong, Lingfei and Wang, Gang and Liu, Naiwei and Bai, Tuoya and He, Jingheng},
  year = {2024},
  doi = {10.1109/DSIT61374.2024.10881002},
  url = {https://doi.org/10.1109/dsit61374.2024.10881002},
  note = {Type: Conference paper},
  keywords = {Accuracy, Analytical models, Data models, Hallucinations, Information filters, Intent Analysis, Large Language Model, Noise, Noise measurement, Optimization, Query Rewriting, Reliability, Retrieval augmented generation, Retrieval-Augemented Generation, Semantics, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{singh_adversarial_2024,
  title = {Adversarial {Training},
  author = {Singh, Sonali and Namin, Akbar Siami},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825933},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825933},
  pages = {3589 -- 3598},
  note = {Type: Conference paper},
  keywords = {Context modeling, Fake news, Fake News, GPT-Neo, Large language Model (LLM), Large language models, Natural language processing, Pipelines, Retrieval augmented generation, Retrieval Augmented Generation (RAG), RoBERTa, Social networking (online), Software development management, Training, Transformers, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{turaga_information_2024,
  title = {An {Information},
  author = {Turaga, Venkata Sai Prathyush and Namin, Akbar Siami},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10826052},
  url = {https://doi.org/10.1109/bigdata62323.2024.10826052},
  pages = {3599 -- 3608},
  note = {Type: Conference paper},
  keywords = {Accuracy, Computational modeling, Dark Web, Dark-BERT, Data models, Decision making, Fake news, Information Reliability, Large language models, LLaMA, Misinformation, RAG, Real-time systems, Reliability, Training, source: IEEE},
  annote = {Cited by: 1},
}

@inproceedings{huang_adapting_2024,
  title = {Adapting {Large},
  author = {Huang, Yinkui and Gao, Tianrun and Zhang, Jiangjiang and Liu, Xiaohong and Wang, Guangyu},
  year = {2024},
  doi = {10.1109/BIBM62325.2024.10822725},
  url = {https://doi.org/10.1109/bibm62325.2024.10822725},
  pages = {5770 -- 5775},
  note = {Type: Conference paper},
  keywords = {Adaptation models, Benchmark testing, Biological system modeling, Costs, Data acquisition, Data models, Decision making, Large language models, Large Language Models, Retrieval augmented generation, Retrieval-Augmented Generation, score documents, Standards, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{mahalakshmi_real-time_2024,
  title = {A {Real},
  author = {Mahalakshmi, M. and Bharadwaj, Shardul and Bhuyan, Aklanta Niraz},
  year = {2024},
  doi = {10.1109/ICAIT61638.2024.10690733},
  url = {https://doi.org/10.1109/icait61638.2024.10690733},
  note = {Type: Conference paper},
  keywords = {Accuracy, AI-powered diagnosis, Computational modeling, ETL (Extract Load Transform), Large language models, Large language models (LLM), Medical services, Natural Language Pro- cessing (NLP), Natural language processing, Pipelines, Real-time medical in- sights, Transforms, source: IEEE},
  annote = {Cited by: 1},
}

@inproceedings{mukhlis_novel_2024,
  title = {A {Novel},
  author = {Mukhlis, Raza and Saleem, Saied and Kwon, Hyunwook and Hussain, Jamil and Aydin, Ahmet Arif and Al-antari, Mugahed A.},
  year = {2024},
  doi = {10.1109/IDAP64064.2024.10710824},
  url = {https://doi.org/10.1109/idap64064.2024.10710824},
  note = {Type: Conference paper},
  keywords = {Accuracy, Comprehensive CAD system, Data models, Heating systems, Large language model (LLM), Location awareness, Medical diagnostic imaging, Meteors, Pulmonary diseases, Retrieval Augmented Generation (RAG), Solid modeling, Text embedding, visual and textual interpretability, Visualization, X-ray imaging, source: IEEE},
  annote = {Cited by: 1},
}

@inproceedings{guo_knowledge_2024,
  title = {A {Knowledge},
  author = {Guo, Youwei and Guo, Yanrong},
  year = {2024},
  doi = {10.1109/ICICML63543.2024.10958051},
  url = {https://doi.org/10.1109/icicml63543.2024.10958051},
  pages = {670 -- 673},
  note = {Type: Conference paper},
  keywords = {Accuracy, Computational modeling, Data models, Depression, depression detection, Employee welfare, knowledge graph, Knowledge graphs, large language model, Large language models, Mental health, Retrieval augmented generation, retrieval-augmented generation, Training, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{saeid_agentfusion_2024,
  title = {{AgentFusion},
  author = {Saeid, Yasser and Kopinski, Thomas},
  year = {2024},
  doi = {10.1109/ICECER62944.2024.10920460},
  url = {https://doi.org/10.1109/icecer62944.2024.10920460},
  note = {Type: Conference paper},
  keywords = {Accuracy, Accuracy Assessment, Autonomous Agents, Benchmark testing, Chunk Size Optimization, Computational modeling, Data models, Domain-Specific Language Models, Embedding Optimization, Finance, Finance Data Processing, Foot-ball Data Analysis, German Language Processing, GPT-4 Omni Benchmark, Inductors, Large language models, Large Language Models (LLMs), Llama Model, Multi-Agent Systems, Natural language processing, Natural Language Processing (NLP), Optimization, Performance Evaluation, Query Optimization, Reactor Decommissioning, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Self-Regulating Systems, Semantic Retrieval, Text Chunking, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{jeong_4bit-quantization_2024,
  title = {4bit-{Quantization},
  author = {Jeong, Taehee},
  year = {2024},
  doi = {10.1109/ICMLA61862.2024.00156},
  url = {https://doi.org/10.1109/icmla61862.2024.00156},
  pages = {1037 -- 1042},
  note = {Type: Conference paper},
  keywords = {Accuracy, Databases, Degradation, Embedding vectors, High-dimensional vectors, Large language models, Machine learning, Memory management, Memory storage, Quantization, Quantization (signal), Retrieval augmented generation, Retrieval-augmented generation, Training data, Vector database, Vector search, Vectors, source: IEEE},
  annote = {Cited by: 0},
}

@inproceedings{wang_research_2024,
  title = {Research on {Exchange},
  author = {Wang, Bocheng and Lu, Yushen and Chen, Long and Li, Jianqing and Qu, Liangzhou},
  year = {2024},
  doi = {10.1109/CAIT64506.2024.10963106},
  url = {https://doi.org/10.1109/cait64506.2024.10963106},
  booktitle = {2024 5th {International},
  pages = {566--572},
  keywords = {Accuracy, Biological system modeling, Computational modeling, Data models, exchange rate prediction, Exchange rates, multi-source heterogeneous data, Predictive models, Real-time systems, retrieval augmented generation, Retrieval augmented generation, Sentiment analysis, SHAP, Transfer learning, source: IEEE},
  abstract = {With the accelerating pace of global economic integration, exchange rate fluctuations have been influenced by various uncertain factors. The information explosion has further complicated the factors influencing exchange rate predictions. To fully utilize multi-source heterogeneous data and further optimize exchange rate prediction performance, this paper constructs an exchange rate prediction method based on Retrieval Augmented Generation (RAG) and Explainable Artificial Intelligence models. During the data processing stage, this paper first uses RAG technology to enhance the situational learning ability of Large Language Models (LLMs). By combining prompts and local sentiment databases, LLMs can perform real-time and accurate sentiment analysis on news headlines. Secondly, transfer learning is used to optimize the Inception(v3) model for sentiment analysis of news images, thereby more efficiently analyzing the sentiment variables contained in texts and images, effectively enriching the sources of multi-source heterogeneous data. Extensive experiments have shown that the multi-source heterogeneous dataset constructed in this paper can effectively improve the accuracy of exchange rate predictions and improve the predictive performance of various models across different time steps.},
  month = {dec},
}

@inproceedings{hasegawa_rag_2024,
  title = {{RAG},
  author = {Hasegawa, Kento and Hidano, Seira and Fukushima, Kazuhide},
  year = {2024},
  doi = {10.1109/ICMLA61862.2024.00133},
  url = {https://doi.org/10.1109/icmla61862.2024.00133},
  booktitle = {2024 {International},
  pages = {912--917},
  note = {ISSN: 1946-0759},
  keywords = {Accuracy, certainty, Computer security, CVSS, Information retrieval, large language model, Large language models, Machine learning, Measurement, Predictive models, Retrieval augmented generation, retrieval-augmented generation, security, source: IEEE},
  abstract = {Large language models (LLMs) have recently been employed for a wide variety of purposes. Retrieval-augmented generation (RAG), in which an LLM generates a response based on context relevant to the prompt, is often used to enable the LLM to adapt to specialized domains. However, sentences generated by a generative LLM may contain incorrect information, known as “hallucinations.” The challenge in identifying hallucinations within the RAG framework involves evaluating the certainty of both context retrieval and LLM outputs. In this paper, we propose a metric called RAG certainty to quantify the certainty of LLM outputs within a RAG framework. The proposed metric is calculated based on certainty scores from both information retrieval and response generation. Experimental results demonstrate that the proposed metric effectively reflects the certainty of information retrieval in a RAG framework. We further validated the proposed metric through a case study that assesses the predicted Common Vulnerability Scoring Sys-tem (CVSS) scores for cybersecurity vulnerabilities and found that errors are mitigated according to the proposed metric.},
  month = {dec},
}

@article{alabbasi_teleoracle_2025,
  title = {{TeleOracle},
  author = {Alabbasi, Nouf and Erak, Omar and Alhussein, Omar and Lotfi, Ismail and Muhaidat, Sami and Debbah, Mérouane},
  year = {2025},
  doi = {10.1109/JIOT.2025.3553161},
  url = {https://doi.org/10.1109/jiot.2025.3553161},
  journal = {IEEE Internet of Things Journal},
  volume = {12},
  number = {10},
  pages = {13170--13182},
  keywords = {3GPP, 6G networks, Accuracy, Adaptation models, AGI, Automation, Benchmark testing, Computational modeling, Context modeling, large language model (LLM), low-rank Adaptation (LoRA), Retrieval augmented generation, retrieval-augmented generation (RAG), Semantics, Telecommunications, source: IEEE},
  abstract = {The telecommunications industry’s rapid evolution demands intelligent systems capable of managing complex networks and adapting to emerging technologies. While large language models (LLMs) show promise in addressing these challenges, their deployment in telecom environments faces significant constraints due to edge device limitations and inconsistent documentation. To bridge this gap, we present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG) system built on the Phi-2 small language model (SLM). To improve context retrieval, TeleOracle employs a two-stage retriever that incorporates semantic chunking and hybrid key-word and semantic search. Additionally, we expand the context window during inference to enhance the model’s performance on open-ended queries. We also employ low-rank adaption for efficient fine-tuning. A thorough analysis of the model’s performance indicates that our RAG framework is effective in aligning Phi-2 to the telecom domain in a downstream question and answer (QnA) task, achieving a 30\% improvement in accuracy over the base Phi-2 model, reaching an overall accuracy of 81.20\%. Notably, we show that our model not only performs on par with the much larger LLMs but also achieves a higher faithfulness score, indicating higher adherence to the retrieved context.},
  issn = {2327-4662},
  month = {may},
}

@inproceedings{wu_multirag_2025,
  title = {{MultiRAG},
  author = {Wu, Wenlong and Wang, Haofen and Li, Bohan and Huang, Peixuan and Zhao, Xinzhe and Liang, Lei},
  year = {2025},
  doi = {10.1109/ICDE65448.2025.00230},
  url = {https://doi.org/10.1109/icde65448.2025.00230},
  booktitle = {2025 {IEEE},
  pages = {3070--3083},
  note = {ISSN: 2375-026X},
  keywords = {Codes, Cognition, Data aggregation, Data engineering, Hallucination Mitigation, Knowledge graphs, Knowledge Graphs, Large language models, Large Language Models, Multi-source Retrieval, Prevention and mitigation, Reliability, Retrieval augmented generation, Retrieval Augmented Generation, Technological innovation, source: IEEE},
  abstract = {Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. Our code is available in https://github.com/wuwenlong123/MultiRAG.},
  month = {may},
}

@inproceedings{mohd_comparative_2024,
  title = {A {Comparative},
  author = {Mohd, Bassam J. and Ahmad Yousef, Khalil M. and Abu Ghalyon, Salah G.},
  year = {2024},
  doi = {10.1109/ACIT62805.2024.10877197},
  url = {https://doi.org/10.1109/acit62805.2024.10877197},
  booktitle = {2024 25th {International},
  pages = {1--6},
  note = {ISSN: 2831-4948},
  keywords = {Accuracy, Adaptation models, Artificial Intelligence, Computational modeling, Focusing, Generative AI, Generative Models, Information technology, large language models, Machine Learning, Natural Language Processing, Neural Networks, Retrieval augmented generation, Retrieval-augmented generation, Testing, Time factors, Time measurement, source: IEEE},
  abstract = {Generative AI creates new content using AI models (i.e., LLMs) trained on large datasets. However, LLMs lack specific sources and can produce inaccurate or misleading information. Retrieval-augmented generation (RAG) improves accuracy by combining LLMs with a content store, providing evidence for responses and making the model more adaptable to changing information. This paper introduces a comparative evaluation of various generative AI models—LLama 3, LLama 3.1, Gemma, Gemma 2, Phi 3, Phi 3.5, and Phi 3.5 Mini based on their performance in answering questions derived from Arabic and English documents using the RAG locally. The models were evaluated based on their ability to answer questions of varying difficulty levels. Questions were categorized as easy, moderate, or complex. The models’ performance was measured in terms of two key metrics: response time to answer each question and the accuracy of their responses (or quality of the answer). Using RAG method, the results show that LLaMA 3.1 consistently outperformed other models in both Arabic and English languages, whereas Gemma 2 demonstrated decent results. Additionally, response time can be misleading indicators of model performance because less accurate responses require less effort and are generated more rapidly than highly accurate ones.},
  month = {dec},
}

@inproceedings{rani_augmenting_2024,
  title = {Augmenting {Code},
  author = {Rani, S Jansi and Deepika, S G and Devdharshini, D and Ravindran, Harini},
  year = {2024},
  doi = {10.1109/SSITCON62437.2024.10796587},
  url = {https://doi.org/10.1109/ssitcon62437.2024.10796587},
  booktitle = {2024 {First},
  pages = {1--7},
  keywords = {Accuracy, code generation, Codes, Computational modeling, Context modeling, grounding, Grounding, language models, Large language models, natural language processing, Natural languages, prompt parameters, Retrieval augmented generation, retrieval-augmented generation, Software systems, Training data, source: IEEE},
  abstract = {The growing demand for efficient code generation has driven research into improving Large Language Models (LLMs). This project presents a novel system designed to enhance code generation by leveraging Retrieval-Augmented Generation (RAG), Grounding techniques, and Prompt Parameters. RAG integrates external knowledge to enrich code outputs, while Grounding methods improve the model’s ability to interpret language with real-world context. Prompt Parameters offer flexibility, enabling customized outputs based on user preferences. These methods were implemented and tested on various code generation tasks, resulting in contextually relevant and accurate outputs. The proposed system streamlines software development workflows, reduces errors, and fosters better collaboration between developers and machine-assisted coding tools. Ultimately, this approach represents a significant advancement in automating code generation from natural language descriptions.},
  month = {oct},
}

@inproceedings{krishnaraj_document_2025,
  title = {Document {Query},
  author = {Krishnaraj, M. and Isaacraj, S. Eben and Soorya, V. Haris Jai},
  year = {2025},
  doi = {10.1109/ESCI63694.2025.10988177},
  url = {https://doi.org/10.1109/esci63694.2025.10988177},
  booktitle = {2025 {International},
  pages = {1--5},
  note = {ISSN: 2996-1815},
  keywords = {Automotive engineering, Chatbots, Companies, embeddings, Informatics, Large language models, Large Language Models (LLM), Privacy, Retrieval augmented generation, Retrieval Augmented Generation(RAG), Transforms, source: IEEE},
  abstract = {The role of automated mechanisms such as chatbots has improved in today’s environment of rapid technological progress which is majorly attributed to the growing use of Artificial Intelligence (AI) based solutions. The ability of such systems to interact with questions concerning PDFs can completely transform support and after sales services in the automotive field where time and resources can be saved. Within this context, this work analyses the application of Large Language Models (LLMs) in the development of RAG based applications to create a smarter and confidentiality-driven chatbot to preserve the privacy of individuals and companies alike.},
  month = {mar},
}

@inproceedings{hayashi_metadata-based_2024,
  title = {Metadata-based {Data},
  author = {Hayashi, Teruaki and Sakaji, Hiroki and Dai, Jiayi and Goebel, Randy},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10826055},
  url = {https://doi.org/10.1109/bigdata62323.2024.10826055},
  booktitle = {2024 {IEEE},
  pages = {6574--6583},
  note = {ISSN: 2573-2978},
  keywords = {data exploration, Databases, dataset search, Decision making, Estimation, Explainable AI, large language model, Large language models, Metadata, Retrieval augmented generation, retrieval-augmented generation, Semantics, Soft sensors, Vectors, source: IEEE},
  abstract = {Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.},
  month = {dec},
}

@inproceedings{nethassanai_mari_2025,
  title = {Mari: {Automating},
  author = {Nethassanai, Thitipa and Seehabong, Nichakon and Roteaim, Saranrat and Mongkolnam, Pornchai and Watanapa, Bunthit},
  year = {2025},
  doi = {10.1109/ECTI-CON64996.2025.11101123},
  url = {https://doi.org/10.1109/ecti-con64996.2025.11101123},
  booktitle = {2025 22nd {International},
  pages = {1--6},
  note = {ISSN: 2837-6471},
  keywords = {chatbot, Chatbots, Computational modeling, Information technology, Large language models, Law, legal, LLM, Prompt engineering, RAG, Retrieval augmented generation, Semantics, Telecommunications, Thai Debt Collection Act, Tropical cyclones, source: IEEE},
  abstract = {Over the years, there have been lots of reports that point out the problems of Thailand’s high personal debt levels. With limited public knowledge of legal debt rights, these problems result in right violation between the creditors and debtors. Mari is designed to address such issues. It is an AI-powered system that offers automated legal consultations pertaining to the Thai Debt Collection Act of 2015. Main focus is on aiding creditors, debtors, or any individual, to understand their rights which contribute a boon of addressing information gaps, and enhancing accessibility to legal knowledge without requiring costly consultations. Mari is a website that uses a large language model (LLM) with integrated Retrieval-Augmented Generation (RAG) to ensure the precise responses for the required scope and semantics of the concerned legal statute. In order to achieve the effectiveness of these responses, we considered several LLM models, namely OpenThaiGPT, Typhoon, and SambaNova under the evaluation based on a set of human-generated multiple-choice and open-end questions. The comparison score showed that SambaNova’s LLM, with English prompt engineering, was the best for this case. Lastly the semantic difference, representing the level of hallucination effect, of the final model is measured by the BERT score with satisfaction of 0.726.},
  month = {may},
}

@inproceedings{pan_raglog_2024,
  title = {{RAGLog},
  author = {Pan, Jonathan and Liang, Wong Swee and Yidi, Yuan},
  year = {2024},
  doi = {10.1109/WFPST58552.2024.00034},
  url = {https://doi.org/10.1109/wfpst58552.2024.00034},
  booktitle = {2024 {IEEE},
  pages = {169--174},
  keywords = {Databases, Digital forensics, Fault diagnosis, Large Language Model, Large language models, Log analysis, Machine learning, Pipelines, Retrieval Augmented Generation, Vectors, source: IEEE},
  abstract = {The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.},
  month = {may},
}

@inproceedings{gijre_enhancing_2024,
  title = {Enhancing {Question},
  author = {Gijre, Supraj and Agrawal, Rishi and Laddha, Priyash and Keswani, Gunjan},
  year = {2024},
  doi = {10.1109/ICAIQSA64000.2024.10882212},
  url = {https://doi.org/10.1109/icaiqsa64000.2024.10882212},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Information retrieval, Knowledge Graph, Knowledge graphs, LangChain, Large language models, Large Language Models, Maintenance, Natural Language Processing, Neo4j, Quantum computing, Retrieval augmented generation, Retrieval Augmented Generation, Soft sensors, Standards, Text analysis, Vectors, source: IEEE},
  abstract = {This paper describes a novel technique to improving Large Language Models (LLMs) for document analysis that employs knowledge graphs and retrieval-augmented generation (RAG). We are working on constructing a chatbot system that can handle and analyze large documents from a variety of fields. Our approach addresses basic LLM issues including context maintenance and hallucination avoidance. The system combines document chunking, vector embedding, and similarity search with graph-based knowledge representation. Users can upload large papers and answer questions accurately. We show that integrating standard information retrieval approaches with graph-based storage and LLM capabilities improves context awareness and response accuracy across a wide range of document genres. This strategy is especially promising for complicated publications such as financial reports.},
  month = {dec},
}

@inproceedings{gangavarapu_evaluating_2025,
  title = {Evaluating {Accuracy},
  author = {Gangavarapu, Rajendra and Srinivasan, Aswath Ram Adayapalam and Moparthi, Venkata},
  year = {2025},
  doi = {10.1109/ICAD65464.2025.11114027},
  url = {https://doi.org/10.1109/icad65464.2025.11114027},
  booktitle = {2025 {IEEE},
  pages = {1--7},
  keywords = {source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) has emerged as a promising approach to mitigate the limitations of Large Language Models (LLMs) in generating factually accurate and consistent text. The main focus of this technical survey is on correct answers as the key performance indicator (KPI) for comparing two well-known RAG methods: Naive RAG and Corrective retrieval augmented Generation (CRAG). Naive RAG exhibits a strong dependence on the relevance of retrieved documents, resulting in suboptimal performance when retrieval quality is compromised. CRAG, on the other hand, adds new features to improve robustness and adaptability, such as a retrieval evaluator, large-scale web searches, and a decompose-then-recompose algorithm. We introduce the Comprehensive RAG Benchmark (CRAG), which encompasses a diverse set of question-answer pairs spanning multiple domains, categories, entity popularities, and temporal dynamics, to facilitate a comprehensive evaluation of RAG models' performance in generating correct answers. Experiments show that adding RAG to LLMs makes a big difference in how many correct answers you get, with CRAG consistently beating Naive RAG. Nevertheless, RAG models, including CRAG, demonstrate lower answer correctness when confronted with questions pertaining to highly dynamic, less popular, or more complex facts. These results make it clear that more research and development is needed to make RAG models more reliable and able to give correct answers in a wider range of situations.},
  month = {jun},
}

@inproceedings{i_vilar_accessible_2025,
  title = {Accessible and {Reliable},
  author = {i Vilar, Guiu Puigcercos and Rashid, Parvez and Tonekaboni, Navid Hashemi},
  year = {2025},
  doi = {10.1109/EDUCON62633.2025.11016497},
  url = {https://doi.org/10.1109/educon62633.2025.11016497},
  booktitle = {2025 {IEEE},
  pages = {1--10},
  note = {ISSN: 2165-9567},
  keywords = {source: IEEE},
  abstract = {This paper addresses the challenge of improving the reliability and accuracy of Large Language Models (LLMs) for assisting students in learning Java programming, a critical component of object-oriented computer science courses. While LLMs have shown promise in generating code, they often produce incorrect or unreliable outputs, which can hinder the learning process. To mitigate these issues, we propose a novel augmentation framework that integrates Retrieval-Augmented Generation (RAG), enabling LLMs to retrieve best-practice Java code examples from external sources to enhance the accuracy of generated solutions. We evaluate this framework using 250 Java coding tasks, covering a range of difficulties. Our findings show that the RAG-augmented model, when applied to a lightweight LLM (Google DeepMind's Gemma), outperformed baseline model by 19 \% for mid and high-difficulty problems. In particular, Augmented Gemma generated accepted code for 11 problems where no other model could provide a valid solution. This suggests that retrieving external best-practice examples is critical in addressing complex coding challenges. These results highlight the potential of lightweight, accessible models enhanced with RAG to provide reliable AI coding assistance in educational settings, facilitating both accurate problem-solving and reinforcement of best coding practices.},
  month = {apr},
}

@inproceedings{tjokro_enhancing_2024,
  title = {Enhancing {Data},
  author = {Tjokro, Vincencius Christiano and Ady Sanjaya, Samuel},
  year = {2024},
  doi = {10.1109/ISRITI64779.2024.10963652},
  url = {https://doi.org/10.1109/isriti64779.2024.10963652},
  booktitle = {2024 7th {International},
  pages = {473--478},
  note = {ISSN: 2832-1456},
  keywords = {Context modeling, Data Traceability, Information technology, Intelligent systems, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Manuals, RAG, Refining, Retrieval augmented generation, Retrieval-Augmented Generation, Seminars, Vector Retrieval, Vectors, source: IEEE},
  abstract = {This study explores the challenges of data traceability within Company ABC’s Master Data Management (MDM) system, where handling data-related requests has become increasingly complex due to inefficient manual processes. Despite MDM’s implementation, business growth has led to the accumulation of documents and difficulties in tracking request lineage, causing bottlenecks in daily operations. The study proposes a solution that integrates knowledge graphs with Retrieval-Augmented Generation (RAG), inspired by LinkedIn’s question-answering system, to improve data management and traceability. The knowledge graph is constructed by manually parsing spreadsheet data related to requests, allowing for more efficient data integration and retrieval. The study compares three approaches—graph-based, vector-based, and hybrid—in terms of answer correctness and context retrieval. The graph-based approach yielded the best performance, improving answer correctness by 21.09\% over the vector-based method and by 19.34\% over the hybrid method. While the hybrid method achieved the highest context precision at 83.49\%, it struggled with context recall, retrieving relevant context only 69.09\% of the time. The study also found that the vector-based method’s retrieval of irrelevant context caused the language model to produce misleading answers, even when accompanied by more relevant graph-based context. In conclusion, integrating RAG with knowledge graphs significantly enhances answer accuracy and context retrieval precision. Future research should focus on refining context-weighting techniques between the vector and graph-based methods and on addressing the issue of irrelevant context to further improve the overall effectiveness of the model in data traceability and information retrieval tasks.},
  month = {dec},
}

@inproceedings{roychowdhury_eratta_2024,
  title = {{ERATTA},
  author = {Roychowdhury, Sohini and Krema, Marko and Mahammad, Anvar and Moore, Brian and Mukherjee, Arijit and Prakashchandra, Punit},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825910},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825910},
  booktitle = {2024 {IEEE},
  journal = {arXiv preprint arXiv …},
  pages = {4605--4610},
  note = {ISSN: 2573-2978},
  keywords = {authentication, hallucination, Large language models, Measurement, Message authentication, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Routing, Scalability, Social networking (online), Soft sensors, sustainability, Sustainable development, TextBison, Unicorn, Water resources, source: IEEE, source: Google Scholar},
  abstract = {Large language models (LLMs) with retrieval augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. Although RAG implemented with AI agents (agentic-RAG) has been recently popularized, its suffers from unstable cost and unreliable performances for Enterprise-level data-practices. Most existing use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user-query routing, data-retrieval and custom prompting for question-answering capabilities from Enterprise-level data tables on sustainability. The source tables here are highly fluctuating and large in size storing carbon footprint, energy and water usage at buildings in regional levels globally and the proposed framework enables structured responses in under 10 seconds per query. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve {\textgreater},
  month = {dec},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{hamzic_evaluation_2024,
  title = {Evaluation and {Comparison},
  author = {Hamzic, Dzenan and Wurzenberger, Markus and Skopik, Florian and Landauer, Max and Rauber, Andreas},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825576},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825576},
  booktitle = {2024 {IEEE},
  pages = {5342--5351},
  note = {ISSN: 2573-2978},
  keywords = {Benchmark testing, Big Data, Context modeling, Data models, Large Language Model, Large language models, LLM Benchmarking, LLM Evaluation, Measurement, Natural language generation, Natural Language Generation Evaluation, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, source: IEEE},
  abstract = {The rapid advancement of Large Language Models (LLMs) has transformed natural language processing, yet comprehensive evaluation methods are necessary to ensure their reliability, particularly in Retrieval-Augmented Generation (RAG) tasks. This study aims to evaluate and compare the performance of open-source LLMs by introducing a rigorous evaluation framework. We benchmark 20 LLMs using a combination of established metrics such as BLEU, ROUGE, BERTScore, along with and a novel metric, RAGAS. The models were tested across two distinct datasets to assess their text generation quality. Our findings reveal that models like nous-hermes-2-solar-10.7b and mistral-7b-instruct-v0.1 consistently excel in tasks requiring strict instruction adherence and effective use of large contexts, while other models show areas for improvement. This research contributes to the field by offering a comprehensive evaluation framework that aids in selecting the most suitable LLMs for complex RAG applications, with implications for future developments in natural language processing and big data analysis.},
  month = {dec},
}

@article{praneeth_optimization_2025,
  title = {Optimization of {Customer},
  author = {Praneeth, Buchepalli and {Mohana},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3588337},
  url = {https://doi.org/10.1109/access.2025.3588337},
  journal = {IEEE Access},
  volume = {13},
  pages = {124319--124332},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, BERT, BM25, Cognition, Customer reviews, Databases, DSPy, Gemini-1.5-Pro, GPT-3.5-Turbo, hierarchical chunk retrieval, LangChain, Large language models, Llama-3-8B-Instruct-Lite, LLM, Question answering (information retrieval), RAG, RAG fusion, Retrieval augmented generation, Reviews, Semantics, Surveys, Vectors, source: IEEE, source: Google Scholar},
  abstract = {Customer feedback, often shared through online reviews, plays a crucial role in shaping business strategies. However, the overwhelming volume of such reviews poses two major challenges: valuable insights often go unnoticed, and manual analysis introduces human bias. To address this, we propose a system that leverages large language models (LLMs) integrated with the LangChain framework to answer natural language queries over customer reviews. A synthetic dataset was created to resemble food delivery reviews typically seen on the Play Store and was stored in a vector database. On receiving a user query, relevant reviews are retrieved using advanced Retrieval-Augmented Generation (RAG) techniques, namely Hierarchical Chunk Retrieval and RAG Fusion that are further refined using the Declarative Self-improving Python (DSPy) framework to generate accurate, grounded responses. The system was evaluated using LLaMA-3-8B-InstructLite, GPT-3.5-Turbo, and Gemini-1.5-Pro, and compared against two non-LLM baselines: BM25 and a fine-tuned BERT model. Results show that our LLM-based pipeline outperforms by an average 15\% in semantic and factual accuracy. Component-level analysis showed that enhanced retrieval strategies showed aggregate improvements of 4.9\% in semantic relevance, 12.1\% in lexical coverage, and 9.9\% in factual consistency over traditional RAG. Further integration of DSPy led to an additional 10.8\% boost in linguistic fluency and a 9.0\% gain in factual alignment. Among the evaluated models, Gemini-1.5-Pro combined with RAG Fusion and DSPy produced the most fluent and factually accurate responses, demonstrating the effectiveness of combining hybrid retrieval with LLM-driven reasoning for query-based feedback response system.},
  issn = {2169-3536},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{sun_irrational_2025,
  title = {The {Irrational},
  author = {Sun, Dachun and Lyu, You and Li, Jinning and Liu, Xinyi and Kara, Denizhan and Lebiere, Christian and Abdelzaher, Tarek},
  year = {2025},
  doi = {10.1109/ICCCN65249.2025.11134012},
  url = {https://doi.org/10.1109/icccn65249.2025.11134012},
  booktitle = {2025 34th {International},
  pages = {1--9},
  note = {ISSN: 2637-9430},
  keywords = {Bounded Rationality, Cognitive Architecture, Computational modeling, Computer architecture, Large language models, Large Language Models, Metaverse, Predictive models, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Robustness, Social networking (online), Technological innovation, Training data, source: IEEE},
  abstract = {This paper advances research on social networks, extended reality, and the metaverse by bringing together innovations from two different communities – AI and cognitive science – to develop LLM-based agents with not only fluent responses but also realistic opinion dynamics that capture a variety of human biases, imperfections, and general departures from rationality. This avenue of investigation can empower applications from social simulation of human opinions in geopolitical hotspots to realistic non-player character interactions in metaverse games. Recent advances in AI have made remarkable progress toward general intelligence with the introduction of large language models (LLMs). They also enabled grounding LLM responses in specialized information stored externally using retrieval-augmented generation (RAG). In a separate line of research, studies on human cognition have produced cognitive architectures that emulate human departures from rationality, such as biases and imperfections, which are crucial to understanding a wide range of social phenomena and human preferences. A critical mechanism in cognitive architectures is the modulation of retrieval weights from (human) memory; we are biased in what we remember. Combining RAG with cognitive model-inspired computation of information retrieval weights, we develop the Irrational LLM – one that weighs information retrieval in RAG systems according to cognitive models, thereby accurately emulating human opinion formation. We implement the novel human cognition-inspired RAG framework (CogRAG) and use it to emulate option developments on different sides of a conflict regarding debated issues. Responses generated by CogRAG (on posts withheld from training data) show close correspondence with real responses posted on social media, suggesting the viability of this approach in approximating biased human opinions. We hope this study paves the way to new directions in AI, social networks, metaverse computing, and human-in-the-loop modeling that better represent diverse human opinions in geopolitical, entertainment, and socio-technical contexts.},
  month = {aug},
}

@inproceedings{dutta_ai_2024,
  title = {{AI},
  author = {Dutta, Ayush and Sarma, Kaushik Kumar and P., Yamini Sahukar},
  year = {2024},
  doi = {10.1109/CSITSS64042.2024.10817061},
  url = {https://doi.org/10.1109/csitss64042.2024.10817061},
  booktitle = {2024 8th {International},
  pages = {1--5},
  note = {ISSN: 2767-1097},
  keywords = {Business, Chatbot Technology, Chatbots, Codes, Indian Penal Code, Large language models, Large Language Models, Law, Legal Information Systems, Natural Language Processing, Navigation, NLP Algorithms, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors, Streamlit, User interfaces, source: IEEE},
  abstract = {The legal framework is a critical component of societal structure, yet its complexity often leaves individuals struggling to understand their rights and obligations. This paper introduces an advanced NLP-based chatbot designed to enhance legal accessibility and comprehension, focusing on the Indian Penal Code (IPC). The proposed system integrates Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to provide precise and contextually relevant legal information. The chatbot utilizes NLP algorithms for interpreting legal texts and generating user-friendly responses, thus facilitating a better understanding of legal articles and statutes. Streamlit is employed to create an interactive user interface, ensuring a seamless experience for users seeking legal advice. The implementation of this system aims to bridge the gap between complex legal language and public understanding, improve the efficiency of accessing legal information, and support individuals and small businesses in navigating legal challenges. This approach is anticipated to significantly enhance legal awareness and accessibility, contributing to a more informed and legally savvy society.},
  month = {nov},
}

@inproceedings{hamid_can_2025,
  title = {Can {Large},
  author = {Hamid, Mohamed Rashad Abdel and El-Regaily, Salsabil Amin and Aref, Mostafa Mahmoud},
  year = {2025},
  doi = {10.1109/MIUCC66482.2025.11196859},
  url = {https://doi.org/10.1109/miucc66482.2025.11196859},
  booktitle = {2025 {International},
  pages = {360--365},
  keywords = {Accuracy, Benchmark testing, Cognition, Factual QA, GRPO, Hallucination Detection, Knowledge graphs, Knowledge Graphs, Large language models, Large Language Models, Multi-hop Reasoning, Optimization, Pipelines, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Subgraph Retrieval, Ubiquitous computing, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding their outputs in external knowledge. However, conventional chunk-based retrieval methods are limited in handling complex multi-hop questions due to their lack of structural reasoning and semantic alignment. In this work, we propose a novel pipeline where an LLM performs multi-hop subgraph retrieval over a knowledge graph (KG), followed by hallucination detection using triplet-level graph alignment. The final answer is generated by conditioning the LLM on both the user query and the verified subgraph. To construct our benchmark, we automatically generate 500 diverse multi-hop questions from 10 real-world websites and construct their corresponding KGs using GPT-4o. Experimental results show that our method significantly outperforms traditional chunking-based RAG in answer accuracy (84.29\% vs. 68.77\%). Furthermore, we introduce a fine-tuning procedure using Group Relative Policy Optimization (GRPO) guided by FactAlign rewards, which further improves answer accuracy to 87.62\% while reducing hallucination by over 8 percentage points. Our approach demonstrates a principled and interpretable alternative to retrieval that leverages the structure and semantics of knowledge graphs, offering stronger factual consistency and reasoning capabilities.},
  month = {sep},
}

@inproceedings{wang_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Wang, Zhiyao and Guo, Xiujing and Tsuchiya, Tatsuhiro},
  year = {2025},
  doi = {10.1109/QRS65678.2025.00022},
  url = {https://doi.org/10.1109/qrs65678.2025.00022},
  booktitle = {2025 25th {International},
  pages = {108--119},
  note = {ISSN: 2693-9177},
  keywords = {Closed box, Databases, Large Language Model, Large language models, Maintenance, Natural language processing, Retrieval augmented generation, Retrieval-Augmented Generation, Runtime, Security, Software quality, Software reliability, Software Requirement, Test Case Generation, source: IEEE},
  abstract = {Testers often need to manually write black-box test cases based on software artifacts such as requirement documents. In agile development, this process is often time-consuming and is further complicated by frequent requirement changes, leading to continuous maintenance overhead. Automating this process is therefore essential. Given the strong natural language understanding and generation capabilities of large language models (LLMs), combined with Retrieval-Augmented Generation (RAG), we propose a RAG-based framework for automated test case generation. Before generation, we embed software artifacts to construct a vectorbased knowledge database. At runtime, software requirements are used as queries to retrieve relevant context, which is integrated into a prompt and passed to the LLM for test case generation. This approach addresses several shortcomings of LLMs, including limited context length, attention dilution over large inputs, and the tendency to hallucinate or over-look key domain-specific constraints. By providing query-specific external knowledge, RAG enhances both accuracy and efficiency. We deploy the framework with different models locally and conduct experiments on two open-source datasets. Compared with the manually written benchmark test cases, our method achieves full requirement coverage with fewer test cases, improved efficiency, reduced error potential, and realized better readability.},
  month = {jul},
}

@inproceedings{arora_revolutionizing_2024,
  title = {Revolutionizing {Third},
  author = {Arora, Shrey and Ramteke, Vidyavati},
  year = {2024},
  doi = {10.1109/ICDCC62744.2024.10961910},
  url = {https://doi.org/10.1109/icdcc62744.2024.10961910},
  booktitle = {2024 {First},
  pages = {16--22},
  keywords = {Automation, Compliance Automation, Data models, Generative AI, Grounding, Manuals, Organizations, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, Risk Assessment, Risk management, Soft sensors, Third Party Risk Management (TPRM), source: IEEE},
  abstract = {This integration of Generative AI and Retrieval Augmented Generation (RAG) will open new frontiers in Third Party Risk Management (TPRM) by making the creation of both risk assessments and compliance checks more accurate, efficient, and scalable. Quite naturally, TPRM is one area wherein organizations must engage in processes to make sure risks emanating from vendors and partners are duly mitigated. Traditionally, the TPRM process has been manual and time-consuming. With the functionality of Generative AI, autonomy in creating relevant content and automation in compliance significantly reduce the labor of these processes. With RAG integrated, the model improves the quality of AI responses through real-time grounding in authoritative data sources, reducing the risk of providing outdated or incorrect information. This dual approach will make sure organizations manage a growing number of third-party relationships with greater precision and timeliness. Indeed, the proposed AI-driven framework will minimize human error and reduce operational costs, besides being scalable to adapt to dynamic regulatory changes-thus offering a competitive advantage in third-party risk management.},
  month = {nov},
}

@inproceedings{zubkova_sugar_2025,
  title = {{SUGAR},
  author = {Zubkova, Hanna and Park, Ji-Hoon and Lee, Seong-Whan},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10890064},
  url = {https://doi.org/10.1109/icassp49660.2025.10890064},
  booktitle = {{ICASSP},
  pages = {1--5},
  note = {ISSN: 2379-190X},
  keywords = {Entropy, large language models, Large language models, Noise measurement, question answering, Question answering (information retrieval), retrieval augmented generation, Retrieval augmented generation, Semantics, Signal processing, Speech processing, Sugar, Uncertainty, uncertainty estimation, source: IEEE},
  abstract = {Bearing in mind the limited parametric knowledge of Large Language Models (LLMs), retrieval-augmented generation (RAG) which supplies them with the relevant external knowledge has served as an approach to mitigate the issue of hallucinations to a certain extent. However, uniformly retrieving supporting context makes response generation source-inefficient, as triggering the retriever is not always necessary, or even inaccurate, when a model gets distracted by noisy retrieved content and produces an unhelpful answer. Motivated by these issues, we introduce Semantic Uncertainty Guided Adaptive Retrieval (SUGAR), where we leverage context-based entropy to actively decide whether to retrieve and to further determine between single-step and multi-step retrieval. Our empirical results show that selective retrieval guided by semantic uncertainty estimation improves the performance across diverse question answering tasks, as well as achieves a more efficient inference.},
  month = {apr},
}

@inproceedings{zhang_meditrir_2025,
  title = {{MediTriR},
  author = {Zhang, Hongzhi and Shafiq, M. Omair},
  year = {2025},
  doi = {10.1109/ICSC64641.2025.00032},
  url = {https://doi.org/10.1109/icsc64641.2025.00032},
  booktitle = {2025 19th {International},
  pages = {187--194},
  note = {ISSN: 2472-9671},
  keywords = {source: IEEE},
  abstract = {Advances in large language models have driven progress in medical question-answering systems, but challenges remain in accuracy and relevance, especially in complex medical settings. To address this problem, we introduce MediTriR. This approach combines knowledge graph triples with a retrieval-augmented generation framework to enhance the performance of large language models in medical multiple-choice question-answering tasks. Our approach transforms the retrieved medical knowledge into structured triples and uses these triples for accurate information retrieval. Furthermore, we dynamically generate new triples based on the properties of triples to enrich the model's reasoning ability. The integration of retrieval-augmented generation enables more accurate and contextual answers by leveraging external medical triples. MediTriR is evaluated against pure large language model approaches using the MedMCQA and Medical QA datasets, showing better performance in terms of accuracy and relevance. These results highlight the potential of our approach to improve medical decision support and education.},
  month = {feb},
}

@inproceedings{he_can_2025,
  title = {Can {RAG},
  author = {He, JiangLong and Kumar, Deepak and Rasamsetty, Pratyusha},
  year = {2025},
  doi = {10.1109/CCWC62904.2025.10903733},
  url = {https://doi.org/10.1109/ccwc62904.2025.10903733},
  booktitle = {2025 {IEEE},
  pages = {00414--00420},
  keywords = {Accuracy, Calibration, Constrastive Learning, Context modeling, Information Extraction, Information retrieval, Knowledge based systems, Large language models, Loans and mortgages, Measurement, Predictive models, Prompt engineering, Retrieval augmented generation, Retrieval-Augmented Generation, Robustness, Training, source: IEEE},
  abstract = {Information extraction (IE) is a crucial process in natural language processing (NLP) that involves automatically retrieving structured information from semi-structured and un-structured text sources. Text sources are semi-structured and unstructured documents, for example, W-2 forms, paystubs, mortgage documents, etc. Due to the recent bloom of LLMs, the accuracy of many NLP-based information extraction systems has improved. Another key attribute of a good information extraction system is certainty. The user must know how certain the system is regarding its extraction. Using LLM suffers from the calibration problem as most LLMs use a decoder-only architecture that produces logits conditioned on previous tokens. The confidence score computed by applying softmax on these logits is mostly off from being calibration. In this paper, we provide a generalizable framework to properly calibrate an LLM-based information extraction system with novel context rejection training tasks to teach LLM to ignore less useful examples. It can be easily extended to other tasks other than information extraction. Our method results in a 17.25\% improvement in F1-score and a 46.49\% improvement in certainty score over the baseline. It also outperforms the state-of-the-art ChatGPT model by a large margin.},
  month = {jan},
}

@inproceedings{shelley_flora-rag_2025,
  title = {Flora-{RAG},
  author = {Shelley, Sayuri and Kaur, Prabhjeet and Aggarwal, Garima and Kaushal, Abhishek and Dutta, Malay Kishore},
  year = {2025},
  doi = {10.1109/ICETM63734.2025.11051979},
  url = {https://doi.org/10.1109/icetm63734.2025.11051979},
  booktitle = {2025 {International},
  pages = {i--vi},
  keywords = {Accuracy, Conversational artificial intelligence, Floriculture, Gemini, ImageBind, KDB.AI, Large Language Model, Large language models, Multimodal, Question answering (information retrieval), Retrieval augmented generation, Retrieval Augmented Generation, Training, Transformers, Vector database, Vectors, Visual databases, Visualization, source: IEEE},
  abstract = {Anthophiles are people who love flowers and want to acquire every inch of information they can get on them. This paper presents study on development of a multi-modal RAGenhanced conversational AI with a database full of Floriculture. This conversational AI multimodal is built in order to explore the preparing, embedding, and storing of both text and image data with the KBD.AI vector database. This vector database is specifically used as a retrieval tool within this RAG pipeline for both textual and visual data, making this conversational AI a multimodal. This paper elucidates the methodology used and the characteristics of the proposed RAG multimodal, which is highly domain-specific. The evaluation of joint training of the RAG’s retrieval and generation components for the domain knowledge task in Open Domain Question Answering is done. With numerous test cases giving accurate and exceptional test results, this study successfully demonstrates the effectiveness of the proposed RAG-enhanced conversational AI having floriculture dataset, while achieving a factual accuracy. This states that the model shows robustness, generalizability and efficiency of the proposed framework for image and text retrieval in domain of floriculture, contributing towards the advancement of agricultural technologies and question answering precision.},
  month = {may},
}

@inproceedings{li_knowledge_2025,
  title = {Knowledge {Graph},
  author = {Li, Bing and Mao, Zhifang and Yan, Rui and Ling, Aojia and Hu, Qin and Zeng, Qiang},
  year = {2025},
  doi = {10.1109/ECIS65594.2025.11087034},
  url = {https://doi.org/10.1109/ecis65594.2025.11087034},
  booktitle = {2025 {IEEE},
  pages = {1--5},
  keywords = {Accuracy, Contrastive learning, Knowledge Completion, Knowledge Graph, Knowledge graphs, Large Language Model, Natural Language Processing, Retrieval augmented generation, Sampling methods, Semantics, Stability analysis, Training, Transforms, Vectors, source: IEEE},
  abstract = {Semantic and structural information are essential in Knowledge graph completion. We propose KRIS (Knowledge Graph Completion using Retrieval-Augmented Generation and Improved Structural Information), a method based on Retrieval-Augmented Generation (RAG) with enhanced structural information. KRIS uses semantic embeddings and RAG to enrich triple semantic information, reducing the hallucination problem, especially for domain-specific knowledge graphs. To address the weakness of semantic-embedding-based methods in leveraging structural information, we conduct joint training via supervised contrastive learning and structural information learning. Our model effectively utilizes both types of information. Experiments on public datasets show that KRIS significantly outperforms baseline models in prediction accuracy.},
  month = {may},
}

@inproceedings{neha_exploring_2025,
  title = {Exploring {AI},
  author = {Neha, Fnu and Bhati, Deepshikha and Shukla, Deepak Kumar and Guercio, Angela and Ward, Ben},
  year = {2025},
  doi = {10.1109/CCWC62904.2025.10903902},
  url = {https://doi.org/10.1109/ccwc62904.2025.10903902},
  booktitle = {2025 {IEEE},
  pages = {00633--00639},
  keywords = {Accuracy, Artificial intelligence, Artificial Intelligence, Ethics, Fake news, Focusing, Generators, Large language models, Large Language Models (LLMs), Natural Language Un-derstanding, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Reviews, Text Detector, Text Generator, source: IEEE},
  abstract = {The rapid development of Artificial Intelligence (AI) has led to the creation of powerful text generation models, such as large language models (LLMs), which are widely used for di-verse applications. However, concerns surrounding AI-generated content, including issues of originality, bias, misinformation, and accountability, have become increasingly prominent. This paper offers a comprehensive overview of AI text generators (AITGs), focusing on their evolution, capabilities, and ethical implications. This paper also introduces Retrieval-Augmented Generation (RAG), a recent approach that improves the contextual relevance and accuracy of text generation by integrating dynamic infor-mation retrieval. RAG addresses key limitations of traditional models, including their reliance on static knowledge and potential inaccuracies in handling real-world data. Additionally, the paper reviews detection tools that help differentiate AI-generated text from human-written content and discusses the ethical challenges these technologies pose. The paper explores future directions for improving detection accuracy, supporting ethical AI development, and increasing accessibility. The paper contributes to a more responsible and reliable use of AI in content creation through these discussions.},
  month = {jan},
}

@inproceedings{sung_new_2024,
  title = {A {New},
  author = {Sung, Chih-Wei and Lee, Yu-Kai and Tsai, Yin-Te},
  year = {2024},
  doi = {10.1109/COMPSAC61105.2024.00371},
  url = {https://doi.org/10.1109/compsac61105.2024.00371},
  booktitle = {2024 {IEEE},
  pages = {2308--2312},
  note = {ISSN: 2836-3795},
  keywords = {Accuracy, Computational modeling, Ethics, Instruction Tuning, Large Language Model, Organizations, Pipelines, Psychiatry, Retrieval-Augmented Generation, Technological innovation, source: IEEE},
  abstract = {With the rapid development of large language models (LLMs) in recent years, there has been an increasing demand for domain-specific Agents that can cater to the unique needs of enterprises and organizations. Unlike general models, which strive for broad coverage, these specialized Agents rely on focused datasets tailored to their intended applications. This research proposes a pipeline that leverages the power of LLMs and the Retrieval-Augmented Generation (RAG) related framework to construct high-quality instruction datasets for finetuning on specific domains using custom document collections. By ingesting domain-specific documents, the pipeline generates relevant and contextually appropriate instructions, thus effectively creating a comprehensive dataset for fine-tuning LLMs on the target domain. This approach overcomes the limitations of traditional dataset creation methods, which often rely on manual curation or web-scraping techniques that may introduce noise and irrelevant data. Notably, our pipeline offers a dynamic solution that can quickly adapt to updates or modifications in the domainspecific document collection, eliminating the need for complete retraining. Additionally, it addresses the challenge of data scarcity by enabling the generation of instruction datasets from a limited set of initial documents, rendering it suitable for unpopular or specialized domains where comprehensive datasets are scarce. As a case study, we apply this approach to the domain of psychiatry, a field requiring specialized knowledge and sensitive handling of patient information. The resulting fine-tuned LLM demonstrates showcases the viability of the proposed approach and underscores its potential for widespread adoption across various industries and domains where tailored, accurate, and contextually relevant language models are indispensable.},
  month = {jul},
}

@inproceedings{s_graph_2025,
  title = {Graph {Based},
  author = {S, Akilesh and Sekar, Rajeev and R, Tulasi Raman and R, Suganya},
  year = {2025},
  doi = {10.1109/SCEECS64059.2025.10940424},
  url = {https://doi.org/10.1109/sceecs64059.2025.10940424},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2688-0288},
  keywords = {Chatbots, Databases, Diet Recommendation, Focusing, GPTs, Healthcare, Information retrieval, Large language models, Large Language Models, Measurement, Medical services, Neo4j, NLP, OpenAI, Recommender systems, Retrieval augmented generation, Retrieval-Augmented Generation, Schedules, source: IEEE},
  abstract = {This paper presents a personalized diet recommendation system that leverages a graph database and retrieval-augmented generation (RAG) with large language models (LLMs) to provide accurate and tailored nutritional plans. The system focuses on vegetarian Indian dishes and analyzes individual health metrics such as physical activity levels, weight age and height to generate customized diet plans. A Neo4j graph database, built from scraped recipe data, stores dish names, ingredients, and nutritional values, enabling precise information retrieval via LLMs. Additionally, the system provides nutritional details and overall health assessments of the dishes. To enhance overall health, the system also suggests basic workout plans based on users' physical metrics and dietary recommendations. This approach aims to provide a low-cost, accessible alternative to traditional diet consultations while mitigating hallucinations in AI outputs through RAG and structured data.},
  month = {jan},
}

@inproceedings{lin_domain_2024,
  title = {Domain {Adaption},
  author = {Lin, Huadong and Chen, Yirong and Tao, Wenyu and Chen, Mingyu and Xu, Xiangmin and Xing, Xiaofen},
  year = {2024},
  doi = {10.1109/SLT61566.2024.10832316},
  url = {https://doi.org/10.1109/slt61566.2024.10832316},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE Spoken …},
  pages = {1047--1052},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Adaptation models, Conferences, Contrastive learning, Data models, Dialog System, Domain Adaption, Knowledge based systems, Large language models, RAG, Retrieval augmented generation, Unified Knowledge Base, source: IEEE, source: Google Scholar},
  abstract = {Retrieval augmented generation (RAG) has emerged as a paradigm to address problems like hallucination in dialog systems based on large language model (LLM). Retrieval model is a key component in RAG framework for recalling relevant information. This paper describes our solution for FutureDial-RAG Challenge Track 1. We identify two primary challenges in this track: domain specificity and heterogeneity of knowledge base. To address the two challenges, we first adopt continual pre-training of a pre-trained retrieval model on both labeled and unlabeled data for domain adaption. Subsequently, we modify and expand the knowledge base, ensuring that each piece of knowledge is uniformly structured in a question-answer (QA) format. Finally, we construct negative samples based on the labeled data and the unified knowledge base, and fine-tune the retrieval model using contrastive learning. Our solution achieves a score of 2.023 on the dev set, which significantly outperforms the baseline.},
  month = {dec},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{xu_design_2025,
  title = {Design {Process},
  author = {Xu, Xiwei and Zhang, Dawen and Zhang, Wenjie and Lu, Qinghua and Zhu, Liming},
  year = {2025},
  doi = {10.1109/ICSA-C65153.2025.00072},
  url = {https://doi.org/10.1109/icsa-c65153.2025.00072},
  booktitle = {2025 {IEEE},
  pages = {482--487},
  note = {ISSN: 2768-4288},
  keywords = {Chatbots, Cognition, Design Process, Financial industry, GenAI, Generative AI, RAG, Retrieval augmented generation, Software architecture, Software systems, Systematics, Trade-offs, Videos, source: IEEE},
  abstract = {Generative AI (GenAI) refers to artificial intelligence systems capable of developing and creating new content such as text, images, audio, or videos based on learned patterns from existing data. Integrating GenAI deeply into more complex software systems poses challenges such as hallucination, out-dated knowledge, non-removable knowledge, and non-traceable reasoning process. Retrieval-Augmented Generation (RAG) has emerged as a promising solution for enabling GenAI systems to incorporate knowledge from external sources. However, there is a lack of systematic guidance on designing a RAG system and identifying critical decision points. This paper presents a design process for a RAG system, including a suitability analysis based on the requirements of specific scenarios and the inherent characteristics of RAG. It outlines the key steps to design a RAG system and discusses critical design decisions. Insights are provided on the impact of design alternatives on the quality attributes of GenAI systems and their tradeoffs. Finally, a feasibility study examines an LLM-based chatbot designed for Q\&A in taxation scenarios.},
  month = {mar},
}

@inproceedings{omrani_hybrid_2024,
  title = {Hybrid {Retrieval},
  author = {Omrani, Pouria and Hosseini, Alireza and Hooshanfar, Kiana and Ebrahimian, Zahra and Toosi, Ramin and Ali Akhaee, Mohammad},
  year = {2024},
  doi = {10.1109/ICWR61162.2024.10533345},
  url = {https://doi.org/10.1109/icwr61162.2024.10533345},
  booktitle = {2024 10th {International},
  pages = {22--26},
  note = {ISSN: 2837-8296},
  keywords = {Benchmark testing, Generative AI, Hybrid power systems, Knowledge engineering, LLM, Measurement, Natural language processing, NLP, Refining, Retrieval Augmented Generation (RAG), Training, source: IEEE},
  abstract = {In the domain of Natural Language Processing (NLP), the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) represents a significant advancement towards enhancing the depth and relevance of model-generated responses. This paper introduces a novel hybrid RAG framework that synergizes the Sentence-Window and Parent-Child methodologies with an innovative re-ranking mechanism, aimed at optimizing the query response capabilities of LLMs. By leveraging external knowledge sources more effectively, the proposed method enriches LLM outputs with greater accuracy, relevance, and information fidelity. We subject our hybrid model to rigorous evaluation against benchmark datasets and metrics, demonstrating its superior performance over existing state-of-the-art RAG techniques. The results highlight our method’s enhanced ability to generate responses that are not only contextually appropriate but also demonstrate a high degree of faithfulness to the source material, thereby setting a new standard for query response enhancement in LLMs. Our study underscores the potential of hybrid RAG models in refining the interaction between LLMs and external knowledge, paving the way for future research in the field of NLP.},
  month = {apr},
}

@inproceedings{amarnath_intelligent_2024,
  title = {An {Intelligent},
  author = {Amarnath, Neeraj Singh and Nagarajan, Rajganesh},
  year = {2024},
  doi = {10.1109/ICSES63445.2024.10762977},
  url = {https://doi.org/10.1109/icses63445.2024.10762977},
  booktitle = {2024 4th {International},
  journal = {2024 4th International …},
  pages = {1393--1398},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, Answer Relevancy, Chatbots, Context Precision, Context Recall, Context Retrieval Pipeline, Faithfulness, Fake news, Ground Truths, Hallucination, Large language models, Navigation, Oral communication, Planning, Real-time systems, Reliability, Retrieval Augmented Generation (RAG), Semantic Similarity Search, Transforms, Vectorstore, source: IEEE, source: Google Scholar},
  abstract = {The growth of Large Language Models (LLM) has showcased both their potential and limitations in domain- specific applications. A significant limitation is their tendency to confidently present false information, known as hallucination, which compromises the integrity of their outputs. Retrieval Augmented Generation (RAG) systems mitigate this by enhancing the Large Language Models' ability to retrieve accurate, source-specific knowledge. Many American students begin preparing for college applications in middle school, seeking guidance on planning their high school years to achieve personal goals. The diversity in students' interests, skills, and circumstances often results in an overwhelming amount of information and advice. This research introduces a novel RAG chatbot designed to provide personalized assistance to high school students using their course guide as the primary source document. By integrating a high school course guide with a RAG system, the chatbot delivers relevant suggestions, helping students navigate their educational paths with greater confidence and precision. This system aims to minimize the risk of hallucination by ensuring that responses are grounded in the accurate and relevant information contained within the course guide. Consequently, it addresses the crucial need for personalized educational planning, enabling students to make informed decisions that align with their unique aspirations and circumstances. This research highlights the transformative potential of RAG systems in educational contexts, offering a reliable tool to support students in their academic and personal growth.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{ju_task-classifying_2025,
  title = {Task-{Classifying},
  author = {Ju, Jingyuan and Zang, Peng and Zhong, Hao and Wang, Lingfeng},
  year = {2025},
  doi = {10.1109/ICAIBD64986.2025.11082008},
  url = {https://doi.org/10.1109/icaibd64986.2025.11082008},
  booktitle = {2025 8th {International},
  pages = {122--127},
  note = {ISSN: 2769-3554},
  keywords = {Accuracy, Adaptation models, HyDE-based Retrieval, Information retrieval, Large language models, Multitasking, Question answering (information retrieval), Reflection, Reliability theory, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Robustness, Task - classifying framework, source: IEEE},
  abstract = {Hallucination phenomena are inevitably present in large language models (LLMs) for generation tasks. Retrieval Augmented Generation (RAG) technology alleviates this issue to some extent by incorporating external retrieved documents. However, existing Retrieval-Augmented Generation (RAG) methods still face limitations in terms of task adaptability and the richness of retrieval inputs. To address these issues, this paper proposes an innovative RAG framework that enhances both task classification and high-quality document retrieval, aiming to improve the accuracy and robustness of generated results. Specifically, we design a task identification module capable of automatically categorizing user inputs into three types of tasks: information retrieval, text generation, and general question answering. This classification then guides the subsequent processing flow. At the retrieval stage, we introduce a HyDE-inspired hypothetical document generation strategy, which constructs task-relevant hypothetical answers to improve the alignment between retrieved documents and user intent. To improve the quality of generation, this article introduces a task adaptive reflection and generation mechanism, implementing differentiated reflection processes and generation strategies for different task types, thereby improving the accuracy and completeness of the model’s response in multi task scenarios. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance across multiple natural language processing tasks, particularly excelling in complex question answering and open-domain information retrieval scenarios.},
  month = {may},
}

@inproceedings{lou_cognitive_2025,
  title = {A {Cognitive},
  author = {Lou, Shanhe and Tan, Runjia and Zhou, Mengchu and Lv, Chen},
  year = {2025},
  doi = {10.1109/ICHMS65439.2025.11154278},
  url = {https://doi.org/10.1109/ichms65439.2025.11154278},
  booktitle = {2025 {IEEE},
  pages = {223--228},
  keywords = {source: IEEE},
  abstract = {While digital twins have made significant strides in creating digital replicas of physical manufacturing systems, their cognitive capabilities remain inadequate to address the dynamic complexities in manufacturing, including process variations, environmental fluctuations, and human interactions. These limitations hinder their applicability in Industry 5.0, which emphasizes Self-X cognitive capabilities. This work proposes an improved five-dimensional framework for developing a cognitive digital twin (CDT) that adopts a Large Language Model (LLM) agent at its core. The LLM agent enhances domain-specific tasksolving capabilities through retrieval-augmented generation (RAG) and in-context learning. RAG compensates for the general LLM's limitations by utilizing external tool libraries and industrial knowledge graphs to establish context awareness, retrieve domain-specific knowledge, and convert human commands into sequential task plans via function calls. Incontext learning further enables the LLM agent to learn specific tasks based on contextual examples without retraining. It empowers CDT to address domain-specific challenges with efficiency, flexibility, and cost-effectiveness. The effectiveness of the proposed CDT is demonstrated in a lab-scale manufacturing unit, highlighting its ability to perform valid task planning and handle dynamic incidents, paving the way for more resilient manufacturing systems aligned with Industry 5.0 objectives.},
  month = {may},
}

@inproceedings{shah_building_2024,
  title = {Building {Generative},
  author = {Shah, Jay Ashok and Iyer, Nisha Ramesh},
  year = {2024},
  doi = {10.1109/UEMCON62879.2024.10754774},
  url = {https://doi.org/10.1109/uemcon62879.2024.10754774},
  booktitle = {2024 {IEEE},
  pages = {79--84},
  keywords = {Artificial Intelligence, Buildings, Chatbots, Generative AI, Large language models, Large Language Models, Machine learning, Mobile communication, OCI Generative AI Service, Oracle Cloud Infrastructure, Regulation, Reliability, Retrieval Augmented Generation, Security, User experience, source: IEEE},
  abstract = {Artificial intelligence (AI) and Generative AI (GenAI) are revolutionizing technology and businesses at an extraordinary speed, providing numerous benefits that can position a company at the forefront of a swiftly changing market. The adoption of Generative AI will strengthen and enhance the role of chatbots in organizations and provide a more efficient and versatile user experience. Successfully integrating these technologies into enterprise systems requires a reliable, scalable, and secure infrastructure. Oracle Cloud Infrastructure (OCI) emerges as a powerful platform designed and crafted to facilitate the building and deployment of Generative AI applications. OCI’s comprehensive suite of AI and machine learning tools like OCI Generative AI service and its exceptional high-performance computing capabilities offers an optimal environment for developing cutting-edge Generative AI solutions. This paper explores the inner workings of Large Language Models (LLMs), OCI Generative AI service and focuses on building Chatbot using pre-trained and custom LLM models with the goal of allowing end users to gain insights seamlessly and efficiently. By presenting a comprehensive guide to leverage OCI for Generative AI, this paper aims to serve as a valuable resource for developers and organizations that are looking to harness cloud-based solutions for innovative AI-driven applications.},
  month = {oct},
}

@inproceedings{sundar_revolutionizing_2024,
  title = {Revolutionizing {Assessment},
  author = {Sundar, Koushik and Manohar, Eashaan and Vijay, K and Prakash, Sajay},
  year = {2024},
  doi = {10.1109/ICSSAS64001.2024.10760285},
  url = {https://doi.org/10.1109/icssas64001.2024.10760285},
  booktitle = {2024 2nd {International},
  pages = {43--48},
  keywords = {Accuracy, Artificial intelligence, Assessment, Context modeling, Databases, Education, Exam evaluation, GEN Artificial Intelligence, Hybrid power systems, Large Language Model, Large language models, Planning, Reliability, Retrieval Augmented Generation, Student Management System (SMS), Vectors, source: IEEE},
  abstract = {The world of Artificial Intelligence is rapidly evolving after the introduction of GEN AI. Artificial Intelligence is being adopted in many fields and to automate complex works which frees up humans to something better. In the field of education, Artificial Intelligence is used in various ways, to tailor the content to meet the needs of individual students, one to one tutoring experience, as teaching assistant, in planning, content development. The scope AI and its subfield can be leveraged further, and it is possible to use it for exam assessment, use the data from Student Management System to generate some meaningful insight on Students behavior, discipline, attendance etc. Natural Language Processing, a subset of Artificial Intelligence can be combined with other AI subsets like Large Language Model to facilitate the Teacher, Parents and Students interaction, Parents can use chatBots to receive candid feedback of their kid’s performance, to generate relative performance improvement compared to previous cycle, suggestion for improvement. All it takes is to connect the Artificial Intelligence subsets with some reference system like Students database. This research analysis is to evaluate student’s performance and provide feedback factoring based on four main areas like exam answer sheet, practicals, attendance and assignments. This research provides technical implementation and guidance to implement the solution using the Retrieval Augmented Generation and Large Language Model, and an architectural overview of combining data from various input modes like student information from Student Management System, text book, digital answer sheet and embedding them in the Vector database.},
  month = {oct},
}

@inproceedings{sharma_dynamic_2024,
  title = {Dynamic {Retriever},
  author = {Sharma, Parth and Mohammad, Aman Kaif},
  year = {2024},
  doi = {10.1109/ICECER62944.2024.10920369},
  url = {https://doi.org/10.1109/icecer62944.2024.10920369},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Accuracy, Computer architecture, Context modeling, Costs, Engines, GenAI, LLM, Measurement, Natural language processing, Q-learning, RAG, Retrieval augmented generation, RL, Training, source: IEEE},
  abstract = {This paper investigates a novel use of Reinforcement Learning (RL) to dynamically choose the best retriever in Retrieval-Augmented Generation (RAG) systems with the goal of improving the performance of natural language processing tasks. RAG systems combine retrieval techniques with pre-trained language models to produce responses that are accurate within their context, but a static retriever selection system results in inefficiencies, in a dynamically changing environment where user preferences and the document corpus evolves. We attempt to solve this problem by proposing an RL-based solution to enable dynamic retriever selection based on document context and user feedback. The reinforcement learning agent is able to adjust to user preferences and a changing document corpus by utilizing Q-Learning. The methodology covers issue formulation, agent architecture and training procedures. Our experiments validate our RL-based approach's performance characterized by metrics like user satisfaction and response accuracy. The discussion highlights the strengths and limitations of the Reinforcement Learning approach and suggests future research directions. This experiment underlines the potential of adaptive mechanisms in NLP, showcasing RL's capability to revolutionize RAG applications by creating more responsive and user-tailored NLP systems. (Abstract)},
  month = {dec},
}

@inproceedings{liu_transrag_2025,
  title = {{TransRAG},
  author = {Liu, Yiqian and Yin, Zexia and Wang, Jing and Li, Kang},
  year = {2025},
  doi = {10.1109/CCDC65474.2025.11090835},
  url = {https://doi.org/10.1109/ccdc65474.2025.11090835},
  booktitle = {2025 37th {Chinese},
  pages = {66--71},
  note = {ISSN: 1948-9447},
  keywords = {Accuracy, Hybrid power systems, Indexing, Large Language Model, Large language models, Public transportation, Public Transportation, Question answering (information retrieval), Real-time systems, Retrieval augmented generation, Retrieval Augmented Generation, Schedules, Videos, source: IEEE},
  abstract = {Large Language Models (LLMs) has significant potential for applications in urban transportation management. However, it faces substantial challenges due to the lack of comprehensive and accurate domain-specific data and knowledge. In this paper, we propose a hybrid Retrieval Augmented Generation framework (TransRAG) specifically designed for the public transportation domain. The framework integrates embeddingbased retrieval with tree and graph structures to enhance both document indexing and question-answering performance. Experimental results on a public transportation dataset demonstrate that TransRAG outperforms existing RAG-based models in retrieval accuracy and user satisfaction, achieving an average improvement of 38.2\%. These findings indicate that TransRAG offers a promising solution for integrating LLMs into urban transportation management systems.},
  month = {may},
}

@inproceedings{yang_ecosmartguide_2024,
  title = {{EcoSmartGuide},
  author = {Yang, Jiun-Yi and Chi, Ren-he and Wu, Chia-Chun and Chen, Li-Ju and Lin, Wei-Ming and Hu, Hsiang-Wei and Cheng, Hui-Ru},
  year = {2024},
  doi = {10.1109/ECBIOS61468.2024.10885500},
  url = {https://doi.org/10.1109/ecbios61468.2024.10885500},
  booktitle = {2024 {IEEE},
  pages = {343--347},
  keywords = {Accuracy, AI, Artificial intelligence, Decision making, ESG reporting, Industries, intelligence research, LLM, Measurement, Medical services, RAG, Stakeholders, Sustainable development, Technological innovation, Transforms, source: IEEE},
  abstract = {EcoSmartGuide leverages the Language Learning Model (LLM) and Retrieval-Augmented Generation-based (RAG) architectures to streamline Environmental, Social, and Governance (ESG) reporting, automating data aggregation and analysis. It shows a citation accuracy of 95\% in reflecting ESG metrics and a coverage rate of 76.23\% in the representation of ESG performance. Stakeholder feedback indicates high user satisfaction and improved decision-making processes. In a case study, EcoSmartGuide's effectiveness is demonstrated in identifying overlooked risks and enhancing decisions. The platform showcases AI's transformative potential in sustainability reporting and decision-making efficiently and accurately. Future research is needed to prioritize longitudinal studies, technological advancements, and diverse industry applications to ensure broader applicability and relevance.},
  month = {jun},
}

@inproceedings{hung_dark_2024,
  title = {Dark {Watchdog},
  author = {Hung, Shing-Li and Chen, Chung-Kuan and Furumoto, Keisuke and Takahashi, Takeshi and Sun, Hung-Min},
  year = {2024},
  doi = {10.1109/ACSACW65225.2024.00010},
  url = {https://doi.org/10.1109/acsacw65225.2024.00010},
  booktitle = {2024 {Annual},
  pages = {11--19},
  keywords = {BERT, Classification, Computer crime, Conferences, cyber security, Dark web, Dark Web, Data privacy, Large language model, Large language models, LLM, Monitoring, Organizations, Privacy, RAG, Real-time systems, Retrieval augmented generation, Retrieval-augmented generation, Tor, source: IEEE},
  abstract = {Personal data breaches have become increasingly common, making the dark web a key marketplace for trading stolen information, often without the immediate awareness of affected organizations. To address this challenge, we introduce Dark Watchdog, a novel system that actively monitors dark web forums and employs a specially fine-tuned BERT classification model to categorize transaction posts into five distinct types of breaches with high accuracy. Dark Watchdog uniquely integrates retrieval-augmented generation (RAG) to efficiently vectorize and analyze dark web data, allowing cyber security analysts to access the latest intelligence on data leaks while preserving privacy by minimizing data exposure to large language models (LLMs). This approach not only improves detection precision but also optimizes computational resources by reducing token usage. Dark Watchdog offers an innovative and practical solution for real-time dark web monitoring, enabling timely insights into ongoing data leak incidents and enhancing the overall effectiveness of cyber security efforts.},
  month = {dec},
}

@inproceedings{cai_2nd_2024,
  title = {The 2nd {Futuredial},
  author = {Cai, Yucheng and Chen, Si and Wu, Yuxuan and Huang, Yi and Feng, Junlan and Ou, Zhijian},
  year = {2024},
  doi = {10.1109/SLT61566.2024.10832299},
  url = {https://doi.org/10.1109/slt61566.2024.10832299},
  booktitle = {2024 {IEEE},
  pages = {1091--1098},
  keywords = {Accuracy, Annotations, Atmospheric measurements, Conferences, Customer services, Dialog systems, Knowledge based systems, Large language models, Market research, Particle measurements, Retrieval augmented generation, source: IEEE},
  abstract = {Recently, increasing research interests have focused on retrieval augmented generation (RAG) to mitigate hallucination for large language models (LLMs). Following this trend, we launch the FutureDial-RAG challenge at SLT 2024, which aims at promoting the study of RAG for dialog systems. The challenge builds upon the MobileCS2 dataset, a real-life customer service datasets with nearly 3000 high-quality dialogs containing annotations for knowledge base query and corresponding results. Over the dataset, we define two tasks, track 1 for knowledge retrieval and track 2 for response generation, which are core research questions in dialog systems with RAG. We build baseline systems for the two tracks and design metrics to measure whether the systems can perform accurate retrieval and generate informative and coherent response. The baseline results show that it is very challenging to perform well on the two tasks, which encourages the participating teams and the community to study how to make better use of RAG for real-life dialog systems.},
  month = {dec},
}

@inproceedings{k_comparative_2025,
  title = {Comparative {Analysis},
  author = {K, Ganesh Vaidyanathan and S, Varun M and Raolji, Shauryadeepsinh Gajendrasinh and Das, Bhaskarjyoti},
  year = {2025},
  doi = {10.1109/ICCRD64588.2025.10962837},
  url = {https://doi.org/10.1109/iccrd64588.2025.10962837},
  booktitle = {2025 {IEEE},
  pages = {216--227},
  keywords = {Advanced RAG, Comparative Analysis, Computer architecture, Faces, Industries, Knowledge based systems, LLMs, Mahabharata, Measurement, Propulsion, Research and development, Retrieval augmented generation, Retrieval Augmented Generation, Routing, Semantics, source: IEEE},
  abstract = {Retrieval Augmented Generation (RAG) is a hot topic and one of the most compute-effective context-extending techniques in the LLM industry. Unfortunately, RAG also faces certain challenges, which make it vulnerable to mistakes. To overcome these challenges, recent studies have come up with numerous methods, which are collectively referred to as advanced RAG. In our study, we test out the effectiveness of individual Advanced RAG methods and compare them when used with a variety of LLMs. We perform this comparative study using the Mahabharata, the Indian Epic, as the external knowledge base. Examining how Advanced RAG techniques can be used in practical applications is this paper’s primary contribution. We then test the efficacy of the techniques using answer-independent evaluation metrics. We conclude by tabulating the performance metrics recorded and provide insights on the results obtained.},
  month = {jan},
}

@inproceedings{hidayaturrahman_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {{Hidayaturrahman},
  year = {2024},
  doi = {10.1109/ICIMCIS63449.2024.10956838},
  url = {https://doi.org/10.1109/icimcis63449.2024.10956838},
  booktitle = {2024 {International},
  pages = {608--613},
  note = {ISSN: 2837-5203},
  keywords = {Auto-Merging Retrieval, Informatics, LLM, Multimedia systems, Natural language generation, RAG, Retrieval augmented generation, Sentence Window Retrieval, Social networking (online), source: IEEE},
  abstract = {This study investigates the efficacy of Retrieval-Augmented Generation (RAG) in generating content for social media platforms. It explores two innovative techniques: sentence window retrieval and auto-merging retrieval. Our study aims to enhance the relevance and credibility of generated content in dynamic and contextually rich environments. Through our extensive trials, we have found that incorporating phrase windows greatly enhances the contextual relevance and groundedness of the generated material. Increasing the size of sentence windows has a significant impact on both elements, but it does not consistently enhance the relevance of the answers. Alternatively, when it comes to auto-merging retrieval, adjusting the size of text chunks used in the process reveals that smaller chunk sizes tend to yield more precise and well-substantiated information. In addition, using a smaller set of chunk types leads to greater context relevance. These findings suggest that there exists a trade-off between the level of detail in the acquired information and its practicality within a specific context. This work is an important addition to the broader field of natural language generation and retrieval. It offers practical insights for efficiently implementing advanced AI models on social media platforms in real-world scenarios.},
  month = {nov},
}

@inproceedings{fayyazi_advancing_2024,
  title = {Advancing {TTP},
  author = {Fayyazi, Reza and Taghdimi, Rozhina and Yang, Shanchieh Jay},
  year = {2024},
  doi = {10.1109/ACSACW65225.2024.00036},
  url = {https://doi.org/10.1109/acsacw65225.2024.00036},
  booktitle = {2024 {Annual},
  journal = {2024 Annual Computer …},
  pages = {255--261},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, Conferences, Context modeling, Cyberattack, Decoding, Large language models, LLM, MITRE ATT\&CK, RAG, Retrieval augmented generation, Surges, TTP, source: IEEE, source: Google Scholar},
  abstract = {Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT\&CK framework can be challenging for cybersecurity practitioners due to presumed expertise and complex dependencies. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. It is, however, unclear how LLMs can be used in an efficient and proper way to provide accurate responses for critical domains such as cybersecurity. This leads us to investigate how to better use two types of LLMs: small-scale encoder-only (e.g., RoBERTa) and large-scale decoder-only (e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended purposes (i.e., tactics) of a cyberattack procedure. This work studies and compares the uses of Supervised Fine-Tuning (SFT) of encoder-only LLMs vs. Retrieval Augmented Generation (RAG) for decoder-only LLMs (without fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with relevant contexts for each cyberattack procedure. Our studies show decoder-only LLMs with RAG achieves better performance than encoder-only models with SFT, particularly when directly relevant context is extracted by RAG. The decoder-only results could suffer low ‘Precision’ while achieving high ‘Recall’, indicating the hallucinations typically occur during decoding phase. Our findings further highlight a counter-intuitive observation that more generic prompts tend to yield better predictions of cyberattack tactics than those that are more specifically tailored. 1},
  month = {dec},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{azher_futuregen_2025,
  title = {{FutureGen},
  author = {Azher, Ibrahim Al and Jannat Mokarrama, Miftahul and Guo, Zhishuai and Choudhury, Sagnik Ray and Alhoori, Hamed},
  year = {2025},
  doi = {10.1109/eScience65000.2025.00087},
  url = {https://doi.org/10.1109/escience65000.2025.00087},
  booktitle = {2025 {IEEE},
  journal = {2025 IEEE …},
  pages = {427--438},
  note = {ISSN: 2325-3703},
  keywords = {Collaboration, Databases, Future Work, Generators, Large language models, LLM, LLM as a Judge, Measurement, Retrieval augmented generation, Text Generation, Vectors, source: IEEE, source: Google Scholar},
  abstract = {The Future Work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from a scientific article. To enrich the generation process with broader insights and reduce the chance of missing important research directions, we use context from related papers using RAG. We experimented with various Large Language Models (LLMs) integrated into Retrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism to enhance the quality of the generated content and introduce an LLM-as-a-judge framework for robust evaluation, assessing key aspects such as novelty, hallucination, and feasibility. Our results demonstrate that the RAG-based approach using GPT-4o mini, combined with an LLM feedback mechanism, outperforms other methods based on both qualitative and quantitative evaluations. Moreover, we conduct a human evaluation to assess the LLM as an extractor, generator, and feedback provider.},
  month = {sep},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{tamascelli_leveraging_2025,
  title = {Leveraging {Large},
  author = {Tamascelli, Nicola and Bhattacharya, Nilavra and Song, Chen and Borrison, Reuben and Gitzel, Ralf},
  year = {2025},
  doi = {10.1109/ETFA65518.2025.11205682},
  url = {https://doi.org/10.1109/etfa65518.2025.11205682},
  booktitle = {2025 {IEEE},
  pages = {1--7},
  note = {ISSN: 1946-0759},
  keywords = {Engines, Human in the loop, knowledge extraction, Large Language Model, Large language models, LLM, Maintenance, maintenance reports, maintenance rule extraction, operator support, RAG, Real-time systems, Reproducibility of results, Retrieval augmented generation, Retrieval Augmented Generation, Robustness, Stochastic processes, Switches, source: IEEE},
  abstract = {Maintenance of medium voltage switchgears is a complex task that requires extensive expertise. However, a substantial portion of this knowledge is often difficult for the next generation of field technicians to access and apply effectively. For instance, maintenance reports – which document system issues and corrective actions – represent a valuable knowledge source yet frequently underutilized due to their unstructured and sparse nature. In this context, recent advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques have been explored to analyze historical maintenance reports and provide real-time guidance to field technicians. However, chatbot-based interactions remain constrained by the stochastic nature of responses, challenges related to domain and industry specificity, and risks of hallucination. This study introduces a novel approach that leverages LLMs to systematically extract structured, machine-readable rules – i.e., sequences of actions used to resolve specific issues – directly from maintenance reports. A human-in-the-loop approach is proposed, where a human expert validates each extracted rule, mitigating inaccuracies and enhancing quality control. These validated rules are eventually stored in a dedicated rule corpus and later deployed in a deterministic rule engine, ensuring reliability and consistency. An industrial case study evaluates the effectiveness of this approach by comparing LLM-generated rules with those manually curated by domain experts. By shifting from conventional chatbot-like interaction to a more human-centric structured rule extraction, our methodology enhances reproducibility, reliability, and accessibility in industrial maintenance knowledge management. This approach addresses hallucinations and increases robustness of LLM applications in industrial AI settings.},
  month = {sep},
}

@article{al-qatf_rag4ds_2025,
  title = {{RAG4DS},
  author = {Al-Qatf, Majjed and Haque, Rafiqul and Alsamhi, Saeed Hamood and Buosi, Samuele and Razzaq, Muhammad Asif and Timilsina, Mohan and Hawbani, Ammar and Curry, Edward},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3545387},
  url = {https://doi.org/10.1109/access.2025.3545387},
  journal = {IEEE Access},
  volume = {13},
  pages = {39510--39522},
  keywords = {Artificial intelligence, Data models, Data privacy, data spaces, Distributed databases, Foundation models, foundational models, Frequency modulation, lifecycle, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Soft sensors, Sustainable development, Technological innovation, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) has gained significant attention from many researchers as an effective solution to address the hallucination issue of Foundational Models (FMs), particularly Large Language Models (LLMs). Although the RAG framework is considered a successful approach for enhancing LLMs by providing a suitable retrieval mechanism to obtain appropriate external knowledge, it still has limitations in acquiring high-quality knowledge from diverse data sources. The complementary integration of RAG and data spaces is proposed to exploit RAG’s capabilities within data spaces. Data spaces provide RAG with the ability to obtain diverse and high quality data sources from several data providers under secure data-sharing mechanisms and direct data exchange negotiations. At the same time, RAG enhances the support services of data spaces. In this paper, we present a high-level architecture for RAG data space models (RAG-DSMs) with a unified lifecycle for RAG and data spaces, highlight the possible challenges of the proposed integration while presenting potential opportunities. Moreover, we present two use cases for leveraging RAG-DSMs in the mobility and health domains.},
  issn = {2169-3536},
}

@article{wahidur_legal_2025,
  title = {Legal {Query},
  author = {Wahidur, Rahman S. M. and Kim, Sumin and Choi, Haeung and Bhatti, David S. and Lee, Heung-No},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3542125},
  url = {https://doi.org/10.1109/access.2025.3542125},
  journal = {IEEE Access},
  volume = {13},
  pages = {36978--36994},
  keywords = {Accuracy, Adaptation models, Hybrid power systems, information retrieval, Law, legal query, LLM agent, Mathematical models, Reliability, Retrieval augmented generation, Retrieval-augmented generation, Semantics, Training, Tuning, source: IEEE},
  abstract = {Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence (AI) for various core tasks. However, these technologies remain in their early stages and face challenges such as understanding complex legal reasoning, managing biased data, ensuring transparency, and avoiding misleading responses, commonly referred to as hallucinations. To address these limitations, this paper introduces Legal Query RAG (LQ-RAG), a novel Retrieval-Augmented Generation framework with a recursive feedback mechanism specifically designed to overcome the critical shortcomings of standard RAG implementations in legal applications. The proposed framework incorporates four key components: a custom evaluation agent, a specialized response generation model, a prompt engineering agent, and a fine-tuned legal embedding LLM. Together, these components effectively minimize hallucinations, improve domain-specific accuracy, and deliver precise, high-quality responses for complex queries. Experimental results demonstrate that the fine-tuned embedding LLM achieves a 13\% improvement in Hit Rate and a 15\% improvement in Mean Reciprocal Rank (MRR). Comparisons with general LLMs reveal a 24\% performance gain when using the Hybrid Fine-Tuned Generative LLM (HFM), the specialized response generation model integrated into the LQ-RAG framework. Furthermore, LQ-RAG achieves a 23\% improvement in relevance score over naive configurations and a 14\% improvement over RAG with Fine-Tuned LLMs (FTM). These findings underscore the potential of domain-specific fine-tuned LLMs, combined with advanced RAG modules and feedback mechanisms, to significantly enhance the reliability and performance of AI in legal practice. The reliance of this study on a proprietary model as the evaluation agent, combined with the lack of feedback from human experts, highlights the need for improvement. Future efforts should focus on developing a specialized legal evaluation agent and enhancing its performance by incorporating feedback from domain experts.},
  issn = {2169-3536},
}

@inproceedings{tevissen_towards_2024,
  title = {Towards {Retrieval},
  author = {Tevissen, Yannis and Guetari, Khalil and Petitpont, Frédéric},
  year = {2024},
  doi = {10.1109/HSI61632.2024.10613524},
  url = {https://doi.org/10.1109/hsi61632.2024.10613524},
  booktitle = {2024 16th {International},
  pages = {1--4},
  note = {ISSN: 2158-2254},
  keywords = {Benchmark testing, Large language models, Manuals, Metadata, Multimedia systems, retrieval augmented generation, Streaming media, video library question answering, video retrieval, Visualization, source: IEEE},
  abstract = {Video content creators need efficient tools to repurpose content, a task that often requires complex manual or automated searches. Crafting a new video from large video libraries remains a challenge. In this paper we introduce the task of Video Library Question Answering (VLQA) through an interoperable architecture that applies Retrieval Augmented Generation (RAG) to video libraries. We propose a system that uses large language models (LLMs) to generate search queries, retrieving relevant video moments indexed by speech and visual metadata. An answer generation module then integrates user queries with this metadata to produce responses with specific video timestamps. This approach shows promise in multimedia content retrieval, and AI-assisted video content creation.},
  month = {jul},
}

@inproceedings{ahmed_codeqa_2024,
  title = {{CodeQA},
  author = {Ahmed, Mohamed and Dorrah, Mostafa and Ashraf, Ahmed and Adel, Yousef and Elatrozy, Abdelrahman and Mohamed, Bahaa Eldin and Gomaa, Wael},
  year = {2024},
  doi = {10.1109/NILES63360.2024.10753267},
  url = {https://doi.org/10.1109/niles63360.2024.10753267},
  booktitle = {2024 6th {Novel},
  journal = {2024 6th Novel …},
  pages = {494--499},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: IEEE, source: Google Scholar},
  abstract = {In light of this complex information for programming environments, this paper explores how effectiveness in Large Language Models and concepts from Retrieval-Augmented Generation can be used to augment reduced hallucinated question-answering systems for programming environments. The present scenario of transformer-based models, though a big player in natural language processing, doesn't seem to have adaptability and context sensitivity, which is, in fact, a very important feature for a specialist domain. Our study, by integrating a sophisticated LLM model within the RAG framework, further improves the precision, effectiveness of retrieval, and sensitivity of the context of responses. The model based on LLM improves over this transformer-based model by scoring higher in accuracy and context awareness, supported by its pretraining on a massive corpus and dynamic document retrieval functionality. This result underlines the possibility of great improvement in QAS performance, handling very complex and specialized queries, through the integration of LLMs with the RAG systems, and thereby helps in indicating promising areas for further research in optimizing these methodologies across different domains.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{irtiza_llm-sentry_2024,
  title = {{LLM},
  author = {Irtiza, Saquib and Akbar, Khandakar Ashrafi and Yasmeen, Arowa and Khan, Latifur and Daescu, Ovidiu and Thuraisingham, Bhavani},
  year = {2024},
  doi = {10.1109/TPS-ISA62245.2024.00036},
  url = {https://doi.org/10.1109/tps-isa62245.2024.00036},
  booktitle = {2024 {IEEE},
  pages = {245--254},
  keywords = {Defense, Gemini, GPT, Human in the loop, Human-In-The-Loop, Intelligent systems, Jailbreaking, Knowledge based systems, Large Language Model, Large language models, Mistral, Model Agnostic, Multilingual, Privacy, Resilience, Retrieval augmented generation, Retrieval Augmented Generation, Robustness, Security, Zero-Shot Classification, source: IEEE},
  abstract = {LLM-Sentry represents a novel black-box defense strategy to safeguard Large Language Models (LLMs) against jailbreaking attacks. A key advantage of our approach is its model-agnostic nature, as it does not rely on specific information about the model’s architecture or parameters, thereby enabling its application to any commercial or open-source language models. Additionally, LLM-Sentry does not require retraining when new jailbreak attacks are discovered; a simple update to the knowledge base equips LLM-Sentry to defend against new threats. The widespread adoption of LLMs is attributed to their high-quality responses and user-friendly nature. However, these models are susceptible to manipulation by malicious actors exploiting vulnerabilities to generate harmful or compromised content. Recent research has identified various jailbreaking methods that exploit vulnerabilities in LLM security measures. Given the increasing complexity of jailbreaking techniques and the ambiguous nature of LLM safeguards, it is imperative to develop unique defense strategies that can seamlessly integrate into existing security frameworks and make them robust.Our work comprehensively analyzes various commercial LLMs to assess their vulnerability to sophisticated, multilingual jailbreaking prompts. We propose a defensive approach that combines a Zero-shot language classifier with the Retrieval Augmented Generation (RAG) technique to screen and filter potentially harmful input prompts before they are processed by the language model for response generation. We adopt a human-in-the-loop approach to gather a dataset comprising harmful and safe prompts, which serves as a knowledge base for the RAG retriever module to extract relevant context. Our investigation includes successful jailbreaking attempts on prominent commercial LLMs like Gemini, Mistral 7B, and ChatGPT, wherein we successfully bypass existing security measures and elicit compromised responses. We conduct a thorough evaluation of our approach against various baseline methods to validate its resilience and superiority against such attacks empirically. Our approach achieves an attack detection accuracy of 97\%, surpassing all other methods in our comparative analysis.},
  month = {oct},
}

@inproceedings{varshney_ragnet_2025,
  title = {{RAGNet},
  author = {Varshney, Nihir and Mantoo, Saksham and Wadhwa, Madhav and Aatif, Mohammad and Bhardwaj, Shweta},
  year = {2025},
  doi = {10.1109/ICETM63734.2025.11051911},
  url = {https://doi.org/10.1109/icetm63734.2025.11051911},
  booktitle = {2025 {International},
  pages = {1--7},
  keywords = {Accuracy, Advanced RAG, Enhanced Modular RAG, Ensemble, Few-Shot Prompting, Large language models, Large Language Models (LLMs), Medical services, Modular RAG, Multi-Hop Retrieval, Naïve RAG, Optimization, Query processing, Question answering (information retrieval), Reliability, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Technological innovation, Training data, source: IEEE},
  abstract = {An Enhanced version of the RAG framework is proposed to improve the Groundedness of Medical Question Answering systems. It achieves higher retrieval effectiveness and response generation due to advanced techniques, such as few-shot prompting, multi-hop retrieval, and query optimization. In this study, the comparison models used are the Naive and Advanced RAG models for large language models on medQA dataset. Application in the field of practice would highlight this topic in medical applications where precision matters and the Enhanced Modular RAG provided one of the most reliable answers to challenging medical questions.},
  month = {may},
}

@inproceedings{li_generating_2025,
  title = {Generating {Is},
  author = {Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10889013},
  url = {https://doi.org/10.1109/icassp49660.2025.10889013},
  booktitle = {{ICASSP},
  pages = {1--5},
  note = {ISSN: 2379-190X},
  keywords = {Data privacy, Databases, Focusing, Large language models, Large Language Models, Membership Inference Attacks, Privacy, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Signal processing, Solids, Speech processing, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG’s external database, with the aim of determining whether a given sample is part of the RAG’s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S2MIA, a Membership Inference Attack that utilizes the Semantic Similarity between a given sample and the content generated by the RAG system. With our proposed S2MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experimental results demonstrate that S2MIA outperforms five existing MIAs, even when the system is protected by three representative defenses.},
  month = {apr},
}

@inproceedings{sanjani_performance_2025,
  title = {Performance {Analysis},
  author = {Sanjani, Lukman Arif and Sarno, Riyanarto and Sungkono, Kelly Rossa and Haryono, Agus Tri and Septiyanto, Abdullah Faqih and Sunaryono, Dwi},
  year = {2025},
  doi = {10.1109/ICoCSETI63724.2025.11018908},
  url = {https://doi.org/10.1109/icocseti63724.2025.11018908},
  booktitle = {2025 {International},
  journal = {2025 International …},
  pages = {152--157},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Adaptation models, Call Center, Chatbot, Chatbots, Companies, Fine-tuning, Large Language Model, Large language models, Medical services, Meteors, Optimization, Performance analysis, Retrieval augmented generation, Retrieval-Augmented Generation, Technological innovation, source: IEEE, source: Google Scholar},
  abstract = {This research investigates the comparative performance of various Large Language Models (LLMs) to determine the most suitable model for XYZ Company, a call center organization aiming to integrate a chatbot solution. Chatbots significantly improve call center operations by streamlining interactions, reducing the workload on human agents, and enhancing the overall customer experience. Unlike many chatbot datasets that are document-based or contain contextual fields, the dataset of XYZ Company is composed of question-answer pairs. This unique data structure requires a customized approach to model selection. Experiments were conducted on multiple models, with RAG using Llama3 emerging as the top performer. Results indicate a BLEU score of 18\%, ROUGE-1 of 46\%, ROUGE-2 of 27\%, ROUGE-L of 36\%, BERTScore Precision of 89\%, Recall of 91\%, F1 of 90\%, and METEOR score of 41\%. These metrics underscore RAG with Llama3 to deliver accurate, efficient responses, supporting the goal of XYZ Company to implement an effective, real-world chatbot solution.},
  month = {jan},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{sun_scrag_2025,
  title = {{SCRAG},
  author = {Sun, Dachun and Lyu, You and Li, Jinning and Chen, Yizhuo and Wang, Tianshi and Kimura, Tomoyoshi and Abdelzaher, Tarek},
  year = {2025},
  doi = {10.1109/SMARTCOMP65954.2025.00062},
  url = {https://doi.org/10.1109/smartcomp65954.2025.00062},
  booktitle = {2025 {IEEE},
  pages = {170--177},
  note = {ISSN: 2693-8340},
  keywords = {Accuracy, Computational modeling, Forecasting, Ideological Embedding, Large Language Model, Large language models, Retrieval augmented generation, Retrieval-Augmented Generation, Robustness, Semantics, Social computing, Social Computing, Social Media Response Forecasting, Social networking (online), Social Networks, Training data, source: IEEE},
  abstract = {This paper introduces SCRAG, a prediction frame-work inspired by social computing, designed to forecast community responses to real or hypothetical social media posts. SCRAG can be used by public relations specialists (e.g., to craft messaging in ways that avoid unintended misinterpretations) or public figures and influencers (e.g., to anticipate social responses), among other applications related to public sentiment prediction, crisis management, and social what-if analysis. While large language models (LLMs) have achieved remarkable success in generating coherent and contextually rich text, their reliance on static training data and susceptibility to hallucinations limit their effectiveness at response forecasting in dynamic social media environments. SCRAG overcomes these challenges by integrating LLMs with a Retrieval-Augmented Generation (RAG) technique rooted in social computing. Specifically, our framework retrieves (i) historical responses from the target community to capture their ideological, semantic, and emotional makeup, and (ii) external knowledge from sources such as news articles to inject time-sensitive context. This information is then jointly used to forecast the responses of the target community to new posts or narratives. Extensive experiments across six scenarios on the X platform (formerly Twitter), tested with various embedding models and LLMs, demonstrate over 10\% improvements on average in key evaluation metrics. A concrete example further shows its effectiveness in capturing diverse ideologies and nuances. Our work provides a social computing tool for applications where accurate and concrete insights into community responses are crucial.},
  month = {jun},
}

@inproceedings{jeon_rag-based_2024,
  title = {{RAG},
  author = {Jeon, Jong-Hee and Koo, Jahoon and Kim, Young-Gab},
  year = {2024},
  doi = {10.1109/TrustCom63139.2024.00098},
  url = {https://doi.org/10.1109/trustcom63139.2024.00098},
  booktitle = {2024 {IEEE},
  pages = {608--615},
  note = {ISSN: 2324-9013},
  keywords = {Computational modeling, cyber threat tracing, graph modeling, Information retrieval, Interoperability, label property graph (LPG), Large language models, Privacy, Ransomware, Resource description framework, resource description framework (RDF), Retrieval augmented generation, retrieval-augmented generation (RAG), Standardization, Tracking, source: IEEE},
  abstract = {Cyber-attacks have become increasingly complex and sophisticated, occurring frequently through various paths. Numerous attack techniques, such as ransomware, advanced persistent threat (APT) attacks, and viruses facilitated by generative artificial intelligence (GAI), are threatening systems. Attackers often remain hidden for extended periods, using networks to infiltrate and spread within internal systems, aiming to steal sensitive information. These stealthy lateral movements are difficult to detect and defend against, necessitating a graph-based approach to effectively track and analyze threats. In this paper, we propose a retrieval-augmented generation (RAG) based threat behavior tracing method that derives more meaningful results through the information retrieval functions in the large language model (LLM). We present a suitable graph model for threat tracing by comparing label property graph (LPG) and resource description framework (RDF) as graph methods for knowledge extraction in RAG.},
  month = {dec},
}

@inproceedings{ananthanarayanan_generating_2025,
  title = {Generating {Medical},
  author = {Ananthanarayanan, Aniruth},
  year = {2025},
  doi = {10.1109/ISEC64801.2025.11147251},
  url = {https://doi.org/10.1109/isec64801.2025.11147251},
  booktitle = {2025 {IEEE},
  pages = {1--5},
  note = {ISSN: 2473-7623},
  keywords = {Ethics, Image color analysis, large language models, Large language models, Medical diagnosis, Medical diagnostic imaging, Medical education, Refining, Reinforcement learning, reinforcement learning from AI feedback, Retrieval augmented generation, retrieval-augmented generation, Training, Transforms, source: IEEE},
  abstract = {Sample medical scenarios play a crucial role in training healthcare professionals by providing structured cases to develop diagnostic reasoning and clinical decision-making skills. However, access to diverse and inclusive sample diagnostic cases remains challenging due to the limited representation of specific conditions and populations in medical education materials, and existing cases are often not equitable due to a lack of representation of minority groups. In this paper, we present a new dataset of medical diagnostic scenarios generated using a combination of reinforcement learning from artificial intelligence feedback and retrieval augment generation techniques. Despite the dataset’s limited size, it offers a unique resource for advancing medical education, particularly in regions with scarce training materials while also emphasizing inclusivity by incorporating a higher representation of people of color and women. Then, we discuss the data generation process, the dataset structure, and potential applications in medical training programs. This work aims to contribute to the development of accessible, high-quality, and inclusive educational tools in the medical field.},
  month = {mar},
}

@inproceedings{jiang_exploring_2025,
  title = {Exploring {Large},
  author = {Jiang, Guangxin and Ma, Chao},
  year = {2025},
  doi = {10.1109/ICAACE65325.2025.11018984},
  url = {https://doi.org/10.1109/icaace65325.2025.11018984},
  booktitle = {2025 8th {International},
  pages = {2670--2674},
  keywords = {Adaptation models, Codes, Error correction, Error Correction, Faces, Focusing, Large language models, LLMs, LoRA, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Structured Query Language, Syntactics, Text-to-SQL, source: IEEE},
  abstract = {The advent of Large Language Models (LLMs) has significantly advanced Text-to-SQL tasks in recent years. However, these models still face considerable challenges when applied in practice, such as hallucinations inherent in LLMs and the rigid syntactic structure of SQL, which often lead to the generation of erroneous SQL queries. Existing approaches typically rely on older models or simplistic LLM prompts for direct error correction, which is a relatively basic strategy. In this paper, we explore the performance of large models in the Text-to-SQL error correction task, focusing on the combination of LoRA fine-tuning and retrieval-augmented methods. Experimental results demonstrate that our approach achieves state-of-the-art (SOTA) performance on the Splash benchmark, outperforming existing methods and providing more accurate error corrections for SQL generation. Notably, despite using a 7B-parameter model, our method surpasses the performance of much larger 72B-scale models, highlighting the efficiency and effectiveness of our approach.},
  month = {mar},
}

@article{chondamrongkul_addressing_2025,
  title = {Addressing {Technical},
  author = {Chondamrongkul, Nacha and Hristov, Georgi and Temdee, Punnarumol},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3531380},
  url = {https://doi.org/10.1109/access.2025.3531380},
  journal = {IEEE Access},
  volume = {13},
  pages = {12846--12858},
  keywords = {Accuracy, Artificial intelligence, Cognition, Decision making, Education, educational application, generative AI, large language model, LLM, Reliability, Retrieval augmented generation, Scalability, Software systems, Testing, Vectors, source: IEEE},
  abstract = {The integration of large language models (LLMs) into educational systems poses significant challenges across several key attributes, including integration, explainability, testability, and scalability. These challenges arise from the complexity of coordinating system components, difficulty interpreting LLM decision-making processes, and the need for reliable, consistent model outputs in varied educational scenarios. Additionally, ensuring scalability requires robust autoscaling mechanisms and suitable architecture design to handle fluctuating workloads. This paper tackles these challenges by proposing tactics to improve system integration, enhance explainability through metadata and an algorithm process, ensure response consistency via regression testing, and facilitate efficient autoscaling through an event-driven microservice architecture. The evaluation results highlight the effectiveness of these tactics, confirming both functional consistency and robust system performance under varying loads.},
  issn = {2169-3536},
}

@inproceedings{shi_improving_2025,
  title = {Improving {LLM},
  author = {Shi, Luyao and Kazda, Michael and Schmitter, Charles and Gupta, Hemlata},
  year = {2025},
  doi = {10.1109/ICLAD65226.2025.00022},
  url = {https://doi.org/10.1109/iclad65226.2025.00022},
  booktitle = {2025 {IEEE},
  pages = {9--15},
  keywords = {Access control, Design automation, EDA, fine-tuning, Large language models, LLM, Personnel, Productivity, RAFT, RAG, Retrieval augmented generation, Synthetic data, source: IEEE},
  abstract = {Electronic design engineers often struggle to efficiently access relevant information for tasks like design verification and technology development. While large language models (LLMs) can enhance productivity as conversational agents, pre-trained open-source LLMs lack domain-specific knowledge for Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG) context, LLMs rely on external context but may still produce inaccurate responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but acquiring labeled question/answer (Q/A) data in EDA is difficult. To address this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our results show that RAFT with synthetic data significantly boosts LLM performance for RAG-based EDA tasks. We also investigate the impact of using real user questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data generation. Additionally, we implement secure access control to ensure sensitive information is only accessible to authorized personnel. Finally, we assess the risk of data leakage and unintended memorization during fine-tuning with synthetic data, providing practical insights.},
  month = {jun},
}

@inproceedings{abu-arqoub_design_2025,
  title = {Design and {Implementation},
  author = {Abu-Arqoub, Mohammad and Alkarim Banna, Abed and El-Khalili, Nuha and Al-Shaikh Hasan, Mohammad},
  year = {2025},
  doi = {10.1109/ICCIAA65327.2025.11013578},
  url = {https://doi.org/10.1109/icciaa65327.2025.11013578},
  booktitle = {2025 1st {International},
  pages = {1--5},
  keywords = {Business intelligence, Business Intelligence, Data systems, Data visualization, Data Visualization, Educational technology, Educational Technology, ILO System, Information retrieval, Large language models, Learning Analytics, Natural languages, RAG, Retrieval augmented generation, Time factors, Transforms, source: IEEE},
  abstract = {This article presents a comprehensive study on designing and implementing a dashboard for enhanced data visualization and query support at the University of Petra. The system leverages retrieval-augmented generation (RAG) and large language models (LLMs) to support diverse document types, including curricula, course descriptions, and program outcomes, alongside intended learning outcome (ILO)-related files. Our implementation demonstrates significant improvements in data accessibility and query response times while maintaining high accuracy in information retrieval and visualization. Through extensive evaluation, we show that this innovative approach transforms data management processes in higher education by enabling natural language interactions with educational data systems, building upon established business intelligence frameworks while introducing advanced AI capabilities.},
  month = {apr},
}

@inproceedings{shrimali_large_2025,
  title = {Large {Language},
  author = {Shrimali, Samyak},
  year = {2025},
  doi = {10.1109/HPSC66065.2025.00012},
  url = {https://doi.org/10.1109/hpsc66065.2025.00012},
  booktitle = {2025 {IEEE},
  pages = {7--12},
  keywords = {Accuracy, Data models, Financial Analytics, Large language models, Large Language Models (LLMs), Manuals, Market research, Numerical models, Pipelines, Prompt engineering, Prompt Engineering, Retrieval-Augmented Generation, Stock Time-Series Analysis, Time series analysis, Time Series Data Retrieval, Time-Series Data Summarization, Transforms, source: IEEE},
  abstract = {Financial time-series data summarization is a critical task for analysts and decision-makers; yet current approaches rely on labor-intensive manual interpretation or bespoke reporting scripts. In this research, a novel multi-tier retrieval pipeline that leverages large language models (LLMs) to automatically generate concise and accurate summaries of financial data is proposed. By integrating traditional templated methods with retrieval-augmented prompts (using domain-specific examples to guide the LLM) the challenge of balancing linguistic fluency with numerical precision is addressed. The pipeline is evaluated on a knowledge base constructed from daily OHLCV data of 20 large-cap companies, with additional testing on a mid-cap ticker not present in the original dataset (COF). Over 15 independent runs were conducted; these experimental results demonstrate that while direct summarization captures general trends, this templated and refined multi-tier retrieval pipeline ensures factual correctness and achieves the best overall performance in terms of ROUGE, BERTScore, and numeric fidelity. This pipeline has the potential to transform time-series analysis by reducing manual workload and improving the reliability of automated financial data summarization.},
  month = {may},
}

@inproceedings{craig_whats_2024,
  title = {What’s the data say? {An},
  author = {Craig, Douglas B. and Drăghici, Sorin},
  year = {2024},
  doi = {10.1109/BIBM62325.2024.10821725},
  url = {https://doi.org/10.1109/bibm62325.2024.10821725},
  booktitle = {2024 {IEEE},
  pages = {1457--1462},
  note = {ISSN: 2156-1133},
  keywords = {Accuracy, Biological system modeling, differential gene analysis, large language models, pathway enrichment analysis, Phenotypes, Planning, Question answering (information retrieval), Reliability, Retrieval augmented generation, retrieval-augmented generation, scientific experiment analysis, Semantics, Tagging, Training, source: IEEE, source: Scopus},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, issues such as hallucinations, implicit extrapolations, and inappropriate interpretations currently limit their wide adoption in scientific research. Retrieval-augmented Generation (RAG) has emerged as a promising technique to enhance the accuracy and reliability of LLM outputs. RAG integrates external information with LLM-generated content, ensuring more dependable results and enabling the inclusion of proprietary or domain-specific knowledge unavailable during the initial LLM training. In this work, we present an extensible LLM RAG architecture designed to answer complex scientific questions that can reference user-provided experimental data. The system uses domain-specific knowledge (RAGdom) to respond to general inquiries, supporting answers with citations from authoritative sources. These responses can be recursively integrated to provide user directed context for experiments (RAGexp), allowing subsequent questions to utilize both general and experiment-specific information. Our system can be easily extended with user-defined domain-specific modules for functionally interrogating quantitative experimental results (RAGfun). We demonstrate its application in analyzing, summarizing, interpreting, and assisting with synthesizing results from a gene expression comparison of two phenotypes. The RAGdom component encompasses biological pathways, genes, and disease-related knowledge. The RAGexp component is derived from user queries of differentially expressed gene (DEG) and pathway enrichment analysis (PEA) results. Additionally, RAGfun modules are designed to interface with DEG and PEA data, enabling quantitative experimental results to be integrated seamlessly into user questions. Unlike traditional LLM outputs, our system’s results are free of hallucinations and include paragraph-level citations to the supporting literature. We compared the quality of our system’s output with those generated by LLM-only models such as GPT-3, GPT-4, Llama2, PaLM2, and BioGPT, and found our approach to be superior.},
  month = {dec},
  annote = {Cited by: 1},
}

@inproceedings{yi_maem_2025,
  title = {{MAEM},
  author = {Yi, Ningyuan and Liu, Chen and Wang, Yue and Yu, Jianjun},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10889603},
  url = {https://doi.org/10.1109/icassp49660.2025.10889603},
  booktitle = {{ICASSP},
  pages = {1--5},
  note = {ISSN: 2379-190X},
  keywords = {Eigen Decomposition, Encoding, Feature extraction, Information filters, Logic gates, Matrix decomposition, Multi-Aspect Extraction, Retrieval augmented generation, Retrieval Augmented Generation, Signal processing, Speech processing, Text Embedding, Training, Vectors, source: IEEE},
  abstract = {Retrieval Augmented Generation can effectively reduce hallucinations in LLMs during question-answering, with embedding models directly influencing its performance. While current embedding models improve encoding through large-scale training, they often overlook the potential or explicit multi-aspect information within the text. To address this, we propose the Multi-Aspect Extraction Model (MAEM), an eigen decomposition-based approach that extracts and integrates text aspects into a unified vector for enhanced retrieval, and introduce a regularization loss function to assist in training. We utilized LLMs to create the Policy-Corpus dataset and validated the model on both Policy-Corpus and FiQA. Incorporating MAEM and regularization into GTEbase improved NDCG@10 by 3.8 points on FiQA and 4.56 points on Policy-Corpus. Respectively, achieving results comparable to larger models using a smaller parameter model.},
  month = {apr},
}

@inproceedings{feng_secllm-intent-driven_2025,
  title = {{SecLLM},
  author = {Feng, Guocong and Pan, Yuan and Zhang, Chunmei and Huang, Kaitian},
  year = {2025},
  doi = {10.1109/CISAT66811.2025.11181750},
  url = {https://doi.org/10.1109/cisat66811.2025.11181750},
  booktitle = {2025 8th {International},
  pages = {225--229},
  keywords = {Collaboration, Computer security, Function Calling, Grounding, Information science, Large language models, Large Language Models, MCP, Production, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, Security Orchestration, SOAR, Vectors, source: IEEE},
  abstract = {Heterogeneous security infrastructures-network-detection, endpoint-detection, threat-intelligence and ticketing systems-remain siloed, impeding real-time collaboration and prolonging incident response. We introduce SecLLM, an intent-driven orchestration framework that couples a 32-billionparameter large-language model with (i) Model-Context-Protocol (MCP) function calling for deterministic, auditable control-plane actions and (ii) retrieval-augmented generation (RAG) for dynamic knowledge grounding. Given a natural-language goal such as "quarantine lateral-movement hosts and patch vulnerable SMB services", SecLLM parses the intent into a typed task-graph, enriches each node with live device capabilities drawn from a 50 M-artifact vector store, then safely executes the plan across NDR, EDR and SOAR devices while reflecting on intermediate results. In a 72-hour replay of the publicly available CIC-IDS2018 network trace plus host logs, SecLLM cuts mean time-to-containment by 80\% (38 → 7.4 min) and reduces false-positive escalations by 42\% relative to a production SOAR baseline. Ablations show that removing RAG slows containment by 19\%, while disabling MCP yields unsafe, non-deterministic actions. These results demonstrate that intent-aligned LLMs, when grounded via RAG and constrained by strongly typed schemas, can safely automate cross-device security operations at scale.},
  month = {jul},
}

@inproceedings{mailach_themes_2025,
  title = {Themes of {Building},
  author = {Mailach, Alina and Simon, Sebastian and Dorn, Johannes and Siegmund, Norbert},
  year = {2025},
  doi = {10.1109/CAIN66642.2025.00011},
  url = {https://doi.org/10.1109/cain66642.2025.00011},
  booktitle = {2025 {IEEE},
  pages = {18--30},
  keywords = {agents, Computer architecture, evaluation, fine-tuning, Large language models, LLM in production, Manuals, Production, prompt engineering, Prompt engineering, RAG, Retrieval augmented generation, retrieval-augmented generation, Software engineering, Systematics, Tuning, Videos},
  abstract = {Background: Large language models (LLMs) have become a paramount interest of researchers and practitioners alike, yet a comprehensive overview of key considerations for those developing LLM-based systems is lacking. This study addresses this gap by collecting and mapping the topics practitioners discuss online, offering practical insights into where priorities lie in developing LLM-based applications. Method: We collected 189 videos from 2022 to 2024 by practitioners actively developing such systems and discussing various aspects they encounter during development and deployment of LLMs in production. We analyzed the transcripts using BERTopic, then manually sorted and merged the generated topics into themes, leading to a total of 20 topics in 8 themes. Results: The most prevalent topics fall within the theme Design \& Architecture, with a strong focus on retrieval-augmented generation (RAG) systems. Other frequently discussed topics include model capabilities and enhancement techniques (e.g., finetuning, prompt engineering), infrastructure and tooling, and risks and ethical challenges. Implications: Our results highlight current discussions and challenges in deploying LLMs in production. This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications. We further highlight topics of interest for academics where further research is needed.},
  month = {apr},
}

@inproceedings{zhou_enabling_2025,
  title = {Enabling {Interactive},
  author = {Zhou, Tianyu and Wan, Yuwei and Liu, Ying and Kumar, Maneesh},
  year = {2025},
  doi = {10.1109/ICE/ITMC65658.2025.11106634},
  url = {https://doi.org/10.1109/ice/itmc65658.2025.11106634},
  booktitle = {2025 {IEEE},
  pages = {1--10},
  note = {ISSN: 2693-8855},
  keywords = {Accuracy, Chatbots, Cognitive load, Cognitive Load Reduction, Collaboration, Decision making, Fifth Industrial Revolution, Generative AI, Human-Machine Collaboration, Human-machine systems, Industry 5.0, Interactive AI, Knowledge Retrieval, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors, source: IEEE},
  abstract = {Industry 5.0 advances sustainable development through human-machine collaboration and personalised manufacturing. The increase in intelligent industrial equipment creates data scalability challenges for human workers who face difficulties in making decisions based on relevant data sources. Advanced interactive AI systems, capable of integrating diverse data sources and delivering real-time, context-aware insights, present promising solutions to the challenges of the industrial environment. This research introduces a retrieval-augmented generation (RAG)-enhanced Generative artificial intelligence (GenAI) chatbot to address these challenges. The system integrates a variety of information sources, including government reports, news websites, academic studies, and industry reports. This industry 5.0 chatbot aims to offer users extensive knowledge of the industrial sector through a Question-and-Answer interface. It provides relevant and accurate information through intuitive, context-aware interactions to reduce cognitive load for users, which improves decision-making efficiency and user experience. Through experimental evaluation, the RAG-enhanced GenAI chatbot significantly improves accuracy, relevance and user satisfaction, outperforming models like ChatGPT-4o. This system presents an innovative practical solution to tackle Industry 5.0 core issues particularly in enhancing human-machine collaboration and decision-making efficiency. This research contributes to the theoretical and practical development of RAG-enhanced AI systems, laying a foundation for future investigations of industrial AI interaction.},
  month = {jun},
}

@inproceedings{rachmat_fine-tuning_2024,
  title = {Fine-{Tuning},
  author = {Rachmat, Hary and Riza, Hammam and Abidin, Taufik Fuadi},
  year = {2024},
  doi = {10.1109/ICIC64337.2024.10956296},
  url = {https://doi.org/10.1109/icic64337.2024.10956296},
  booktitle = {2024 {Ninth},
  pages = {1--5},
  keywords = {Computational modeling, Data models, Fine-tuning, Generative AI, Informatics, Large Language Model, Large language models, Measurement, RAG, Retrieval augmented generation, Soft sensors, Testing, Training, source: IEEE},
  abstract = {USK Mistral 7B is a large language model designed to answer basic admission questions at Universitas Syiah Kuala (USK). The model was fine-tuned using the open-source model of Mistral 7B using collected data from admissions and lectures at the university. The QLoRA and RAG techniques were used to train the model and retrieve relevant information from external data sources. The results were evaluated using the ROUGE score. Responses were generated with a score of {\textgreater},
  month = {oct},
}

@inproceedings{mulakala_evaluation_2025,
  title = {Evaluation of {LLMs},
  author = {Mulakala, Benarji and Saini, Madan Lal and Bhukya, Vamsi and {Siddhartha},
  year = {2025},
  doi = {10.1109/WorldSUAS66815.2025.11199211},
  url = {https://doi.org/10.1109/worldsuas66815.2025.11199211},
  booktitle = {2025 {World},
  pages = {1--5},
  keywords = {Cognition, Faithfulness, Hallucination, Large language models, Large Language Models, Performance metrics, Pipelines, Question answering (information retrieval), Reasoning, Retrieval augmented generation, Retrieval-Augmented Generation, Semantic Answer Similarity, Semantics, Soft sensors, Synthetic aperture sonar, Test pattern generators, source: IEEE},
  abstract = {Large Language Models (LLMs) have shown remarkable capabilities in various Natural Language Processing Tasks, but are usually suffered by hallucinations which refers to generating factually incorrect content. Retrieval-Augmented Generation (RAG) has shown itself to be a promising approach for minimizing such hallucinations in by accessing external data sources. However, the impact of LLM’s reasoning capabilities such as Chain-of-thought (CoT) prompting in RAG systems is unclear. Three different LLMs, o1-mini, o3-mini, and gpt-4o, with the same RAG pipeline are evaluated on the same PubMed dataset across three different performance metrics Mean Reciprocal Rank (MRR), Faithfulness, and Semantic Answer Similarity (SAS). All three models scored a perfect MRR score, meaning good document retrieval but the reasoning models performed better than non-reasoning baseline in faithfulness and SAS. O1-mini model recorded the optimal somatic quality versus roundedness tradeoff. The outcome indicates that it is possible to improve the quality and credibility of produced answers by using the reasoning-capable LLMs for test generation in RAG systems.},
  month = {aug},
}

@inproceedings{liu_comprehensive_2024,
  title = {Comprehensive {Evaluation},
  author = {Liu, Zhenyao and Kou, Jieren and Zhang, Wuyang and Gu, Chuqiao and Fang, Xinyi and Huang, Zhenqian and Yuan, Hao and Li, Hanxia and Lu, Xiuyuan and Yin, Aobing and Liu, Chang and Zhao, Chenjun and Lin, Wenjie and Zhang, Lifeng and Wang, Zhongda and Huang, Haoyang and Duan, Xiaoman and Fang, Yajun},
  year = {2024},
  doi = {10.1109/UV63228.2024.11189137},
  url = {https://doi.org/10.1109/uv63228.2024.11189137},
  booktitle = {2024 7th {International},
  pages = {1--136},
  keywords = {AI hallucination, alignment bias, Artificial intelligence, benchmark construction, Calibration, confidence miscalibration, creative AI, Decoding, decoding strategies, dynamic feedback loops, emergent hybrid hallucinations, evaluation metrics, explainability, factuality, faithfulness, Feedback loop, hallucination detection, hallucination mitigation, hallucination taxonomy, human–AI interaction, Large language models, large language models (LLMs), logical consistency, mechanism-aware evaluation, multimodal alignment, multimodal large language models (MLLMs), Prevention and mitigation, reinforcement learning from human feedback (RLHF), responsible AI, Retrieval augmented generation, retrieval-augmented generation (RAG), robustness, safe and reliable AI, Surveys, Taxonomy, trustworthiness, Uncertainty, uncertainty calibration, Universal Village (UV) framework, source: IEEE},
  abstract = {this work, we present a comprehensive evaluation of hallucination phenomena in LLMs and multimodal systems. First, we propose a structured taxonomy encompassing factuality-based, faithfulness-based, logical-based, and emergent hybrid forms, extending to multimodal-specific risks such as cross-modal inconsistencies, visual overinterpretation, and modality dominance effects. Second, we conduct a mechanistic analysis that traces hallucinations across the full model lifecycle—data-level origins (knowledge gaps, misinformation, annotation noise), training-induced mechanisms (distributional mimicry, reward bias, alignment forgetting), and inference-time vulnerabilities (confidence miscalibration, decoding failures, prompt-induced errors). This perspective reveals how independent mechanisms interact to produce cascade effects, amplifying initial flaws into elaborate but unreliable narratives. Third, we critically assess existing detection and evaluation approaches, highlighting limitations of current benchmarks, taxonomic ambiguities, and the lack of mechanism-aware evaluation protocols. We argue that future evaluation must integrate both surface-level manifestations and their generative causes to achieve more robust measurement.Beyond evaluation, we survey mitigation strategies organized into mechanism-based, phase-based, and hybrid approaches. These range from lightweight prompt engineering and decoding constraints to resource-augmented methods such as retrieval-augmented generation and knowledge graph integration, as well as higher-level frameworks for controllability and uncertainty calibration. We analyze the trade-offs among effectiveness, scalability, interpretability, and creative freedom, emphasizing the importance of context-specific tolerances: hallucinations that are unacceptable in medicine or finance may be tolerable, or even beneficial, in creative applications.Building on these insights, we propose a novel UV-oriented framework for safe and trustworthy AI, inspired by the Universal Village vision of harmonizing human, technological, and environmental systems. In this framework, hallucination is conceptualized not only as an isolated model error but as a systemic vulnerability in information flow and decision-making loops. We design a multi-level dynamic system integrating sensing, communication, decision-making, action, and evaluation, supported by closed feedback loops and user-specific hallucination tolerance levels. This architecture enables adaptive mitigation through mechanism-informed strategies, retrieval and verification integration, and consensus mechanisms, ensuring resilience across diverse application contexts.Our contributions are threefold: (1) we present the most comprehensive taxonomy of hallucination to date, linking manifestations with mechanistic drivers; (2) we unify detection, evaluation, and mitigation strategies into a coherent survey that highlights both current progress and pressing gaps; and (3) we introduce a UV-oriented framework that reframes hallucination control as part of a broader, feedback-driven ecosystem for reliable AI. We conclude by outlining open challenges, including classification ambiguities, multimodal alignment risks, deployment-specific vulnerabilities, and the need to reconcile reliability with creativity. Addressing these challenges is essential to advancing from powerful yet fallible generative models toward AI systems that are not only safe and responsible but also aligned with human values and societal needs.},
  month = {oct},
}

@inproceedings{yang_timerag_2025,
  title = {{TimeRAG},
  author = {Yang, Silin and Wang, Dong and Zheng, Haoqi and Jin, Ruochun},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10889933},
  url = {https://doi.org/10.1109/icassp49660.2025.10889933},
  booktitle = {{ICASSP},
  pages = {1--5},
  note = {ISSN: 2379-190X},
  keywords = {Accuracy, Dynamic Time Warping (DTW), Forecasting, Knowledge based systems, Large Language Model(LLM), Predictive models, Retrieval augmented generation, Retrieval-Augmented Generation(RAG), Signal processing, Speech processing, Time measurement, Time series analysis, Time Series Forecasting, Training, source: IEEE},
  abstract = {Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97\% on average.},
  month = {apr},
}

@inproceedings{rani_enhance_2024,
  title = {To {Enhance},
  author = {Rani, Maneeha and Mishra, Bhupesh Kumar and Thakker, Dhavalkumar and Khan, Mohammad Nouman},
  year = {2024},
  doi = {10.1109/ICOSST64562.2024.10871140},
  url = {https://doi.org/10.1109/icosst64562.2024.10871140},
  booktitle = {2024 18th {International},
  journal = {2024 18th International …},
  pages = {1--6},
  note = {ISSN: 2770-8225},
  keywords = {Accuracy, Cognition, Diabetes, Graph-based Retrieval-augmented generation, Healthcare, Knowledge based systems, Knowledge graph, Knowledge graphs, Large language model, Pipelines, Real-time systems, Retrieval augmented generation, Retrieval-augmented generation, Training data, Vectors, source: IEEE, source: Google Scholar},
  abstract = {Large language models have demonstrated exceptional performance in multiple domains. However, practical deployment in the healthcare sector has distinctive challenges. These challenges include hallucination, inconsistency, explainability, reasoning, authenticity, and validity of information sources. Hallucinations in LLM often emerge due to unstructured and obsolete training data and the incompetence to upgrade the model data post-training. Retrieval-augmented generation (RAG) integration with LLM decision-making helps access real-time information from external resources. However, further improvements are needed to improve accurate response generation. A knowledge Graph is a structured data comprising nodes as entities and edges as relationships. When integrated with RAG, Knowledge Graph-based retrieval offers better contextu-ally relevant responses, traceability, and explainability of generated responses than RAG alone. This study proposes a novel knowledge graph-based RAG framework with a refined retrieval pipeline, robust chunking mechanism, and source traceability for enhanced diabetes-focused LLM. The retrieval pipeline integrates three robust retrieval strategies: keyword, graph, and vector. To ensure the authenticity of responses, a knowledge base focusing on diabetes is designed from validated sources. This verified knowledge base is preprocessed and converted to a knowledge graph to design A graph-based RAG pipeline. The empirical results demonstrate effective performance in diabetes-focused LLM, achieving a Rouge 1 score of 82.19\%.},
  month = {dec},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{zhang_cara-rag_2025,
  title = {{CARA},
  author = {Zhang, Zhonghao and Yang, Xiaochen},
  year = {2025},
  doi = {10.1109/ICIMCT.2025.00039},
  url = {https://doi.org/10.1109/icimct.2025.00039},
  booktitle = {2025 {International},
  pages = {161--164},
  keywords = {Accuracy, Civil Aviation Regulation, Industries, Large Language Models (LLMs), Real-time systems, Regulation, Reliability, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Security, System performance, Testing, Visualization, source: IEEE},
  abstract = {The civil aviation regulatory system is huge and complex, and the existing Question Answering (QA) system is difficult to balance the retrieval efficiency and generation quality. This paper introduces CARA-RAG, which improves the reliability and authoritativeness of regulation QA by combining customized corpus construction with Retrieval-Augmented Generation (RAG), and reinforced by security verification mechanism. A domain-specific evaluation dataset was built, combining automatic generation and expert verification to ensure data quality. Experimental results show that, CARA-RAG has better relevance, accuracy, Completeness and Faithfulness than the LLM-only generation and naïve RAG models. The system also integrates a lightweight WebUI to realize real-time interaction and visual display, providing a complete and scalable solution for civil aviation regulation QA system.},
  month = {may},
}

@inproceedings{lin_athena_2025,
  title = {Athena: {A},
  author = {Lin, Yadanar and Ferdous Khan, M. Fahim and Sakamura, Ken},
  year = {2025},
  doi = {10.1109/ICCT-Pacific63901.2025.11012850},
  url = {https://doi.org/10.1109/icct-pacific63901.2025.11012850},
  booktitle = {2025 1st {International},
  pages = {1--4},
  keywords = {Chatbots, Digital divide, Education, educational chatbot, Fake news, Generative AI, Generative artificial intelligence (GenAI), Hands, large language model (LLM), Large language models, programming education, Programming profession, Reliability, Retrieval augmented generation, retrieval augmented generation (RAG), source: IEEE},
  abstract = {With the rapid growth of generative artificial intelligence (GenAI), it is important to find ways to utilize them for academic advantage. GenAI tools embody immense potential in providing personalized feedback to students any time anywhere, and hence can provide a reliable helping hand to teachers who often experience burnout in large classes and are burdened with administrative tasks. While current GenAI tools like ChatGPT are helpful, they occasionally offer misinformation - a phenomenon known as hallucination, undermine critical thinking by providing direct answers to questions, and, as paid services, can further widen the digital divide. Against the backdrop of these problems, this research introduces Athena, a GenAI programming mentor based on an open-source large language model (LLM), constructed to guide programming learners to think critically and provide reliable information leveraging retrieval augmented generation. Its impact on learning outcomes was measured by feedback from programming students. Most students have given a positive response, saying that their motivation to keep learning and their confidence in their abilities have increased. These results imply that having a reliable AI mentor that can guide students at all times can have a positive impact in self-directed learning process.},
  month = {mar},
}

@inproceedings{xiong_research_2025,
  title = {Research on {Document},
  author = {Xiong, Simiao and Ouyang, Caixiao},
  year = {2025},
  doi = {10.1109/ICRCA64997.2025.11011072},
  url = {https://doi.org/10.1109/icrca64997.2025.11011072},
  booktitle = {2025 9th {International},
  pages = {367--370},
  keywords = {Accuracy, Automation, document detection, image description, Large language models, Layout, LLM, RAG, Retrieval augmented generation, Robots, Semantics, source: IEEE},
  abstract = {Improve the accuracy of Retrieval-Augmented Generation (RAG) for Large Language Model (LLM), this paper proposes a document layout detection and description network model (Doc-LDNet) for RAG. Doc-LDNet can accomplish two tasks: one is to conduct document layout detection which can accurately identifying the positions and boundaries of different regions in the document, such as text blocks and tables. The other is to describe the images within the document by converting the image content into textual descriptions. Applying Doc-LDNet as a document parsing tool in RAG can effectively enrich the results of document parsing. The experimental results show that applying the Doc-LDNet in RAG can effectively improve the accuracy of LLM in professional domain knowledge question-answering.},
  month = {mar},
}

@inproceedings{phukon_localized_2024,
  title = {Localized {Open},
  author = {Phukon, Pratiksha and Lokhar, Yogesh and Ray, Partha Pratim},
  year = {2024},
  doi = {10.1109/BITCON63716.2024.10985396},
  url = {https://doi.org/10.1109/bitcon63716.2024.10985396},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Accuracy, Chatbots, Codes, Documentation, IEEE Constitution, Large Language Model, Large language models, Law, Legal Documents, Localized Interface, Question-Answer Visualizer, Retrieval augmented generation, Retrieval-Augmented Generation, Training data, source: IEEE},
  abstract = {Large Language Models (LLMs) are advanced artificial intelligence systems which are designed to generate human-like language based on their training data. Retrieval- Augmented Generation (RAG) is a framework that enhances the response of LLM by incorporating external information sources, leading to more accurate, context-relevant responses. However, LLMs commonly struggle with contextual relevance, especially in particular fields such as legal documentation, where specific local laws are crucial. Furthermore, there is limited research on the use of RAG technology in the legal area, particularly for producing and analyzing legal documents. This study develops a chatbot using RAG to answer questions on the Indian Constitution (IC) and the Indian Penal Code (IPC), with a Streamlit-based interface to display legal queries and their generated responses. This study analyzes the evaluation of various LLMs integrated with RAG including (including (i) Llama3 8b, (ii) Mistral 7b, (iii) Gemma2 7b, (iv) Phi3 3.8b, and (v) Qwen2 7b and it evaluates their performance in terms of relevance, faithfulness, context recall, and context precision.},
  month = {dec},
}

@inproceedings{mestre_ragnar_2024,
  title = {{RAGNAR},
  author = {Mestre, António and Marques, Ruben and Fernandes, Alexander and Silva, Bruno},
  year = {2024},
  doi = {10.1109/ISAS64331.2024.10845598},
  url = {https://doi.org/10.1109/isas64331.2024.10845598},
  booktitle = {2024 8th {International},
  pages = {1--6},
  keywords = {Complexity theory, Faces, Generative AI, Large language models, LLM, RAG, Relational Database, Relational databases, Retrieval augmented generation, Soft sensors, Training, source: IEEE},
  abstract = {The technological evolution carried out in recent years has enabled significant developments in various areas of Artificial Intelligence (AI), such as Generative AI. Large Language Models (LLMs) are becoming increasingly complex, allowing for better results and enhancing their real-world applicability. However, these models still face issues such as hallucination or outdated information. This last one occurs due to the temporal gap between the training process and the model’s use. A Retrieval-Augmented Generation (RAG) architecture can address these issues since the information source used is not involved in the training phase, which also facilitates the reuse of models for different applications. One of the challenges of RAG is its applicability when the data source is a relational database, becoming even more challenging as the database size and complexity increase. This article proposes a potential architecture and approach for solving this problem and implementing a RAG architecture using a relational database as the data source.},
  month = {dec},
}

@inproceedings{ayala_task_2025,
  title = {Task {Decomposition},
  author = {Ayala, Orlando Marquez},
  year = {2025},
  doi = {10.1109/CAIN66642.2025.00049},
  url = {https://doi.org/10.1109/cain66642.2025.00049},
  booktitle = {2025 {IEEE},
  journal = {2025 IEEE/ACM 4th International Conference on AI …},
  pages = {279--280},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Artificial intelligence, Best practices, design patterns, generative ai, Large language models, llms, Production, rag, Retrieval augmented generation, Safety, Software engineering, Software quality, Software systems, task decomposition, source: IEEE, source: Google Scholar},
  abstract = {AI technologies are moving rapidly from research to production. Compared to traditional AI-based software, systems employing Large Language Models (LLMs) are more difficult to design due to their scale and versatility. This makes it necessary to document best practices, known as design patterns in software engineering, that can be used across LLM-based applications. While Task Decomposition and Retrieval-Augmented Generation (RAG) are well-known techniques, their formalization as design patterns for LLM-based systems benefits AI practitioners. These techniques should be considered not only from a scientific perspective, but also from the standpoint of desired software quality attributes such as safety and modularity. This will help bridge the gap between AI and software engineering as those fine-tuning or prompting LLMs will be aware of the impact that modern techniques have on the overall system.},
  month = {apr},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{zhao_research_2025,
  title = {Research on {Intelligent},
  author = {Zhao, Tao and Yang, Xiaojing and Gao, Jun and Pan, Haiyan},
  year = {2025},
  doi = {10.1109/CAIBDA65784.2025.11183015},
  url = {https://doi.org/10.1109/caibda65784.2025.11183015},
  booktitle = {2025 5th {International},
  pages = {389--393},
  keywords = {Accuracy, Adaptation models, LLM, Model Fine-tuning, Power industry, Prompt engineering, Prompt Engineering, Question answering (information retrieval), Question Answering System, RAG, Real-time systems, Retrieval augmented generation, System performance, Technological innovation, Vectors, source: IEEE},
  abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in conversation, reasoning, and knowledge retention. However, they still face challenges in handling knowledge-intensive tasks within the power domain, including insufficient factual accuracy, difficulties in knowledge updating, and a scarcity of high-quality domain-specific datasets. In order to address these challenges, this paper aims to design specifically a system for knowledge-intensive question answering tasks in the electric power domain. The proposed system incorporates an enhanced Retrieval-Augmented Generation (RAG) strategy that integrates a hybrid retrieval strategy and a finetuned model to provide more efficient knowledge acquisition and updating capabilities. Additionally, the system incorporates LLMbased prompt engineering techniques to enhance the coherence of responses to questions. Experimental results show that the question answering system effectively improves the accuracy of retrieving and answering power-related knowledge and reduces hallucinations. Furthermore, ablation experiments indicate that RAG module has the greatest impact on system performance, with the overall accuracy decreasing by 18.6 \% without this component.},
  month = {jun},
}

@inproceedings{chan_optimizing_2025,
  title = {Optimizing {Confidence},
  author = {Chan, Wing Tung and Hung, Kevin and Ho, Raymond and Man-Tat Man, Gary},
  year = {2025},
  doi = {10.1109/ISCAIE64985.2025.11081158},
  url = {https://doi.org/10.1109/iscaie64985.2025.11081158},
  booktitle = {2025 {IEEE},
  pages = {540--545},
  note = {ISSN: 2836-4317},
  keywords = {source: IEEE},
  abstract = {The global shortage of skilled personnel in technical support has spurred significant interest in AI-driven solutions, particularly Large Language Model (LLM)-based customer service chatbots. However, a critical challenge in deploying these systems lies in addressing AI hallucination, wherein models generate responses that are plausible yet factually incorrect. This study investigates a prompt engineering approach to enhance confidence estimation and mitigate AI hallucinations in LLM chatbots. Three distinct prompt strategies—Basic, Advanced, and Combo prompts–are systematically evaluated to improve response reliability. Given that LLMs inherently lack the ability to explicitly express uncertainty (e.g., by stating “I don't know”), a structured confidence scoring mechanism is employed to refine accuracy and reduce the Expected Calibration Error (ECE). Experimental results reveal that Basic prompts achieve an accuracy of 69.33\% (ECE: 23.33), Advanced prompts improve accuracy to 75.33\% (ECE: 14.87), and Combo prompts further enhance accuracy to 81.33\% while reducing ECE to 8.4. These findings underscore the efficacy of prompt engineering in mitigating AI hallucinations and advancing the performance of LLM chatbots in real-world customer support applications.},
  month = {may},
}

@inproceedings{guan_external_2024,
  title = {External {Knowledge},
  author = {Guan, Wenbo and Lu, Jiyu and Feng, Qinyu and Li, Xiaoqian and Zhou, Jun},
  year = {2024},
  doi = {10.1109/AIIM64537.2024.10934284},
  url = {https://doi.org/10.1109/aiim64537.2024.10934284},
  booktitle = {2024 4th {International},
  pages = {882--886},
  keywords = {adaptive RAG method, hallucination, large language model, Large language models, Manufacturing, Noise, Retrieval augmented generation, source: IEEE},
  abstract = {Retrieval augmented generation (RAG), by integrating external knowledge with large language models (LLMs), has become a common practice to alleviate LLMs’ hallucination problem. The performance of RAG, however, depends on the capability of the adopted information retriever to a large extent. Specifically, a good information retriever can manage to obtain the most useful information from the external knowledge, which will then enhance the quality of generated content from LLMs and vice versa. Since the vanilla RAG will always leverage the retrieved information to assist with LLMs’ content generation despite of its usefulness, when combined with a retriever of limited capacity, therefore, the vanilla RAG will not benefit LLMs’ content generation sometimes but introduce additional noise, having a negative impact on the final performance. To address this problem, this paper proposes a novel adaptive RAG method which first uses LLMs to determine the usefulness of the retrieved information and then let LLMs to generate content on their own without the retrieved information if it is useless. Experimental results on four datasets demonstrate the effectiveness of the proposed adaptive RAG method with information retrievers of various capabilities, improving the performance of the vanilla RAG by an obvious margin.},
  month = {dec},
}

@inproceedings{kashyap_rag-em_2025,
  title = {{RAG},
  author = {Kashyap, Priyank and Rouf, Nirjhor and Choi, Yongjin and Franzon, Paul and Cheng, Chris},
  year = {2025},
  doi = {10.1109/EMCSIPI52291.2025.11169861},
  url = {https://doi.org/10.1109/emcsipi52291.2025.11169861},
  booktitle = {2025 {IEEE},
  pages = {158--163},
  note = {ISSN: 2158-1118},
  keywords = {Codes, Electromagnetic, Electromagnetics, Large language models, LLM, Microstrip, Natural languages, RAG, Retrieval augmented generation, Scattering parameters, Solid modeling, Stripline, Three-dimensional displays, Via, source: IEEE},
  abstract = {As large language models (LLMs) gain broader adoption, incorporating domain knowledge via proprietary data is of the utmost importance. However, data such as platform design guides (PDGs) have heavy restrictions in the design flow, meaning that the data must remain on-premise. Thus, using existing language models over the Internet is infeasible without compromising the data. This paper examines using local LLMs with domain knowledge, especially PDGs, with relevant design information and custom tool interfaces. In order to provide the LLM with domain-relevant knowledge, we use modified retrieval augmented generation (RAG). For domain-relevant knowledge, we provide the LLM with numerous PDGs and Python scripts that enable the control of an electromagnetic tool for PCB design. We demonstrate the ability of the model to perform domain-specific question/answering (QA) and generate code for manipulating 3D EM structures such as striplines and vias. We show the advantage of the on-prem RAG-based approach over finetuning by introducing new tasks/instructions.},
  month = {aug},
}

@article{hindi_enhancing_2025,
  title = {Enhancing the {Precision},
  author = {Hindi, Mahd and Mohammed, Linda and Maaz, Ommama and Alwarafy, Abdulmalik},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3550145},
  url = {https://doi.org/10.1109/access.2025.3550145},
  journal = {IEEE Access},
  volume = {13},
  pages = {46171--46189},
  keywords = {Accuracy, Complexity theory, Generators, Information retrieval, Knowledge graphs, large language model (LLM), Large language models, Law, legal technology, Pipelines, prompt engineering, Retrieval augmented generation, retrieval-augmented generation (RAG), Surveys, Transformers, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) is a promising solution that can enhance the capabilities of large language model (LLM) applications in critical domains, including legal technology, by retrieving knowledge from external databases. Implementing RAG pipelines requires careful attention to the techniques and methods implemented in the different stages of the RAG process. However, robust RAG can enhance LLM generation with faithfulness and few hallucinations in responses. In this paper, we discuss the application of RAG in the legal domain. First, we present an overview of the main RAG methods, stages, techniques, and applications in the legal domain. We then briefly discuss the different information retrieval models, processes, and applied methods in current legal RAG solutions. Then, we explain the different quantitative and qualitative evaluation metrics. We also describe several emerging datasets and benchmarks. We then discuss and assess the ethical and privacy considerations for legal RAG and summarize various challenges, and propose a challenge scale based on RAG failure points and control over external knowledge. Finally, we provide insights into promising future research to leverage RAG efficiently and effectively in the legal field.},
  issn = {2169-3536},
}

@inproceedings{hachicha_recommendation_2024,
  title = {Recommendation {Module},
  author = {Hachicha, Mohamed and Omezine, Mohamed Aziz and Zghal, Mohamed Khalil and Riahi, Mohamed Hedi and Ncib, Lotfi},
  year = {2024},
  doi = {10.1109/AIR63653.2024.00025},
  url = {https://doi.org/10.1109/air63653.2024.00025},
  booktitle = {2024 {Artificial},
  pages = {68--73},
  keywords = {CVs, Data models, Data preprocessing, Databases, LLM, Machine learning, machine learning techniques, recom-mend internships, Recommender systems, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Scalability, Soft sensors, Testing, Vectors, web scraping, source: IEEE},
  abstract = {In this research, we investigate the implementation of Retrieval-Augmented Generation (RAG) technology to recommend suitable engineering internships to individuals who are looking for an internship based on their CVs by improving the search for correspondence between candidates and employers, ensuring that both parties find the best adequacy. This innovative approach leverages web scraping, data preprocessing and advanced machine learning techniques including vector databases designed to handle high-dimentional vector data allowing for searches based on the proximity or similarity of vectors, and embedding methods used to convert high-dimentional data into vector representations that can be stored in these vector databases. By analyzing and understanding the content of each CV and internship offer, our methodology aims to deliver personalized and relevant internship opportunities, enhancing candidate-employer matching.},
  month = {oct},
}

@article{collini_context-aware_2025,
  title = {Context-{Aware},
  author = {Collini, Enrico and Indra Kurniadi, Felix and Nesi, Paolo and Pantaleo, Gianni},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3614553},
  url = {https://doi.org/10.1109/access.2025.3614553},
  journal = {IEEE Access},
  volume = {13},
  pages = {170065--170080},
  keywords = {Accuracy, Context-aware retrieval augmented generation (CA-RAG), cosine similarity, Costs, domain-specific tasks, dot product, Large language models, large language models (LLMs), Retrieval augmented generation, retrieval-augmented generation (RAG), Robustness, Semantics, Strips, Training, Training data, Vectors, source: IEEE},
  abstract = {Large Language Models (LLMs) have transformed natural language processing by offering human-like responses. However, issues such as incorrect information (hallucinations) and errors in specific subject areas remain, especially in Retrieval Augmented Generation (RAG) systems. This study introduces a Context-Aware Retrieval Augmented Generation (CA-RAG), which simplifies the process by removing the need to separately find relevant chunks of a document. Instead, after dividing the document into chunks, both the question and chunks are directly given to the LLMs to produce answers. The method then focuses on improving the answers through additional post-processing, aiming to reduce errors and make the answers more relevant to the question. To evaluate the effectiveness of CA-RAG, two scenarios have been designed. Scenario 1 involved experiments using widely adopted and recognized benchmark datasets, such as TriviaQA, Natural Questions, AmbigQA and Stanford Question Answering Dataset (SQuAD). In this context, the proposed CA-RAG method, combined with similarity measure (either cosine similarity or dot product) between generated answers and chunks, achieved the highest F1-score in TriviaQA and AmbigQA. Scenario 2 tested CA-RAG robustness using a custom dataset comprising domain-specific and unstructured documents. Results from automated and manual evaluations revealed that CA-RAG with post-processing consistently outperformed traditional RAG. These findings highlight the critical role of post-processing techniques and similarity measures in improving the accuracy and relevance of generated answers. CA-RAG demonstrates strong potential as a reliable and versatile solution for Retrieval Augmented Generation tasks across diverse datasets and domains.},
  issn = {2169-3536},
}

@inproceedings{zhao_empirical_2024,
  title = {An {Empirical},
  author = {Zhao, Yuetong and Cao, Hongyu and Zhao, Xianyu and Ou, Zhijian},
  year = {2024},
  doi = {10.1109/ISCSLP63861.2024.10800207},
  url = {https://doi.org/10.1109/iscslp63861.2024.10800207},
  booktitle = {2024 {IEEE},
  pages = {436--440},
  keywords = {source: IEEE},
  abstract = {Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.},
  month = {nov},
}

@inproceedings{bhiwgade_integrating_2025,
  title = {Integrating {Open},
  author = {Bhiwgade, Akash Wamanrao and Nagrale, Nilesh},
  year = {2025},
  doi = {10.1109/NGISE64126.2025.11085188},
  url = {https://doi.org/10.1109/ngise64126.2025.11085188},
  booktitle = {2025 {International},
  volume = {1},
  pages = {1--5},
  keywords = {Accuracy, AI in Healthcare, Clinical Decision Support, Data models, Databases, Gynecology, Information retrieval, Information Retrieval, Knowledge based systems, Large language models, Large Language Models, Machine Learning, Natural Language Processing, OBGYN, Obstetrics, Retrieval augmented generation, Retrieval Augmented Generation, Text Generation, Vectors, Women’s Health, source: IEEE},
  abstract = {In this research paper, we address the integration of Large Language Models (LLM) with Retrieval Augmented Generation (RAG) to enhance clinical decision support and address patient doubts in the Obstetrics and Gynecology (OBGYN) domain. The research mainly tries to explore on How open source LLM’s can effectively retrieve and generate the relevant responses in the OBGYN domain. The research methodology includes two components: Data Ingestion, which reads the input text data and stores it to a vector database in an embedded format and Data Retriever-Generation, which retrieves the relevant information from the vector database and use it for accurate response generation. The method includes data collection from esteemed medical databases, LLM model selection, integration with RAG and evaluation of the generated outputs. The methodology uses Bio-Mistral 7B fine-tuned LLM with PubMed Bert embeddings. The LLM responses are evaluated using the Ragas framework, context precision and context recall to measure the performance of retrieval system, faithfulness to measure hallucinations and answer relevancy to measure how relevant the answers are to the input query. The evaluated results confirm that the research has improved the accuracy as well as the contextual relevancy of the information in the OBGYN domain. This research provides a robust architecture for integration of Artificial Intelligence for supporting clinical decision making and information retrieval in specialized medical domains. This research can be extended with the upcoming advancements in the field of Artificial Intelligence and Data Science.},
  month = {mar},
}

@article{hu_icca-rag_2025,
  title = {{ICCA},
  author = {Hu, Rong and Liu, Sen and Qi, Panpan and Liu, Jingyi and Li, Fengyuan},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3544408},
  url = {https://doi.org/10.1109/access.2025.3544408},
  journal = {IEEE Access},
  volume = {13},
  pages = {39711--39726},
  keywords = {Accuracy, Codes, Companies, Complexity theory, Customs declaration assistance, dynamic regulation adaptation, Inspection, intelligent question-answering system, large language model (LLM), Large language models, multimodal document parsing, Retrieval augmented generation, retrieval-augmented generation (RAG), semantic retrieval, Semantics, Tariffs, Vectors, source: IEEE},
  abstract = {Document processing and query generation tasks in customs declaration scenarios face key challenges such as the complexity of multimodal data, adaptability to dynamic regulations, and ambiguity in query semantics. This study proposes a Retrieval-Augmented Generation system (ICCA-RAG) that addresses the core issues of processing complex customs documents and dynamically generating queries through multimodal document parsing, sparse-dense hybrid storage, and context-driven large language model generation. In terms of multimodal document parsing, the system supports comprehensive parsing of PDFs, images, tables, and text, which are uniformly transformed into semantic vectors and keyword indices for hybrid storage. By combining the retrieval and generation modules, the ICCA-RAG system achieves significant improvements in contextual relevance and generation accuracy. Compared to traditional methods, the ICCA-RAG system demonstrates a 20.1\% increase in answer correctness, a 15.3\% increase in answer relevancy, and an 18.7\% increase in the faithfulness of generated content, with outstanding performance in noisy query scenarios. The research findings validate the ICCA-RAG system’s advancement and applicability in handling complex document processing and professional domain question-answering tasks, while also providing a transferable technical framework for other fields, such as law and healthcare.},
  issn = {2169-3536},
}

@inproceedings{chen_lire_2024,
  title = {{LiRe},
  author = {Chen, Chengfeng and Huang, Xiaodong and Xu, Yangzhen and Lin, Runfeng and Li, Gangliang and Liu, Shouqiang},
  year = {2024},
  doi = {10.1109/CEI63587.2024.10871553},
  url = {https://doi.org/10.1109/cei63587.2024.10871553},
  booktitle = {2024 4th {International},
  pages = {27--31},
  keywords = {Accuracy, Agent, Computer science, ConQA, CQR, Filtering, Hallucinations, Information retrieval, Intelligent control, Large language models, LLM, Question answering (information retrieval), RAG, Retrieval augmented generation, Robustness, source: IEEE},
  abstract = {With the continuous advancement of information retrieval technologies, query rewriting has become a key technique for enhancing retrieval effectiveness. In the context of the widespread application of large language models and retrieval-augmented generation technologies, high-quality query rewriting plays a crucial role in tasks like open-domain question answering. Conventional Conversational Query Rewriting (CQR) methods often prioritize human-friendly query formulation, which may not consistently produce optimal retriever-friendly results. To address this limitation, we present LiRe, a lightweight and training-free query rewriting system. LiRe employs a retrieval-oriented strategy to refine queries, optimizing them for retrievers. This approach allows for seamless integration into diverse Retrieval-Augmented Generation (RAG) systems. Our approach involves problem categorization, keyword augmentation from contextual information, and integration with the filtering process. LiRe-enhanced queries demonstrate superior performance compared to existing CQR methods, as evidenced by experimental outcomes.},
  month = {nov},
}

@inproceedings{ko_rerag_2024,
  title = {{ReRag},
  author = {Ko÷, Robin and Gürkan, Mustafa Kağan and Yarman Vural, Fatoş T.},
  year = {2024},
  doi = {10.1109/UBMK63289.2024.10773428},
  url = {https://doi.org/10.1109/ubmk63289.2024.10773428},
  booktitle = {2024 9th {International},
  journal = {2024 9th International …},
  pages = {961--965},
  note = {ISSN: 2521-1641},
  keywords = {Computational modeling, Computer architecture, Computer science, Databases, Generative AI, infor-mation retrieval, Large language models, Retrieval-Augmented Generation systems, Standards, Vectors, source: IEEE, source: Google Scholar},
  abstract = {As the application areas of Generative AI models become widespread, new problems arise. One of these problems is the “hallucination” observed in Large Language Models (LLM). Recently, Retrieval-Augmented Generation systems (RAG) have been introduced to cope with this problem. In this paper, we propose a new method, called “ReRag for Retrieval Augmented Generation”, to reduce the hallucination effect of LLM responses. This method assumes that a small dataset of queries and their man-made ideal responses are available for a specific application domain. It attempts to optimize the hyperparameters of the vector database in the RAG system by generating close-to-ideal responses concerning this ideal dataset, in which the questions and the ideal answers are provided. ReRag measures the similarity between the generated and ideal responses and then modifies the hyperparameters iteratively to increase the similarity between these two responses. Experimental results show that the ReRag method reduces the hallucination problem and generates relatively close to ideal responses compared to raw models.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{su_leveraging_2025,
  title = {Leveraging {Knowledge},
  author = {Su, Tianfeng and Ma, Chenjing and Chen, Xin and Wang, Xiaohan},
  year = {2025},
  doi = {10.1109/FASTA65681.2025.11138748},
  url = {https://doi.org/10.1109/fasta65681.2025.11138748},
  booktitle = {2025 4th {Conference},
  pages = {1590--1595},
  keywords = {source: IEEE},
  abstract = {To address the challenges of insufficient text data utilization and low manual diagnosis efficiency in urban rail transit signaling system fault analysis, particularly the difficulty in handling novel faults under unmanned train operations, this study proposes a dual-engine framework integrating Knowledge Graph (KG) and Large Language Model (LLM). Based on preprocessed fault diagnosis logs from Nanjing Metro's signaling system, a multi-dimensionally annotated dataset was constructed. The CasRel joint extraction model was employed to overcome the limitations of traditional pipeline methods in recognizing nested terms and mitigating error propagation, building a high-density domain-specific KG. Subsequently, a three-tiered question-answering paradigm (fact query, enumeration verification, causal verification) and multi-granularity intent classification system were innovatively integrated to enable semantic retrieval and dynamic reasoning over the KG. To tackle data dependence and logical hallucinations in vertical domain LLMs, a domain-adapted LLM was developed using QLoRA parameter-efficient fine-tuning technology, complemented by a zero-shot autonomous reasoning mechanism based on prompt engineering. By deconstructing signal transmission logic, cross-system association rules, and device temporal relationships, causal chain prompts, analogical inference templates, and dynamic temporal context cues were generated to guide the LLM in performing root cause deduction along domain knowledge-constrained pathways without historical data. Concurrently, the reasoning logic underwent real-time verification leveraging the KG's security protocol rule library to filter domain-violating hypotheses. Graph Retrieval-Augmented Generation (GraphRAG) technology further embedded KG retrieval results and dynamic prompts into the LLM's input, forming a closed-loop “knowledge anchoring, prompt guidance, verification and generation” decision process. Experimental results demonstrate that the framework significantly enhances novel fault diagnosis accuracy and reliability, achieving security-compliant root cause localization in zero-shot scenarios while outperforming traditional methods. It autonomously transforms new fault resolutions into KG nodes, triggering LLM incremental learning to enable continuous evolution of the knowledge system. This research provides an interpretable and scalable diagnostic decision-support paradigm for intelligent rail transit operations, offering technical substantiation for deploying industrial LLMs in safety-critical systems.},
  month = {jul},
}

@inproceedings{xiao_arag_2025,
  title = {{ARAG},
  author = {Xiao, Yixiong and Cao, Jingjia and Jiang, Yangxin and Zhou, Jingbo},
  year = {2025},
  doi = {10.1109/ICDE65448.2025.00368},
  url = {https://doi.org/10.1109/icde65448.2025.00368},
  booktitle = {2025 {IEEE},
  pages = {4624--4627},
  note = {ISSN: 2375-026X},
  keywords = {Chatbots, Cognition, data analysis, Data analysis, Data engineering, Fake news, Information retrieval, Large language models, LLM, Question answering (information retrieval), RAG, Retrieval augmented generation, Socioeconomics, source: IEEE},
  abstract = {Recent advancements in Large Language Models (LLMs) have significantly impacted the field of question answering systems, particularly with LLM-based data analysis and Retrieval-Augmented Generation (RAG). Yet, applying them independently has limited their effectiveness in scenarios that require a synthesis of both data analysis and contemporary information retrieval. To bridge this gap, we introduce the Analysis and Retrieval Augmented Generation (ARAG) framework, which integrates data analysis with the retrieval of up-to-date information. Based on the framework, we build a system to showcase how ARAG interprets the dynamics of socioeconomic indicators by examining correlated data and retrieving relevant information from news sources. The comparison of ARAG with ChatGPT Search and Perplexity showed that ARAG significantly outperformed them in delivering indepth analytical insights. Moreover, ARAG is observed to have a stronger ability to verify facts and reject misinformation in users' queries, thus reducing LLM's susceptibility to hallucination.},
  month = {may},
}

@article{saha_quim-rag_2024,
  title = {{QuIM},
  author = {Saha, Binita and Saha, Utsha and Zubair Malik, Muhammad},
  year = {2024},
  doi = {10.1109/ACCESS.2024.3513155},
  url = {https://doi.org/10.1109/access.2024.3513155},
  journal = {IEEE Access},
  volume = {12},
  pages = {185401--185410},
  keywords = {Accuracy, ChatGPT, Computational modeling, Data models, Faces, GPT-3.5-turbo, hallucination mitigation, Indexes, information dilution, Large language models, Large language models (LLMs), Meta-LLaMA3-8B-instruct, Prototypes, Question answering (information retrieval), question answering (QA), Reliability, retrieval-augmented generation (RAG), Vectors, source: IEEE},
  abstract = {This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics.},
  issn = {2169-3536},
}

@inproceedings{nishikawa_enhancing_2024,
  title = {Enhancing {Source},
  author = {Nishikawa, Kazu and Koreki, Genta and Kanuka, Hideyuki},
  year = {2024},
  doi = {10.1109/APSEC65559.2024.00061},
  url = {https://doi.org/10.1109/apsec65559.2024.00061},
  booktitle = {2024 31st {Asia},
  pages = {467--471},
  note = {ISSN: 2640-0715},
  keywords = {Accuracy, Codes, Dictionaries, Fitting, Large Language Model, Large language models, Open source software, Retrieval augmented generation, Retrieval-Augmented Generation, Software development management, Software engineering, Source coding, Source-code Comment Generation, source: IEEE},
  abstract = {Effective software development depends on clear code comments for better understanding. We introduce a method for generating automated source-code comments using retrieval-augmented generation (RAG) with a design document term dictionary. This method aligns terms and their meanings from design documents with lines of source code, producing comments that clearly reflect the source-code's intent and functionality. Our evaluations on the open-source software iDempiere show significant improvements: context precision increased by 22\% and faithfulness by 17 \% compared with conventional RAG methods These results confirm our method's validity. Therefore, we plan to explore its application to different software contexts in future work.},
  month = {dec},
}

@inproceedings{gu_lens_2024,
  title = {{LENS},
  author = {Gu, Chuqiao and Zhang, Wuyang and Huang, Zhenqian and Kou, Jieren and Liu, Zhenyao and Zhao, Chenjun and Liu, Chang and Zhang, Lifeng and Lin, Wenjie and Wang, Zhongda and Deng, Jianwei and Xie, Yuhuan and Huang, Guoxin and Zhang, Charles and Lu, Xiuyuan and Wang, Chengming and Zhang, Zejun and Yuan, Hao and Duan, Xiaoman and Fang, Yajun},
  year = {2024},
  doi = {10.1109/UV63228.2024.11189150},
  url = {https://doi.org/10.1109/uv63228.2024.11189150},
  booktitle = {2024 7th {International},
  pages = {1--85},
  keywords = {Accuracy, actionable error attribution, adversarial robustness, Analytical models, Benchmark testing, causal attribution, causal reasoning capability, Cognition, compositional complexity, comprehensive hallucination evaluation, computational efficiency, cross-domain evaluation, cross-modal inconsistencies, domain-specific benchmarks, domain-specific queries, dynamic knowledge, evidence grounding, evolving benchmarks, factuality assessment, faithfulness evaluation, generative AI evaluation, ground truth ambiguity, hallucination evaluation, hallucination mitigation, hierarchical query decomposition, horizontal vs vertical evaluation, Large language models, LENS framework, Lenses, mechanistic interpretability, metacognitive evaluation, multi-dimensional metrics, multi-layered assessment structures, multimodal hallucination, post-hoc error detection, Prevention and mitigation, proactive risk assessment, reasoning failures, reliability and robustness, retrieval-augmented generation, Robustness, surface-level accuracy, system theory, temporal drift, tool necessity detection, tool selection accuracy, traceable evidence chains, Transforms, transparency and interpretability, tree-of-thoughts, trustworthy AI, Uncertainty, uncertainty quantification, Universal Village, user-centric benchmark construction, vision-language models, visual-textual misalignment, source: IEEE},
  abstract = {Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate remarkable capabilities but remain vulnerable to hallucinations—producing plausible yet factually incorrect content—with error rates ranging from as low as 1.47\% in clinical applications [1] to as high as 75\% in domain-specific queries [2]. Despite growing attention, existing hallucination evaluation frameworks remain insufficient to meet critical needs. Through a comprehensive survey of over 100 evaluation methods spanning six methodological paradigms (probe-based, adversarial testing, causal intervention, uncertainty-guided, internal state analysis, and online evaluation), we identify fundamental limitations: current approaches fall short of enabling objective model comparison, providing diagnostic insights into failure modes, supporting domain-evolving benchmark construction, and guiding targeted mitigation strategies. Our analysis reveals that evaluations remain fragmented, operating either horizontally—comparing models across tasks and domains—or vertically—probing reasoning chains within single outputs. This fragmentation limits holistic assessment: horizontal evaluations provide breadth but risk superficiality, while vertical assessments deliver depth but lack generalizability. Moreover, we identify five critical gaps: (1) dimensional poverty reducing hallucinations to binary metrics, (2) failure to integrate horizontal breadth with vertical depth, (3) metacognitive blind spots overlooking when models should seek external verification, (4) adaptability crisis from static benchmarks, and (5) transparency deficits providing scores without actionable insights. Beyond surveying the landscape, this paper articulates eight fundamental challenges confronting comprehensive evaluation—from epistemological difficulties in defining ground truth and computational complexity of scaling assessment, to attribution opacity obscuring causal mechanisms and dynamic knowledge evolution rendering benchmarks obsolete. These challenges span multimodal complexity, scale and diversity requirements, adversarial robustness, and human alignment considerations. We present LENS (Layers of Evaluation of Hallucination in GenAI Systems), a unified framework addressing these gaps through hierarchical, tree-based query decomposition. LENS transforms complex evaluation tasks into multi-layered assessment structures via a six-stage pipeline (task formulation, decomposition, tool-augmented execution, structured generation, multi-dimensional scoring, and trace analysis), enabling MRI-like scanning of inference processes to reveal where and why hallucinations originate. The framework introduces four key innovations: (1) Tool Necessity Detection and Selection (TND/TSA) – explicitly evaluating when models should consult external sources versus relying on parametric memory, addressing a fundamental hallucination source. (2) Multi-Dimensional Metrics – assessing degree (accuracy, faithfulness, tool appropriateness), quantity (coverage, completeness), stability (consistency, robustness), and risk (uncertainty quantification) beyond binary detection. (3) User-Centric Benchmark Construction – empowering organizations to design custom evaluations from their evolving knowledge bases while maintaining methodological rigor. (4) Actionable Error Attribution – providing hierarchical decomposition traces with causal attribution, evidence chains, and OpenTelemetry-based reproducibility for transparent auditing. Our systematic taxonomy unifies previously fragmented approaches across evaluation targets (task-specific, modality-based, hallucination-type, domain-specific), dimensions (factuality, faithfulness, consistency, robustness, causal reasoning, interpretability), and methodologies. We introduce unified metrics transcending individual dimensions and present mitigation-aware evaluation strategies integrating RAG, parameter-efficient fine-tuning, knowledge distillation, preference optimization, and temporal intervention approaches. By combining horizontal breadth (across domains and architectures) with vertical depth (into reasoning processes), LENS advances hallucination evaluation from post-hoc error detection to proactive risk assessment. Case studies in medical diagnosis, legal analysis, and financial reasoning demonstrate the framework’s transformative impact, enabling objective model comparison, informed selection, diagnostic insights, domain-evolving benchmarks, and targeted mitigation development—fostering calibrated trust in AI systems deployed in safety-critical applications where accuracy, interpretability, and accountability are indispensable.},
  month = {oct},
}

@inproceedings{heydari_context_2024,
  title = {Context {Awareness},
  author = {Heydari, Mohammad Hassan and Hemmat, Arshia and Naman, Erfan and Fatemi, Afsaneh},
  year = {2024},
  doi = {10.1109/IKT65497.2024.10892659},
  url = {https://doi.org/10.1109/ikt65497.2024.10892659},
  booktitle = {2024 15th {International},
  pages = {260--264},
  note = {ISSN: 2476-2180},
  keywords = {Accuracy, Context awareness, Context modeling, Hallucination, Large language models, Large Language Models, Logic gates, Open Domain Question Answering, Pipelines, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation, Statistical analysis, Vectors, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to mitigate the limitations of large language models (LLMs) in answering domain-specific questions. Previous research has predominantly focused on improving the accuracy and quality of retrieved data chunks to enhance the overall performance of the generation pipeline. However, despite ongoing advancements, the critical issue of retrieving irrelevant information-which can impair a model's ability to utilize its internal knowledge effectively-has received minimal attention. In this work, we investigate the impact of retrieving irrelevant information in open-domain question answering, highlighting its significant detrimental effect on the quality of LLM outputs. To address this challenge, we propose the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLM's input prompt based on whether the user query necessitates external context retrieval. Additionally, we introduce the Vector Candidates method, a core mathematical component of CAG that is statistical, LLM-independent, and highly scalable. We further examine the distributions of relationships between contexts and questions, presenting a statistical analysis of these distributions. This analysis can be leveraged to enhance the context retrieval process in retrieval-augmented generation (RAG) systems.},
  month = {dec},
}

@inproceedings{danuarta_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Danuarta, Leo and Mawardi, Viny Christanti and Lee, Viciano},
  year = {2024},
  doi = {10.1109/ICIC64337.2024.10957676},
  url = {https://doi.org/10.1109/icic64337.2024.10957676},
  booktitle = {2024 {Ninth},
  journal = {2024 Ninth International …},
  pages = {1--6},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, Chatbots, Context modeling, Educational chatbot, Large Language Model, Large language models, Relevance, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Safety, Sports, Training, Uncertainty, source: IEEE, source: Google Scholar},
  abstract = {In the digital age, educational chatbots are increasingly essential for providing personalized learning experience. This study explores the potential of the Retrieval-Augmented Generation (RAG) method to enhanced the performance of educational chatbots. By integrating Large Language Models (LLMs) with real-time data retrieval from external sources, the RAG approach improves the accuracy, relevance, and safety of chatbot responses. Tested using an Indonesian elementary school e-book dataset on Physical Education, Sports, and Health (PJOK), the RAG model demonstrated significantly improvements in generating accurate and contextually appropriate answers while minimizing harmful content. The model achieved a mean context precision of 0.69, context recall of 0.70, faithfulness of 0.60, answer relevancy of 0.86, and harmfulness reduction to 0.26. The findings underscore the effectiveness of RAG advancing educational chatbot performance.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{deng_intelligent_2024,
  title = {Intelligent {Orientation},
  author = {Deng, Xinying and Yang, Dongju and Zhang, Yuan},
  year = {2024},
  doi = {10.1109/ICAICE63571.2024.10863858},
  url = {https://doi.org/10.1109/icaice63571.2024.10863858},
  booktitle = {2024 5th {International},
  pages = {779--782},
  keywords = {Data collection, Embodied Intelligence, Information security, Intelligent robots, Knowledge based systems, LangChain, Large language models, Large Language Models, Protocols, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Touch sensitive screens, User experience, WebSocket, source: IEEE},
  abstract = {To address the issues of hallucination and knowledge security in large language models (LLMs) during Question and Answer processes, this project developed a Q\&A platform integrating Retrieval-Augmented Generation (RAG) with LLMs and enabled communication with intelligent robots. This project, applied to the university orientation scenario, built an orientation knowledge base comprising 11 categories of documents, about 300,000 words in all. By leveraging RAG for precise retrieval and using LLMs to generate logical and traceable answers, the system enhances both the accuracy and reliability of responses. The project also established remote communication with robots through the WebSocket protocol, offering flexible voice and touchscreen interaction, greatly improving the user experience. This platform not only improves the logical consistency of text-based Q\&A but also strengthens the interaction between robots and users, providing a certain technical foundation for future multi-scenario intelligent interactions.},
  month = {nov},
}

@inproceedings{meng_analysis_2025,
  title = {Analysis of {Text},
  author = {Meng, Qi and Wu, Zhenglong and Zhao, Zhongshi and Lian, Xi'nan},
  year = {2025},
  doi = {10.1109/SGAI64825.2025.11009349},
  url = {https://doi.org/10.1109/sgai64825.2025.11009349},
  booktitle = {2025 2nd {International},
  pages = {204--208},
  keywords = {source: IEEE},
  abstract = {Text generation technology based on Pre-trained Language Model (PLM) has made significant progress in recent years, but it still faces many challenges such as semantic inconsistency and insufficient computing power. In view of this phenomenon, Retrieval Augmented Generation (RAG) and Fine-tuning strategy is studied, the general framework design for the text generation system of fusion strategy is put forward, several typical systems in specific fields are analyzed. The discussion of the realization and development direction of technology provides a reference for studying fusion strategy, which can improve the quality of text generation system design.},
  month = {mar},
}

@inproceedings{sng_novel_2024,
  title = {A {Novel},
  author = {Sng, Grace and Zhang, Yanming and Mueller, Klaus},
  year = {2024},
  doi = {10.1109/URTC65039.2024.10937570},
  url = {https://doi.org/10.1109/urtc65039.2024.10937570},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE MIT Undergraduate …},
  pages = {1--6},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, Automation, Cause effect analysis, Large language models, Logic, Pipelines, Retrieval augmented generation, Surveys, source: IEEE, source: Google Scholar},
  abstract = {The increasing use of large language models (LLMs) in causal discovery as a substitute for human domain experts highlights the need for optimal model selection. This paper presents the first hallucination survey of popular LLMs for causal discovery. We show that hallucinations exist when using LLMs in causal discovery so the choice of LLM is important. We propose using Retrieval Augmented Generation (RAG) to reduce hallucinations when quality data is available. Additionally, we introduce a novel method employing multiple LLMs with an arbiter in a debate to audit edges in causal graphs, achieving a comparable reduction in hallucinations to RAG.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{lu_initial_2025,
  title = {Initial {Evaluation},
  author = {Lu, Bingyan and Reinking, Caleb and Laneman, J. Nicholas},
  year = {2025},
  doi = {10.1109/DySPAN64764.2025.11115912},
  url = {https://doi.org/10.1109/dyspan64764.2025.11115912},
  booktitle = {2025 {IEEE},
  pages = {1--5},
  note = {ISSN: 2473-070X},
  keywords = {Benchmark testing, Dynamic spectrum access, FCC, Information retrieval, Knowledge graphs, Large language models, LLM, NTIA, RAG, Retrieval augmented generation, spectrum policy, System performance, source: IEEE},
  abstract = {Spectrum policy documents, such as notices and comments in Federal Communications Commission (FCC) and the National Telecommunications and Information Administration (NTIA) proceedings, are often lengthy and complex, making them difficult to access and understand. Combining large language models (LLMs) with retrieval-augmented generation (RAG) techniques offers a promising way to address these challenges by improving information retrieval and processing. This paper evaluates the use of RAG in spectrum policy analysis. We establish an open-source knowledge dataset of policy comments that has been cleaned and annotated to work effectively with RAG systems. We also develop a corresponding test dataset of question-answer pairs to evaluate system performance. By benchmarking four RAG-based systems, we show that RAG techniques significantly enhance the ability of LLMs to interpret and respond to complex spectrum policy queries, demonstrating their potential to make these important documents more accessible.},
  month = {may},
}

@inproceedings{lin_novel_2024,
  title = {Novel {Preprocessing},
  author = {Lin, Yu-Chen and Kumar, Akhilesh and Chang, Norman and Zhang, Wenliang and Zakir, Muhammad and Apte, Rucha and He, Haiyang and Wang, Chao and Jang, Jyh-Shing Roger},
  year = {2024},
  doi = {10.1109/LAD62341.2024.10691715},
  url = {https://doi.org/10.1109/lad62341.2024.10691715},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE LLM …},
  pages = {1--5},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {code generation, Codes, Conferences, data preprocessing, data renovation, data splitter, domain-specific, Information retrieval, Knowledge engineering, Large language models, Large language models (LLMs), MapReduce, prompt engineering, RedHawk-SC (RH-SC), Retrieval-Augmented Generation (RAG), Semantics, Software, source: IEEE, source: Google Scholar},
  abstract = {We introduce four principal contributions to augment the capabilities of Large Language Models (LLMs) in generating domain-specific code: (i) leveraging LLM-based data splitting and data renovation techniques to refine the semantic representation within the embedding space; (ii) proposing an effective method for refactoring existing scripts, enabling the generation of new and high-quality scripts with the aid of LLMs; (iii) developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt technique; and (iv) showcasing the efficacy of our data pre-processing approach through a case study using engineering simulation software RedHawk-SC. Our contributions collectively advance the Retrieval-Augmented Generation (RAG) framework, enabling more relevant and precise information retrieval. An arena-style evaluation by 28 domain experts and 182 votes confirms the significant effectiveness of our methods. Notably, our approach achieves up to 1.43 times the improvement in code generation for MapReduce applications compared to the Chain-of-Thought (CoT) technique.},
  month = {jun},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{zhou_exploring_2024,
  title = {Exploring the {Application},
  author = {Zhou, Lijie and Yan, Shoujun and Li, Zhongjie and Ma, Jie},
  year = {2024},
  doi = {10.1109/CSIS-IAC63491.2024.10919351},
  url = {https://doi.org/10.1109/csis-iac63491.2024.10919351},
  booktitle = {2024 {International},
  pages = {664--669},
  keywords = {Aging, Best practices, Boosting, Complex systems, Defense Technology Intelligence, Focusing, Large language models, Large Language Models, National security, Retrieval augmented generation, Retrieval-Augmented Generation, Security, Systematics, source: IEEE},
  abstract = {Large language models (LLMs) are impeded by issues such as spurious generation, knowledge obsolescence, and domain expertise deficiency, which constrain their efficacy in defense technology intelligence applications. Mitigating these challenges is imperative for enhancing the utility of these models in this domain. Grounded in the core requirements of defense technology intelligence, this paper provides an incisive analysis of the significance of Retrieval-Augmented Generation (RAG) in boosting the performance and applicability of LLMs. The paper explores the primary scenarios and deployment paradigms for RA G in defense technology intelligence. Additionally, it synthesizes the technical obstacles encountered during implementation and elucidates the countermeasures devised to overcome them. Our research demonstrates that RAG technology can significantly enhance the efficiency, precision, relevance, and timeliness of intelligence gathering, enabling LLMs to better meet the demands of defense technology intelligence. These findings highlight the promising potential of RAG in augmenting the capabilities of language models for critical defense applications, paving the way for future advancements in this emerging field.},
  month = {sep},
}

@inproceedings{de_castro_human---loop_2024,
  title = {Human-in-the-loop knowledge base upkeep for retrieval augmented generation applications},
  author = {de Castro, Pedro Baptista and Sukeda, Hiroko and Takashige, Soichi},
  year = {2024},
  doi = {10.1109/ISM63611.2024.00053},
  url = {https://doi.org/10.1109/ism63611.2024.00053},
  booktitle = {2024 {International},
  pages = {232--233},
  keywords = {Grounding, Human in the loop, human-in-the-loop, knowledge base upkeep, Knowledge based systems, Large language models, Noise, retrieval augmented generation, Retrieval augmented generation, Semantics, Visualization, source: IEEE},
  abstract = {Retrieval augmented generation (RAG) has been proposed as a way of grounding the large language models (LLMs) with factual and updated sources of information. However, the quality of the chunks in the knowledge base is important. In this work, we propose a human-in-the-loop knowledge base visualization and upkeep system that aims to maintain the quality of the knowledge base by preassigning tags to chunks based on their structural forms, rather than semantics. This allows a human reviewer to quickly identify pieces of chunks that could be considered noise, and easily remove unwanted chunks that will naturally come, due to the process of document chunking in RAG.},
  month = {dec},
}

@inproceedings{curto_building_2025,
  title = {Building {Software},
  author = {Curto, Hayala Nepomuceno},
  year = {2025},
  doi = {10.1109/RE63999.2025.00077},
  url = {https://doi.org/10.1109/re63999.2025.00077},
  booktitle = {2025 {IEEE},
  pages = {612--616},
  note = {ISSN: 2332-6441},
  keywords = {AI in Software Engineering, Iterative methods, Large language models, Large Language Models, Medical services, Multi-agent systems, Multi-Agent Systems, Project management, Requirements engineering, Requirements Engineering, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Software engineering, Software quality, source: IEEE},
  abstract = {This work explores the automated elicitation of functional software requirements using Large Language Models (LLMs). The proposed approach employs multiple LLMs in conjunction with Retrieval-Augmented Generation (RAG) across iterative refinement rounds. This process enables the generation of candidate requirements from high-level software descriptions, with validation through similarity analysis. The method addresses key challenges in current AI-based elicitation approaches, including lack of iterative refinement, hallucination control, and semantic convergence. Preliminary results demonstrate that multi-round feedback and cross-model integration improve alignment with expert-defined requirements. This study marks the initial phase of a broader doctoral research project titled Agent Family for Software Engineering Teams, which aims to develop a set of intelligent agents to support various stages of the software engineering lifecycle, including requirement elicitation, project management, and software quality, ultimately forming a cohesive and responsible AI ecosystem for software teams.},
  month = {sep},
}

@inproceedings{sarnikar_using_2025,
  title = {Using {LLMs},
  author = {Sarnikar, Arhanth},
  year = {2025},
  doi = {10.1109/SCEECS64059.2025.10940780},
  url = {https://doi.org/10.1109/sceecs64059.2025.10940780},
  booktitle = {2025 {IEEE},
  pages = {1--5},
  note = {ISSN: 2688-0288},
  keywords = {source: IEEE},
  abstract = {With over 2,000 bills being passed in the California State Legislature each year, it can be very confusing for the average citizen to understand what is being approved. This process can be made much easier with recent advancements to large language models (LLMs). Specifically, using Retrieval Augmented Generation (RAG). This research paper will study two different approaches to using LLMs to answer questions about bills passed in our state legislature. The first approach will employ standard RAG protocol to scan and retrieve any information relevant to the user’s prompt and summarize it in an easy-to-read manner. Our second approach expands the context window of the model to consider a wider range of legislature when scanning for keywords entered by the user. By analyzing the performance of each LLM, we can find the best model that guarantees contextually rich answers to any questions surrounding the state legislature.},
  month = {jan},
}

@inproceedings{obaid_seerahgpt_2024,
  title = {{SeerahGPT},
  author = {Obaid, Surayya and Bawany, Narmeen Zakaria},
  year = {2024},
  doi = {10.1109/ICOSST64562.2024.10871159},
  url = {https://doi.org/10.1109/icosst64562.2024.10871159},
  booktitle = {2024 18th {International},
  pages = {1--7},
  note = {ISSN: 2770-8225},
  keywords = {source: IEEE},
  abstract = {Large Language Models(LLMs) have become widely recognized in recent years for their exceptional performance in language generation capabilities. As a result, an unprecedented rise is seen in its use cases in various domains specifically involving Natural Language Processing(NLP). These models however perform suboptimally when exploited in the field of studies where authenticity of the generated content is a critical aspect. One such domain is usage of LLM for exploration of the life of Prophet Muhammad S.A.W(commonly referred to as Seerah). It is of utmost significance to ensure the authenticity and reliability in the sources used and reported by the LLM due to the sensitive nature of the domain. The contemporary LLMs, however, lack the explainability in their response due to their inherent black-box nature. In our study, we have presented a novel LLM named SeerahGPT that addresses this challenge with the help of retrieval-augmented generation (RAG). This technique enables the model to utilize both parametric and nonparametric memories for generating response of queries. Our model, built on the Llama-2-7b architecture, employs Sentence Transformer embedding to effectively retrieve relevant information. The model's capabilities are augmented by integrating it with a corpus having Islamic texts such as the Quranic translation and Hadith collections, and historical accounts. The model's performance is benchmarked against its base model using both quantitative and qualitative metrics. The comparative analysis with Llama-2-7b revealed that SeerahGPT incorporation with external knowledge sources, provided more authentic and verifiable responses, despite the others exhibiting greater fluency. Performance metrics such as BLEU, ROUGE, and METEOR indicated SeerahGPT's better accuracy and contextual handling. This study paves way for analysis of such sensitive domains in more efficient way that can be utilized in other complex domains such as Islamic theology and Fiqh or legal system.},
  month = {dec},
}

@inproceedings{huang_medeeprag_2025,
  title = {{MedeepRAG},
  author = {Huang, Hoying},
  year = {2025},
  doi = {10.1109/ICETCI64844.2025.11083988},
  url = {https://doi.org/10.1109/icetci64844.2025.11083988},
  booktitle = {2025 {IEEE},
  pages = {146--150},
  keywords = {Biological system modeling, Biomedical tasks, Data processing, DeepSeek, Heuristic algorithms, Large language models, Large Language Models, Medical diagnostic imaging, Natural language processing, Oral communication, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Stability analysis, source: IEEE},
  abstract = {Large Language Models (LLMs) represent a major breakthrough in Artificial Intelligence (AI), with models like DeepSeek from China demonstrating effectiveness in various natural language processing (NLP) tasks. These models, trained on large datasets, capture the intricate relationships between words in textual data. Retrieval-Augmented Generation (RAG) is a novel approach that combines retrieval-based and generation-based models to enhance text generation quality. However, LLMs have yet to achieve optimal performance in biomedical tasks, where domain-specific expertise is crucial. To address this gap, this paper proposes MedeepRAG, a model built on the distilled version of DeepSeek-R1-Distill-1.5B. MedeepRAG is designed to balance inference power and deployment efficiency with only 1.5 billion parameters. It is trained on the Medical-R1-Distill-Data-Chinese, a high-quality, structurally labeled Chinese medical dataset. Experimental results show that MedeepRAG produces responses with more reliable medical knowledge, supporting the integration of Artificial Intelligent (AI) into the medical field for enhanced Question and Answer (Q\&A) in clinical settings. Future work will focus on clinical validation, multimodal data processing, and improving nested medical concept handling.},
  month = {may},
}

@inproceedings{zhang_large_2025,
  title = {A {Large},
  author = {Zhang, Qi and Zhang, Zhi},
  year = {2025},
  doi = {10.1109/MRAI65197.2025.11135758},
  url = {https://doi.org/10.1109/mrai65197.2025.11135758},
  booktitle = {2025 {International},
  pages = {359--362},
  keywords = {Accuracy, Fault diagnosis, Knowledge graph, large language model, Large language models, Mechatronics, Power system faults, Predictive models, Question answering (information retrieval), RAG, Retrieval augmented generation, Robots, Soft sensors, source: IEEE},
  abstract = {This paper aims to construct a knowledge graph-enhanced power system fault diagnosis question-answering system based on Retrieval-Augmented Generation (RAG), addressing challenges in power equipment fault diagnosis such as fragmented data sources, over-reliance on expert experience, low diagnostic accuracy, and insufficient intelligent capabilities. By integrating large language models (LLMs) with domain-specific knowledge bases, the proposed system leverages the open-source framework RAGFlow to retrieve relevant information and generate diagnostic recommendations, thereby enhancing the model’s domain expertise and prediction accuracy to deliver an intelligent solution for power system fault diagnosis. Experimental validation demonstrates that the system achieves significant improvements in diagnostic efficiency and accuracy.},
  month = {jun},
}

@inproceedings{hecking_architecture_2025,
  title = {An {Architecture},
  author = {Hecking, Tobias and Sommer, Thorsten and Felderer, Michael},
  year = {2025},
  doi = {10.1109/ICSA-C65153.2025.00012},
  url = {https://doi.org/10.1109/icsa-c65153.2025.00012},
  booktitle = {2025 {IEEE},
  pages = {31--35},
  note = {ISSN: 2768-4288},
  keywords = {Buildings, Computer architecture, Data models, Data protection, Distributed Systems, Knowledge based systems, Large language models, Large Language Models, Protocols, Retrieval augmented generation, Retrieval Augmented Generation, Software architecture, Software systems, source: IEEE},
  abstract = {Retrieval-augmented generation (RAG) has become a widely adopted approach for the integration of knowledge bases and large language models (LLMs). This paper proposes a decentralized software architecture for RAG, where retrieval, augmentation, and generation components are operated independently by distributed entities. This approach addresses challenges like efficient usage of resources when building LLM-based software systems as well as data protection issues in centralized systems. By allowing data providers to implement their own retrieval mechanisms, they fully retain control over data access and the flexibility to use their own data infrastructure. An interaction platform implements communication between client applications, data providers, and model providers. To standardize this process, the paper introduces introduces an architecture and protocol called External Retrieval Interface (ERI), which ensures compatibility of services, enforces data restrictions, and simplifies the development of decentralized RAG systems.},
  month = {mar},
}

@inproceedings{joldea_multimodal_2025,
  title = {Multimodal {AI},
  author = {Joldea, Andrei-Răzvan and Cernăzanu-Glăvan, Diana and Sârbu, Vlad and Bulzan, Andrei-Ştefan},
  year = {2025},
  doi = {10.1109/SACI66288.2025.11030141},
  url = {https://doi.org/10.1109/saci66288.2025.11030141},
  booktitle = {2025 {IEEE},
  pages = {000189--000194},
  note = {ISSN: 2765-818X},
  keywords = {Assistant, Chatbot, Chatbots, Computational modeling, Domain-specific fine-tuning, Informatics, Information retrieval, Large language models, LLM, Question answering (information retrieval), RAG, Retrieval augmented generation, Romanian, Software development management, Speech to text, STT, Text to speech, TTS, source: IEEE},
  abstract = {The era of large language models (LLMs) brings forth a new wave of automation to many fields of activity. In this work we employ the AI advancements catalyzed by these LLMs to create a smart university assistant. A chatbot that comes to assist university enrolled students and staff on administrative, legislative and public interest topics. To this end, we develop a platform that combines Large Language Models, Retrieval Augmented Generation, Speech-to-Text and Text-to-Speech technologies to automate accessibility to university-related information. We start from openly available models and resources, adapt and finetune them to our target - Romanian question answering with information retrieval - and then release our solutions publicly at https://github.com/Andrei481/RomanianChatbot.},
  month = {may},
}

@inproceedings{agrawal_mindful-rag_2024,
  title = {Mindful-{RAG},
  author = {Agrawal, Garima and Kumarage, Tharindu and Alghamdi, Zeyad and Liu, Huan},
  year = {2024},
  doi = {10.1109/FLLM63129.2024.10852457},
  url = {https://doi.org/10.1109/fllm63129.2024.10852457},
  booktitle = {2024 2nd {International},
  pages = {607--611},
  keywords = {Accuracy, Cognition, Hallucinations, Knowledge graphs, Knowledge Graphs (KG), Large language models, Law, LLMs, Medical services, Points of Failure, Reliability, Retrieval augmented generation, Retrieval Augmented Generation (RAG), source: IEEE},
  abstract = {Large Language Models (LLMs) excel at generating coherent text but often struggle with knowledge-intensive queries, particularly in domain-specific and factual question-answering tasks. Retrieval-augmented generation (RAG) systems have emerged as a promising solution by integrating external knowledge sources, such as structured knowledge graphs (KGs). While KG-based RAG approaches have demonstrated value, current state-of-the-art solutions frequently fall short, failing to deliver accurate and reliable answers even when the necessary factual knowledge is available. In this paper, we present a critical analysis of failure points in existing KG-based RAG methods, identifying eight key areas of concern, including misinterpretation of question context, incorrect relation mapping, and ineffective ambiguity resolution. We argue that these failures primarily stem from design limitations in current KG-RAG systems, such as inadequate attention to discerning user intent and insufficient alignment of retrieved knowledge with the contextual demands of the query. Based on this analysis, we propose a new approach for KG-RAG systems, termed Mindful-RAG, which re-engineers the retrieval process to be more intent-driven and contextually aware. By enhancing reasoning capabilities, improving constraint identification, and addressing the structural limitations of knowledge graphs, we aim to improve the reliability and effectiveness of KG-RAG systems. To validate this approach, we developed a proof-of-concept by integrating the principles of Mindful-RAG into an existing KG-RAG system. The Mindful-RAG approach seeks to deliver more robust, accurate, and contextually aligned AI-driven knowledge retrieval systems, with potential applications in critical domains such as healthcare, legal, research, and scientific discovery, where precision and reliability are paramount.},
  month = {nov},
}

@inproceedings{li_deepseek-med-8b_2025,
  title = {{DeepSeek},
  author = {Li, Chenxing and Mao, Jiawei and Liu, Bin and Luo, Wei},
  year = {2025},
  doi = {10.1109/CISAT66811.2025.11181817},
  url = {https://doi.org/10.1109/cisat66811.2025.11181817},
  booktitle = {2025 8th {International},
  pages = {240--244},
  keywords = {Grounding, Healthcare Natural Language Processing, Information science, Large language models, Large Language Models, Medical diagnostic imaging, Medical services, Natural language processing, Quantization (signal), Real-time systems, Reinforcement learning, Reinforcement Learning with AI and Human Feedback, Retrieval augmented generation, Retrieval-Augmented Generation, Supervised Fine-Tuning, source: IEEE},
  abstract = {The uneven distribution of medical resources in China poses significant challenges, especially in rural areas. While large language models (LLMs) offer potential for clinical support, existing systems like GPT-4 and Med-PaLM suffer from hallucinations, English-centric biases, and lack real-time physician integration. We present DeepSeek-Med-8B, a Chinese medical conversational agent based on the DeepSeek-R1-DistillLlama-8B architecture.DeepSeek-Med-8B is trained through: (i) Supervised Fine-Tuning (SFT) on curated Chinese medical corpora; (ii) Reinforcement Learning with AI and Doctor Feedback (RLAIF) for factuality, empathy, and referral quality; and (iii) Retrieval-Augmented Generation (RAG) for real-time grounding in physician databases.Across eight clinical tasks, DeepSeek-Med-8B achieves a top1 mean score of 66.9 on GPT-4o-based benchmarks and a 74\% top-3 doctor match rate, outperforming rule-based baselines. The model runs efficiently on a single RTX 4090 GPU via INT8 quantization.},
  month = {jul},
}

@inproceedings{zhang_dynamic_2024,
  title = {A {Dynamic},
  author = {Zhang, Yanjun and Li, Dapeng and Peng, Gaojun and Guo, Shuang and Dou, Yu and Yi, Ruheng},
  year = {2024},
  doi = {10.1109/IALP63756.2024.10661194},
  url = {https://doi.org/10.1109/ialp63756.2024.10661194},
  booktitle = {2024 {International},
  pages = {372--376},
  note = {ISSN: 2159-1970},
  keywords = {Accuracy, border inspection LQA, Filling, Inspection, large language model, Law, Meteors, Question answering (information retrieval), retrieval-augmented generation, source: IEEE},
  abstract = {Border inspection legal question answering (LQA) is designed for specific legal scenarios, aiming to address legal questions related to border inspections and provide accurate and practical legal guidance to the public and border inspection departments. Current research requires a retrieval process regardless of whether the necessary knowledge already exists, and does not evaluate the legal information in the generated responses, leading to errors and low-quality content. we propose the Dynamic Retrieval-Augmented Generation framework for Border Inspection LQA (DRAG-BILQA), which generates responses first and then dynamically controls retrieval based on confidence scores. In this framework, we innovatively introduce a legal factor recognition module to ensure the legal accuracy of generated answers and improve the quality of the generation. We also present the BorderLegal-QA dataset, filling the gap in border inspection legal datasets. We conducted extensive experiments on both the BorderLegal-QA and the general LQA dataset JEC-QA. Experimental results show that DRAG-BILQA outperforms Sota models in terms of legal factor recognition accuracy and retrieval performance, achieving a METEOR score of 39.5 and improving generation quality. Case analysis also demonstrate that DRAG-BILQA effectively Augmented the legal accuracy of the generated content.},
  annote = {Cited by: 1},
  month = {aug},
}

@inproceedings{joshi_robust_2024,
  title = {Robust {Multi},
  author = {Joshi, Pankaj and Gupta, Aditya and Kumar, Pankaj and Sisodia, Manas},
  year = {2024},
  doi = {10.1109/ICAAIC60222.2024.10574972},
  url = {https://doi.org/10.1109/icaaic60222.2024.10574972},
  booktitle = {2024 3rd {International},
  pages = {993--999},
  keywords = {Accuracy, Analytical models, Computational modeling, GenAI(Generative AI), Knowledge based systems, LLM(Large Language Models), MuRAG(Multimodal Retrieval Augmented Generation), Pipelines, RAG(Retrieval Augmented Generation), Testing, source: IEEE},
  abstract = {RAG (Retrieval Augmented Generation) is generally used for generating results from the existing knowledge-base. RAG refers to finding references (R), Adding references (A) and improving generation(i.e, answers to the question) (G). MultiModel-RAGs are used for generation of results over the documents which contain images and texts. There exists multiple different Multimodel-RAGs but these are not still efficient in generation of the results from the documents which contain relationships between images and texts. This study has proposed the solution to enable effective retrieval and generation of results, which includes the relationship between images and texts. The comparison of proposed Multimodal RAG with four different datasets (i.e., Short-form-type-QA, Long-form-type-QA, MCQ-type-QA, True-False-type-QA) shows the proposed solution improves the effectiveness of the existing Multimodal RAGs. Testing of proposed Multimodal RAG over two different other multimodal LLM i.e, Open-AI \& Gemini helps in deciding whether the proposed solution fits best with LLM in different cases.},
  month = {jun},
}

@inproceedings{viboonsang_leveraging_2025,
  title = {Leveraging {Large},
  author = {Viboonsang, Pranodnard and Kosolsombat, Somkiat and Ratanavilisagul, Chiabwoot},
  year = {2025},
  doi = {10.1109/ICMSS64503.2025.00028},
  url = {https://doi.org/10.1109/icmss64503.2025.00028},
  booktitle = {2025 9th {International},
  pages = {68--73},
  keywords = {Accuracy, Chatbots, Computer security, Context modeling, Cybersecurity, Generative AI, Knowledge based systems, Large language models, Large Language Models (LLMs), Refining, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Translation, source: IEEE},
  abstract = {Cybersecurity operations in Thailand face evolving challenges, including a surge in data volume and diverse threats that exceed human analytical capacity. This study explores the potential of Large Language Models (LLMs) to augment Thai-language cybersecurity analysis and response. Using OpenThaiGPT integrated with Retrieval-Augmented Generation (RAG), we developed a generative AI chatbot tailored for cybersecurity inquiries. By leveraging a curated dataset from authoritative sources, the results demonstrate that the RAG-integrated model significantly improves response accuracy and relevance compared to the base model. However, limitations such as dataset size, translation challenges, and hallucination risks highlight areas for further research. Proposed future work includes expanding datasets, refining translation processes, and exploring alternative LLM architectures. This study establishes a foundational framework for deploying LLMs in Thai cybersecurity, contributing to the development of effective AI-driven solutions for cybersecurity landscape in Thailand.},
  month = {jan},
}

@inproceedings{bhatia_optimization_2024,
  title = {Optimization {Techniques},
  author = {Bhatia, Tushar and Gupta, Ashish and Varshney, Dhruv and Mutreja, Tvisha and Kumar, Sumit and Vijh, Surbhi},
  year = {2024},
  doi = {10.1109/EmergIN63207.2024.10961084},
  url = {https://doi.org/10.1109/emergin63207.2024.10961084},
  booktitle = {2024 {International},
  pages = {497--502},
  keywords = {Accuracy, AI in Journalism, Content Personalization, Large language models, Large Language Models (LLMs), Natural language processing, Natural Language Processing (NLP), News Automation, News Report Generation, Optimization, Personalization in Content Generation, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Sustainable development, Technological innovation, Text Coherence and Accuracy, Writing, Zero shot learning, source: IEEE},
  abstract = {This paper aims to explore optimization techniques in Large Language Models (LLMs) for automated news report generation, utilizing methods such as Retrieval-Augmented Generation (RAG), Zero-Shot Learning, ICL (In Context Learning), and reAct agents. Using these techniques, the system collects real-time information based on the user context and the images from the internet, to improve factual accuracy and enhance reliability. Various LLMs such as Llama 3.1, Llama 3.2, Qwen 2.5, Gemma, LLaVa, Aya were utilized in the system to generate the News Reports. Among these, Llama3.1 demonstrated the best performance with Context Precision as 92\%, Answer Relevancy as 90.84 \% and Faithfulness as 12.5\%, closely followed by Qwen 2.5, which showed a notable improvement in Faithfulness as 43\%. Fine-tuning the models can improve the various metrics such as Context Precision, Recall, Answer Relevancy, BLEU and ROUGE Scores as well as improve the adoption to various writing styles as seen in News Reports.},
  month = {dec},
}

@inproceedings{ahmed_quality_2025,
  title = {Quality {Assurance},
  author = {Ahmed, Bestoun S. and Baader, Ludwig Otto and Bayram, Firas and Jagstedt, Siri and Magnusson, Peter},
  year = {2025},
  doi = {10.1109/ICSTW64639.2025.10962487},
  url = {https://doi.org/10.1109/icstw64639.2025.10962487},
  booktitle = {2025 {IEEE},
  journal = {… on Software Testing …},
  pages = {200--207},
  note = {ISSN: 2159-4848},
  keywords = {AI Quality Assurance, Extra-Functional Properties, Large language models, Large Language Models (LLM), ML System Testing, Quality assurance, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantics, Software Quality Testing, Standards organizations, Syntactics, System performance, System testing, Systematics, Temperature, source: IEEE, source: Google Scholar},
  abstract = {This paper presents a comprehensive framework for testing and evaluating quality characteristics of Large Language Model (LLM) systems enhanced with Retrieval-Augmented Generation (RAG) in tourism applications. Through systematic empirical evaluation of three different LLM variants across multiple parameter configurations, we demonstrate the effectiveness of our testing methodology in assessing both functional correctness and extra-functional properties. Our framework implements 17 distinct metrics that encompass syntactic analysis, semantic evaluation, and behavioral evaluation through LLM judges. The study reveals significant information about how different architectural choices and parameter configurations affect system performance, particularly highlighting the impact of temperature and top-p parameters on response quality. The tests were carried out on a tourism recommendation system for the Varmland region in Sweden, utilizing standard and RAG-enhanced configurations. The results indicate that the newer LLM versions show modest improvements in performance metrics, though the differences are more pronounced in response length and complexity rather than in semantic quality. The research contributes practical insights for implementing robust testing practices in LLM-RAG systems, providing valuable guidance to organizations deploying these architectures in production environments.},
  month = {mar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{xu_chat_2024,
  title = {A {Chat},
  author = {Xu, Liwei and Liu, Jiarui},
  year = {2024},
  doi = {10.1109/IWCEAA63616.2024.10823979},
  url = {https://doi.org/10.1109/iwceaa63616.2024.10823979},
  booktitle = {2024 8th {International},
  pages = {125--129},
  note = {Type: Conference paper},
  keywords = {Accuracy, Chatbots, Context modeling, Large Language Model (LLM), Large language models, Maintenance, Pollution, Prompt Engineering, Question answering (information retrieval), Retrieval augmented generation, Retriever\_Augmented Generation (RAG), Search engines, Web and internet services, Web search, source: IEEE, source: Scopus},
  abstract = {Recent advancements in large language models (LLMs) have established pre-training on extensive textual cor-pora as a foundational methodology. However, in specialised applications such as admissions systems, the focus shifts from general knowledge-based reasoning to ensuring accuracy and relevance in domain-specific responses. This study presents the development of an automated admissions system for Xi'an Jiaotong-Liverpool University, leveraging GLM-4 in conjunction with Retrieval-Augmented Generation (RAG) to handle targeted queries. The implementation of RAG mitigates the occurrence of hallucinations often seen in LLM outputs, thereby enhancing the reliability and alignment of generated responses with real-world data, which is critical for prospective students and their parents. This paper details the construction of the RAG corpus and cue word methodology, and provides an empirical comparison of the efficacy of various major language models with and without RAG integration. The results demonstrate the potential of RAG to significantly improveresponse accuracy in domain-specific tasks, and suggest directions for future research in optimising LLMs for admissions processes.},
  month = {nov},
  annote = {Cited by: 2},
}

@inproceedings{zeeshan_rag_2025,
  title = {{RAG},
  author = {Zeeshan, Hafiz Muhammad Ali and Faizan, Muhammad and Zia, Usman and Gohar, Abdullah},
  year = {2025},
  doi = {10.1109/ComTech65062.2025.11034531},
  url = {https://doi.org/10.1109/comtech65062.2025.11034531},
  booktitle = {2025 {International},
  pages = {1--6},
  note = {ISSN: 2996-3621},
  keywords = {Codes, Cognition, Communications technology, Databases, Large language models, Large Language Models (LLM), Market research, Question answering (information retrieval), Question Answering (QA), RAG Applications and Challenges, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Reviews, Text summarization, source: IEEE},
  abstract = {Large language models (LLMs) are evolving to excel in challenging tasks such as text generation, mathematical reasoning, code generation, question answering, text summarization, etc. However, the responses generated by LLMs are prone to hallucinations, out-of-the-date knowledge, and nontransparent and untraceable reasoning. Retrieval augmented generation (RAG) addresses these shortcomings by incorporating external knowledge in the LLM prompt. RAG combines parametric knowledge of LLM with non-parametric knowledge from external databases by efficient retrieval techniques. This comprehensive review paper provides a detailed overview of the significance and emergence of RAG. Moreover, the building blocks of RAG are thoroughly discussed along with the evolution, challenges, and current research trends of deploying RAG-based applications. Finally, we explore the RAG performance enhancement strategies while also underlining the potential future research directions.},
  month = {apr},
}

@inproceedings{dong_how_2025,
  title = {How to {Build},
  author = {Dong, Chenxi and Yuan, Yimin and Chen, Kan and Cheng, Shupei and Wen, Chujie},
  year = {2025},
  doi = {10.1109/ICEIT64364.2025.10975937},
  url = {https://doi.org/10.1109/iceit64364.2025.10975937},
  booktitle = {2025 14th {International},
  pages = {152--157},
  keywords = {Adaptation models, Artificial intelligence, Biological system modeling, Education, Generative AI, Information technology, Intelligent Tutoring Systems, Knowledge graphs, Large language model, Large language models, Measurement, Retrieval augmented generation, Retrieval-Augmented generation, Semantics, source: IEEE},
  abstract = {Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems (ITS) presents transformative opportunities for personalized education. However, current implementations face two critical challenges: maintaining factual accuracy and delivering coherent, context-aware instruction. While Retrieval-Augmented Generation (RAG) partially addresses these issues, its reliance on pure semantic similarity limits its effectiveness in educational contexts where conceptual relationships are crucial. This paper introduces Knowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel framework that integrates structured knowledge representation with context-aware retrieval to enable more effective AI tutoring. We present three key contributions: (1) a novel architecture that grounds AI responses in structured domain knowledge, (2) empirical validation through controlled experiments (n=76) demonstrating significant learning improvements (35\% increase in assessment scores, p{\textless},
  month = {mar},
}

@inproceedings{guntupalli_integrating_2024,
  title = {Integrating {Generative},
  author = {Guntupalli, Jayesh and Watanabe, Kentarou},
  year = {2024},
  doi = {10.1109/ETFA61755.2024.10710979},
  url = {https://doi.org/10.1109/etfa61755.2024.10710979},
  booktitle = {2024 {IEEE},
  pages = {1--4},
  note = {ISSN: 1946-0759},
  keywords = {Generative AI, Large Language Model, Manufacturing automation, Pipelines, Prototypes, Retrieval Augmented Generation, Streaming media, System analysis and design, System Design, System Engineering, source: IEEE},
  abstract = {This work delves into the potential of applying Generative AI techniques to the system design phase, aiming to streamline processes and augment human expertise. Gen AI is used to automate the creation of complex design elements and is emerging as a powerful tool for system engineers. Also, with LLM -generated content, evaluating and verifying the correctness of the responses is a challenge. The SE Assistant designed for system engineers intends to create detailed system design documents quickly and accurately along with its evaluation. At the core of the SE Assistant is a sophisticated system that combines the power of GPT-4 with a Multimodal Retrieval Augmented Generation (RAG) pipeline capable of understanding text, images, and tables to provide valuable context. The evaluation uses the strength of strong LLMs in analyzing content based on design-specific criteria. The SE Assistant prototype demonstrates its ability to streamline the system design process, from initial data gathering to the final design output, making it an invaluable tool for system engineers.},
  month = {sep},
}

@article{dangsungnoen_opg-spell_2025,
  title = {{OPG},
  author = {Dangsungnoen, Lapatrada and Thongprasant, Kwansawan and Wiratchawa, Kannika and Hirunchavarod, Natthanich and Sributsayakarn, Natnicha and Pornprasertsuk-Damrongsri, Suchaya and Jirarattanasopha, Varangkanar and Piumsomboon, Thammathip and Intharah, Thanapong},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3600956},
  url = {https://doi.org/10.1109/access.2025.3600956},
  journal = {IEEE Access},
  volume = {13},
  pages = {146690--146706},
  keywords = {Accuracy, Artificial intelligence, Brain modeling, Dentistry, Diagnostic radiography, Explainable AI, Explainable AI (XAI), model trust, model understanding, orthopantomogram, Predictive models, Reliability, Retrieval augmented generation, Visualization, source: IEEE},
  abstract = {As AI models become increasingly powerful, the field of Explainable AI (XAI) has gained significant attention. However, the effectiveness of explanations often varies depending on the audience’s domain knowledge. This paper introduces OPG-SPELL (understanding of OrthoPantomoGram’s artificial intelligence prediction through ShaPlEy value and LLms text generation), an approach combining visual and textual explanations to enhance understanding and trust in AI models. OPG-SPELL focuses on an AI model trained to classify sex based on orthopantomograms (panoramic radiographs of human jaw structure). Our method integrates visual explanations using the OPG-SHAP technique with textual explanations generated through Large Language Models’ Retrieval-Augmented Generation. This dual-modality approach aims to bridge the gap between expert and non-expert users, making AI decisions more interpretable across diverse audiences. We conducted a user study with 24 participants to evaluate the influence of both visual and textual explanations on understanding, trust, and willingness to use the XAI tool. Our findings demonstrate the potential of OPG-SPELL to enhance user comprehension and confidence in AI-driven sex prediction from orthopantomograms, contributing to the broader field of intelligent user interfaces and Explainable AI.},
  issn = {2169-3536},
}

@inproceedings{ran_driver-guide_2025,
  title = {Driver-{Guide},
  author = {Ran, Yabing and Gao, Bingzhao and Yu, Qiankun},
  year = {2025},
  doi = {10.23919/CCC64809.2025.11179498},
  url = {https://doi.org/10.23919/ccc64809.2025.11179498},
  booktitle = {2025 44th {Chinese},
  pages = {8670--8675},
  note = {ISSN: 1934-1768},
  keywords = {Accuracy, Autonomous Driving, Autonomous vehicles, Human computer interaction, Knowledge based systems, Large language models, Multimodal Large Language Model, Question answering (information retrieval), Retrieval augmented generation, Scene understanding, Semantics, Vehicles, source: IEEE},
  abstract = {The research proposes a driving scene understanding agent based on a Multimodal Large Language Model (MLLM) to enhance scene understanding and human-machine interaction capabilities in autonomous driving. A high-quality multimodal driving scene dataset has been constructed, covering tasks such as scene description, question answering, and action suggestions. Two types of multimodal feature mappers (MLP with two hidden layers and Q-Former) were compared, and experiments demonstrated the superior performance of Q-Former in complex scene. Additionally, by incorporating a traffic rules knowledge base and Retrieval-Augmented Generation (RAG), the system effectively reduces hallucination phenomena, ensuring the accuracy and compliance of model responses. Experimental results indicate that the proposed system excels in complex scene semantic understanding and interaction tasks, offering a novel solution to address the limitations of traditional methods in complex driving scene understanding.},
  month = {jul},
}

@inproceedings{patra_tailored_2024,
  title = {Tailored {Resume},
  author = {Patra, Komal Prakashchandra and Diwakar, Manoj and Arya, Chandrakala},
  year = {2024},
  doi = {10.1109/PDGC64653.2024.10984036},
  url = {https://doi.org/10.1109/pdgc64653.2024.10984036},
  booktitle = {2024 {Eighth},
  pages = {848--853},
  note = {ISSN: 2573-3079},
  keywords = {BERTscore, Computational modeling, Conversational Memory, Internet, Job specification, Large Language Model (LLM), Large language models, LLAMA, Memory, Mixtral, Performance evaluation, Prompt engineering, Prompt Engineering, RAGAS, Refining, Resume Generation, Resumes, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Vectors, source: IEEE},
  abstract = {The traditional resume being common for each specific job applications which result to rejection for job seekers. These research tailors the resume based on different domain of job applications using Retrieved Augmented Generation (RAG) and LLMs. The automated system starts from resume parsing for information retrieval as per the sections using LLM model like Mixtral, Google Gemma and LLAMA-3. The resume has been further stored into vector databases such as Pinecone and MongoDB with JSON resume further retrieved using re-ranking methods such as BM25 and Cross encoder. The prompt techniques used to construct the model to make them understand better. For providing the memory to the LLM models and refining the tailored resume, the conversational buffer memory is implemented. The efficiency and the performance of the methodology is showcase through the performance evaluation metrics such as BERTscore, RAGAs Metrics and Custom metrics such as Content preservation and Job Alignment. By comparing the cosine similarity for content preservation between LLM and RAG with LLM are 72\% and 89\% respectively. Compared to LLM models using RAG with LLM models generates consistent and relevant tailored resumes. For a better user friendly and seamless UI, the chatbot is developed.},
  month = {dec},
}

@inproceedings{hussain_mitigating_2025,
  title = {Mitigating {Values},
  author = {Hussain, Waqar},
  year = {2025},
  doi = {10.1109/RAIE66699.2025.00006},
  url = {https://doi.org/10.1109/raie66699.2025.00006},
  booktitle = {2025 {IEEE},
  pages = {9--12},
  keywords = {AI Bias, AI Safety, Ethical AI, Ethics, Generative AI, Hallucination, HHH Framework, Industries, Medical services, Reliability engineering, Responsible AI, Safety, Shape, Symbiosis, Systems engineering and theory, Transportation, Values Debt, source: IEEE},
  abstract = {Generative AI technologies are rapidly transforming industries such as healthcare, education, and transportation. However, this progress often incurs a Values Debt-ethical and operational deficits due to insufficient ethical considerations during development. This paper examines Values Debt in Gen-erative AI and introduces the Helpful, Honest, Harmless (HHH) framework to align AI systems with human values. In developing GRAISE, a Graph RAG-based chatbot for aviation safety, the HHH framework is applied to integrate ethical practices through-out the development process. This case study demonstrates how the HHH framework addresses ethical challenges and provides reliable contextual information to enhance pilot communication, exemplifying responsible AI engineering. These findings advocate for the broader adoption of ethical AI frameworks across various sectors, promoting trust and integrity in AI applications.},
  month = {apr},
}

@inproceedings{yang_apicoder_2025,
  title = {{APICoder},
  author = {Yang, Conghui and Yu, Lei and Su, Huafeng and Zhou, Xiang},
  year = {2025},
  doi = {10.1109/ICWS67624.2025.00109},
  url = {https://doi.org/10.1109/icws67624.2025.00109},
  booktitle = {2025 {IEEE},
  pages = {849--851},
  note = {ISSN: 2836-3868},
  keywords = {source: IEEE},
  abstract = {With the rapid advancement of large language models (LLMs) in code generation, the task of API service invocation code generation faces increasing challenges such as inaccurate context understanding, improper API selection, and outdated data. To address these issues, this paper constructs a high-quality dataset, APIDataset, and fine-tunes the DeepSeek-Coder-V2-Lite-Instruct model based on this dataset. We further integrate a retrieval-augmented generation (RAG) approach to enhance the model's contextual comprehension capabilities. Moreover, we propose APICoder, a multi-role collaborative framework that optimizes different stages of code generation, significantly improving the quality of generated API invocation code. Finally, we develop the APITest benchmark and conduct two rounds of evaluation combining both API-specific and general code generation test sets. Experimental results demonstrate that APICoder achieves superior performance in both API invocation and general code generation tasks, validating its effectiveness and practical potential.},
  month = {jul},
}

@inproceedings{singh_understanding_2025,
  title = {Understanding and {Mitigating},
  author = {Singh, Ravneet and Singh, Parminder and Malik, Arun and Sukmawan, Dede},
  year = {2025},
  doi = {10.1109/ICMCTC62214.2025.11196493},
  url = {https://doi.org/10.1109/icmctc62214.2025.11196493},
  booktitle = {2025 {International},
  pages = {1--10},
  keywords = {and Large Language Models (LLMs) Benchmarks, Benchmark testing, Ethics, Generative AI, Hallucinations in LLMs, Large language models, Market research, Metaverse, Multimodal Alignment, Prevention and mitigation, Prompt engineering, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Systematic literature review, Trustworthy AI, source: IEEE},
  abstract = {The analysis of hallucinations in Large Language Models (LLMs), sequences that are coherent but factually false and semantically conflicting, has implications for the credibility and practical value of such systems. To address this concerning issue, the current paper presents a three-tier systematic review of 30 foundational and state-of-the-art publications, summarizing the existing knowledge on the error’s nature of hallucinations in LLMs and their antecedents and management. The classification of hallucinations is analyzed as intrinsic, extrinsic, compounding, and multimodal errors; focusing on their phenomenon in tasks of questions answering, summarization, and other multimodal cases. Current mitigation strategies are discussed at the data level (e.g., dataset cleaning and ad hoc knowledge injection), at the model level (e.g., fine-tuning and regularization), and at the inference level (e.g., retrieval-augmented generation and prompt engineering). Benchmarks such as HALUEVAL and TruthfulQA are compared to determine the usefulness and drawbacks for measuring the incidence of hallucinations. Some of the recurring issues such as the scarcity of domain-related corpora, adaptability in the LM inspired by scarce resources, and the ethical use of purposeful hallucinations are observed together with the prospects of a combination of approaches and large-scale evaluation frameworks. It is proposed that this review be used as a guide for future research in the domain of LLMs and as a reference that highlights gaps and possible directions for continuing work towards the development of more dependable, domain-adaptive, and ethically consistent LLMs.},
  month = {apr},
}

@inproceedings{richard_slm-based_2025,
  title = {{SLM},
  author = {Richard, Roman M. and Villanueva, Alonica R.},
  year = {2025},
  doi = {10.1109/ICSP65755.2025.11086925},
  url = {https://doi.org/10.1109/icsp65755.2025.11086925},
  booktitle = {2025 10th {International},
  pages = {1157--1160},
  keywords = {Big Data, Generative AI, Graphics processing units, Hybrid power systems, Information retrieval, Information Retrieval, Large language models, Natural language processing, Reliability, Retrieval augmented generation, Semantics, Signal processing, source: IEEE},
  abstract = {Large Language Models (LLMs) have significantly advanced the accessibility of natural language processing across diverse applications. However, susceptibility to hallucinations and reliance on outdated data limit their reliability. Retrieval-Augmented Generation (RAG) systems address this by providing access to external, up-to-date information during inference. Despite this improvement, existing dense, sparse, and hybrid retrieval strategies often struggle to balance semantic relevance and efficiency. In this study, we implement a hybrid retrieval approach that utilizes a small language model (SLM)-based reranking. We compare its performance against traditional hybrid retrieval methods combining FAISS HNSW with BM25 and TF-IDF reranking on a 500,000-document subset from OSCAR, evaluated under realistic compute limits using a single-GPU Colab environment (L4 GPU, 22.5 GB VRAM) to simulate resource-constrained RAG settings. We evaluate retrieval quality (precision, Recall) and efficiency (seconds-per-query, query-per-seconds). Results show that hybrid BM25 (0.05 SPQ, 21.96 QPS) and TF-IDF (0.01 SPQ, 71.17 QPS) methods are more efficient, but the SLM-based hybrid achieves superior retrieval accuracy. It substantially outperforms both traditional approaches in top-5 relevance, achieving a Precision@5 of 0.028 and a Recall of 0.102.},
  month = {may},
}

@inproceedings{fan_towards_2025-1,
  title = {Towards {Retrieval},
  author = {Fan, Wenqi and Wu, Pangjing and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Qing},
  year = {2025},
  doi = {10.1109/ICDE65448.2025.00341},
  url = {https://doi.org/10.1109/icde65448.2025.00341},
  booktitle = {2025 {IEEE},
  pages = {4509--4512},
  note = {ISSN: 2375-026X},
  keywords = {Data engineering, Data management, Knowledge engineering, Large Language Model, Large language models, Query processing, Real-time systems, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, System analysis and design, Technological innovation, Tutorials, source: IEEE},
  abstract = {Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by integrating external, reliable, and up-to-date knowledge. This addresses critical limitations such as hallucinations and outdated internal information. This tutorial delves into the evolution and frameworks of RAG, emphasizing the pivotal role of data management technologies in optimizing query processing, storage, indexing, and efficiency. It explores how RAG systems can deliver high-quality, context-aware outputs through efficient retrieval and integration, covering key topics such as retrieval-augmented LLM (RA-LLM) architectures, retrieval techniques, learning methodologies, and applications in NLP and domain-specific tasks. Challenges like customized query and generation, real-time retrieval, and trustworthy RAG are discussed alongside future directions and opportunities for innovation. Designed for students, researchers, and industry practitioners with basic artificial intelligence and data engineering knowledge, this tutorial offers practical insights into designing data management-powered RAG systems. It inspires the exploration of novel solutions in this rapidly evolving field.},
  month = {may},
}

@inproceedings{munir_leveraging_2024,
  title = {Leveraging {Multimodal},
  author = {Munir, Muhaimin Bin and Cai, Yuchen and Khan, Latifur and Thuraisingham, Bhavani},
  year = {2024},
  doi = {10.1109/TPS-ISA62245.2024.00046},
  url = {https://doi.org/10.1109/tps-isa62245.2024.00046},
  booktitle = {2024 {IEEE},
  pages = {341--350},
  keywords = {Accuracy, Cyberattack, Databases, Knowledge based systems, Large Language Model, MITRE ATT\&CK, Multimodal RAG, Retrieval augmented generation, Retrieval Augmented Generation, Sensor systems, Sensors, Target tracking, Threat assessment, Training, Transit System, Transportation Security, source: IEEE},
  abstract = {Large Language Models (LLMs) often tend to hallucinate, one of the reasons is due to limitations in their training datasets. These datasets are vast, and the training process is resource-intensive, making LLMs unreliable for generating accurate responses for recent information. To address this issue, Retrieval-Augmented Generation (RAG) uses indexed text chunks from relevant, up-to-date knowledge databases to generate more accurate and current responses. Our project explores the use of RAG in the domain of transit security. Transit security systems include physical objects such as video and audio surveillance, alarms, threat sensors, and infrastructure monitoring sensors, which scan the environment for potential threats and relay this information to the Transit Management Center, Transit Vehicles, Emergency Management Center, etc. We aimed to predict potential cyber threats to these information flows that adversaries might exploit to infiltrate the systems. By utilizing the description of the information flow and other characteristics of the data, we leveraged LLMs with RAG to map possible cyber attack techniques from the MITRE ATT\&CK knowledge-base. As the MITRE ATT\&CK technique database is continuously updated to keep track of the new cyberattack techniques, using RAG enhances our ability to predict how adversaries might target transit security information flows. We analyzed information flows of transit systems from the USDOT public website, manually annotating possible attack techniques to establish a benchmark. Our multimodal RAG model achieved an F-1 score of 40.5\% and a precision of 42.5\%, representing a 73.65\% improvement over the baseline approach. These results demonstrate the effectiveness of integrating LLMs with RAG and incorporating multimodality in predicting cyber threats in transit cybersecurity.},
  month = {oct},
}

@inproceedings{darji_enhancing_2024,
  title = {Enhancing {Financial},
  author = {Darji, Abhishek and Kheni, Fenil and Chodvadia, Dhruvil and Goel, Parth and Garg, Dweepna and Patel, Bankim},
  year = {2024},
  doi = {10.1109/ICACRS62842.2024.10841711},
  url = {https://doi.org/10.1109/icacrs62842.2024.10841711},
  booktitle = {2024 3rd {International},
  pages = {754--760},
  keywords = {Accuracy, Analytical models, Computational modeling, Financial Risk Analysis, Gemini-1.5-flash, Generative AI, GPT-4o, Large language Model (LLM), Large language models, Machine learning, Natural language processing, Ollama, Renewable energy sources, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Reviews, Risk analysis, source: IEEE},
  abstract = {The rapid development of Generative AI has brought major changes in way of functioning of different sectors throughout the world. Many research work has been done in the field of financial sector to increase the efficiency and reduce the errors due to human intervention. However, the current financial risk analysis relies on manual reviews and conventional machine learning models which repeatedly failing to process financial risk data. This study investigates how Retrieval-Augmented Generation (RAG) approach can help Large Language Models (LLM) to generate risk analysis reports for audit reports which extract detailed information from the audit reports and avoid overlooking of small details, which was a major drawback in the earlier system. This research study covers how Retrieval Augmented Generation (RAG) enhances the performance financial risk analysis of audit reports using different LLMs like GPT-4o, Gemini-1.5-flash, and LlaMa3.1. This research work includes the performance of LLMs beyond multiple metrics, including faithfulness, context precision-recall-relevancy, and answer relevance. The research findings imply that LlaMa3.1 is a great model in terms of faithfulness of the generated report with a score of 78.26\%. In terms of retrieval of the documents and its context, Llama had a very strong performance by getting the score of 79.62\% in context-precision, 78.26\% in context-recall and 86.99\% in context-relevancy. In terms of generated report, the Llama3.1 model have the score of 37.83\% for answer-relevancy and Gemini-1.5-flash have a score of 58.64\% for answer-correctness.},
  month = {dec},
}

@inproceedings{kong_document_2024,
  title = {Document {Embeddings},
  author = {Kong, Yongle and Yang, Zhihao and Luo, Ling and Ding, Zeyuan and Wang, Lei and Liu, Wei and Zhang, Yin and Xu, Bo and Wang, Jian and Sun, Yuanyuan and Zhao, Zhehuan and Lin, Hongfei},
  year = {2024},
  doi = {10.1109/BIBM62325.2024.10822781},
  url = {https://doi.org/10.1109/bibm62325.2024.10822781},
  booktitle = {2024 {IEEE},
  pages = {962--967},
  note = {ISSN: 2156-1133},
  keywords = {Accuracy, Bioinformatics, Biological system modeling, biomedical question answering, document embedding, large language model, Large language models, Question answering (information retrieval), Refining, Reliability, retrieval augmented generation, Retrieval augmented generation, Semantics, Training, source: IEEE},
  abstract = {Large language models (LLMs) perform well in many NLP tasks but frequently generate inaccurate information in the biomedical domain, due to hallucination issues. Retrieval-Augmented Generation (RAG) has been introduced to address this issue by integrating external knowledge, enhancing the factual accuracy of outputs. However, naive RAG encounters challenges in effectively utilizing retrieved content, particularly in specialized domains like biomedicine. LLMs often struggle to integrate retrieved content as irrelevant information can interfere with the model’s judgment. Even if relevant documents are retrieved, the model may be unable to accurately comprehend and utilize the domain-specific features due to its inherent knowledge limitations. To overcome these limitations, we propose Document Embeddings Enhanced Biomedical RAG (DEEB-RAG), a framework that incorporates document embeddings along with the original retrieved text. DEEB-RAG uses MedCPT to generate document embeddings and these embeddings are then aligned with the LLM’s semantic space using a two-stage training process on a simple projector. Experimental results on biomedical QA datasets show that DEEB-RAG improves accuracy, with an average performance increase of 2.3\% over naive RAG. This demonstrates DEEB-RAG’s ability to mitigate the challenges of utilizing complex biomedical information, thereby enhancing the reliability and effectiveness of LLMs in biomedical domain.},
  month = {dec},
}

@inproceedings{yoon_implementation_2025,
  title = {Implementation of {Multi},
  author = {Yoon, Min-Joo and Yoo, Sun-Mo and Park, Jong-Hwa and Park, Ki-Woong},
  year = {2025},
  doi = {10.1109/ICCT-Pacific63901.2025.11012876},
  url = {https://doi.org/10.1109/icct-pacific63901.2025.11012876},
  booktitle = {2025 1st {International},
  pages = {1--4},
  keywords = {Analytical models, Common Vulnerabilities and Exposures, Data integration, Data models, Databases, Large Language Model, Measurement, Meteors, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Soft sensors, Software development management, source: IEEE},
  abstract = {Traditional vulnerability analysis models often rely on a single data source, which limits their ability to comprehensively detect and analyze vulnerabilities. In this study, we propose a multi-level Retrieval-Augmented Generation (RAG) model that progressively integrates multiple vulnerability databases to overcome these limitations. By incorporating core and peripheral data sources at different levels, our approach aims to enhance the quality of vulnerability analysis. We integrate four major vulnerability databases-National Vulnerability Database, CERT Vulnerability Notes Database, GitHub Advisory Database, and Exploit Database-into a Level 4 RAG model. This integration demonstrates an average performance improvement of 6.4\% compared to a Level 1 model that only refers to the National Vulnerability Database. The evaluation is based on several metrics, including METEOR, BERT Score, Faithfulness, and Answer Relevancy, which collectively highlight the benefits of a multi-source, incremental integration strategy for more effective vulnerability analysis. These results highlight the potential of multi-level RAG models in enhancing vulnerability analysis through the integration of complementary vulnerability data sources.},
  month = {mar},
}

@inproceedings{mao_scrag_2025,
  title = {{scRAG},
  author = {Mao, Yuren and Zhu, Yifan and Liu, Qing and Liu, Peigen and Yu, Haoran and Gao, Yunjun},
  year = {2025},
  doi = {10.1109/ICDE65448.2025.00352},
  url = {https://doi.org/10.1109/icde65448.2025.00352},
  booktitle = {2025 {IEEE},
  pages = {4560--4563},
  note = {ISSN: 2375-026X},
  keywords = {source: IEEE},
  abstract = {An average person usually contains more than 10 trillion human cells, and each cell's transcriptome can be profiled by a single-cell RNA sequence (scRNA-seq). The huge volume and high complexity of scRNA-seq data put challenges on fundamental tasks of scRNA-seq data analysis, i.e., cell type identification and new cell type discovery. In this paper, we demonstrate scRAG, which can efficiently remove batch effect in cell-type identification and enable reliable new cell discovery, facilitated by GPU-based scRNA-seq data management and Large Language Models (LLMs). The GPU-based scRNA-seq data management enables high throughput scRNA-seq data retrieval and update, while the LLM utilizes the retrieval results to remove the batch effect and discover novel cells. We demonstrate scRAG for its: (a) interfaces, (b) GPU-based scRNA-seq data management, and (c) applications in batch effect removal and cancer cell discovery.},
  month = {may},
}

@inproceedings{abolhasani_ontokgen_2025,
  title = {{OntoKGen},
  author = {Abolhasani, Mohammad Sadeq and Pan, Rong},
  year = {2025},
  doi = {10.1109/RAMS48127.2025.10935139},
  url = {https://doi.org/10.1109/rams48127.2025.10935139},
  booktitle = {2025 {Annual},
  pages = {1--6},
  note = {ISSN: 2577-0993},
  keywords = {source: IEEE},
  abstract = {Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.},
  month = {jan},
}

@inproceedings{alshammary_rfpg_2024,
  title = {{RFPG},
  author = {Alshammary, Mitha and Uddin, Md Nahiyan and Khan, Latifur},
  year = {2024},
  doi = {10.1109/CIC62241.2024.00023},
  url = {https://doi.org/10.1109/cic62241.2024.00023},
  booktitle = {2024 {IEEE},
  pages = {107--116},
  keywords = {Accuracy, Artificial Intelligence (AI), Atmospheric modeling, Collaboration, Computational modeling, Faces, Focusing, Generative AI, Hallucination, Internet, Large language models, Large Language Models (LLMs), Low-Resource Languages, Question Answering, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Standards, source: IEEE},
  abstract = {Since Large Language Models (LLMs) face several challenges, including hallucinations, the Retrieval-Augmented Generation (RAG) model has been proposed as a solution. While RAG has been widely used for English, it remains underexplored in low-resource languages like Arabic. This study enhances the RAG model by focusing on a specific domain within Arabic, a low-resource language. Our proposed framework, RFPG, addresses question-answering by integrating (a) fact-checking into the retrieval process and (b) customized and innovative prompts. Our model was tested on 123 questions and it was able to answer with an accuracy of 100\% and reference the sources with a precision of 98\%, outperforming both RAG and standard LLMs, including the latest models like GPT-4o, GPT-4o mini, and GPT-4, in Arabic text question-answering.},
  month = {oct},
}

@inproceedings{shayaninasab_enhancing_2024,
  title = {Enhancing {Patient},
  author = {Shayaninasab, Minoo and Zahoor, Maryiam and Yalçin, Özge Nilay},
  year = {2024},
  doi = {10.1109/ACIIW63320.2024.00053},
  url = {https://doi.org/10.1109/aciiw63320.2024.00053},
  booktitle = {2024 12th {International},
  pages = {256--264},
  keywords = {Chatbots, Conferences, Conversational Agents, Depression, Large language models, Large Language Models (LLM), Measurement, Medical services, Mental health, Mental Health, MIMICs, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Safety, source: IEEE},
  abstract = {In this paper, we develop and evaluate an empathic chatbot, called EmoBot, for a mental healthcare patient in-take scenario. EmoBot employs Retrieval-Augmented Generation (RAG) methods with Large Language Models (LLMs), to provide empathic support and advice, while ensuring the intake process is completed. We evaluate EmoBot's performance both by automated metrics using available datasets and through dialogues with a simulated patient model that mimics varying levels of depression severity. Our evaluations showed good agreement between EmoBot's categorization of depression severity levels and human raters. EmoBot's topic classification system achieved 78.48\% accuracy on the Primate2022 dataset [1] without fine-tuning. EmoBot's responses showed a 0.93 similarity with patient inputs, proving contextual relevance. These findings highlight the effectiveness of RAG-based methods in guiding and providing safety guardrails for LLMs and their potential as supportive tools for health assessment and support.},
  month = {sep},
}

@article{song_enhancing_2025,
  title = {Enhancing {RAG},
  author = {Song, Minchae},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3569872},
  url = {https://doi.org/10.1109/access.2025.3569872},
  journal = {IEEE Access},
  volume = {13},
  pages = {85072--85083},
  keywords = {Accuracy, Adaptation models, Cognition, Context modeling, Finance, Generative AI, Hierarchical nodes, Pipelines, Retrieval augmented generation, retrieval-augmented generation (RAG), Soft sensors, table retrieval, Training, vector database, source: IEEE},
  abstract = {Recent developments in retrieval-augmented generation (RAG) techniques have aimed at integrating structured tabular data with external data sources. Nevertheless, because existing approaches have difficulty effectively retrieving and reasoning from diverse and complex table structures, there is a growing demand for innovative methods that surpass traditional retrieval paradigms. This study introduced hierarchical node-based header representation models designed to enhance the processing and integration of external table data, addressing the key limitations of existing RAG techniques when working with structured table sources. By classifying table types designed for the structural complexity and the level of domain knowledge required for their interpretation, this study explores the impact of using embedded information to improve retrieval accuracy. The findings of this study demonstrate that presenting richer and more meaningful information within table headers significantly enhances RAG performance. This improvement is attributed to the utilization of table header representations, which allow for a more precise identification of hierarchical node-based headers during the table retrieval process. Furthermore, this study confirms that vector database achieves greater accuracy when employing a distributed table collection (DTC) structure than a unified context collection (UCC). These results indicate that the structure of the information extracted from text and tables varies considerably. Our experiment results highlight that separating and organizing such information using the DTC structure proved to be an effective strategy for improving the accuracy of table-specific responses.},
  issn = {2169-3536},
}

@inproceedings{bharambe_exploring_2025,
  title = {Exploring {Opportunities},
  author = {Bharambe, Uiwala and Patil, Kaushal and Ingle, Pushkar and Bhangale, Ujwala},
  year = {2025},
  doi = {10.1109/AI2E64943.2025.10983078},
  url = {https://doi.org/10.1109/ai2e64943.2025.10983078},
  booktitle = {2025 {International},
  pages = {1--6},
  keywords = {Accuracy, Cancer, Cancer Care, Chatbot, Chatbots, Clinical Decision Support, Cognition, Computer architecture, Data visualization, Ethics, Knowledge graphs, Knowledge Graphs, Large Language Model, Large language models, Patient Monitoring, Retrieval augmented generation, Retrieval Augmented Generation, source: IEEE},
  abstract = {Chatbots offer essential support for patients undergoing radiation therapy by providing timely advice, tracking symptoms, and suggesting coping strategies for side effects. They act as a bridge between patients and their care teams, ensuring continuous guidance during treatment. This requires accurate, context-aware, and explainable responses, which is challenging when designing chatbots. New features and capabilities have recently been introduced to chatbot functionality via Large Language Models (LLMs) using technologies such as Retrieval-Augmented Generation (RAG), chatbots can retrieve real-time, relevant information, generating contextually relevant interactions. Traditional LLMs and RAG models still suffer from limitations such as generating hallucinated information and difficulty reasoning over complex relationships. These challenges can be addressed with knowledge graphs, which offer structured, interconnected data representations for chatbots to deliver grounded, understandable responses. Designing a chatbot that integrates knowledge graphs with RAG presents significant opportunities and challenges for developing intelligent, patient-centric conversational systems. Knowledge graphs enhance structured reasoning and accuracy, while RAG supports dynamic and personalized interactions. The paper examines architectural considerations, technological possibilities, and practical applications of such systems, especially in healthcare domains like oncology, where precision and reliability are crucial.},
  month = {feb},
}

@inproceedings{nickel_integrating_2025,
  title = {Integrating {LLMs},
  author = {Nickel, Lucas Immanuel and Hohmann, Lorenz and Stolbov, Nick and Gerstacker, Lukas and Rieger, Sebastian},
  year = {2025},
  doi = {10.1109/NOMS57970.2025.11073741},
  url = {https://doi.org/10.1109/noms57970.2025.11073741},
  booktitle = {{NOMS},
  pages = {1--6},
  note = {ISSN: 2374-9709},
  keywords = {Accuracy, Automation, Complexity theory, Documentation, Intent-based Networking, Large Language Model, Large language models, Monitoring, Network Automation, Network Operations, Nonhomogeneous media, Retrieval augmented generation, Retrieval-Augmented Generation, Translation, source: IEEE},
  abstract = {The operation of modern network infrastructures presents network operations engineers with considerable challenges, esp. in multi-vendor environments, due to their inhomogeneity and technological diversity. Intent-based networking (IBN) offers a solution to reduce this complexity by allowing users to specify desired network states as intents, which are then translated into network policies and continuously monitored. This paper presents an approach for the translation and implementation of intents based on Retrieval-Augmented Generation (RAG) that is integrated with the network automation tools Netmiko and NetBox. Intents can be used to express and perform changes in these tools, e.g., to automate otherwise tedious and hence error-prone tasks to assist human network administrators. The evaluation shows a 90.2\% success rate for individual phases (Intent Conception, Request Construction, Response Correctness) and 81.2\% end-to-end. To safeguard network availability and integrity, the Large Language Model (LLM) references manufacturer documentation online and prompts operators for confirmation. The implementation is provided as open source. Potential network management and operations use cases benefiting from LLMs and RAG are discussed based on the accuracy, correctness, and practicability of the presented testbed.},
  month = {may},
}

@inproceedings{cheng_rule_2025,
  title = {Rule {Verification},
  author = {Cheng, Haoyuan and Wang, Ming and Liu, Yuanlong and Li, Naiyong and Mao, Chenxu and Wang, Wenxin and Liu, Enren and Wang, Kai},
  year = {2025},
  doi = {10.1109/AAIEE64965.2025.11100921},
  url = {https://doi.org/10.1109/aaiee64965.2025.11100921},
  booktitle = {2025 {IEEE},
  pages = {892--897},
  keywords = {error-prevention knowledge graph (EPKG), Green energy, Knowledge based systems, Knowledge graphs, Large language models, LLM, Load forecasting, Power grids, power operation order, RAG, Retrieval augmented generation, Rule verification method, Substations, Topology, Vectors, source: IEEE},
  abstract = {In order to identify the temporary additional rules that may appear for specific operation tasks to prevent errors in substation operation instructions, a rule verification method for power operation order instructions is established based on a large language model (LLM). First, the paper uses the graph isomorphism algorithm to form an error-prevention knowledge graph (EPKG) that integrates the physical structure information and rule information of the power grid, so as to improve the analysis ability of the LLM on the power grid topology; the paper also uses retrieval-augmented generation (RAG) technology, which is used to improve the understanding ability of the LLM on the operation rules, to build a vector database for the temporary additional rules of the operation task. Then, by extracting the argument information of operation instructions and using the argument post-processing method, the operation instruction graph is obtained which can be located on the EPKG to realize the graph-based instruction verification. Furthermore, the corresponding additional rules related to an instruction that can be understood by LLM are extracted from the RAG knowledge base to realize RAG-based instruction verification. Finally, in the case analysis, the operation of a certain device operation instruction is simulated and deduced, and interpretability of the graph-RAG-based rule verification method is demonstrated.},
  month = {apr},
}

@inproceedings{purba_towards_2025,
  title = {Towards {Automated},
  author = {Purba, Moumita Das and Chu, Bill and French, Will},
  year = {2025},
  doi = {10.1109/DSN64029.2025.00067},
  url = {https://doi.org/10.1109/dsn64029.2025.00067},
  booktitle = {2025 55th {Annual},
  pages = {664--677},
  note = {ISSN: 2158-3927},
  keywords = {Cognition, Cyber threat intelligence, Database languages, Generative AI, GPT-4, Kibana, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, OpenAI o1, Prompt, Prototypes, RAG, Real-time systems, Retrieval augmented generation, Security, Threat hunting, source: IEEE},
  abstract = {This paper describes an attempt to automate threat hunting, taking cyber threat intelligence messages as input and generating queries to search logs for attack evidence using a popular query language, Kibana, used in Security Operations Centers (SOC). Our prototype implementation, AIThreatTrack, uses GPT-4 to extract actionable threat intelligence from real-time messages like X and Slack. The core idea is to explain the extracted intelligence in terms of MITRE ATT\&CK TTPs using a knowledge graph containing "is-a" and "part-of" relationships (extracted using GPT-4) with the following benefits:(a) Significantly reduced hallucinations from 47\% (GPT-4) to 1.5\% using two orthogonal ways to cross-check answers. (b) Gaining analysts’ trust with explained results. (c) Using chain-of-knowledge prompting to significantly improve query generation accuracy. This approach supports expanding the scope of the knowledge graph to further improve query generation. Our approach significantly outperforms the Retrieval-Augmented Generation (RAG) approach and chain-of-thought reasoning LLM in reducing hallucinations.},
  month = {jun},
}

@inproceedings{khan_enhancing_2024,
  title = {Enhancing {Large},
  author = {Khan, Nasik Sami and Hasan, Md Mahibul and Towhid, Md. Shamim and Basnet, Saroj and Shahriar, Nashid},
  year = {2024},
  doi = {10.1109/GCWkshp64532.2024.11100791},
  url = {https://doi.org/10.1109/gcwkshp64532.2024.11100791},
  booktitle = {2024 {IEEE},
  pages = {1--7},
  note = {ISSN: 2166-0077},
  keywords = {source: IEEE},
  abstract = {This paper presents a comprehensive approach for fine-tuning large language models (LLMs) for domain-specific tasks in the telecommunications field. We utilize a dataset with 1,827 multiple-choice questions (MCQs) from 3GPP standard documents. A publicly available LLM named "Phi-2" is used to answer the MCQs correctly. We develop a Retrieval-Augmented Generation (RAG) pipeline to improve Phi-2 model's performance. The RAG pipeline comprises document segmentation, synthetic question-answer (QA) generation, custom fine-tuning of the embedding model, and incremental fine-tuning of Phi-2. Our experiments show that accuracy greatly increased by combining all the above-mentioned steps in the RAG pipeline. The proposed approach outperforms the baseline Phi-2 model by 45.20\% in terms of accuracy. This study identifies the limitations of instruction fine-tuning in specialized fields and explores the possibility of using sophisticated data processing with fine-tuned models to improve performance even more.},
  month = {dec},
}

@inproceedings{nezafat_fake_2024,
  title = {Fake {News},
  author = {Nezafat, Mohammad Vatani and Samet, Saeed},
  year = {2024},
  doi = {10.1109/FLLM63129.2024.10852474},
  url = {https://doi.org/10.1109/fllm63129.2024.10852474},
  booktitle = {2024 2nd {International},
  journal = {2024 2nd International Conference on …},
  pages = {160--167},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, Costs, Data models, Fake news, Fake News Detection, Large Language Model, Large language models, Real-time systems, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Sparse Mixture of Experts, Testing, Training, source: IEEE, source: Google Scholar},
  abstract = {The rapid spread of false information on social media has grown to be a serious problem that influences public opinion and decision-making. Fake news spreads rapidly and extensively, often outpacing efforts to debunk or mitigate its effects. Traditional methods for detecting fake news face numerous challenges, including the necessity for extensive model training and the potential for inherent biases. Although Large Language Models (LLMs) have seen substantial improvements recently, their use in fake news detection poses the risk of producing false or misleading information due to their possible hallucinations. This study presents a new strategy to combat fake news by integrating Mixtral-8x7B, a Sparse Mixture of Experts (SMoE) Large Language Model, with a Retrieval-Augmented Generation (RAG) framework. Our framework employs Google’s search API to retrieve relevant articles in real time, harnessing Mixtral’s sophisticated language processing capabilities and RAG’s ability to access current information dynamically. Initial results are promising, indicating that our approach performs comparably to established fake news detection techniques. Our method operates without the need for extensive model training, offering significant cost savings and contributing to developing more efficient tools for detecting misinformation in the digital era, which will help stop the spread of misleading data more efficiently.},
  month = {nov},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{sun_rabbit_2025,
  title = {Rabbit: {Retrieval},
  author = {Sun, Wenwen and Pan, Zhicheng and Hu, Zirui and Liu, Yu and Yang, Chengcheng and Zhang, Rong and Zhou, Xuan},
  year = {2025},
  doi = {10.1109/ICDE65448.2025.00284},
  url = {https://doi.org/10.1109/icde65448.2025.00284},
  booktitle = {2025 {IEEE},
  pages = {3807--3820},
  note = {ISSN: 2375-026X},
  keywords = {Bayesian optimization, Data processing, Databases, Indexes, Knob tuning, Large language models, LLM, Optimization, Rabbits, RAG, Retrieval augmented generation, Sparks, Tuners, Tuning, source: IEEE},
  abstract = {The large language model (LLM)-based knob tuning method has attracted considerable attention due to its excellent in-context learning ability and generalizability. However, the existing LLM-based tuning methods do not effectively harmonize multi-source external knowledge, leading to missed opportunities for enhanced knob tuning. In light of this, we propose Rabbit, a novel approach that leverages Retrieval-augmented generation to enhance database knob tuning tools, which seamlessly integrates structured historical tuning experience with graph-encoded static knowledge. First, we introduce an experience-driven knob selection strategy, enhanced by dependency-aware external knowledge integration, to systematically select key knobs. Second, we develop a cutting-edge multi-agent knob domain pruning method, which ensures the reduced search space remains compact yet effective. Finally, we leverage the few-shot capabilities of LLMs to act as surrogate models, enabling rapid exploration of the pruned search space, followed by incremental optimization that expands the search space using historical insights. Moreover, we also design an adaptive strategy to transition between these two search spaces, striking an optimal balance between exploration and exploitation. Extensive experiments on well-established bench-marks demonstrate that Rabbit outperforms the state-of-the-art methods in both effectiveness and efficiency, pointing to a new paradigm for this area.},
  month = {may},
}

@inproceedings{kumar_development_2025,
  title = {Development of {Interactive},
  author = {Kumar, P and M, Haresh and V, Hayagreevan},
  year = {2025},
  doi = {10.1109/ICCCIT62592.2025.10928137},
  url = {https://doi.org/10.1109/icccit62592.2025.10928137},
  booktitle = {2025 {International},
  pages = {265--269},
  keywords = {source: IEEE},
  abstract = {With the increase in capabilities and availability of large language models, it is inevitable that it finds its way into application in a variety of domains. Education domain has seen remarkable improvements in both efficiency and capabilities with the incorporation of innovative and latest technology to improve the experience for both teachers as well as students and when talking about latest technologies it is impossible to not talk about large language models. Their ability to contemplate huge volumes of data as well as generate human-like responses to queries related to data it possesses has made many industries rethink certain aspects of their workflow to integrate this powerful technology to improve the working efficiency. Education sector can benefit from this technology in various aspects. This project aims to integrate the usage of locally run large language models using Ollama and LangChain to build a conversational platform application for the usage by students for assisted learning. The model also utilizes Retrieval Augmented generation techniques for creating an updated and dynamic large language model. To promote transparency stateof-the-art open-source technologies are preferred rather than closed source technologies like OpenAI's ChatGPT.},
  month = {feb},
}

@inproceedings{ku_enhancing_2025,
  title = {Enhancing {Autonomous},
  author = {Ku, Jaebin and Kim, Sanha and Lee, Eunkyu and Zaman, Umar and Kim, Kyungsup},
  year = {2025},
  doi = {10.1109/ICAIIC64266.2025.10920831},
  url = {https://doi.org/10.1109/icaiic64266.2025.10920831},
  booktitle = {2025 {International},
  pages = {0420--0426},
  note = {ISSN: 2831-6983},
  keywords = {source: IEEE},
  abstract = {This study introduces a novel architecture designed to enhance the performance and cost-efficiency of Large Language Models (LLMs) in autonomous ship communication systems. Autonomous ships require high accuracy and rapid response, yet their operational constraints-limited computational resources and lack of internet connectivity-pose significant challenges for traditional LLMs. To address these issues, we integrate Retrieval-Augmented Generation (RAG) with Decision Trees. This integration improves the efficiency of RAG's data retrieval and processing, significantly reduces the computational overhead of LLMs, and enhances response accuracy. Evaluations using 200 hours of real-world maritime communication data demonstrate that the proposed system outperforms existing methods in speed and accuracy under resource-constrained conditions. This research advances the practical application and reliability of LLMs in autonomous ship communication, providing a strong foundation for improving automation and ensuring safety in the maritime industry.},
  month = {feb},
}

@inproceedings{pathak_utilizing_2024,
  title = {Utilizing {Large},
  author = {Pathak, Rudransh and Vald, Gabriel and Sermet, Yusuf and Demir, Ibrahim},
  year = {2024},
  doi = {10.1109/URTC65039.2024.10937521},
  url = {https://doi.org/10.1109/urtc65039.2024.10937521},
  booktitle = {2024 {IEEE},
  pages = {1--5},
  keywords = {Accuracy, Chain of Thought Prompting, Codes, Hallucination, Large language models, Large Language Models, Medical diagnosis, Medical diagnostic imaging, Prompt engineering, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Training, Transforms, source: IEEE},
  abstract = {The application of Large Language Models (LLMs) has been a deeply explored topic, but with little focus on utilizing LLMs for predicting ICD-10 patient diagnoses in the medical field. Using LLMs to predict these patient codes has significant potential to streamline medical processes and reduce human burden drastically. In this work, we use GPT-4o to test four prompting techniques: Base GPT-4o, GPT-4o with Chain of Thought (CoT) prompting, GPT-4o with Retrieval-Augmented Generation (RAG), and GPT-4o with both CoT and RAG. Our results show that combining CoT and RAG significantly improve predictive accuracy of GPT-4o in patient diagnosis.},
  month = {oct},
}

@inproceedings{mishra_daignostic-_2025,
  title = {{dAIgnostic},
  author = {Mishra, Deepasikha and Rayasam, Venkat Harsha and Kandukuri, Sai Akshay and Sai, Koneru Pranav and Ashray, N. V. Ravi},
  year = {2025},
  doi = {10.1109/AIMV66517.2025.11203542},
  url = {https://doi.org/10.1109/aimv66517.2025.11203542},
  booktitle = {2025 {International},
  pages = {1--6},
  keywords = {Clinical Decision Support, Diagnostics, Electronic Health Records (EHRs), Electronic medical records, Ethics, Face recognition, Generative AI, Healthcare, Large Language Models (LLMs), Market research, Medical services, Natural language processing, Natural Language Processing (NLP), Regulation, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Streaming media, source: IEEE},
  abstract = {Generative Artificial Intelligence (Gen-AI) is dramatically changing the healthcare sector through improved diagnosis and streamlined administrative processes. Gen-AI technologies meet these needs in the face of mounting pressures arising from cost levels, patient expectations, and the pressure for more individualised treatment. It can identify trends by analysing large datasets, such as electronic health records and imaging data, allowing for trend recognition and providing clinical decision support, thereby improving the quality of both patient care and operational efficiency. Despite the promise that Gen-AI presents, there are inherent risks associated with integrating Gen-AI into healthcare. These include issues regarding data privacy, biases in algorithms, and lack of appropriate frameworks for ethical decision-making. Governance structures can ensure compliance with privacy regulations and enhance user trust. Ongoing evaluations are also necessary about Gen-AI and its impact on clinical outcomes to maximize benefits and minimize risks. The end-to-end solutions that would be developed using Gen-AI include automated report generation and facial recognition to reduce administrative burdens and improve patient experiences. This promises to revolutionize clinical practices while requiring mature exploration of the ethics and data protection policies with the deployment of these technologies in healthcare organizations.},
  month = {aug},
}

@inproceedings{abualhaija_llm-assisted_2025,
  title = {{LLM},
  author = {Abualhaija, Sallam and Ceci, Marcello and Sannier, Nicolas and Bianculli, Domenico and Lannier, Salomé and Siclari, Martina and Voordeckers, Olivier and Tosza, Stanisław},
  year = {2025},
  doi = {10.1109/RE63999.2025.00023},
  url = {https://doi.org/10.1109/re63999.2025.00023},
  booktitle = {2025 {IEEE},
  pages = {142--154},
  note = {ISSN: 2332-6441},
  keywords = {Accuracy, Collaboration, General Data Protection Regulation, General Data Protection Regulation (GDPR), Large language models, Large Language Models (LLMs), Law, Natural language processing, Natural Language Processing (NLP), Privacy Requirements, Requirements Generation, Retrieval augmented generation, Retrieval Augmented Generation (RAG), Software systems, Temperature distribution, Training data, source: IEEE},
  abstract = {Modern software systems increasingly rely on personal data. Despite the enforcement of the European General Data Protection Regulation (GDPR) and the growing awareness about privacy and data protection, many individuals’ rights remain unsatisfactorily implemented in software systems. This is partially due to the knowledge gap between legal interpretation and software development.In this paper, we address this gap first by extracting, in close collaboration with legal experts, a list of 108 requirements pertinent to the right of access (ACC) and the right to portability (PRT), two fundamental rights under the GDPR. We further propose the XTRAREG approach, which utilizes large language models (LLMs) and retrieval augmented generation (RAG) to provide automated assistance in extracting privacy requirements from predefined legal sources.Compared to the manually extracted requirements, XTRAREG can automatically generate requirements with an accuracy of 81.8\% for ACC and 85.7\% for PRT. Our empirical evaluation reveals two notable observations: (i) A skewed performance in terms of coverage in the favor of ACC, indicating the significant impact of abundant training data of the LLM, (ii) despite explicit exposure of legal references through RAG, the LLM generates requirements predominantly from the GDPR.},
  month = {sep},
}

@inproceedings{hajaghaie_leveraging_2025,
  title = {Leveraging {Large},
  author = {Hajaghaie, Ahmadreza and Thulasiram, Ruppa K.},
  year = {2025},
  doi = {10.1109/CiFer64978.2025.10975739},
  url = {https://doi.org/10.1109/cifer64978.2025.10975739},
  booktitle = {2025 {IEEE},
  pages = {1--7},
  keywords = {Adaptation models, Analytical models, Asset Allocation, Biological system modeling, Computational Finance, Data models, Generative AI, Investment, Large language models, Large Language Models, Llama, Portfolio management, Portfolios, Resource management, Retrieval augmented generation, Training data, source: IEEE},
  abstract = {This study assesses the Large Language Models (LLMs) in creating investment portfolios. We implement a few-shot learning technique, followed by Retrieval Augmented Generation (RAG) enhanced with comprehensive up-to-date financial data, using Meta's latest LLM, Llama 3.1-8b. In the first phase, We assess the models' efficacy using key financial indicators, including total returns, annualized volatility, riskadjusted performance (Sharpe ratio), potential loss estimates (value-at-risk), and their pre-training knowledge with the S\&P 500 Index performance baseline. In the second phase, we enhance the LLM's knowledge base by RAG and the latest historical and statistical metrics (such as earnings per share (EPS), dividends per share (DPS), profit margin, and many more) for each asset from different classes. The study constrains model inputs to specific sets of financial assets, such as equities, exchangetraded funds (ETFs), commodities, cryptocurrencies, and bonds. To evaluate model performance and adaptability, we analyzed across two distinct time frames: (1) within the models' training data cutoff, and (2) from the cutoff date to the present. This approach enables the assessment of model generalization to past and present market conditions. The research quantifies LLMs’ capabilities in financial asset allocation, comparing baseline performance against RAG-augmented strategies. Our results demonstrate that RAG-enhanced LLM significantly outperforms vanilla LLM in portfolio construction across various asset classes. We contemplate that these results could influence AI-driven financial decision-making processes such as automated trading, real-time sentiment analysis, and investment management.},
  month = {mar},
}

@inproceedings{hou_llm-based_2024,
  title = {{LLM},
  author = {Hou, Fuqing and Chen, Yibing and Shi, Xin and Li, Min and Zhao, Xueqing},
  year = {2024},
  doi = {10.1109/CoST64302.2024.00038},
  url = {https://doi.org/10.1109/cost64302.2024.00038},
  booktitle = {2024 {International},
  pages = {151--155},
  note = {Type: Conference paper},
  keywords = {Accuracy, Analytical models, Costs, Databases, Large language model, Large language models, Prototypes, Questions and answers, Retrieval augmented generation, User requirements, Vectors, source: IEEE, source: Scopus},
  abstract = {The new technology of artificial intelligence can bring users a fresh experience and accelerate the rapid development of cultural and tourism integration. In this paper, we use a large language model (LLM) to mine user implicit demands and implement intelligent Q\&A reasoning for the Chang’an Twelve Hours Scenic Area. First, multi-faceted data related to Chang’an Twelve Hours is collected to construct a vector database. Next, the LLM is utilized to mine the implicit requirements of user questions by combining attribute features such as gender and age. Then, the user demand vectors are retrieved and matched with the content vectors in the vector database. Finally, the similar content obtained from user demand and retrieval feedback is used to populate the prompt template and input into the LLM, realizing intelligent Q\&A and reasoning for the Twelve Hours of Chang’an. On this basis, a prototype of an intelligent Q\&A system tailored to the Twelve Hours of Chang’an is constructed. Experimental comparisons with the traditional Retrieval-Augmented Generation (RAG) technique demonstrate that using an LLM enables finer mining of user requirements and generates more accurate answers. The intelligent Q\&A system for Chang’an Twelve Hours developed in this paper effectively addresses users’ customized inquiries, thereby enhancing the user experience and improving the operational efficiency of the scenic spot. Additionally, this system can serve as a reference model for implementation at other cultural tourism attractions.},
  month = {aug},
  annote = {Cited by: 0},
}

@inproceedings{kamra_enhancing_2024,
  title = {Enhancing {Document},
  author = {Kamra, Vikas and Gupta, Lakshya and Arora, Dhruv and Yadav, Ashwin Kumar},
  year = {2024},
  doi = {10.1109/C2I663243.2024.10895931},
  url = {https://doi.org/10.1109/c2i663243.2024.10895931},
  booktitle = {2024 5th {International},
  pages = {1--7},
  keywords = {Cognition, Context modeling, Document Retrieval, Evaluation, Graph Neural Networks, Graph-Based RAG, Industries, Information processing, Knowledge Graphs, Large language models, Neural networks, Question answering (information retrieval), Recommender systems, Retrieval augmented generation, Surveys, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) has emerged as a potent method for enhancing the capabilities of large language models (LLMs) by integrating them with external knowledge sources. While traditional RAG models rely heavily on textual similarity for retrieval, often leading to issues like context drift and hallucinations, graph-based RAG offers a more sophisticated approach. By representing documents and their relationships within a graph structure, graph-based RAG enables more context-aware retrieval, reducing hallucinations, and facilitating multi-hop reasoning. This abstract provides an overview of the RAG landscape, contrasting traditional and graph-based approaches, and highlights the advantages of graph-based RAG in addressing the limitations of traditional methods. The application of graph-based RAG to various domains, such as question answering, dialogue systems, and recommendation systems, is also explored. The abstract concludes by emphasizing the potential of graph-based RAG to revolutionize information access and retrieval in diverse AI applications.},
  month = {dec},
}

@inproceedings{pratami_llm-based_2025,
  title = {{LLM},
  author = {Pratami, Rahmat and Ruhallah, Muhammad Lutfi and Gozali, Alfian Akbar},
  year = {2025},
  doi = {10.1109/ICoDSA67155.2025.11157184},
  url = {https://doi.org/10.1109/icodsa67155.2025.11157184},
  booktitle = {2025 {International},
  pages = {979--984},
  keywords = {Benchmark testing, chatbot, Chatbots, IT helpdesk, large language models, Multilingual, Pragmatics, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Scalability, Statistical analysis, Time factors, User experience, source: IEEE},
  abstract = {The growing need for streamlined IT service management in higher education has highlighted the limitations of traditional helpdesk models, which depend heavily on manual processes and human operators. These legacy systems often suffer from slow response times, accumulating service requests, and staff fatigue. To overcome these challenges, this study introduces an intelligent, LLM-powered chatbot that integrates the DeepSeek-v3 API with a Retrieval-Augmented Generation (RAG) strategy tailored to an Indonesian university’s IT service desk environment. Built upon a modular architecture, the proposed solution automates document ingestion with multi-format preprocessing, dynamic chunking, and embedding generation, which are stored in Supabase for efficient top-k contextual retrieval. When a user submits an inquiry, pertinent knowledge segments are retrieved in real time and synthesized into coherent, contextually appropriate responses by the DeepSeek-v3 language model. The system was evaluated through a comprehensive experimental framework, including functional and hardware benchmarking, performance testing, and a standardized User Experience Questionnaire (UEQ). Functional tests confirmed stable, 24/7 operation with over 98\% retrieval accuracy, while performance benchmarking demonstrated an average response time of 2.1 ± 0.2 seconds—a 35\% improvement over a legacy helpdesk baseline. User studies with 26 IT staff and students yielded a high UEQ score of 4.5 ± 0.3 (out of 5), reflecting strong satisfaction across pragmatic (efficiency, dependability) and hedonic (stimulation, novelty) dimensions. In addition, nine prompting strategies were compared via statistical analysis to optimize response quality. These findings demonstrate the viability and scalability of integrating LLMs with institutional IT infrastructures and offer practical guidance for deploying RAG-based chatbots in resource-constrained academic settings. By detailing implementation best practices and comparative evaluations, this work advances AI-driven automation in academic IT support and lays the groundwork for future enhancements in multilingual and low-resource environments.},
  month = {jul},
}

@inproceedings{j_intelligent_2024,
  title = {Intelligent {Conversations},
  author = {J, Ravichandran and Khalaf, Hameed Hassan and Mohammed, Nada Qasim and Al-Hussein, Rouaida Kadhim A. and Pirani, Safzan and Nithin, Enthala and Pujitha, Kasoju},
  year = {2024},
  doi = {10.1109/ARIIA63345.2024.11051881},
  url = {https://doi.org/10.1109/ariia63345.2024.11051881},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Accuracy, Artificial Intelligence, Deep Learning, Knowledge based systems, Large Language Model, Large language models, Measurement, Natural Language Processing, Oral communication, Question answering (information retrieval), Real-time systems, Reliability, Retrieval augmented generation, Retrieval Augmented Generation, Scalability, source: IEEE},
  abstract = {This research tries to implement the RAG approach to combine LLMs with other techniques and create a system that can talk directly to the system. The core of this endeavor hinges on the combination of the LLMs ability to generate meaningful responses and the dynamism of the model to recall and incorporate contextual information stored in a corpus of documents during the discussion. This combination tackles lack of latest knowledge and specific information that can be addressed by integrating LLMs with an updated knowledge base which is kept external. The project shows how the interaction of RAG and LLMs is a precondition for the creation of an agent with a broader capacity and greater accuracy for communication in academic and professional fields. When we equip them with these abilities, we open the door to deeper and more context-sensitive interactions as they can directly refer to and retrieve data from specific records and other sources. The system is identified to be beneficial in education institutions where it will assist students/researchers to interact with academic text, research paper and other educational content through conversation hence achieving a simpler, interesting and more effective learning process. In addition, this project requires the establishment of a robust retriever, an integration of this retriever with current LLMs technology, and a set up of an interface that users can communicate with the system via spoken language. The project combines both retrieval techniques with LLMs to use as a basis and conduct research aimed at making AI systems the way they are more interactive and resourceful while handling real-world information.},
  month = {dec},
}

@article{agarwal_toward_2025,
  title = {Toward {Inclusive},
  author = {Agarwal, Ishita and Sakthivel, V. and Prakash, P.},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3594218},
  url = {https://doi.org/10.1109/access.2025.3594218},
  journal = {IEEE Access},
  volume = {13},
  pages = {136420--136432},
  keywords = {Accuracy, Artificial intelligence, Chatbots, Gemini, generative AI, large language models, medical chatbot, medical diagnosis, Medical diagnostic imaging, Medical services, Oral communication, Random forests, retrieval augmented generation, Retrieval augmented generation, Training, Vectors, source: IEEE},
  abstract = {This paper presents the design and development of a multimodal medical chatbot that leverages Gemini-2.0-Flash Model alongside a novel Retrieval-Augmented Generation (RAG) architecture to support preliminary medical diagnosis and recommendations. The system integrates textual prompt analysis and medical image interpretation, aiming to improve healthcare accessibility, particularly for underserved populations. Focused on data-rich medical conditions, the chatbot generates reliable diagnostic insights based on natural language inputs and/or medical images, requiring minimal user expertise. The proposed RAG-based architecture incorporates a curated medical knowledge base and structured retrieval mechanisms, significantly reducing hallucinations and enhancing response credibility compared to direct Large Language Model (LLM) querying. By demonstrating the efficacy of multimodal reasoning in conjunction with structured retrieval, this work paves the way for more accessible, accurate, and scalable AI-driven health support systems.},
  issn = {2169-3536},
}

@inproceedings{rachha_llm-enhanced_2024,
  title = {{LLM},
  author = {Rachha, Ashwin and Seyam, Mohammed},
  year = {2024},
  doi = {10.1109/FIE61694.2024.10893211},
  url = {https://doi.org/10.1109/fie61694.2024.10893211},
  booktitle = {2024 {IEEE},
  pages = {1--9},
  note = {ISSN: 2377-634X},
  keywords = {AI in Education, ChatGPT, Codes, Computer science education, Computer Science Education, Data models, Data structures, Education, Encoding, Guardrails, Large language models, Large Language Models, Retrieval augmented generation, Retrieval Augmented Generation, Reviews, Technological innovation, source: IEEE},
  abstract = {In this Innovative Practice full paper, we introduce Gurukul, an innovative coding platform designed to support teaching Data Structures and Algorithm (DSA) course by integrating advanced Large Language Models (LLMs). LLMs have emerged as powerful tools in Computer Science Education (CSEd), offering unparalleled opportunities for enhancing student comprehension and engagement. However, their use in educational settings presents challenges, including tendencies toward hallucination, contextual inaccuracies, and the risk of undermining critical thinking by providing explicit solutions. To address these challenges, and to explore how specialized LLMs can bolster learner engagement, we present Gurukul, a platform featuring dual innovations: Retrieval-Augmented Generation (RAG) and Guardrails. Gurukul offers a hands-on practice feature where students can solve DSA problems within a code editor, supported by a dynamically Guardrailed LLM that prevents the delivery of explicit solutions. Additionally, the platform's study feature utilizes RAG, drawing from OpenDSA as a trusted source, to ensure accurate and contextually relevant information is provided. To assess the platform's effectiveness, we conducted a User Study with students, and a User Expert Review with faculty from a U.S. public state university specializing in DSA courses. Our analysis of student usage patterns and perceptions, along with insights from instructors, reveal that Gurukul positively impacted student engagement and learning in DSA, demonstrating the potential of specialized LLMs to enhance educational outcomes in this field.},
  month = {oct},
}

@inproceedings{syeda_llm-based_2024,
  title = {{LLM},
  author = {Syeda, Masooma Zehra and Bukhari, Syed Usama Khalid and Hussain, Maqbool and Khan, Wajahat Ali and Shah, Syed Sajid Hussain},
  year = {2024},
  doi = {10.1109/EMBC53108.2024.10782599},
  url = {https://doi.org/10.1109/embc53108.2024.10782599},
  booktitle = {2024 46th {Annual},
  pages = {1--4},
  note = {ISSN: 2694-0604},
  keywords = {Biological system modeling, decision support, Diseases, information retrieval, Kidney, kidney diseases, large language models, Large language models, Measurement, Medical diagnosis, Medical diagnostic imaging, natural language processing, Prompt engineering, retrieval augmented generation, Retrieval augmented generation, Standards, source: IEEE},
  abstract = {Large language models revolutionize the recent paradigm in the medical field and its contributing to various applications, diversified from clinical decision support to information extraction and summarization. The substantial linguistic understanding and contextual awareness allow language models to process and evaluate decision tasks. Concurrently, it addresses the challenges encountered by pathologists in disease diagnosis by adeptly retrieving precise and accurate facts from an external knowledge base. In this paper, we propose a framework which incorporates advanced retrieval augmented generation with prompt engineering techniques, contain prompting levels and structured prompts, which enables the model to extract refine, and customize responses. The model has been equipped with a large corpus of several kidney diseases clinical data which is collected from the vast information sources of kidney diagnostic books. The utilization of varied prompt techniques, exemplified by standard prompts like few-shots and the Reasoning Act (ReAct), manifests notable improvements in disease diagnosis responses. Structured prompts are designed to provide pathologists with specific instructions for formulating questions that effectively enhance the performance of the model. In the evaluation of prompt performance, three key metrics are employed answer relevance, faithfulness, and context relevance. Notably, in the context relevance metric, an optimal performance score of 1.0 was attained indicating perfect alignment with the conversational context.},
  month = {jul},
}

@article{xu_chattf_2024,
  title = {{ChatTf},
  author = {Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
  year = {2024},
  doi = {10.1109/ACCESS.2024.3485877},
  url = {https://doi.org/10.1109/access.2024.3485877},
  journal = {IEEE Access},
  volume = {12},
  pages = {162638--162650},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Accuracy, Cognition, Cultural differences, Knowledge engineering, Knowledge graph, Knowledge graphs, large language model, Large language models, Ontologies, question answering, Reliability, retrieval-augmented generation, Semantics, traditional folklore, Training, source: IEEE, source: Google Scholar},
  abstract = {Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q\&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q\&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7\% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.},
  issn = {2169-3536},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{quynh_nhu_rag-smartvuln_2025,
  title = {{RAG},
  author = {Quynh Nhu, Nguyen Dang and Minh Quan, Le and Van, Thai Hung and Minh Trung, Doan and Duy, Phan The},
  year = {2025},
  doi = {10.1109/MAPR67746.2025.11134018},
  url = {https://doi.org/10.1109/mapr67746.2025.11134018},
  booktitle = {2025 {International},
  pages = {1--6},
  note = {ISSN: 2770-6850},
  keywords = {Accuracy, Benchmark testing, Computer bugs, Faces, Fine-tuning, Large language models, Large Language Models, Pattern recognition, RAG, Retrieval augmented generation, Security, Smart Contract, Smart contracts, Solid modeling, Vulnerabilities Detection, source: IEEE},
  abstract = {The burgeoning adoption of economically incentivized smart contracts faces persistent security vulnerabilities, resulting in significant financial losses due to their immutability post-deployment. This paper presents a novel framework integrating fine-tuned large language models (LLMs) with Retrieval-Augmented Generation (RAG) to enhance the precision and explainability of smart contract vulnerability detection. By fine-tuning an open-source LLM and employing RAG, our model dynamically incorporates domain-specific external knowledge during inference, significantly improving threat identification. On two public benchmarks, SolidiFI-Benchmark and Smart Bugs Curated, our fine-tuned Qwen2.5-Coder-14B model (QC-14B-FT) outperforms zero-shot LLMs (GPT-3.5 with and without RAG) in terms of F1-score. Specifically, QC-14B-FT achieves an F1-score of 0.64 on SolidiFI, surpassing GPT-3.5-RAG by 9\% and GPT-3.5 by 10\%. On Smart Bugs Curated, QC-14B-FT achieves an F1-score of 0.73, outperforming GPT-3.5-RAG by 14\% and GPT-3.5 by 19\%. These results demonstrate the effectiveness of combining RAG with fine-tuning to provide accurate and clear smart contract security assessments.},
  month = {aug},
}

@inproceedings{panday_puaa_2024,
  title = {{PUAA},
  author = {Panday, Sijan and Ghosh, Robin and Rahman, Md Atiqur},
  year = {2024},
  doi = {10.1109/IEMCON62851.2024.11093120},
  url = {https://doi.org/10.1109/iemcon62851.2024.11093120},
  booktitle = {2024 {IEEE},
  pages = {066--073},
  keywords = {Accuracy, Artificial intelligence, Chatbot, Chatbots, Digital transformation, Education, Knowledge acquisition, LLM, Quality of service, RAG, Retrieval augmented generation, Technological innovation, University AI assistant, User experience, source: IEEE},
  abstract = {A significant improvement in knowledge acquisition has been made by introducing Large Language Models (LLMs) like Chat-GPT and Bard, which provide a more efficient way to acquire information than the labor-intensive procedure of going through various online checkpoints. This new tendency in LLMs makes the widely used rule-based chatbots that colleges use look antiquated and inadequate. To better serve the needs of students looking for information about their universities, this research project suggests incorporating LLM technology into university websites. Our research introduces Personal University AI Assistant (PUAA) with methodology leveraging the RetrievalAugmented Generation (RAG) framework, utilizing the power of the LangChain in combination with cutting-edge LLMs like Mistral-7B, which is made available by the Hugging Face as an open-source option, and Chat GPT 3.5 as the most optimal solution for the problem of fast knowledge acquisition. PUAA improves the students’ experience of accessing university knowledge by providing instant, accurate information while reducing the workload of administrative staff and allowing them to focus on more complex inquiries and tasks. PUAA aims to promote the adoption of artificial intelligence (AI) in educational institutions, demonstrating the viability and benefits of advanced AI tools in enhancing the academic experience and information accessibility for students and staff.},
  month = {oct},
}

@inproceedings{saha_special_2025,
  title = {Special {Session},
  author = {Saha, Dipayan and Al Shaikh, Hasan and Tarek, Shams and Farahmandi, Farimah},
  year = {2025},
  doi = {10.1109/VTS65138.2025.11022932},
  url = {https://doi.org/10.1109/vts65138.2025.11022932},
  booktitle = {2025 {IEEE},
  pages = {1--5},
  note = {ISSN: 2375-1053},
  keywords = {Cognition, Complexity theory, Computational efficiency, Computational modeling, Computer architecture, Hardware security, Hardware Security and Trust, LLM, Manuals, Retrieval augmented generation, Security Test Plan Generation, Security Threat Modeling, Threat modeling, Very large scale integration, source: IEEE},
  abstract = {Current hardware security verification processes predominantly rely on manual threat modeling and test plan generation, which are labor-intensive, error-prone, and struggle to scale with increasing design complexity and evolving attack methodologies. To address these challenges, we propose ThreatLens, an LLM-driven multi-agent framework that automates security threat modeling and test plan generation for hardware security verification. ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant security knowledge, LLM-powered reasoning for threat assessment, and interactive user feedback to ensure the generation of practical test plans. By automating these processes, the framework reduces the manual verification effort, enhances coverage, and ensures a structured, adaptable approach to security verification. We evaluated our framework on the NEORV32 SoC, demonstrating its capability to automate security verification through structured test plans and validating its effectiveness in real-world scenarios.},
  month = {apr},
}

@inproceedings{caglayan_structured_2025,
  title = {Structured {Financial},
  author = {Çağlayan, Alperen and Gökçe, Saliha Nur and Ayata, Değer},
  year = {2025},
  doi = {10.1109/UBMK67458.2025.11207079},
  url = {https://doi.org/10.1109/ubmk67458.2025.11207079},
  booktitle = {2025 10th {International},
  pages = {539--544},
  note = {ISSN: 2521-1641},
  keywords = {Cognition, Financial NLP, Financial Question Answering, Fine-Tuning, Large Language Models, LLaMA, LoRA, Market research, Measurement, Numerical models, Pipelines, Question answering (information retrieval), RAG, Retrieval augmented generation, Retrieval-Augmented Generation, Search engines, Semantics, Structured QA, Turkish Financial Data, Vectors, source: IEEE},
  abstract = {This paper provides a comparative study of two major language model (LLM) strategies (instruction-based fine-tuning and retrieval-augmented generation (RAG)) for corporate financial question answering. A modular AI pipeline is designed where each financial domain (e.g. liquidity, investment, credit analysis) is treated as an independent QA module that operates on structured company-level data. The dataset includes Turkish public company financial statements from 2008 to 2025. The fine-tuned variant based on LLaMA 3 8B is trained on domain-specific prompts in Turkish using LoRA adapters. The RAG-based variant utilizes a vector search engine to retrieve relevant financial pieces. In addition, tabular reasoning is integrated using pandas to provide dynamic, code-based access to structured data and is developed as a more advanced version (ragenh). To evaluate the quality of generated answers, a set of metrics are applied that capture semantic similarity, numerical accuracy, and directional consistency—such as ROUGE-L, BERTScore, and domain-specific number and trend alignment scores. The results obtained from these metrics show that while the fine-tuned models perform well in interpretive and trend-based tasks, ragenh outperforms both baselines in ground truth and opinion-based reasoning. This work provides a scalable framework for building interpretable financial assistants in under-resourced language environments by combining modular QA design, hybrid architectures, and custom evaluation. The findings contribute to developing robust, context-aware LLM applications for financial decision support.},
  month = {sep},
}

@inproceedings{pouhela_context-aware_2025,
  title = {Context-{Aware},
  author = {Pouhela, Franc and Krummacker, Dennis and Schotten, Hans D.},
  year = {2025},
  doi = {10.1109/ICCWorkshops67674.2025.11162197},
  url = {https://doi.org/10.1109/iccworkshops67674.2025.11162197},
  booktitle = {2025 {IEEE},
  pages = {505--511},
  note = {ISSN: 2694-2941},
  keywords = {6G, AI, Communication systems, Conferences, Hands, Internet of Things, IoT, Knowledge graphs, Large language models, LLM, Natural language processing, RAG, Real-time systems, Retrieval augmented generation, Technological innovation, source: IEEE},
  abstract = {A major challenge to the advancement of the Internet of Things (IoT) as we know it, is related to the need of a context-aware communication framework that can dynamically adapt to environments with constantly changing requirements. The paper at hand presents a potential avenue to address this limitation. The study proposes an innovative approach to enable context-awareness in dynamic IoT settings by leveraging the Natural Language Processing (NLP) capabilities of the Large Language Models (LLMs) in conjunction with the Knowledge Graphs (KGs) within a custom Retrieval-Augmented Generation (RAG) system to enable real-time understand and processing of the Natural Language (NL) instructions. This approach makes the communication system flexible and holds the potential for future innovations in the realm of IoT technologies.},
  month = {jun},
}

@inproceedings{azam_cancerbot_2024,
  title = {{CancerBot},
  author = {Azam, Ayesha and Naz, Zubaira and Khan, Muhammad Usman Ghani},
  year = {2024},
  doi = {10.1109/ICOSST64562.2024.10871155},
  url = {https://doi.org/10.1109/icosst64562.2024.10871155},
  booktitle = {2024 18th {International},
  pages = {1--6},
  note = {ISSN: 2770-8225},
  keywords = {Cancer, Chatbot, Chatbots, Databases, Large language models, Llama2, LLM, Medical services, RAG, Reliability engineering, Retrieval augmented generation, Standards, Testing, Vector Databases, Vectors, source: IEEE},
  abstract = {Cancer is the second leading cause of death globally, highlighting the urgent need for innovative solutions to support patients and healthcare providers. Remote and digital cancer care interventions, such as chatbots, are more relevant than ever, offering timely information and personalized assistance. With the rise of conversational AI, chatbots have emerged as a potential solution to bridge this gap, offering 24/7 assistance and personalized responses. This paper introduces CancerBot, a Retrieval-Augmented Generation (RAG) system designed to address the need for reliable, contextually relevant cancer-related information. Built on top of a large language model LLaMA-2, and integrated with a vector database, CancerBot retrieves relevant medical literature to generate accurate responses, minimizing the risk of hallucinations. Through synthetic testing and evaluation using metrics such as faithfulness, context relevancy, and response relevancy, our results demonstrate that the RAG-based CancerBot significantly outperforms the standard LLaMA-2 model in addressing cancer-specific queries. This advancement showcases the potential of RAG systems to improve patient care by providing reliable, context-driven information that enhances patient engagement and healthcare decision-making.},
  month = {dec},
}

@inproceedings{ates_semantic_2025,
  title = {Semantic {Chunking},
  author = {Ateş, Yiğit and Sayar, Alperen and Bozlar, İbrahim Umut and Ertuğrul, Seyit and Arslan, Suayb S.},
  year = {2025},
  doi = {10.1109/MLSP62443.2025.11204203},
  url = {https://doi.org/10.1109/mlsp62443.2025.11204203},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2161-0371},
  keywords = {Conferences, Generative AI, information retrieval, Information retrieval, large language models, Large language models, Machine learning, Metadata, natural language processing, Natural language processing, retrieval augmented generation, Retrieval augmented generation, semantic chunking, Semantics, Signal processing, source: IEEE},
  abstract = {This paper presents a novel two-phase semantic chunking methodology designed to enhance document processing within Retrieval-Augmented Generation (RAG) systems. The proposed approach utilizes Large Language Models (LLMs) and Chain of Thought (CoT) to systematically generate and refine document chunks, while concurrently producing associated metadata, such as hypothetical user queries and contextual tags. By integrating established information retrieval techniques-namely Best Matching 25 (BM25)-with the advanced semantic understanding capabilities of LLMs, the proposed method substantially improves the relevance and quality of retrieved context for Generative Artificial Intelligence (GenAI) applications. Empirical evaluations reveal that this approach yields significant improvements in response accuracy and contextual relevance when compared to traditional chunking techniques. The implementation utilizes the open-source Qwen 2.572 B model for its semantic processing operations, demonstrating how state-of-the-art language models can be effectively deployed in practical RAG systems.},
  month = {aug},
}

@inproceedings{koh_clara_2025,
  title = {Clara: {Context},
  author = {Koh, Chan Young and DeMedeiros, Kyle and Hendawi, Abdeltawab},
  year = {2025},
  doi = {10.1109/MDM65600.2025.00035},
  url = {https://doi.org/10.1109/mdm65600.2025.00035},
  booktitle = {2025 26th {IEEE},
  journal = {2025 26th IEEE …},
  pages = {1--8},
  note = {ISSN: 2375-0324},
  keywords = {Anomaly detection, Data analysis, Health Monitoring, Large language models, Mobile Sensors, Monitoring, Quality control, Retrieval augmented generation, Retrieval-Augmented Generation, Sensors, Smart phones, Statistical analysis, Vectors, source: IEEE, source: Google Scholar},
  abstract = {Mobile devices are equipped with various sensors that continuously gather data about user activity and environmental conditions. Detecting anomalies in this sensor data is crucial for both health monitoring applications and technical quality control. This paper presents a novel framework, namely CLARA, that leverages Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) to detect anomalies in mobile device sensor data. Using the ExtraSensory dataset, which contains labeled sensor data from smartphones and smartwatches, we demonstrate how RAG enhances LLM-based anomaly detection by retrieving relevant historical patterns and domain knowledge to provide context-rich analysis. Our framework serves dual purposes: (1) providing health and lifestyle insights to end-users through anomaly detection in their daily activities and (2) offering manufacturers a tool for identifying technical sensor malfunctions during quality control processes. The framework delivers rich, contextual explanations of detected anomalies, making the results actionable for end-users and technical teams, addressing key industry challenges in sensor data analysis.},
  month = {jun},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_llm-based_2025,
  title = {{LLM},
  author = {Chen, Mingkai and Sun, Zhende and He, Xitao and Wang, Lei and Al-Dulaimi, Anwer},
  year = {2025},
  doi = {10.1109/LWC.2025.3583053},
  url = {https://doi.org/10.1109/lwc.2025.3583053},
  journal = {IEEE Wireless Communications Letters},
  volume = {14},
  number = {10},
  pages = {3029--3033},
  keywords = {Accuracy, Adaptation models, Data mining, Encoding, hallucination, Knowledge based systems, LLMs, Pipelines, prompt, Retrieval augmented generation, Semantic communication, semantic triples, Semantics, Triples (Data structure), source: IEEE},
  abstract = {Semantic communication represents a paradigm shift in 6G communications, emphasizing the transmission of meaning rather than solely syntactic elements. Despite this advancement, current approaches exhibit limitations in generality, adaptability to dynamic environments, and dependence on static knowledge bases. To address these challenges, we propose a novel Large Language Model-based Semantic Communication (LLM-SemCom). LLM-SemCom incorporates three key innovations: 1) structured semantic triple representation that mitigates LLM hallucinations unlike existing unstructured approaches, 2) knowledge-base-free LLM semantic processing that adapts dynamically without static domain constraints, and 3) Retrieval-Augmented Generation-enhanced personalization that maintains semantic fidelity while enabling user-specific adaptation. Experimental results demonstrate that LLM-SemCom significantly outperforms existing methods, achieving up to a 22.7\% improvement in sentence similarity while maintaining consistent performance across diverse languages and channel conditions.},
  issn = {2162-2345},
  month = {oct},
}

@inproceedings{gaddameedi_rag_2025,
  title = {{RAG},
  author = {Gaddameedi, Suvardhan Dileep and Aryan, Sudeep},
  year = {2025},
  doi = {10.1109/ICCC64910.2025.11077193},
  url = {https://doi.org/10.1109/iccc64910.2025.11077193},
  booktitle = {2025 6th {International},
  pages = {1--7},
  keywords = {Aerodynamics, Artificial intelligence, Chatbots, Computer architecture, Filtering, Lang Chain, Libraries, LlamaIndex, Logic, Navigation, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation, source: IEEE},
  abstract = {The Retrieval-Augmented Generation (RAG) HUB framework introduces a visionary, streamlined approach to simplify AI development by addressing complex challenges that often dissuade new developers. The unique selling point of RAG HUB lies in its question-driven methodology utilizing the libraries provided by robust frameworks such as LangChain and LlamaIndex. This methodology allows users with varying expertise to create efficient AI architectures that are tailored to their use case with tailored prompts. For instance, developers working on creating chatbots or retrieving data can progressively follow a structured path from broad problem statements like a chatbot with SharePoint-stored data to niche-specific solution-based choices. Thus, it allows for the removal of trial and error, enabling faster and more precise solutions.RAG HUB uses dynamic filtering, decision tree logic, and an extensive repository of validated techniques to concentrate the development process across various AI tasks, from data ingestion to embedding and retrieval. The framework eliminates irrelevant methods as developers navigate through guided queries, ensuring that only the most applicable techniques remain. This segmented strategy proves to be optimal in all ways of implementation adapting itself to real-time the ever-evolving requirements and also in reducing the learning curve and time invested in modeling advanced AI frameworks. Further enhancements include automating the process from problem statement input and real-time user feedback aiming to make RAG HUB more responsive and intuitive, thereby solidifying its role as an imperative tool in the rapidly changing AI realm.},
  month = {may},
}

@inproceedings{huang_chatdc_2025,
  title = {{ChatDC},
  author = {Huang, Jin and Sun, Yu and Dou, Jinhui and Zhou, Haibo},
  year = {2025},
  doi = {10.1109/VTC2025-Spring65109.2025.11174324},
  url = {https://doi.org/10.1109/vtc2025-spring65109.2025.11174324},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2577-2465},
  keywords = {data center, Data centers, Faces, Industries, Large language models, Large Language Models, Measurement, multi-agent, multimodal, operations, Personnel, Retrieval augmented generation, Retrieval-Augmented Generation, Scalability, Testing, Vehicular and wireless technologies, source: IEEE},
  abstract = {Data centers (DC), as the computational core, play a pivotal role in driving the development of various industries. However, their 24/7 operational workload places immense pressure on operations staff, and traditional AI for IT Operations technologies due to their inability to communicate with operations personnel through language, are unable to effectively alleviate this pressure. In contrast, Retrieval-Augmented Generation (RAG), by integrating external knowledge bases with Large Language Models, enables effective communication with operations teams. Nevertheless, the direct application of RAG in DC currently faces various challenges, such as limited text representation, context length limitations, and poor scalability for large-scale databases. In this paper, we propose multi-modal multi-agent framework for DC facility operations. This framework is primarily composed of a manager and multiple specialized experts, which supports providing intuitive multi-modal information to assist in responses and optimizes response quality in terms of both depth and breadth, delivering more comprehensive and precise answers to offer an innovative solution for enhancing DC operations. Finally, we build a DC operations question-answering dataset and conduct comparative testing of the proposed framework using multiple evaluation metrics.},
  month = {jun},
}

@inproceedings{hsu_addressing_2024,
  title = {Addressing {LLM},
  author = {Hsu, Hao-Hsuan and Huang, Nen-Fu},
  year = {2024},
  doi = {10.1109/ICAIRC64177.2024.10900148},
  url = {https://doi.org/10.1109/icairc64177.2024.10900148},
  booktitle = {2024 4th {International},
  journal = {2024 4th International Conference on …},
  pages = {283--287},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Costs, Detectors, Information retrieval, keyword network, Large language models, LLM challenges, Logistic regression, machine learning, Question answering (information retrieval), question answering system, Reliability, Retrieval augmented generation, Robots, Standards, source: IEEE, source: Google Scholar},
  abstract = {Detecting similar or duplicate texts or questions is a critical subtask in information retrieval. Despite the remarkable advancements achieved by large language models (LLMs) in the question answering (QA) domain in recent years, challenges such as unreliable responses caused by LLM hallucinations and outdated information have highlighted the continued importance of information retrieval techniques. The integration of retrieval-augmented generation (RAG) with LLMs has mitigated some of these limitations, enhancing the reliability of applying LLMs to domain-specific knowledge. This paper uses Chinese educational questions as a case study and introduces an innovative framework for duplicate question detection, termed the Balanced Duplicate Question Detector (BDQD). The BDQD incorporates multiple extensible detectors and a machine learning-based balancer. Experimental results reveal that detecting duplicate questions involves an inherent trade-off between precision and recall. Logistic regression proves effective in balancing thresholds across different detectors, establishing a more stable and reliable standard for duplicate question determination. Finally, we propose a hybrid QA framework that considers both cost and efficiency, integrating the lightweight question retrieval architecture developed in this study with RAG and LLMs. This framework offers practical recommendations for building robust and efficient QA systems.},
  month = {dec},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{s_leveraging_2024,
  title = {Leveraging {Technology},
  author = {S, Anju and Krishnan, Anuja G and G, Veena},
  year = {2024},
  doi = {10.1109/GCAT62922.2024.10923869},
  url = {https://doi.org/10.1109/gcat62922.2024.10923869},
  booktitle = {2024 5th {IEEE},
  pages = {1--7},
  keywords = {Accuracy, Agriculture, COSINE SIMILARITY, Databases, EMBED-DING, Farming, Focusing, Knowledge based systems, Large language models, LLMS, NLP, Oral communication, PROMPTING, RAG, Retrieval augmented generation, VECTOR DATABASE, Vectors, source: IEEE},
  abstract = {This research addresses the critical need to bridge the gap between traditional agricultural knowledge and modern findings, particularly focusing on millet cultivation in India. Millet, being a staple grain in the region, holds significant importance for both farmers and agricultural researchers. However, the lack of accessible technology exacerbates the dissemination of crucial information to farmers. To tackle this challenge, our study proposes a comprehensive solution leveraging cutting-edge technologies such as Large Language Models (LLMs), Prompting, and vector database management. The core of our approach centers on the utilization of RAG techniques, which involves scraping and segmenting millet-related data into manageable chunks. These chunks are then transformed into vectors using LLMs and stored in the vector database. Similarity measures such as cosine similarity to obtain relevant vectors in response to questions posed by farmers. Prompting is then employed to provide responses that understood by humans. Crucially, the technology prevents the spread of false information by refusing to respond to questions that are not related to agriculture, thus guaranteeing accuracy.},
  month = {oct},
}

@inproceedings{bouchekir_hallucination_2025,
  title = {Hallucination {Detection},
  author = {Bouchekir, Radouane and Faghih, Fathiyeh and Beyene, Tewodros A.},
  year = {2025},
  doi = {10.1109/DSN-W65791.2025.00076},
  url = {https://doi.org/10.1109/dsn-w65791.2025.00076},
  booktitle = {2025 55th {Annual},
  pages = {274--281},
  note = {ISSN: 2325-6664},
  keywords = {source: IEEE},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing, enabling high-quality content generation. However, they remain prone to hallucinations, instances where generated outputs deviate from factual truth. In this paper, we propose a novel approach that integrates Beam Search Sampling (BSS) with Semantic Consistency Analysis to systematically detect factual hallucinations. Our method leverages BSS to generate multiple candidate answers, capturing the model's confidence distribution across different plausible answers. These answers are then clustered based on semantic similarity, and a Natural Language Inference (NLI) model is applied to assess entailment and contradiction relationships. To quantify hallucinations, we introduce a scoring mechanism that combines token probabilities with semantic similarity metrics. Additionally, for cases where BSS produces a single answer, we incorporate a Chain-of-Verification (CoVe) mechanism to perform self-consistency checks. We evaluate our approach using Llama3.8B on the TruthfulQA dataset, achieving a precision of 0.87 and an AUROC of 0.81 for multi-answer generation. For single-answer verification with CoVe, we report a precision of 0.64 and accuracy of 0.71. Our approach outperforms conventional semantic entrony-based methods.},
  month = {jun},
}

@inproceedings{mawardi_indonesian_2025,
  title = {Indonesian {Educational},
  author = {Mawardi, Viny Christanti and Trilaksono, Bambang Riyanto and Purwarianti, Ayu},
  year = {2025},
  doi = {10.1109/ICoDSA67155.2025.11157494},
  url = {https://doi.org/10.1109/icodsa67155.2025.11157494},
  booktitle = {2025 {International},
  pages = {667--672},
  keywords = {Accuracy, Chatbots, educational chatbot, embedding, Filtering, Generators, Large language models, LLM, Measurement, PJOK, RAG, Reliability, Retrieval augmented generation, Semantics, Sports, source: IEEE},
  abstract = {This study evaluates the effect of retrieval embeddings and similarity metrics at the generative component (LLaMA3). No fine-tuning or decoding variation was applied to the generator, ensuring that any performance differences were due to retrieval quality alone. This design enables a focused evaluation of how retrieval configurations influence the final output in a controlled RAG setup, particularly for primary school topics in sports and health (PJOK). We point out that RAG is a technique that helps large language models make fewer mistakes by using outside information, which improves the accuracy of facts and the relevance of the context. The research compares various embedding models and retrieval similarity metrics to evaluate their effectiveness in retrieving relevant information. The baai/bge-m3 and gte-multilingual-base models demonstrate strong performance with 0.98 precision, while the indo-bge-m3 model shows high recall but lower precision. From 5 embeddings, Llama Embedding performs significantly worse than the other models. The dot product has higher recall than Euclidean and cosine similarity metrics but embedding model selection significantly influences overall performance. The study underscores the importance of embedding choice and retrieval quality in improving chatbot responses. Further refinements are suggested, including better filtering, reranking retrieved chunks, and fine-tuning the retriever, or LLaMA, to align with the target language domain, such as Bahasa Indonesia. The results show that even though RAG improves how well things are generated, the quality of what is retrieved is still a major problem, which means we need to keep making improvements for the best results in educational uses.},
  month = {jul},
}

@inproceedings{qiu_stella_2024,
  title = {{SteLLA},
  author = {Qiu, Hefei and White, Brian and Ding, Ashley and Costa, Reinaldo and Hachem, Ali and Ding, Wei and Chen, Ping},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825385},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825385},
  booktitle = {2024 {IEEE},
  pages = {8154--8163},
  note = {ISSN: 2573-2978},
  keywords = {Big Data, Biology, Data mining, Error analysis, Knowledge based systems, Large language models, LLM-based ASAG system, QA-based evaluation, RAG, Reliability, Retrieval augmented generation, structured evaluation, source: IEEE},
  abstract = {Large Language Models (LLMs) have shown strong general capabilities in many applications. However, how to make them reliable tools for some specific tasks such as automated short answer grading (ASAG) remains a challenge. We present SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval Augmented Generation (RAG) approach is used to empower LLMs specifically on the ASAG task by extracting structured information from the highly relevant and reliable external knowledge based on the instructor-provided reference answer and rubric, b) an LLM performs a structured and question-answering-based evaluation of student answers to provide analytical grades and feedback. A real-world dataset which contains students’ answers in an exam was collected from a college-level Biology course. Experiments show that our proposed system can achieve substantial agreement with the human grader while providing break-down grades and feedback on all the knowledge points examined in the problem. A qualitative and error analysis of the feedback generated by GPT4 shows that GPT4 is good at capturing facts while may prone to inferring too much implication from the given text in the grading task which provides insights into the usage of LLMs in the ASAG system.},
  month = {dec},
}

@inproceedings{sun_fusion-based_2025,
  title = {Fusion-{Based},
  author = {Sun, Yumeng and Zhang, Renhan and Meng, Renzi and Lian, Lian and Wang, Heyi and Quan, Xuehui},
  year = {2025},
  doi = {10.1109/CISAT66811.2025.11181772},
  url = {https://doi.org/10.1109/cisat66811.2025.11181772},
  booktitle = {2025 8th {International},
  pages = {116--120},
  keywords = {Accuracy, Buildings, cross-domain question answering, Knowledge fusion, Large language models, Question answering (information retrieval), Retrieval augmented generation, retrieval enhancement generation, Solids, Stability analysis, Standards organizations, structured knowledge, Systematics, Terminology, source: IEEE},
  abstract = {This paper proposes a Retrieval-Augmented Generation (RAG) model that integrates structured and unstructured knowledge. The aim is to enhance the knowledge coverage and generation accuracy of large language models in complex question-answering tasks. The method introduces a dual-channel knowledge retrieval mechanism. One channel targets structured knowledge sources such as knowledge graphs and databases. The other focuses on unstructured textual resources such as documents and paragraphs. A unified knowledge fusion network integrates both types of heterogeneous information into a coherent generation context. The model performs multi-level modeling across key components. These include query representation generation, knowledge retrieval, representation alignment, and fusion expression construction. As a result, the generation stage produces text that is semantically rich and logically consistent. Under low-resource conditions, the method significantly improves the accuracy and linguistic quality of generated outputs. It also shows strong stability and generalization in cross-domain tasks. Systematic experiments were conducted on the proportion of structured knowledge, types of knowledge sources, and fusion strategies. The results demonstrate the effectiveness of the fusion architecture in enhancing knowledge representation in language models. This study provides a methodological foundation and empirical support for building controllable and trustworthy knowledge-enhanced natural language generation systems.},
  month = {jul},
}

@inproceedings{zeng_federated_2024,
  title = {Federated {Recommendation},
  author = {Zeng, Huimin and Yue, Zhenrui and Jiang, Qian and Wang, Dong},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825302},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825302},
  booktitle = {2024 {IEEE},
  pages = {8078--8087},
  note = {ISSN: 2573-2978},
  keywords = {Chatbots, Feature extraction, federated recommendation, Hands, Hybrid power systems, large language models, Large language models, Process mining, Recommender systems, Retrieval augmented generation, sequential recommendation, Servers, Training data, source: IEEE},
  abstract = {Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as incomplete recommendation and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, in the second stage, the results returned by hybrid retrieval are converted into text prompts and fed into GPT for re-ranking. Under GPT-FedRec, the privacy of both local training data and global test data is well protected, as there is no data exchange across any clients or the global server. For test users, GPT-FedRec executes inference only on the global server: given the historical data of a test user, GPT-FedRec performs hybrid retrieval and GPT-based re-ranking, without exposing test data to any other clients. Our proposed hybrid retrieval mechanism and LLM-based re-ranking aim to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. Finally, the RAG nature of GPT-FedRec also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods. Our code is available at https://github.com/huiminzeng/GPT-FedRec.git.},
  month = {dec},
}

@inproceedings{li_knowledge_2025-1,
  title = {Knowledge {Graph},
  author = {Li, Fei and Xu, Yin and Wang, Yanyan and Zhao, Long},
  year = {2025},
  doi = {10.1109/AIoTC66747.2025.11198680},
  url = {https://doi.org/10.1109/aiotc66747.2025.11198680},
  booktitle = {2025 4th {International},
  pages = {592--596},
  keywords = {Cognition, Computer architecture, Computers, Internet of Things, Knowledge Graph, Knowledge graphs, Large language models, Limiting, Predictive models, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation, source: IEEE},
  abstract = {Large Language Models (LLMs) have achieved widespread success in various Natural Language Processing (NLP) tasks, especially in the Question Answering (QA) task. Despite their impressive performance, they often struggle with issues such as hallucinations and lack of domain knowledge, limiting their ability to accurately reasoning within specialized fields. Knowledge Graphs (KGs), as structured sources of factual knowledge, offer a valuable resource to address these issues. However, existing KG-augmented LLM methods mainly focus on the open-domain multi-hop QA task. Notably, they fail to account for other types of com plex questions, such as comparing and parallel questions. In this paper, we propose a novel method KGPCQA, which introduces a joint fine-tuning task for extracting topic entities and predicting relation paths to enhance domain knowledge understanding of the model. This improves reasoning path retrieval and enables the model to effectively answer do main questions using questions and pruned knowledge. Experiments on the operator field dataset prove that our method outperforms existing advanced approaches, achieving Hit@1 accuracy and F1\_score of 95.2 and 90.8, respectively.},
  month = {aug},
}

@inproceedings{topalis_langbo_2025,
  title = {{LangBO},
  author = {Topalis, Philip and Schieseck, Marvin and Gehlhoff, Felix},
  year = {2025},
  doi = {10.1109/ETFA65518.2025.11205749},
  url = {https://doi.org/10.1109/etfa65518.2025.11205749},
  booktitle = {2025 {IEEE},
  pages = {1--4},
  note = {ISSN: 1946-0759},
  keywords = {Automated machine learning, AutoML, Bayes methods, Bayesian Optimization, Convergence, Large language models, Large Language Models, Manufacturing automation, Natural languages, Optimization, Prior Knowledge Integration, Retrieval augmented generation, Retrieval-Augmented Generation, Self-evaluation, source: IEEE},
  abstract = {The development of high-performance machine learning models has traditionally required extensive expertise, thereby excluding domain experts without a formal AI background. To overcome this barrier, we propose LangBO, a novel framework that systematically integrates domain-specific prior knowledge into the Bayesian Optimization (BO) process via natural language input. By leveraging Large Language Models (LLMs) in combination with Retrieval-Augmented Generation (RAG) and a self-evaluation mechanism, unstructured domain expert knowledge is transformed into a structured Dirichlet prior distribution, thereby guiding the optimization of neural architectures and hyperparameters. Initial experiments on a real-world classification task demonstrate accelerated convergence without compromising final model performance while improving interpretability and sample-efficient AutoML for users lacking machine learning expertise.},
  month = {sep},
}

@inproceedings{celik_hybridizing_2025,
  title = {Hybridizing {Large},
  author = {Çelik, Yusuf and Bilge, Alper},
  year = {2025},
  doi = {10.1109/UBMK67458.2025.11206808},
  url = {https://doi.org/10.1109/ubmk67458.2025.11206808},
  booktitle = {2025 10th {International},
  pages = {28--33},
  note = {ISSN: 2521-1641},
  keywords = {Accuracy, Computational modeling, Dual-Layer Retrieval, Fine-Tuning, Gemma 2, History, Large language models, Large Language Models (LLM), Metadata, Motion pictures, Movie Rating Prediction, Predictive models, Recommender systems, Recommender Systems, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Semantics, source: IEEE},
  abstract = {This study introduces a novel approach to movie rating prediction by fine-tuning the Gemma 2 (9B) Large Language Model (LLM) within a Retrieval-Augmented Generation (RAG) framework. The model is tuned using structured prompts derived from the MovieLens 100K dataset. To enable context-aware predictions, we employ a dual-layer RAG architecture that retrieves structured user history from PostgreSQL and semantic movie metadata from a Qdrant vector database. The proposed system demonstrates that hybridizing LLMs with structured and semantic retrieval mechanisms is an effective strategy for building accurate, context-aware recommender systems.},
  month = {sep},
}

@inproceedings{reeves_robustness_2025,
  title = {Robustness of {Question},
  author = {Reeves, Andrew and Dong, Hang},
  year = {2025},
  doi = {10.1109/ICIT64950.2025.11049250},
  url = {https://doi.org/10.1109/icit64950.2025.11049250},
  booktitle = {2025 12th {International},
  pages = {247--252},
  note = {ISSN: 2831-3399},
  keywords = {Data integrity, Generative AI, Information technology, Large language models, Perturbation methods, Predictive models, Prevention and mitigation, Question answering, Question answering (information retrieval), Retrieval augmented generation, Robustness, source: IEEE},
  abstract = {Robustness is a critical consideration when integrating artificial intelligence (AI) systems into decision-making processes. This concern is particularly relevant for generative AI systems, which are designed to consistently produce convincing outputs but can be prone to hallucinations, risking overconfidence in their outputs. This paper investigates the performance of fine-tuning and retrieval augmented generation (RAG) under external data quality perturbations, including typographical errors and factual inaccuracies. Results from experiments using the BioASQ Task 12b question answering dataset and PubMed articles showed nuanced trade-offs, with either RAG or fine-tuning performing better for different scenarios. Furthermore, an analysis of LLMs’ self-reported confidence scores indicated a tendency toward overconfidence, particularly in the presence of inconsistent or erroneous context data. A novel mitigation strategy, leveraging an LLM for data quality error correction was evaluated, but the results demonstrated limited effectiveness, highlighting the need for more advanced correction techniques.},
  month = {may},
}

@inproceedings{jadon_enhancing_2025,
  title = {Enhancing {Domain},
  author = {Jadon, Aryan and Patil, Avinash and Kumar, Shashank},
  year = {2025},
  doi = {10.1109/IAICT65714.2025.11100564},
  url = {https://doi.org/10.1109/iaict65714.2025.11100564},
  booktitle = {2025 {IEEE},
  pages = {445--452},
  note = {ISSN: 2834-8249},
  keywords = {Biological system modeling, Chunk Optimization, Cognition, Computer security, Data models, Domain-Specific NLP, Evaluation Metrics, Finance, Measurement, Optimization, Pipelines, Reasoning Models, Retrieval augmented generation, Retrieval-Augmented Generation, Synthetic data, Synthetic Data Generation, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) systems face significant performance gaps when applied to technical domains requiring precise information extraction from complex documents. Current evaluation methodologies relying on document-level metrics inadequately capture token-resolution retrieval accuracy that is critical for domain-related documents. We propose a framework combining granular evaluation metrics with synthetic data generation to optimize domain-specific RAG performance.First, we introduce token-aware metrics PrecisionΩ and Intersection-over-Union (IoU) that quantify context preservation versus information density trade offs inherent in technical texts. Second, we develop a reasoning model driven pipeline using instruction-tuned LLMs (DeepSeek-R1, DeepSeek-R1 distilled variants and Phi-4) to generate context-anchored QA pairs with discontinuous reference spans across three specialized corpora: SEC 10-K filings (finance), biomedical abstracts (PubMed), and APT threat reports (cybersecurity).Our empirical analysis reveals critical insights: smaller chunks (less than 10 tokens) improve precision by 31–42\% (IoU=0.071 vs. baseline 0.053) at recall costs (–18\%), while domain-specific embedding strategies yield 22\% variance in optimal chunk sizing (5–20 tokens). The DeepSeek-R1-Distill-Qwen-32B model demonstrates superior concept alignment (+14\% mean IoU over alternatives), though no configuration universally dominates financial texts favor larger chunks for risk factor coverage (Recall=0.81@size=20), whereas cybersecurity content benefits from atomic segmentation (PrecisionΩ=0.28@size=5).We open-source this toolkit enabling reproducible optimization of chunking strategies through automated synthetic dataset generation and multi-metric analysis pipelines. This work bridges critical gaps between generic RAG architectures and enterprise requirements for precision-sensitive domains. Our code is available on https://github.com/aryan-jadon/Synthetic-Data-Generation-and-Evaluation-using-Reasoning-Models.},
  month = {jul},
}

@inproceedings{abrahamyan_stackrag_2024,
  title = {{StackRAG},
  author = {Abrahamyan, Davit and Fard, Fatemeh H.},
  year = {2024},
  doi = {10.1109/ICSME58944.2024.00098},
  url = {https://doi.org/10.1109/icsme58944.2024.00098},
  booktitle = {2024 {IEEE},
  pages = {893--897},
  note = {ISSN: 2576-3148},
  keywords = {Accuracy, Chatbots, Large language models, LLM, Multiagent tool, RAG-based tool, Search engines, Software engineering, Software maintenance, Software reliability, Stack Overflow, source: IEEE},
  abstract = {Developers spend much time finding information that is relevant to their questions. Stack Overflow has been the leading resource, and with the advent of Large Language Models (LLMs), generative models such as ChatGPT are used frequently. However, there is a catch in using each one separately. Searching for answers is time-consuming and tedious, as shown by the many tools developed by researchers to address this issue. On the other, using LLMs is not reliable, as they might produce irrelevant or unreliable answers (i.e., hallucination). In this work, we present StackRAG, a retrieval-augmented Multiagent generation tool based on LLMs that combines the two worlds: aggregating the knowledge from SO to enhance the reliability of the generated answers. Initial evaluations show that the generated answers are correct, accurate, relevant, and useful. A description video can be found here11Please note that as the tool requires API keys, we are not able to set up a live demo of StackRAG..},
  month = {oct},
}

@article{guan_external_2025,
  title = {From {External},
  author = {Guan, Wenbo and Liu, Hangchen and Li, Xiaoqian and Zhou, Jun and Yan, Yonghong},
  year = {2025},
  doi = {10.1109/CSCWD64889.2025.11033484},
  url = {https://doi.org/10.1109/cscwd64889.2025.11033484},
  booktitle = {2025 28th {International},
  journal = {Proceedings of the International Conference on Computer Supported Cooperative Work in Design, CSCWD},
  number = {2025},
  pages = {1399 -- 1405},
  note = {Type: Conference paper},
  keywords = {source: Scopus, Entropy-based internal consistency, Federated learning, Focusing, Hallucination, Large language models, Measurement, Reliability engineering, Retrieval augmented generation, Sorting},
  abstract = {Artificial Intelligence Generated Content (AIGC) has emerged as a mainstream research direction with the development of Large Language Models (LLMs). The hallucination of LLMs, however, always interweaves the generated content with outdated or fabricated information, making it hard to be fully trusted and severely hindering LLMs form being widely applied in real-life scenarios. To address this problem, Retrieval Augmented Generation (RAG) has been proposed, which incorporates external knowledge to assist LLMs with content generation and significantly alleviates the hallucination problem. Nonetheless, the vanilla RAG uses similarity as the sole criterion for selecting external knowledge, neglecting the problem of internal inconsistency within the knowledge itself, which may distract LLMs from focusing on the most important information during the content generation process and, therefore, has a negative impact on the generated content's reliability. In this paper, we propose a novel metric, Entropy-based Internal Consistency (EIC), to measure the internal consistency of the external knowledge which is then integrated with similarity to mutually determine the knowledge's importance. Experimental results demonstrate that the proposed metric can provide a more fine-grained signal for external knowledge selection, thereby enhancing the reliability of generated content.},
  annote = {Cited by: 0},
  month = {may},
}

@inproceedings{russo_europeanlawadvisor_2024,
  title = {{EuropeanLawAdvisor},
  author = {Russo, Raffaele and Russo, Diego and Orlando, Gian Marco and Romano, Antonio and Riccio, Giuseppe and Gatta, Valerio La and Postiglione, Marco and Moscato, Vincenzo},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10826025},
  url = {https://doi.org/10.1109/bigdata62323.2024.10826025},
  booktitle = {2024 {IEEE},
  pages = {4751--4756},
  note = {ISSN: 2573-2978},
  keywords = {Accuracy, Europe, Explainable AI, Generative AI, Large language models, Law, Legislation, Nearest neighbor methods, NLP in Legal Industry, Retrieval augmented generation, Retrieval-Augmented Generation (RAG), Search engines, Search problems, Software development management, source: IEEE},
  abstract = {Legal Artificial Intelligence has emerged as an essential field, focusing on AI technologies that facilitate various legal tasks and alleviate the workload of legal professionals. Despite advancements in Legal Artificial Intelligence, there remains a critical gap in systems that can provide both comprehensive and contextually accurate retrieval tailored to the intricate structure of EU legislation. We propose EuropeanLawAdvisor, an efficient and user-friendly legal information retrieval system designed to deliver tailored responses to legal queries. This system utilizes open-source Large Language Models within a Retrieval-Augmented Generation framework, facilitating precise and relevant information retrieval. The system employs a robust retrieval approach that integrates multi-match, k-nearest neighbors, hybrid methods, and TF-IDF search strategies across both complete documents and segmented text indexes, ensuring comprehensive retrieval for diverse query types. The implementation of the framework has demonstrated significant improvements in the accuracy and relevance of responses to EU legal queries, enhancing both the retrieval of relevant legal documents and the generation of precise responses. We show that EuropeanLawAdvisor, leveraging open-source models like Phi3-mini-3B and LLaMa-3-8B, achieves competitive Faithfulness and Relevance compared to GPT-4-Turbo. The performance gap narrows significantly in zero-shot scenarios, and our approach outperforms GPT-4-Turbo in the percentage of answered questions. We publicly release our code on GitHub: https://github.com/raffaele-russo/EuropeanLawAdvisor.},
  month = {dec},
}

@inproceedings{ongris_towards_2024,
  title = {Towards an {Open},
  author = {Ongris, Jaycent Gunawan and Tjitrahardja, Eduardus and Darari, Fariz and Ekaputra, Fajar J.},
  year = {2024},
  doi = {10.1109/ISRITI64779.2024.10963661},
  url = {https://doi.org/10.1109/isriti64779.2024.10963661},
  booktitle = {2024 7th {International},
  journal = {2024 7th International …},
  pages = {44--49},
  note = {ISSN: 2832-1456},
  keywords = {GraphRAG, KG, Knowledge graphs, Large language models, Linked data, LLM, Natural languages, Open systems, Pipelines, RAG, Retrieval augmented generation, Semantic search, Seminars, Technological innovation, Wikidata, source: IEEE, source: Google Scholar},
  abstract = {The rise of large language models (LLMs) has significantly advanced information retrieval, yet challenges like the limitation of knowledge updating ability, lack of openness, and hallucination issues persist. To address these, Retrieval-Augmented Generation (RAG) has been introduced but remains limited in interpretability due to its reliance on vector-based representations. This paper presents a question-answering (QA) system using GraphRAG, a RAG system with knowledge graphs (KGs) as its base. We develop a natural language interface (NLI) for QA over Wikidata, a popular, open, and crowdsourced KG. Our approach employs LLM chaining, i.e., a paradigm that leverages multiple LLM calls sequentially, to generate SPARQL queries, with the aim of creating an open system that ensures transparency and allows direct inspection of its components. Utilizing an experimental research approach, we evaluated the generated SPARQL queries and found that incorporating a broader set of property candidates into the prompts significantly boosts performance, achieving a Jaccard similarity score of 0.7806. These findings demonstrate the system’s effectiveness in SPARQL query generation, highlighting its potential for further development. However, we consider the limitation of the LLM’s context window and the hallucination phenomenon as the major challenges that limit the system’s performance.},
  month = {dec},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{kusuma_trisurya_2024,
  title = {Trisurya: {A},
  author = {Kusuma, Arya Raditya and Wongso, Carleano Ravelza and Aldrich, Darren and Darari, Fariz},
  year = {2024},
  doi = {10.1109/ICIMCIS63449.2024.10956359},
  url = {https://doi.org/10.1109/icimcis63449.2024.10956359},
  booktitle = {2024 {International},
  pages = {601--607},
  note = {ISSN: 2837-5203},
  keywords = {source: IEEE},
  abstract = {E-government initiatives have evolved in various forms in Indonesia over the past several decades. However, the services remain far from optimal; information sources are still fragmented and inaccessible to people who are not fluent in Indonesian. Thus, Trisurya is introduced as a chatbot capable of providing information from government and public service officials in local languages, in addition to Indonesian. Our system is built on Large Language Models (LLMs), specifically GPT API, enhanced by Retrieval-Augmented Generation (RAG) and leveraged by graph (Neo4j) and relational (PostgreSQL) databases to ensure the accuracy of the information provided. This study also compares two versions of the GPT API, GPT-4 and GPT-4o, to determine which version performs better as the LLM backend of the Trisurya system. While both offer similar performance, GPT-4o is more cost-effective. The evaluation results of Trisurya demonstrate its superiority over other publicly available and widely used chatbots, i.e., the vanilla ChatGPT-4o, ChatGPT-3.5, Gemini-1.0-Pro, and Claude-3-Sonnet. Based on the result, Trisurya demonstrated clear superiority during quantitative and qualitative assessments by generating accurate and high-quality responses in Javanese, Sundanese, and Balinese. This research represents a breakthrough in technological innovation by enhancing the efficiency of public services and revolutionizing the management and dissemination of government information in an inclusive manner through Trisurya's omnichannel chatbot.},
  month = {nov},
}

@inproceedings{de_araujo_luz_junior_generative_2025,
  title = {Generative {AI},
  author = {De Araújo Luz Junior, Jonas and Prado Saldanha Ribeiro, Guadalupe and Pessoa, Rafael Fonseca and Huarastaca Taveira Magalhães, Alberto and Formico Rodrigues, Maria Andréia},
  year = {2025},
  doi = {10.1109/GAS66647.2025.00007},
  url = {https://doi.org/10.1109/gas66647.2025.00007},
  booktitle = {2025 {IEEE},
  pages = {9--16},
  note = {ISSN: 2996-5187},
  keywords = {3D Character Animation, Animation, Artificial Intelligence, Conferences, Encoding, Facial Expressions, Games, Generative AI, Large language models, Large Language Models, Retrieval augmented generation, Retrieval-Augmented Generation, Software engineering, Testing, Three-dimensional displays, source: IEEE},
  abstract = {This paper examines a Retrieval-Augmented Generation (RAG) approach for generating facial expressions in 3D game characters using artificial intelligence. By integrating large language models within a RAG-based architecture, we developed a proof-of-concept system that animates expressions based on Facial Action Coding System (FACs) action units. Testing demonstrates the potential of RAG-driven animations to create immersive, adaptive experiences with contextually appropriate expressions that enhance perceived emotional responsiveness in Non-Playable Characters, highlighting RAG’s promise for dynamic character interactions and AI-driven personalization in game development.},
  month = {apr},
}

@inproceedings{liang_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Liang, Xun and Niu, Simin and Zhang, Sensen and Li, Zhiyu and Zhang, Xuan and Wu, Bo and Xiong, Feiyu and Tang, Bo and Wang, Hanyu and Song, Shichao and Wang, Mengwei and Yang, Jiawei},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10888712},
  url = {https://doi.org/10.1109/icassp49660.2025.10888712},
  booktitle = {{ICASSP},
  pages = {1--5},
  note = {ISSN: 2379-190X},
  keywords = {Acoustics, Applications of Generative AI, Citation Generation, Generative AI, Generators, Knowledge based systems, Large language models, Measurement, Multilingual, Reliability, Retrieval-Augmented Generation, Signal processing, Speech processing, source: IEEE},
  abstract = {Retrieval-augmented citation generation (RACG) helps users trust the large language model output by retrieving evidence from reliable sources. However, most current RACG research focuses on single-language tasks, particularly in English, and overlooks the need for cross-lingual evidence retrieval and utilization in real-world applications. To address this issue, we introduce a plug-and-play Retrieval-Augmented Multilingual Citation Generation method (RAMCG) which uses a multilingual retriever to identify relevant evidence from a multilingual knowledge base. The evidence is then combined with the query and processed by a multilingual citation generator. The result is citations that are both accurate and comprehensive. Experiments show that RAMCG outperforms baseline methods in multilingual citation generation and is well-suited for practical use.},
  month = {apr},
}

@inproceedings{tvarozek_what_2025,
  title = {What {If},
  author = {Tvarožek, Rastislav and Haffner, Oto},
  year = {2025},
  doi = {10.1109/KI64036.2025.10916478},
  url = {https://doi.org/10.1109/ki64036.2025.10916478},
  booktitle = {2025 {Cybernetics},
  pages = {1--6},
  note = {ISSN: 2767-875X},
  keywords = {Accuracy, Adaptation models, Data models, Knowledge gap, Large language models, LLM, Natural language processing, Prompt engineering, RAG, Real-time systems, Reliability, Retrieval augmented generation, Training data, source: IEEE},
  abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing, demonstrating impressive capabilities across a wide range of tasks. However, there are instances where LLMs fall short, either due to outdated training data or the absence of domain-specific knowledge. This article explores the challenges and solutions when an LLM “doesn’t know” or lacks sufficient information to provide accurate responses. We discuss various strategies to address these gaps, including fine-tuning with additional datasets and integrating Retrieval-Augmented Generation (RAG). RAG, which combines the strengths of LLMs with external knowledge sources, stands out as a powerful solution, enabling real-time retrieval of relevant data during model inference. This hybrid approach ensures that LLMs can provide more up-to-date, contextually accurate, and domain-specific responses by leveraging external databases or knowledge systems. The article highlights the potential of RAG as a scalable, adaptable method for enhancing LLM performance in environments where generalistic models alone may be insufficient, offering practical guidelines for implementation and future research directions in this area.},
  month = {feb},
}

@inproceedings{liu_research_2025,
  title = {Research on {Aerospace},
  author = {Liu, Bo and Chen, Fei and Wen, Zhonghua},
  year = {2025},
  doi = {10.1109/AINIT65432.2025.11034989},
  url = {https://doi.org/10.1109/ainit65432.2025.11034989},
  booktitle = {2025 {IEEE},
  pages = {1582--1586},
  keywords = {Accuracy, Aerospace field, ChatGLM model, Context modeling, Data models, Fine tuning of large models, Large language models, Prompt project, question answering, Question answering (information retrieval), RAG technology, Real-time systems, Retrieval augmented generation, Seminars, Training, Tuning, source: IEEE},
  abstract = {With the rapid development of aerospace technology in our country, news reports in related fields are increasingly common. However, the aerospace domain is characterized by vast, complex, and highly specialized data, making it difficult for the general public and researchers to obtain the information they need in a short time. To help people more conveniently acquire and understand knowledge in the aerospace field, this paper constructs a high-precision dialogue model by integrating innovative dataset construction methods, model fine-tuning techniques, and retrieval-augmented generation techniques. To address the challenges of manually constructing datasets, the study employed chain-of-thought prompting techniques combined with the GLM-4 large language model to generate high-quality question-and-answer datasets from raw data. To improve the accuracy of the model in text generation tasks in the aerospace field, the LoRA fine-tuning technique was applied to fine-tune and compare the ChatGLM3-6B and ChatGLM2-6B large language models. To resolve the “hallucination” issue that may occur in complex dialogues after model fine-tuning, retrieval-augmented generation techniques were applied to establish a dedicated local knowledge base and optimize retrieval strategies, ultimately increasing the accuracy of the question-and-answer model to 89\%. Experimental results indicate that the proposed construction scheme significantly enhances the accuracy and practicality of the aerospace domain question-and-answer model. Through automated dataset construction, domain-specific model fine-tuning, and retrieval augmentation techniques, an efficient and accurate aerospace information dialogue model has been successfully built.},
  month = {apr},
}

@inproceedings{abdullah_p4omp_2025,
  title = {{P4OMP},
  author = {Abdullah, Wali Mohammad and Kabir, Azmain},
  year = {2025},
  doi = {10.1109/HPEC67600.2025.11196284},
  url = {https://doi.org/10.1109/hpec67600.2025.11196284},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2643-1971},
  keywords = {Benchmark testing, C++ languages, Code Parallelization, Codes, Grounding, HPC, Large language models, LLM, OpenMP, Pipelines, RAG, Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Syntactics, Tutorials, source: IEEE},
  abstract = {We present P4OMP, a retrieval-augmented framework for transforming serial C/C++ code into OpenMP-annotated parallel code using large language models (LLMs). To our knowledge, this is the first system to apply retrieval-based prompting for OpenMP pragma correctness without model fine-tuning or compiler instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with structured instructional knowledge from OpenMP tutorials to improve the reliability of prompt-driven code generation. By grounding generation in the retrieved context, P4OMP improves syntactic correctness compared to baseline prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline, GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites. P4OMP achieves 100\% compilation success on all parallelizable cases, while the baseline fails to compile in 20 out of 108 cases. Six cases that rely on non-random-access iterators or thread-unsafe constructs are excluded due to fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP consistently avoids scoping errors, syntactic misuse, and invalid directive combinations that commonly affect baseline-generated code. We further demonstrate strong runtime scaling across seven compute-intensive benchmarks on an HPC cluster. P4OMP offers a robust, modular pipeline that significantly improves the reliability and applicability of LLM-generated OpenMP code.},
  month = {sep},
}

@inproceedings{lovtsov_automated_2025,
  title = {Automated {Mobile},
  author = {Lovtsov, Vladimir A. and Skvortsova, Maria A.},
  year = {2025},
  doi = {10.1109/REEPE63962.2025.10971107},
  url = {https://doi.org/10.1109/reepe63962.2025.10971107},
  booktitle = {2025 7th {International},
  pages = {1--6},
  note = {ISSN: 2831-7262},
  keywords = {Accuracy, Adaptation models, Biological system modeling, Customer services, language model adaptation, Large language models, LLM, machine learning, mobile operator, RAG, Retrieval augmented generation, Security, Telecommunications, telecoms, Training, Training data, source: IEEE},
  abstract = {Large Language Models (LLMs) have made a real breakthrough in the field of artificial intelligence and have been rapidly integrated into our daily lives, including the telecom domain. The use of Retrieval Augmented Generation (RAG) reduces the likelihood of generating incorrect or outdated data in LLMs and improves the understanding of the query context and the generation of more relevant answers. The aim of this research is to improve the quality of automated customer service for mobile operator customers by using RAG system in combination with different language models. The paper makes a comprehensive analysis of open-source LLMs, the main methods of model adaptation and pre-training. Conclusions are drawn on the applicability of the analyses in the study. The architecture of the RAG system and the deployment diagram are designed. The main stages of system training are described, system results and performance evaluation for different LLMs are given. Major conclusions are drawn on the achievement of the research objective and further development.},
  month = {apr},
}

@inproceedings{haldar_rubra_2025,
  title = {{RUBRA},
  author = {Haldar, Subhajit and Sengupta, Souvik and Das, Asit Kumar},
  year = {2025},
  doi = {10.1109/CIACON65473.2025.11189329},
  url = {https://doi.org/10.1109/ciacon65473.2025.11189329},
  booktitle = {2025 {International},
  pages = {1--6},
  keywords = {Accuracy, Agentic AI, ASAG, Computer architecture, Context modeling, Deep learning, Intelligent systems, Large language models, LLM, Mathematical models, Natural language processing, NLP, RAG, Retrieval augmented generation, Semantics, source: IEEE},
  abstract = {The proliferation of deep Learning applications in natural language processing has facilitated automated evaluation of short-answer questions, providing more transparent, interpretable and unbiased evaluation of students’ responses. In this work, we introduce RUBRA, an agentic AI system for Automatic Short Answer Grading (ASAG) using Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques. The system incorporates domain-specific context information retrieved from textbooks and study materials through a RAG framework and a dynamically generated rubric from reference answers provided by human tutors. Then, by aligning student responses with the rubric via semantic similarity scoring, the system computes an explainable and fair grade and provides transparent feedback to the learner. We discuss the architecture, components, and potential effectiveness of the proposed agentic AI system and tested our results over the SCIENTSBANK dataset. It is observed that the proposed approach not only outperformed the existing approaches in prediction accuracy but also significantly improved the autonomy and adaptability of the system.},
  month = {jul},
}

@inproceedings{jiang_food_2025,
  title = {Food {Safety},
  author = {Jiang, Zhiying and Zhang, Zeyu and Ji, Gang and Li, Fang},
  year = {2025},
  doi = {10.1109/DDCLS66240.2025.11065829},
  url = {https://doi.org/10.1109/ddcls66240.2025.11065829},
  booktitle = {2025 {IEEE},
  pages = {1681--1686},
  note = {ISSN: 2767-9861},
  keywords = {Accuracy, Food safety, Knowledge based systems, Knowledge engineering, Large Language Model, Large language models, Learning systems, Medical services, News Summarization, Prompt engineering, Prompt Engineering, Retrieval augmented generation, Retrieval Augmented Generation, Semantics, source: IEEE},
  abstract = {The issue of food safety is becoming increasingly severe, and generating accurate and understandable news summaries is crucial for raising public awareness and supporting decision-making. However, current summarization methods lack effective integration of domain-specific knowledge and often suffer from issues such as factual errors, omission of key information, or misinterpretations, failing to meet the demand for high-quality news summaries in the food safety domain. This paper proposes a food safety news summarization generation paradigm based on the combination of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). First, the model extracts the "5W1H" (Who, What, When, Where, Why, How) from the news, providing key information for subsequent classification and knowledge retrieval. Then, based on the extracted "5W1H", the model classifies the article and narrows the knowledge retrieval scope, improving retrieval accuracy and ensuring the acquisition of domain-relevant knowledge. Finally, combining the original text, extracted "5W1H", and retrieved knowledge, the model generates the final summary, ensuring the summary accurately reflects the core content of the news and integrates domain knowledge. This paper constructs a knowledge base containing 4500 items of food safety knowledge and applies it to a self-built news data set from domestic authoritative food safety websites. Experimental results show that the proposed method has significant advantages in food safety news summary generation, especially in terms of relevance and information accuracy, performing better than traditional methods. Compared with the reference summary, the BLEU-4 is improved by 2.63\% and ROUGE-L is improved by 1.67\%. This approach is not only applicable to the food safety field but also has potential applications in automatic summary generation for other vertical domains.},
  month = {may},
}

@inproceedings{xu_research_2024,
  title = {Research on the {Construction},
  author = {Xu, Hao and Kang, Zhenyuan and Zhang, Yan and Jin, Zhenqiang and Wang, Mulan and Ge, Linlin and Lu, Haihua},
  year = {2024},
  doi = {10.1109/DSIT61374.2024.10881483},
  url = {https://doi.org/10.1109/dsit61374.2024.10881483},
  booktitle = {2024 7th {International},
  pages = {1--8},
  keywords = {Accuracy, Adaptation models, Data models, Data science, Fault diagnosis, Information technology, Knowledge based systems, Large Language Model, Large language models, Low-Rank Adaptation Fine-Tuning, Power Transformer, Power transformers, Question-Answering System, Retrieval-Augmented Generation, source: IEEE},
  abstract = {The QA system can provide various professionals with fast and accurate knowledge support, significantly improving work efficiency. The introduction of LLMs has further enhanced the accuracy and efficiency of QA systems. A QA system based on LLMs for power transformers can effectively improve the efficiency of fault diagnosis and the resolution of technical issues in the power transformer domain. This study first proposes an automatic dataset construction method for QA based on LLMs, through which a QA dataset of basic knowledge about power transformers is obtained. This dataset then performs low-rank adaptive fine-tuning on the LLM. Subsequently, an external knowledge base covering fundamental knowledge and fault cases of power transformers is built. Finally, with the aid of prompt texts, knowledge responses were generated by RAG in combination with the fine-tuned LLM. Experimental results show that compared with the QA systems driven by general LLMs, the method proposed in this paper generates more concise and professional responses, thus promoting, to some extent, the application of LLMs in the power domain.},
  month = {dec},
}

@inproceedings{guo_streamlining_2025,
  title = {Streamlining {Research},
  author = {Guo, Jinghui and Akbar, Khandakar Ashrafi and Khan, Latifur and Thuraisingham, Bhavani and Irtiza, Saquib},
  year = {2025},
  doi = {10.1109/ACDSA65407.2025.11166476},
  url = {https://doi.org/10.1109/acdsa65407.2025.11166476},
  booktitle = {2025 {International},
  pages = {1--10},
  keywords = {Accuracy, Complexity theory, Generative AI, In-Context Learning, Large language models, Learning (artificial intelligence), LLMs, Measurement, Prompt-Engineering, Retrieval augmented generation, Retrieval-Augmented Generation, source: IEEE},
  abstract = {The emergence of generative artificial intelligence in language processing has made it possible to automate complex human tasks, such as research. By breaking down these intricate tasks, large language models (LLMs) can gain a deeper understanding of how to tackle problems effectively. Emphasizing hierarchical information and contrasting perspectives is crucial for enhancing LLMs’ contextual awareness, leading to more informed and accurate outcomes. We propose an innovative approach to help LLMs generate novel research ideas and assess their quality across various dimensions. Our case studies and experiments show that simplifying research narratives enables LLMs to generate more concrete, high-quality ideas. We conduct extensive automated experiments on the generated novel ideas, evaluating them against various metrics and demonstrating their effectiveness based on different qualitative factors.},
  month = {aug},
}

@inproceedings{nazar_nextg-gpt_2025,
  title = {{NextG},
  author = {Nazar, Ahmad M. and Selim, Mohamed Y. and Qiao, Daji and Zhang, Hongwei},
  year = {2025},
  doi = {10.1109/ICCCN65249.2025.11133874},
  url = {https://doi.org/10.1109/icccn65249.2025.11133874},
  booktitle = {2025 34th {International},
  pages = {1--9},
  note = {ISSN: 2637-9430},
  keywords = {ARA, Benchmark testing, Generative AI, GPT, Knowledge based systems, Large language models, LLM, Next generation networking, RAG, Real-time systems, Retrieval augmented generation, Technological innovation, Telecommunications, Wireless networks, source: IEEE},
  abstract = {Artificial intelligence (AI) and wireless networking advancements have created new opportunities to enhance network efficiency and performance. In this paper, we introduce Next-Generation GPT (NextG-GPT), an innovative framework that integrates retrieval-augmented generation (RAG) and large language models (LLMs) within the wireless systems’ domain. By leveraging state-of-the-art LLMs alongside a domain-specific knowledge base, NextG-GPT provides context-aware real-time support for researchers, optimizing wireless network operations. Through a comprehensive evaluation of LLMs—including Mistral-7B, Mixtral-8×7B, LLaMa3.1-8B, and LLaMa3.1-70B—we demonstrate significant improvements in answer relevance, contextual accuracy, and overall correctness. In particular, LLaMa3.1-70B achieves a correctness score of 86.2\% and an answer relevancy rating of 90.6\%. By incorporating diverse datasets such as ORAN-13K-Bench, TeleQnA, TSpecLLM, and Spec5G, we improve NextG-GPT’s knowledge base, generating precise and contextually aligned responses. This work establishes a new benchmark in AI-driven support for next-generation wireless network research, paving the way for future innovations in intelligent communication systems.},
  month = {aug},
}

@inproceedings{medeiros_tailoring_2025,
  title = {Tailoring {RAG},
  author = {Medeiros, Thaís and Medeiros, Morsinaldo and Andrade, Matheus and Silva, Marianne and Silva, Ivanovitch and Gaffurini, Massimiliano and Brandão, Dennis and Ferrari, Paolo},
  year = {2025},
  doi = {10.1109/ETFA65518.2025.11205618},
  url = {https://doi.org/10.1109/etfa65518.2025.11205618},
  booktitle = {2025 {IEEE},
  pages = {1--8},
  note = {ISSN: 1946-0759},
  keywords = {Complexity theory, Fourth Industrial Revolution, Industrial Automation, Industry 4.0, Large language models, Large Language Models (LLM), Measurement, Meteors, PROFIBUS, Protocols, Question answering (information retrieval), Retrieval augmented generation, Retrieval-Augmented Generation, Semantics, Technical Document Retrieval, Testing, source: IEEE},
  abstract = {The advancement of Industry 4.0 has intensified the demand for intelligent systems that can efficiently access and interpret technical information critical to industrial operations. However, recovering knowledge from extensive and complex technical documentation remains a significant challenge. This study examines the effectiveness of various Retrieval-Augmented Generation (RAG) strategies, combined with different Large Language Models (LLMs), in extracting and generating answers from industrial technical documents. A case study was conducted based on PROFIBUS, a widely adopted digital communication protocol in automation networks, with technical documents organized into categories for engineers and developers. Twenty questions of varying complexity were formulated, and responses were generated using three RAG strategies (Basic, Decomposition, and HyDE) combined with two LLMs (Gemma 3 and GPT-4o-mini). The outputs were compared against reference answers generated by the NotebookLM system and evaluated using automatic metrics, including ROUGE, METEOR, BERTScore, and MATTR. The results indicate that the Decomposition and HyDE strategies achieved superior semantic similarity scores when combined with more capable models. That model’s performance varied depending on the complexity of the document profile. These findings highlight the importance of tailored RAG strategies in enhancing intelligent information retrieval in industrial domains, which supports safer and more efficient operational environments.},
  month = {sep},
}

@inproceedings{parthasarathy_engineering_2025,
  title = {Engineering {LLM},
  author = {Parthasarathy, Kannan and Vaidhyanathan, Karthik and Dhar, Rudra and Krishnamachari, Venkat and Kakran, Adyansh and Akshathala, Sreemaee and Arun, Shrikara and Karan, Amey and Muhammed, Basil and Dubey, Sumant and Veerubhotla, Mohan},
  year = {2025},
  doi = {10.1109/CAIN66642.2025.00031},
  url = {https://doi.org/10.1109/cain66642.2025.00031},
  booktitle = {2025 {IEEE},
  pages = {201--211},
  keywords = {Accuracy, AI Engineering, Autonomous CloudOps, Generative AI, LLM, Multi-Agent Framework, Multi-agent systems, Optimization, Prevention and mitigation, Retrieval augmented generation, Security, Soft sensors, Software architecture, Software Architecture, Software reliability, source: IEEE},
  abstract = {Cloud Operations (CloudOps) is a rapidly growing field focused on the automated management and optimization of cloud infrastructure which is essential for organizations nav-igating increasingly complex cloud environments. MontyCloud Inc. is one of the major companies in the CloudOps domain that leverages autonomous bots to manage cloud compliance, security, and continuous operations. To make the platform more accessible and effective to the customers, we leveraged the use of GenAl. Developing a GenAl-based solution for autonomous CloudOps for the existing MontyCloud system presented us with various challenges such as i) diverse data sources; ii) orchestration of multiple processes and iii) handling complex workflows to automate routine tasks. To this end, we developed MOYA, a multi-agent framework that leverages GenAI and balances autonomy with the necessary human control. This framework integrates various internal and external systems and is optimized for factors like task orchestration, security, and error mitigation while producing accurate, reliable, and relevant insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our multi-agent system with the help of practitioners as well as using automated checks demonstrate enhanced accuracy, responsiveness, and effectiveness over non-agentic approaches across complex workflows.},
  month = {apr},
}

@inproceedings{singireddi_nnrag_2025,
  title = {{nnRAG},
  author = {Singireddi, Siva and Pandey, Sarthak and Pardasani, Rohit and V, Raj Kiran and Awasthi, Navchetan},
  year = {2025},
  doi = {10.1109/ICHI64645.2025.00036},
  url = {https://doi.org/10.1109/ichi64645.2025.00036},
  booktitle = {2025 {IEEE},
  pages = {243--250},
  note = {ISSN: 2575-2634},
  keywords = {Accuracy, Adaptive retrieval, Artificial neural networks, Human in the loop, Human-in-the-loop, Informatics, Large language model, Large language models, Medical data, Medical services, Neural Network, Reliability, Retrieval augmented generation, Retrieval-Augmented Generation, Testing, Training, source: IEEE},
  abstract = {Medical data presents unique challenges for large language models (LLMs), especially because any inaccurate or incomplete information may pose a risk of serious harm to the patient. While Retrieval-Augmented Generation (RAG) architectures offer promising solutions for improving the context and accuracy of LLM-generated responses, it is crucial that the retrievals are highly precise. Due to the potential consequences of errors, Human-in-the-Loop (HITL) processes are essential to ensure reliability and relevance in medical data retrieval. In this work, we propose a first-of-its-kind approach for retrieval workflows that incorporates Human Feedback for a Neural Network-Driven Similarity Search within the RAG architecture (nnRAG). This novel approach stores and integrates user feedback-based reward scores directly into the retrieval process to make it adaptive and refine the accuracy of context selection. We conducted a preliminary study to verify the functionality of this approach, initially evaluating it on a non-medical dataset and subsequently testing its feasibility with a medical dataset. The study demonstrated the ability of the nnRAG system to dynamically update its selection from non- or less-relevant sections to highly precise sections, post-training, on both non-medical and medical datasets. The system was able to perform this precise retrieval, even when the top-k was kept to 1, after a single iteration of training. Concluding, this approach offers a unique solution by introducing dynamism into the retrieval workflows, leveraging user feedback and potentially ensuring improved relevance and utility in the context of diverse and evolving user needs.},
  month = {jun},
}

@inproceedings{gopalan_leveraging_2024,
  title = {Leveraging {LLMs},
  author = {Gopalan, Sathyanarayanan and V, Vijay and Kalyanaraman, Sreenidhi and Raj, Daniel Joseph and V, Sneha and Thomas, Anto N},
  year = {2024},
  doi = {10.1109/CICT64037.2024.10899487},
  url = {https://doi.org/10.1109/cict64037.2024.10899487},
  booktitle = {2024 {IEEE},
  pages = {1--6},
  keywords = {Information and communication technology, Information retrieval, large language models, Linked data, Monitoring, multiple data sources, Natural languages, Organizations, Pipelines, process mining, Process mining, retrieval augmented generation, Retrieval augmented generation, Soft sensors, source: IEEE},
  abstract = {Organizations are fundamentally based on business processes that allow the streamlined functioning and achievement of goals. These business processes depend on various IT systems and tools that manage their performance. While techniques like process mining enable the analysis of these processes, the presence of multiple sources of contextually linked data makes it difficult to derive valuable insight with ease. This research aimed to establish contextual connections between multiple sources of data and perform information retrieval and analytical tasks using LLMs with a focus on process mining. We introduce a novel approach that utilizes a custom-built Python-based Retrieval Augmented Generation (RAG) pipeline that has been designed to overcome the limitations of existing LLM-based approaches, mainly focusing on handling multiple data sources and process-oriented queries. We have incorporated a unique table and row retrieval mechanism, providing the LLM with rich context and enabling it to reason about data from a process-aware perspective. In conclusion, this paper proves the ability to contextually link process logs with additional process-related data from varied sources to enable LLMs in providing relevant process insights which can be used to plan, monitor, and improve operations all through natural language querying.},
  month = {dec},
}

@inproceedings{gozukara_rag_2025,
  title = {{RAG},
  author = {Gözükara, Hamza and Patel, Jay and Kara, Erkan and Yıldız, Ayşenur and Köseoğlu, Ozan and Makaroğlu, Didem and Drias, Yassine and Özlem, Şirin and Çakar, Tuna},
  year = {2025},
  doi = {10.1109/UBMK67458.2025.11206833},
  url = {https://doi.org/10.1109/ubmk67458.2025.11206833},
  booktitle = {2025 10th {International},
  pages = {1350--1355},
  note = {ISSN: 2521-1641},
  keywords = {Accuracy, Chatbots, Computer architecture, embeddings, Filters, interactive assistant, Knowledge based systems, large language models, Large language models, Motion pictures, movie recommendation, retrieval augmented generation, Retrieval augmented generation, Semantic search, User experience, source: IEEE},
  abstract = {The proliferation of content within video streaming services presents a significant challenge for users seeking personalized recommendations and specific information. This research addresses this challenge by developing a Retrieval-Augmented Generation (RAG) chatbotn designed to enhance user experience through conversational AI. The primary contribution of this work is a novel Retrieval-Augmented Generation (RAG) architecture featuring a dual-retrieval system that combines semantic search for descriptive requests and structured queries for fact based inquiries. This approach grounds the Large Language Model (LLM) in a factual knowledge base, mitigating the risk of hallucinations. The system is engineered to handle empty data retrieval scenarios by dynamically relaxing search filters, ensuring a robust user experience. The effectiveness of this RAG approach was validated through a comprehensive set of automated evaluations. The system demonstrates high precision in ranked list retrieval with questions like "Recommend me the top 5 action movies with highest IMDb scores", achieving an average NDCG@k of 0.837. While the chatbot shows strong semantic understanding by achieving 91\% accuracy with contextual clues such as "Which Batman movies are directed by Christopher Nolan?", its performance with more ambiguous, plot-only queries (59.5\% accuracy) indicates clear opportunities for future refinement. These results confirm that the dual-tool architecture successfully combines the flexibility of semantic search with the precision of structured queries, paving the way for more intuitive and efficient content discovery on streaming platforms.},
  month = {sep},
}

@inproceedings{jeong_lightweight_2025,
  title = {Lightweight {Relevance},
  author = {Jeong, Taehee},
  year = {2025},
  doi = {10.1109/ICICT64582.2025.00037},
  url = {https://doi.org/10.1109/icict64582.2025.00037},
  booktitle = {2025 8th {International},
  pages = {198--203},
  note = {ISSN: 2769-4542},
  keywords = {Accuracy, Computational modeling, Databases, Fine-tuning, Information retrieval, Large language models, Media, Memory management, Pipelines, Relevance grader, Retrieval augmented generation, Retrieval-augmented generation, Vector database, Vector search, Vectors, source: IEEE},
  abstract = {Retrieval-augmented generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents is a challenge. To address this, a secondary model, known as a relevant grader, is used to verify document relevance. To reduce computational requirements, a lightweight small language model can be used as a relevant grader. This work aims to improve the capability of such a model, achieving a significant increase in precision from 0.1038 to 0.7750 using llama-3.2-1b, outperforming llama-3.1-70b and gpt4o-mini. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.},
  month = {mar},
}

@inproceedings{boronat_mdre-llm_2025,
  title = {{MDRE},
  author = {Boronat, Artur and Mustafa, Jawad},
  year = {2025},
  doi = {10.1109/SANER64311.2025.00090},
  url = {https://doi.org/10.1109/saner64311.2025.00090},
  booktitle = {2025 {IEEE},
  pages = {850--854},
  note = {ISSN: 2640-7574},
  keywords = {Aging, Analytical models, Collaboration, Documentation, domain model recovery, large language models, Large language models, RAG, Reverse engineering, Software systems, Source coding, Systematics, source: IEEE},
  abstract = {Understanding and maintaining software systems often requires extracting high-level abstractions, such as domain models, from source code. MDRE-LLM addresses this challenge by integrating Large Language Models (LLMs) with traditional Model-Driven Reverse Engineering (MDRE) techniques, offering an innovative approach to automate and enhance domain model recovery. The tool supports flexible granularity strategies and validates LLM -generated models against deterministic baselines. MDRE-LLM addresses diverse use cases, including analyzing legacy systems with minimal documentation, rapidly compre-hending large-scale codebases, and validating LLM performance in reverse engineering tasks. These capabilities have the potential to improve software analysis and refactoring while advance AI-driven research and education by fostering systematic experimentation and collaboration. The tool and a webcast are available at https://zenodo.org/uploads/14072106.},
  month = {mar},
}

@inproceedings{wang_mainstream_2025,
  title = {Mainstream {AI},
  author = {Wang, Jiajun and Wang, Jing},
  year = {2025},
  doi = {10.1109/CCDC65474.2025.11090774},
  url = {https://doi.org/10.1109/ccdc65474.2025.11090774},
  booktitle = {2025 37th {Chinese},
  pages = {2261--2266},
  note = {ISSN: 1948-9447},
  keywords = {source: IEEE},
  abstract = {In the process of digital transformation, enterprises and institutions require intelligent resources that can be flexibly adjusted in accordance with business development. AI Agents are well-suited to meet the diverse needs of current and future businesses. With the assistance of AI Agents, enterprises can eliminate repetitive task execution, reduce human error, alleviate process bottlenecks, and prevent personnel overload. This paper, based on the author's ongoing research projects and a substantial body of relevant information, analyzes mainstream artificial intelligence technologies required for building digital consultants in the vertical field of the industry. The content encompasses Knowledge Graph (KG), Large Language Model (LLM), Retrieval-Augmented Generation (RAG), classification and characteristics of AI Agents, and selection principles for constructing digital consultants in the industry vertical field. It is anticipated that this paper will contribute to time and cost savings for similar vertical AI applications.},
  month = {may},
}

@inproceedings{shafi_personalized_2025,
  title = {Personalized {Mental},
  author = {Shafi, Fozle Rabbi and Hossain, M. Anwar and Choudhury, Salimur},
  year = {2025},
  doi = {10.1109/COMPSAC65507.2025.00109},
  url = {https://doi.org/10.1109/compsac65507.2025.00109},
  booktitle = {2025 {IEEE},
  pages = {815--823},
  note = {ISSN: 2836-3795},
  keywords = {Accuracy, artificial intelligence, Computational modeling, Data mining, Emotion recognition, fine-tuning, function calling, large language model, Large language models, Mental health, mental health assistance, Privacy, retrieval augmented generation, Retrieval augmented generation, Soft sensors, source: IEEE},
  abstract = {Mental health challenges continue to rise globally, yet access to effective and personalized support remains insufficient. While Large Language Models (LLMs) have shown its promise in this area, most existing solutions lack personalization, pose privacy risks, and often generate unreliable or generic responses. In this study, we present a novel approach that enhances LLM-based mental health support through fine-tuning on a combination of public and synthetically generated mental health datasets. We further propose a dynamic prompt strategy that extracts relevant mental health entities from patient conversations, such as symptoms and emotions, and retrieves relevant information from diverse data sources. We leverage function calling with Retrieval-Augmented Generation (RAG) to produce context-aware, personalized responses. Empirical comparisons with existing models demonstrate that our approach achieves higher accuracy and generates responses that are better aligned with individual user needs.},
  month = {jul},
}

@inproceedings{guettala_building_2024,
  title = {Building {Advanced},
  author = {Guettala, Manel and Bourekkache, Samir and Kazar, Okba and Harous, Saad},
  year = {2024},
  doi = {10.1109/ICCDA64887.2024.10867361},
  url = {https://doi.org/10.1109/iccda64887.2024.10867361},
  booktitle = {2024 2nd {International},
  pages = {1--7},
  keywords = {Accuracy, context-aware Learning, Deep Learning, human-computer Interaction (HCI), Large language models, Natural language processing, Natural Language Processing (NLP), Pipelines, Retrieval augmented generation, Scalability, Security, Soft sensors, Training data, Transfer Learning, User experience, source: IEEE},
  abstract = {The integration of Large Language Models (LLMs) in Question-Answering (QA) systems has made significant progress, yet they often fail to generate precise answers for queries beyond their training data and hallucinating. To address this, our study develops an advanced Retrieval-Augmented Generation (RAG) pipeline method using the LangChain framework, featuring a decision-making agent that dynamically selects the most effective tools and data sources for accurate and contextually relevant responses. By incorporating multiple data sources and diverse tools, our system mitigates the limitations of traditional LLMs, enhancing their effectiveness in ubiquitous learning environments. Evaluation through various case studies shows significant improvements in accuracy, relevance, and contextual appropriateness. The multi-search agent RAG system efficiently retrieves and synthesizes information, supporting continuous and context-aware learning and enriching the user experience. This research advances AI-based educational technologies, delivering a powerful solution for information retrieval and synthesis, as well as laying the foundations for intelligent question-and-answer systems of the future.},
  month = {nov},
}

@inproceedings{sandakelum_real-time_2025,
  title = {Real-{Time},
  author = {Sandakelum, Dishan and Rajapakse, Chathura and Jayalath, Nirasha},
  year = {2025},
  doi = {10.1109/SCSE65633.2025.11030999},
  url = {https://doi.org/10.1109/scse65633.2025.11030999},
  booktitle = {2025 {International},
  pages = {1--6},
  note = {ISSN: 2997-7363},
  keywords = {Accuracy, banking, Banking, chatbot, Chatbots, Customer services, knowledge management, Knowledge management, Modeling, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Training, Translation, source: IEEE},
  abstract = {In the corporate sector, the experience of an employee is significant to the service they provide. Especially in sectors such as banking, where domain knowledge is crucial in providing efficient service in internal and external operations. However, in recent years within Sri Lanka, experienced professionals have started to migrate and change jobs due to the financial crisis. This affects organizations because they have to constantly work with new recruits. This research aims to provide a solution to this problem by finding a method to develop a chatbot using the RAG method that can provide assistance to new recruits with domain knowledge. The RAG is supposed to store knowledge, such as information about products, policies, and relevant domain knowledge as its context. Provides answers from the domain-specific knowledge rather than the general knowledge of an LLM. Furthermore, this research focuses on optimizing the RAG model in main aspects of the RAG such as query translation, retrieval, in order to provide more accurate and reliable outputs to the user. The results of the research are to build a chatbot that helps fresh recruits find knowledge to answer customer queries during their training period. In addition, the study highlights ways to improve the response of the RAG model to be accurate and relevant for the domain of the banking sector in Sri Lanka.},
  month = {apr},
}

@inproceedings{okutan_leveraging_2024,
  title = {Leveraging {RAG},
  author = {Okutan, Ahmet and Merten, Samuel and Michael, Christoph C. and Ryjikov, Ben},
  year = {2024},
  doi = {10.1109/ICAA64256.2024.00024},
  url = {https://doi.org/10.1109/icaa64256.2024.00024},
  booktitle = {2024 {International},
  journal = {… Conference on Assured …},
  pages = {102--105},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {C++ languages, C++ to Rust, Codes, Computer languages, Ecosystems, Grounding, Large language models, Philosophical considerations, Program processors, Source coding, Syntactics, Transpilation, source: IEEE, source: Google Scholar},
  abstract = {Despite their similarities, translating C++ code into the Rust language is a challenging task on account of differences in syntax, ecosystem, philosophy, and idioms. Large Language Models (LLMs) using Retrieval Augmented Generation (RAG) are effective at solving challenging programming language tasks, including source code generation and program translation. We leveraged RAG LLMs for translating C++ code into Rust and created Cpp2Rust, an LLM-based C++-to-Rust transpiler grounding the OpenAI ChatGPT4 model with similar C++ and Rust code snippet pairs from LeetCode. Preliminary analysis results show that 84\% of the solutions produced by Cpp2Rust that had no compile errors were accepted as correct solutions on LeetCode.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{sharma_integrating_2025,
  title = {Integrating {Blockchain},
  author = {Sharma, Sujan and Koirala, Ravi and Matalonga, Santiago and Dahal, Keshav},
  year = {2025},
  doi = {10.1109/SKIMA66621.2025.11155428},
  url = {https://doi.org/10.1109/skima66621.2025.11155428},
  booktitle = {2025 {International},
  pages = {1--6},
  keywords = {Accuracy, auction, blockchain, Blockchains, Data mining, LLM, Natural language processing, Pipelines, PQE, RAG, Real-time systems, Retrieval augmented generation, Security, Structured Query Language, Time factors, source: IEEE},
  abstract = {Real-time data retrieval and analytics play a crucial role in digital systems, requiring efficient, secure, and automated mechanisms for extracting insights from structured and unstructured data. Traditional methods rely on centralized databases and SQL (Structure Query Language) queries, which may lack security, transparency, adaptability and automation. Blockchain (BC) technology introduces a decentralized and tamper-proof ledger that enhances data integrity, while large language models (LLMs) leverage natural language processing (NLP) and excel in contextual understanding to facilitate more intuitive and flexible querying mechanisms. Implementing these technologies brings promising solutions, but some challenges remain, particularly in retrieving contextual information and data analysis. Hence, this research paper explores the integration of BC with LLM in an e-auction system using three different integration alternatives: the Pandas Query Engine (PQE), Retrieval-Augmented Generation (RAG) and Custom Query Pipeline (CQP). We compare the accuracy, and the time elapsed on querying of these methods in extracting relevant information from online auction event logs. Among three integration alternatives, RAG provides better result in terms of accuracy and PQE provides quicker result in terms of response time. Our results provide guidance on selecting retrieval strategies for AI-driven blockchain analytics, with implications for bid monitoring, regulatory compliance, fraud detection and recommendations to users of e-auction systems about efficient bidding. Further, we have discussed about cost of using these techniques.},
  month = {jun},
}

@inproceedings{lu_synergizing_2024,
  title = {Synergizing {Internal},
  author = {Lu, Gewei and He, Chaofan and Shen, Liping},
  year = {2024},
  doi = {10.1109/SMC54092.2024.10831420},
  url = {https://doi.org/10.1109/smc54092.2024.10831420},
  booktitle = {2024 {IEEE},
  pages = {1217--1223},
  keywords = {Accuracy, Cognition, Knowledge based systems, Knowledge graphs, Large language models, Prompt engineering, Question answering (information retrieval), Retrieval augmented generation, Training, Training data, source: IEEE},
  abstract = {Large language models (LLMs), such as ChatGPT, have demonstrated remarkable capability in question answering but face challenges when it comes to knowledge-based rea-soning, such as limited training data and hallucination. To address these challenges, integrating LLMs with knowledge graphs (KGs) has emerged as a promising solution. However, the cost associated with training and inference of LLMs is high. Our method integrates the Retrieval-Augmented Generation (RAG) paradigm, incorporating relevant information from KGs alongside the question to enhance LLMs' reasoning process without training. Moreover, we propose a novel concept of self-knowledge motivation to reduce the overhead of inference, which prompts LLMs to integrate retrieved information with their internal knowledge for reasoning before seeking additional queries to KGs. Experimental results showcase improvements in answer accuracy and a reduction in LLMs' API calls compared to the latest published state-of-the-art (SOTA) method employing an identical paradigm, underscoring the efficiency and effectiveness of our method.},
  month = {oct},
}

@article{guo_enhancing_2025,
  title = {Enhancing {Code},
  author = {Guo, Jing-Ming and Liu, Po-Yang and Zeng, Yi-Chong and Chen, Ting-Ju},
  year = {2025},
  doi = {10.1109/TCE.2025.3565294},
  url = {https://doi.org/10.1109/tce.2025.3565294},
  journal = {IEEE Transactions on Consumer Electronics},
  volume = {71},
  number = {1},
  pages = {2342--2346},
  keywords = {Accuracy, Adaptation models, C\# languages, Codes, Data mining, Deep learning, large language model, Large language models, natural language processing, Retrieval augmented generation, retrieval-augmented fine-tuning, Syntactics, Training, Tuning, vision transformer, source: IEEE},
  abstract = {Large language models (LLMs) have made substantial advancements in knowledge reasoning and are increasingly utilized in specialized domains such as code completion, legal analysis, and medical transcription, where accuracy is paramount. In such applications, document-specific precision is more critical than general reasoning capabilities. This paper proposes a novel approach based on Retrieval-Augmented Fine-Tuning (RAFT) to enhance model-generated outputs, particularly in code transformation tasks. RAFT integrates domain-specific knowledge, optimizing in-domain retrieval-augmented generation by training the model to discern the relationship between prompts, retrieved documents, and target outputs. This enables the model to extract relevant information while mitigating the impact of noise. Experimental results demonstrate that the proposed method improves accuracy of 2.4\% and CodeBLEU of 1.3\% for VB-to-C\# code conversion, highlighting its effectiveness in domain-specific applications.},
  issn = {1558-4127},
  month = {feb},
}

@inproceedings{pratap_harnessing_2025,
  title = {Harnessing {Large},
  author = {Pratap, Ayush and Sharma, Nidhi and Sardana, Neha and Husing, Pao-Ann},
  year = {2025},
  doi = {10.1109/IS3C65361.2025.11130957},
  url = {https://doi.org/10.1109/is3c65361.2025.11130957},
  booktitle = {2025 {Seventh},
  pages = {1--4},
  note = {ISSN: 2770-0496},
  keywords = {source: IEEE},
  abstract = {The integration of Large Language Models (LLMs) into materials and manufacturing offers a transformative approach to achieving ISO 9001 -compliant product quality, aligning with the goals of Industry 5.0. This work proposes MatManQ (Large Language Model in Material and Manufacturing for Product Quality and Control), a framework that leverages LLMs for quality control, assurance, and agentic AIdriven decision-making. A case study on alloy behavior using a self-curated dataset demonstrates the framework's capabilities. Five open-source LLMs-Mixtral-8x7B-327, TinyLlama-1.1B, deepset/roberta, Gemini, and FLAN-T5-were fine-tuned using a Retrieval-Augmented Generation (RAG) approach. Mixtral-8x7B-327 achieved the highest F1 score of {\textbackslash},
  month = {jun},
}

@inproceedings{zeng_not_2024,
  title = {Not {Just},
  author = {Zeng, Yating and Guo, Bin and Jing, Yao and Wang, Hao and Ding, Yasan and Liang, Yunji and Yu, Zhiwen},
  year = {2024},
  doi = {10.1109/SWC62898.2024.00194},
  url = {https://doi.org/10.1109/swc62898.2024.00194},
  booktitle = {2024 {IEEE},
  pages = {1234--1241},
  note = {ISSN: 2993-396X},
  keywords = {Boosting, Collaboration, Human computer interaction, human-computer interaction, Knowledge graphs, large language models, Large language models, Prevention and mitigation, Retrieval augmented generation, role-playing, source: IEEE},
  abstract = {In this paper, we focus on enhancing the credibility and personalization of large language models in role-playing scenarios, aiming to facilitate natural and believable humancomputer interaction. Current dialogue systems often fall short in accurately portraying character styles, leading to issues of role illusion. To address this, we introduce an innovative method, the Retrieve Fine-Tuned Collaborative Large Language Model (RFTC-LLM), which employs a two-stage strategy combining fine-tuning with retrieval-augmented generation (RAG). By employing instruction-based fine-tuning, the model is directed to learn the character’s expression style and preferences, and the use of a role-specific knowledge graph aids in constraining responses to ensure relevance and avoid misleading answers. This approach effectively resolves the confusion surrounding role-specific knowledge, thereby boosting the credibility and authenticity of the dialogue system. Experimental results demonstrate the superiority of our method across multiple role-playing dialogue datasets, particularly in significantly reducing instances of role illusion.},
  month = {dec},
}

@inproceedings{garza_privcomp-kg_2024,
  title = {{PrivComp},
  author = {Garza, Leon and Elluri, Lavanya and Piplai, Aritran and Kotal, Anantaa and Gupta, Deepti and Joshi, Anupam},
  year = {2024},
  doi = {10.1109/TPS-ISA62245.2024.00021},
  url = {https://doi.org/10.1109/tps-isa62245.2024.00021},
  booktitle = {2024 {IEEE},
  journal = {… Conference on Trust …},
  pages = {97--106},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Law, Organizations, Policy Compliance, Privacy, Privacy Policy, Regulation, Retrieval augmented generation, Security, Semantic Web, Standards organizations, source: IEEE, source: Google Scholar},
  abstract = {Regulatory documents are complex and lengthy, making full compliance a challenging task for businesses. Similarly, privacy policies provided by vendors frequently fall short of the necessary legal standards due to insufficient detail. To address these issues, we propose a solution that leverages a Large Language Model (LLM) in combination with Semantic Web technology. This approach aims to clarify regulatory requirements and ensure that organizations’ privacy policies align with the relevant legal frameworks, ultimately simplifying the compliance process, reducing privacy risks, and improving efficiency. In this paper, we introduce a novel tool, the Privacy Policy Compliance Verification Knowledge Graph, referred to as PrivComp-KG. PrivComp-KG is designed to efficiently store and retrieve comprehensive information related to privacy policies, regulatory frameworks, and domain-specific legal knowledge. By utilizing LLM and Retrieval Augmented Generation (RAG), we can accurately identify relevant sections in privacy policies and map them to the corresponding regulatory rules. Our LLM-based retrieval system has demonstrated a high level of accuracy, achieving a correctness score of 0.9, outperforming other models in privacy policy analysis. The extracted information from individual privacy policies is then integrated into the PrivComp-KG. By combining this data with contextual domain knowledge and regulatory rules, PrivComp-KG can be queried to assess each vendor’s compliance with applicable regulations. We demonstrate the practical utility of PrivComp-KG by verifying the compliance of privacy policies across various organizations. This approach not only helps policy writers better understand legal requirements but also enables them to identify gaps in existing policies and update them in response to evolving regulations.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{sahin_llm_2024,
  title = {{LLM},
  author = {Şahin, Gürkan and Varol, Karya and Pak, Burcu Kuleli},
  year = {2024},
  doi = {10.1109/UBMK63289.2024.10773564},
  url = {https://doi.org/10.1109/ubmk63289.2024.10773564},
  booktitle = {2024 9th {International},
  journal = {2024 9th International Conference …},
  pages = {1--6},
  note = {ISSN: 2521-1641},
  keywords = {Accuracy, Companies, Computer science, generative artificial intelligence, GPT, Information security, Knowledge management, large language models, Measurement, natural language processing, question answering, Question answering (information retrieval), retrieval augmented generation (RAG), Usability, Vectors, source: IEEE, source: Google Scholar},
  abstract = {Large language models (LLM) have become integral to many natural language processing applications, particularly in the area of automatic question answering. In this study, a question answering system was developed to enable Adesso Turkiye employees to access internal company information quickly and accurately. A Retrieval Augmented Generation (RAG)-based question answering framework was constructed by utilizing multiple large language models and embedding techniques, along with content curated by experts in human resources and information security. The performance of the system was evaluated using ROUGE, BLEU, and accuracy metrics, and the results indicated high levels of success. Future work will focus on enhancing performance through the use of different language models, enriching the system with datasets from various domains, and integrating the developed system into MS Teams to ensure accessibility for employees.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{agliata_generative_2024,
  title = {Generative {AI},
  author = {Agliata, Antonio and Pilato, Antonio and Mariacarmen, Sorrentino and Bottiglieri, Salvatore and Nardo, Emanuel Di and Ciaramella, Angelo},
  year = {2024},
  doi = {10.1109/ISCC61673.2024.10733569},
  url = {https://doi.org/10.1109/iscc61673.2024.10733569},
  booktitle = {2024 {IEEE},
  pages = {1--4},
  note = {ISSN: 2642-7389},
  keywords = {Drug discovery, Electronic medical records, Emotional Well-Being, Ethics, Generative adversarial networks, Generative AI, Generative Pre-trained Transformer, Guidelines, Medical services, NLP, Protocols, Retrieval-Augmented Generation, Technological innovation, Transformers, User-Centered AI Design, source: IEEE},
  abstract = {Generative artificial intelligence (AI) is poised to revolutionize the healthcare sector by enhancing research methodologies, diagnostic procedures, and treatment protocols. This paper investigates the application of key generative AI technologies, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Pre-trained Generative Transformer models, with a particular focus on their implementation in medical imaging, drug discovery, and electronic health record management. Utilizing the Haystack framework, this study integrates these technologies to optimize data access and retrieval. The research addresses the primary challenges in healthcare AI adoption, such as data quality, model interpretability, ethical considerations, and regulatory compliance. By leveraging the Haystack framework, we identify future research opportunities, emphasizing the integration of multimodal data, the personalization of treatments, and the development of transparent AI systems. The study underscores the critical role of interdisciplinary collaboration between AI researchers and healthcare professionals in maximizing the benefits of these technologies, managing their complexities, and ensuring their successful integration into healthcare systems. Our findings demonstrate the potential of generative AI to significantly improve clinical decision-making and patient care, while also highlighting the importance of ethical guidelines and robust data security measures.},
  month = {jun},
}

@inproceedings{tomkou_bridging_2025,
  title = {Bridging {Industrial},
  author = {Tomkou, Despina and Fatouros, George and Andreou, Andreas and Makridis, Georgios and Liarokapis, Fotis and Dardanis, Dimitrios and Kiourtis, Athanasios and Soldatos, John and Kyriazis, Dimosthenis},
  year = {2025},
  doi = {10.1109/DCOSS-IoT65416.2025.00158},
  url = {https://doi.org/10.1109/dcoss-iot65416.2025.00158},
  booktitle = {2025 21st {International},
  pages = {1050--1056},
  note = {ISSN: 2325-2944},
  keywords = {Conversational AI, eXtended Reality, Extended reality, Knowledge Management, Large language models, Large Language Models, Performance evaluation, Remote Assistance, Retrieval augmented generation, Retrieval-Augmented Generation, Robotic assembly, Semantics, Smart manufacturing, Smart Manufacturing, Smart systems, Training, Vectors, source: IEEE},
  abstract = {This paper introduces a novel integration of Retrieval-Augmented Generation (RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR) technologies to address knowledge transfer challenges in industrial environments. The proposed system embeds domain-specific industrial knowledge into XR environments through a natural language interface, enabling hands-free, context-aware expert guidance for workers. We present the architecture of the proposed system consisting of an LLM Chat Engine with dynamic tool orchestration and an XR application featuring voice-driven interaction. Performance evaluation of various chunking strategies, embedding models, and vector databases reveals that semantic chunking, balanced embedding models, and efficient vector stores deliver optimal performance for industrial knowledge retrieval. The system's potential is demonstrated through early implementation in multiple industrial use cases, including robotic assembly, smart infrastructure maintenance, and aerospace component servicing. Results indicate potential for enhancing training efficiency, remote assistance capabilities, and operational guidance in alignment with Industry 5.0's human-centric and resilient approach to industrial development.},
  month = {jun},
}

@inproceedings{grislain_rag_2025,
  title = {{RAG},
  author = {Grislain, Nicolas},
  year = {2025},
  doi = {10.1109/CAI64502.2025.00150},
  url = {https://doi.org/10.1109/cai64502.2025.00150},
  booktitle = {2025 {IEEE},
  pages = {847--852},
  keywords = {Artificial Intelligence, Differential privacy, Electronic medical records, Ethics, Intelligent agents, Knowledge based systems, Knowledge retrieval, Large language models, Privacy, Protection, Retrieval augmented generation, Security, Security and Privacy Protection, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to provide Large Language Models (LLM) with fresh and relevant context, mitigating the risk of hallucinations and improving the overall quality of responses in environments with large and fast moving knowledge bases. However, the integration of external documents into the generation process raises significant privacy concerns. Indeed, when added to a prompt, it is not possible to guarantee a response will not inadvertently expose confidential data, leading to potential breaches of privacy and ethical dilemmas. This paper explores a practical solution to this problem suitable to general knowledge extraction from personal data. It shows differentially private token generation is a viable approach to private RAG.},
  month = {may},
}

@inproceedings{bhuva_interviewedge_2025,
  title = {{InterviewEdge},
  author = {Bhuva, Jaykumar R. and Parmar, Tanya V. and Prajapati, Harshadkumar B. and Dabhi, Vipul K.},
  year = {2025},
  doi = {10.1109/SCEECS64059.2025.10940590},
  url = {https://doi.org/10.1109/sceecs64059.2025.10940590},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2688-0288},
  keywords = {source: IEEE},
  abstract = {A structured, objective approach to non-coding technical interviews can enhance fairness and effectiveness in hiring, for which the latest AI technology can be applied instead of human evaluation. Towards this, this paper presents InterviewEdge: A Smart Interview Assistant, an AI-powered system designed to automate the evaluation of theoretical answers in programming languages. The proposed system employs Artificial Intelligence (AI) technologies such as the Retrieval Augmented Generation (RAG) and Large Language Model (LLM) for the retrieval of relevant content for technical assessment of the candidates’ responses. The proposed work is evaluated using criteria such as technical accuracy, response completeness, display of language-specific knowledge, and use of clarity \& structure. InterviewEdge provides for unbiased evaluation by ensuring consistent marking of the responses. Feedback to the candidates may help them understand how to improve specific areas of their answers.},
  month = {jan},
}

@inproceedings{gogineni_llm-powered_2025,
  title = {{LLM},
  author = {Gogineni, Vineeth},
  year = {2025},
  doi = {10.1109/AIRC64931.2025.11077480},
  url = {https://doi.org/10.1109/airc64931.2025.11077480},
  booktitle = {2025 6th {International},
  pages = {452--456},
  keywords = {Accuracy, Collaboration, Convergence, Databases, Knowledge engineering, knowledge retrieval, Large language models, multi-agent systems, prompt engineering, Prompt engineering, retrieval-augmented generation, Robots, Semantics, vector databases, Vectors, source: IEEE},
  abstract = {This paper presents a comprehensive technical framework for constructing effective multi-agent systems powered by large language models (LLMs). We examine how multiple LLM-based agents can collaborate to solve complex problems beyond the capabilities of single agents through specialized knowledge integration and optimized communication protocols. Our novel architecture enables efficient collaboration between heterogeneous LLM agents, each with domain-specific capabilities and knowledge bases. Experimental results demonstrate that our multi-agent LLM system achieves 42\% higher accuracy on complex knowledge tasks, 37\% reduction in hallucinations, and 29\% faster convergence on collaborative problem-solving compared to baseline approaches. By implementing optimized knowledge retrieval mechanisms and structured communication patterns, our system reduces token usage by 45\% while maintaining semantic fidelity across agent interactions. These findings establish key design principles for building more effective and reliable collaborative LLM-based multi-agent systems for enterprise applications},
  month = {may},
}

@inproceedings{yang_enhancing_2025,
  title = {Enhancing {LLM},
  author = {Yang, Te-Lun and Liu, Jyi-Shane and Tseng, Yuen-Hsien and Jang, Jyh-Shing and Chang, Ming-Ching and Chen, Wei-Chao},
  year = {2025},
  doi = {10.1109/AVSS65446.2025.11149916},
  url = {https://doi.org/10.1109/avss65446.2025.11149916},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2643-6213},
  keywords = {Accuracy, Knowledge based systems, Large language models, Pipelines, Question answering (information retrieval), Regulation, Retrieval augmented generation, Servers, Vectors, Visualization, source: IEEE},
  abstract = {Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing Large Language Models (LLMs) by incorporating external knowledge through information retrieval (IR) techniques. However, in question-answering tasks, RAG often retrieves documents that are only semantically similar to the query, which may not provide the most relevant information for generating accurate responses. To address this limitation, we propose an improved retrieval pipeline that combines dense vector search with a re-ranking mechanism to more effectively identify and extract highly relevant knowledge from the retrieved content. We evaluated our approach on two Chinese datasets, TTQA and TMMLU+, using 17 different LLMs. Experimental results show that our method improves performance by up to 21.24\% over baseline approaches, particularly on two finance-related subsets, after incorporating domain-specific financial regulations to enhance the knowledge base used in the TMMLU+ dataset.},
  month = {aug},
}

@inproceedings{ailapuram_development_2025,
  title = {Development of a {Medical},
  author = {Ailapuram, Raveena and Sharif, Mhd Saeed and Elmedany, Wael},
  year = {2025},
  doi = {10.1109/FiCloud66139.2025.00068},
  url = {https://doi.org/10.1109/ficloud66139.2025.00068},
  booktitle = {2025 12th {International},
  pages = {445--450},
  note = {ISSN: 2996-1017},
  keywords = {Accuracy, Chatbots, Databases, Generative AI, Large language models, Medical Chatbot, Medical services, Mistral-7B-Instruct-v0.3, Natural Language Processing (NLP), Pinecone Vector Database, Reliability, Retrieval augmented generation, RetrievalAugmented Generation, Standards, Vectors, source: IEEE},
  abstract = {Quick and accurate access to medical information is crucial for patients and healthcare professionals alike. Traditional search engines often fail to concise and context-specific responses to medical queries. This paper introduces a medical chatbot that combines large language models (LLMs) with retrieval-augmented generation (RAG) techniques to generate reliable and contextually relevant answers. The system integrates the Mistral-7B-Instructv0.3 model hosted on Hugging Face and uses Pinecone, a vector database, for efficient semantic search. Built using the Flask web framework, the chatbot retrieves the most relevant medical texts, constructs dynamic prompts, and generates accurate, user-friendly responses. Experiments with common medical questions demonstrate the chatbot’s ability to deliver timely, medically sound answers, highlighting the advantages of combining retrieval mechanisms with generative models in healthcare applications.},
  month = {aug},
}

@inproceedings{quddus_enhanced_2024,
  title = {Enhanced {VLSI},
  author = {Quddus, Hafiz Abdul and Hossain, Md Sanowar and Cevahir, Ziya and Jesser, Alexander and Amin, Md Nur},
  year = {2024},
  doi = {10.30420/566438009},
  url = {https://doi.org/10.30420/566438009},
  booktitle = {{DVCon},
  journal = {DVCon Europe 2024 …},
  pages = {57--62},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: IEEE, source: Google Scholar},
  abstract = {Assertion-based verification (ABV) is widely used in VLSI design. However, manual assertion writing is timeconsuming and may not adhere to high-level specifications. Generative AI techniques like LLMs automate this but can introduce hallucination. We propose an automatic assertion generation framework using Retrieval-Augmented Generation (RAG) and LLMs. It generates assertions from designer-tailored specifications, ensuring conformance with high-level specifications and reducing hallucinations. We applied this to an AXI4-Lite protocol case study, verifying SystemVerilog Assertions (SVAs) against golden RTL using Bounded Model Checking (BMC). Results showed improved accuracy, conformance, and integration with ABV.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{rasaq_large_2025,
  title = {Large {Language},
  author = {Rasaq, Lukmon and Yadav, Om Prakash and Siddula, Madhuri and Walthall, Rhonda},
  year = {2025},
  doi = {10.1109/RAMS48127.2025.10935195},
  url = {https://doi.org/10.1109/rams48127.2025.10935195},
  booktitle = {2025 {Annual},
  pages = {1--6},
  note = {ISSN: 2577-0993},
  keywords = {Accuracy, Airworthiness Directives, Atmospheric modeling, AviationQA, Boeing 767, Failure analysis, Finetuning, Large Language Model, Large language models, LoRA, Maintenance, Organizations, PEFT, Random access memory, Retrieval augmented generation, Retrieval Augmented Generation, Training, Tuning, source: IEEE},
  abstract = {This paper presents a Large Language Model (LLM) framework to assess and map each failure mechanism and failure mode of individual Airworthiness Directives (ADs) available in the historical dataset. In addition, this framework can provide insights into related maintenance procedures and practices to help maintenance crews react faster and more effectively.},
  month = {jan},
}

@inproceedings{selva_kumar_overcoming_2024,
  title = {Overcoming {LLM},
  author = {Selva Kumar, S and Khan, Afifah Khan Mohammed Ajmal and Banday, Imadh Ajaz and Gada, Manikantha and Shanbhag, Vibha Venkatesh},
  year = {2024},
  doi = {10.1109/ICETCS61022.2024.10543859},
  url = {https://doi.org/10.1109/icetcs61022.2024.10543859},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Government, GPT-3.5, LLM, NLP, Object detection, Object Detection, Pesticides, Precision Agriculture, Production, RAG, Real-time systems, Reliability engineering, Scalability, YOLOv8, source: IEEE},
  abstract = {This research introduces an innovative AI-driven precision agriculture system, leveraging YOLOv8 for disease identification and Retrieval Augmented Generation (RAG) for context-aware diagnosis. Focused on addressing the challenges of diseases affecting the coffee production sector in Karnataka, The system integrates sophisticated object detection techniques with language models to address the inherent constraints associated with Large Language Models (LLMs). Our methodology not only tackles the issue of hallucinations in LLMs, but also introduces dynamic disease identification and remediation strategies. Real-time monitoring, collaborative dataset expansion, and organizational involvement ensure the system’s adaptability in diverse agricultural settings. The effect of the suggested system extends beyond automation, aiming to secure food supplies, protect livelihoods, and promote eco-friendly farming practices. By facilitating precise disease identification, the system contributes to sustainable and environmentally conscious agriculture, reducing reliance on pesticides. Looking to the future, the project envisions continuous development in RAG-integrated object detection systems, emphasizing scalability, reliability, and usability. This research strives to be a beacon for positive change in agriculture, aligning with global efforts toward sustainable and technologically enhanced food production.},
  month = {apr},
}

@inproceedings{yadav_aeroquery_2024,
  title = {{AeroQuery},
  author = {Yadav, Surendra},
  year = {2024},
  doi = {10.1109/CONECCT62155.2024.10677028},
  url = {https://doi.org/10.1109/conecct62155.2024.10677028},
  booktitle = {2024 {IEEE},
  pages = {1--6},
  note = {ISSN: 2766-2101},
  keywords = {Aerospace, Aerospace electronics, Chatbots, Generative AI, Industries, Industry-Specific AI, Information retrieval, Large language models, Large Language Models, Protocols, Retrieval-Augmented Generation, Text summarization, source: IEEE},
  abstract = {In the realm of avionics and aerospace, the demand for swift access to critical data is hindered by vast documentation, causing hallucinations, delays, and inefficiencies.To address this issue, our approach leverages the concepts of Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).Our approach aims to overcome the limitations of LLMs by incorporating real-time data retrieval capabilities through RAG, enabling seamless access to current information. This envisioned chatbot utilizes advanced natural language processing and proactive pattern identification to streamline information retrieval and communication across various aerospace domains.By leveraging advancements in text summarization and utilizing models like Google’s PaLM2, Facebook’s LLaMA, or OpenAI’s GPT-4, we aim to enhance the performance of chatbots in information retrieval. This involves generating training examples and improving text summarization to efficiently address general inquiries related to standards and communication protocols within the aerospace sector but not limited to this.For instance, an aerospace engineer can quickly obtain relevant information on industry standards or communication protocols through the chatbot equipped with RAG, effortlessly taps into external sources to provide up-to-date and relevant information, reducing the need for exhaustive explanations and improving efficiency in information retrieval and communication processes.},
  month = {jul},
}

@inproceedings{yu_textual_2024,
  title = {Textual {Differential},
  author = {Yu, Junwei and Zhou, Jieyu and Ding, Yepeng and Zhang, Lingfeng and Guo, Yuheng and Sato, Hiroyuki},
  year = {2024},
  doi = {10.1109/COMPSAC61105.2024.00135},
  url = {https://doi.org/10.1109/compsac61105.2024.00135},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE 48th Annual …},
  pages = {988--997},
  note = {ISSN: 2836-3795},
  keywords = {Cognition, Computer architecture, Context-Aware Reasoning, Differential privacy, Large Language Model, Large language models, Named -Entity Recognition, Privacy, Retrieval-Augmented Generation, Robustness, Systematics, source: IEEE, source: Google Scholar},
  abstract = {Large language models (LLMs) have demonstrated proficiency in various language tasks but encounter difficulties in specific domain or scenario. These challenges are mitigated through prompt engineering techniques such as retrieval-augmented generation, which improves performance by integrating contextual information. However, concerns regarding the privacy implications of context-aware reasoning architectures persist, particularly regarding the transmission of sensitive data to LLMs service providers, potentially compromising personal privacy. To mitigate these challenges, this paper introduces Tex-tual Differential Privacy, a novel paradigm aimed at safeguarding user privacy in LLMs-based context-aware reasoning. The proposed Differential Embedding Hash algorithm anonymizes sensitive information while maintaining the reasoning capability of LLMs. Additionally, a quantification scheme for privacy loss is proposed to better understand the trade-off between privacy protection and loss. Through rigorous analysis and experimentation, the effectiveness and robustness of the proposed paradigm in mitigating privacy risks associated with context-aware reasoning tasks are demonstrated. This paradigm addresses privacy concerns in context-aware reasoning architectures, enhancing the trust and utility of LLMs in various applications.},
  month = {jul},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{tommasel_semantic_2024,
  title = {Semantic grounding of {LLMs},
  author = {Tommasel, Antonela and Assent, Ira},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10826117},
  url = {https://doi.org/10.1109/bigdata62323.2024.10826117},
  booktitle = {2024 {IEEE},
  pages = {4048--4057},
  note = {ISSN: 2573-2978},
  keywords = {Benchmark testing, Grounding, Information retrieval, knowledge graphs, Knowledge graphs, large language models, Law, medical documents, Natural language processing, Noise, Pipelines, query expansion, Resource management, Retrieval augmented generation, Semantics, source: IEEE},
  abstract = {The widespread adoption of electronic health records has generated a vast amount of patient-related data, mostly presented in the form of unstructured text, which could be used for document retrieval. However, querying these texts in full could present challenges due to their unstructured and lengthy nature, as they may contain noise or irrelevant terms that can interfere with the retrieval process. Recently, large language models (LLMs) have revolutionized natural language processing tasks. However, despite their promising capabilities, their use in the medical domain has raised concerns due to their lack of understanding, hallucinations, and reliance on outdated knowledge. To address these concerns, we evaluate a Retrieval Augmented Generation (RAG) approach that integrates medical knowledge graphs with LLMs to support query refinement in medical document retrieval tasks. Our initial findings from experiments using two benchmark TREC datasets demonstrate that knowledge graphs can effectively ground LLMs in the medical domain.},
  month = {dec},
}

@inproceedings{neupane_questions_2024,
  title = {From {Questions},
  author = {Neupane, Subash and Hossain, Elias and Keith, Jason and Tripathi, Himanshu and Ghiasi, Farbod and Golilarz, Noorbakhsh Amiri and Amirlatifi, Amin and Mittal, Sudip and Rahimi, Shahram},
  year = {2024},
  doi = {10.1109/FIE61694.2024.10892994},
  url = {https://doi.org/10.1109/fie61694.2024.10892994},
  booktitle = {2024 {IEEE},
  pages = {1--9},
  note = {ISSN: 2377-634X},
  keywords = {Accuracy, Chatbot, Chatbots, information access, Large language models, LLM, Measurement, Pipelines, RAG, Reliability, Retrieval augmented generation, Surveys, University resources, Usability, User experience, source: IEEE},
  abstract = {This research-to-practice full paper presents Bark-plug v.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings. The objective of Barkplug v.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion. Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks. We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment (RAGAS). Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS). Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and satisfactory user experience, as validated by usability assessments.},
  month = {oct},
}

@article{zeeshan_llm-based_2025,
  title = {{LLM},
  author = {Zeeshan, Hafiz Muhammad Ali and Umer, Muhammad and Akbar, Muhammad and Kaushik, Aryan and Jamshed, Muhammad Ali and Jung, Haejoon and Hassan, Syed Ali},
  year = {2025},
  doi = {10.1109/MCOM.001.2400774},
  url = {https://doi.org/10.1109/mcom.001.2400774},
  journal = {IEEE Communications Magazine},
  volume = {63},
  number = {10},
  pages = {60--67},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {6G mobile communication, Augmented reality, Information retrieval, Large language models, Market research, Resource management, Wireless networks, source: IEEE, source: Google Scholar},
  abstract = {The increasing complexity of wireless networks, driven by diverse use cases and the evolution toward 6G, demands advanced resource optimization techniques for efficient and reliable communication. Traditional methods often struggle with adaptability and scalability in such dynamic environments. This article explores large language models (LLMs) as a potential approach to wireless resource optimization. Trained on massive datasets and capable of human-like reasoning, LLMs offer unique advantages in handling complex, multi-faceted problems. We propose a retrieval-augmented generation (RAG)-based framework that combines LLMs with domain-specific knowledge extracted from wireless standards documents, research articles, network-specific data, and other documents. This integration enhances the accuracy and reliability of LLM-driven decisions, enabling effective solutions to real-world challenges. We demonstrate the effectiveness of our RAG-based approach through a case study on spectrum management, showcasing its potential to outperform traditional methods and adapt to evolving network conditions. Lastly, we conclude with a discussion on the challenges and future directions in this promising area.},
  issn = {1558-1896},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{ta_fine-tuning_2025,
  title = {Fine-tuning a {Large},
  author = {Ta, Rishabh and Salunke, Ramit and R, Rahul and Nv, Ritvik and Upadhyaya, Sujatha R},
  year = {2025},
  doi = {10.1109/INFOTEH64129.2025.10959207},
  url = {https://doi.org/10.1109/infoteh64129.2025.10959207},
  booktitle = {2025 24th {International},
  pages = {1--6},
  note = {ISSN: 2767-9470},
  keywords = {Accuracy, Adaptation models, Benchmark testing, Computational modeling, Context modeling, Large language models, Law, Measurement, Reliability, Retrieval augmented generation, source: IEEE},
  abstract = {This paper introduces an application based on a large language model customized for the Indian legal system, leveraging the LLama 3.1 8B foundational model. The model is pre-trained on a diverse corpus of legal texts and subsequently fine-tuned with curated Indian legal data to enhance accuracy and contextual relevance in legal responses. Advanced techniques such as Low-Rank Adaptation and Quantized Low-Rank Adaptations are employed to optimize the model’s efficiency while minimizing computational costs during fine-tuning. Pruning, as a compression method is utilized to enhance the model’s performance further and enable its deployment in resource-constrained environments. Additionally, the Retrieval Augmented Generation module is strategically implemented for document-specific queries, ensuring contextually accurate responses when processing legal documents. This research work is backed up by extensive experiments measuring the effectiveness across many precision metrics. The application is also tested against HaluEval, a Hallucination Evaluation Benchmark for factual reliability, and has demonstrated significant improvements in the model’s effectiveness as a resource for the legal domain. This AI-driven tool is the first step in simplifying the legal advisory services and decision-support systems in the Indian judiciary. It also goes a long way in enhancing the legal services experience.},
  month = {mar},
}

@inproceedings{rorseth_rage_2024,
  title = {{RAGE},
  author = {Rorseth, Joel and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw},
  year = {2024},
  doi = {10.1109/ICDE60146.2024.00430},
  url = {https://doi.org/10.1109/icde60146.2024.00430},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE 40th …},
  pages = {5469--5472},
  note = {ISSN: 2375-026X},
  keywords = {Data engineering, Data models, Explainable AI, Large language models, Navigation, Retrieval-Augmented Generation, source: IEEE, source: Google Scholar},
  abstract = {This paper demonstrates RAGE, an interactive tool for explaining Large Language Models (LLMs) augmented with retrieval capabilities; i.e., able to query external sources and pull relevant information into their input context. Our explanations are counterfactual in the sense that they identify parts of the input context that, when removed, change the answer to the question posed to the LLM. RAGE includes pruning methods to navigate the vast space of possible explanations, allowing users to view the provenance of the produced answers.},
  month = {may},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{gopi_enhancing_2024,
  title = {Enhancing {Engineering},
  author = {Gopi, Sreekanth and Sreekanth, Devananda and Dehbozorgi, Nasrin},
  year = {2024},
  doi = {10.1109/FIE61694.2024.10893146},
  url = {https://doi.org/10.1109/fie61694.2024.10893146},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE Frontiers in …},
  pages = {1--8},
  note = {ISSN: 2377-634X},
  keywords = {Accuracy, AI quiz generation, Databases, engineering education, GPT-4, Large Language Models, Learning (artificial intelligence), LLM evaluation, Market research, Mathematical models, personalized learning, prompt engineering, RAG, STEM, Systematic literature review, Systematics, Vec-tara, Vectors, source: IEEE, source: Google Scholar},
  abstract = {This research-to-practice study aims to develop an Artificial Intelligence (AI) MCQ generation system for engineering students, with a focus on adaptive learning, educational technology, and innovative assessment tools, to enhance personalized learning. Engineering education faces significant academic performance challenges, with first-year retention rates in STEM fields ranging between 27\% to 46\%, largely due to poor academic achievements. Multiple Choice Questions (MCQs) identify misconceptions, reinforce knowledge retention, and offer efficient assessment methods for engineering education. This interactive method improves attention and memory retention, reinforces knowledge, and improves comprehension. In this context, the emergence of Large Language Models (LLMs) such as GPT-4 has marked a significant advancement. Our literature review method employed a systematic approach, analyzing peer-reviewed articles, conference papers, and authoritative reports to uncover the trends and challenges in AI-driven quiz generation. The notable gap identified in our literature review is the lack of LLM-based adaptive quiz generation methods specifically for engineering education. Our methodology involved sourcing relevant structured datasets, data pre-processing, embedding generation, vector database storage, hybrid-search retrieval, LLM query results feed, prompt engineering, and context-based response. In this research, we adopted Vectara as a vector database tool for its automatic data ingestion capabilities and seamless integration with generative AI applications. Prompt engineering involves a dual-prompt approach, where the Contextual Question Prompt formulates questions based on user topics and chat history, while the Answer Question Prompt manages MCQ responses with explanations, ensuring relevant and contextually accurate interactions. Evaluation includes topic relevancy, answer relevancy, and a contextual relevancy score. Preliminary results indicate promising results for the generation of accurate and contextually appropriate questions with minimal hallucinations. The quiz generation system was deployed using Streamlit cloud-based architecture to showcase the functionality. Looking forward, we aim to expand the dataset to include more diverse engineering disciplines and to refine the retrieval algorithms to better handle complex diagrams and mathematical expressions commonly found in engineering texts.},
  month = {oct},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{martinez-romo_generative_2025,
  title = {Generative {AI},
  author = {Martinez-Romo, Juan and Araujo, Lourdes and Plaza, Laura and López-Ostenero, Fernando},
  year = {2025},
  doi = {10.1109/EDUCON62633.2025.11016446},
  url = {https://doi.org/10.1109/educon62633.2025.11016446},
  booktitle = {2025 {IEEE},
  pages = {1--9},
  note = {ISSN: 2165-9567},
  keywords = {Accuracy, computer science, Computer science, Engineering education, formative feedback, generative IA, Iterative methods, Prompt engineering, Retrieval augmented generation, Self-assessment tools, source: IEEE},
  abstract = {The application of generative AI in education has shown significant potential to enhance learning outcomes by providing personalized, adaptive feedback to students. In this work, we present a novel Retrieval-Augmented Generation (RAG) system designed to improve the explanations and feedback provided to students during self-assessment activities. The system we developed is grounded in the course's reference material, ensuring that the feedback remains accurate, consistent, and contextually relevant to the student's curriculum. The system retrieves information directly from the textbook, reducing ambiguity and interpretation errors, and generates responses tailored to the specific needs of each student. The feedback is not only designed to correct misconceptions but also to reinforce key concepts, making the system a valuable tool for self-guided learning. In this study, we also explore the importance of prompt engineering in creating effective AI-generated feedback. We detail the iterative process used to optimize the prompts and the strategies employed to ensure high-quality, interpretable responses. The findings from this work suggest that generative AI, when integrated with subject-specific textbooks and careful prompt engineering, can significantly enhance the educational experience by providing dynamic, and contextually accurate feedback. This approach opens new possibilities for AI-driven education tools, contributing to more personalized and effective learning experiences.},
  month = {apr},
}

@inproceedings{beining_generating_2024,
  title = {Generating {Commit},
  author = {Beining, Yang and Alassane, Samba and Guillaume, Fraysse and Sihem, Cherrared},
  year = {2024},
  doi = {10.23919/CNSM62983.2024.10814636},
  url = {https://doi.org/10.23919/cnsm62983.2024.10814636},
  booktitle = {2024 20th {International},
  pages = {1--7},
  note = {ISSN: 2165-963X},
  keywords = {5G mobile communication, Automation, Commit Message Generation, Data mining, Large Language Model, Large language models, Measurement, Meteors, network automation, Retrieval augmented generation, source: IEEE},
  abstract = {Network automation is crucial for improving network performance. Commit messages describes the different actions of the modification of network configuration files and deployments. This paper presents experiments and studies on automated commit message generation in the deployment of 5G networks. We extracted data from repositories of various projects engineered in Orange’s 5G network. We then developed five prompts for experiments to identify the most suitable methods for this task. To select large language models, we used an in-house GPT-4 interface provided by Orange, and locally deployed popular large models such as Llama3, Mistral. We used both automated and human evaluation methods, selecting BLEU, ROUGE, and METEOR as our metrics for automated assessment. Our experiments shows that commit messages for configuration files generated by Large Language Models (LLMs) have better scores when using one-shot and Retrieval-Augmented Generation (RAG) technologies, for messages generated both by humans and bots.},
  month = {oct},
}

@inproceedings{yao_assertgpt_2025,
  title = {{AssertGPT},
  author = {Yao, Ying and Tian, Le and Pan, Fan and Hu, Yuxiang and Yang, Xingyuan and Zhan, Qi and Yue, Qiqiang},
  year = {2025},
  doi = {10.23919/APNOMS67058.2025.11181349},
  url = {https://doi.org/10.23919/apnoms67058.2025.11181349},
  booktitle = {2025 25th {Asia},
  pages = {1--6},
  note = {ISSN: 2576-8565},
  keywords = {Assertion Generation, Assertion-based, Feedback loop, Iterative methods, Large Language Model, Large language models, Limiting, Manuals, Natural language processing, Network Verification, Programmable Network, Retrieval augmented generation, Scalability, Semantics, Syntactics, source: IEEE},
  abstract = {Assertion-based verification is essential for ensuring that the operational behavior of programmable networks aligns with their intended design specifications. However, reliance on manual assertion generation prolongs verification cycles and introduces deployment barriers due to its labor intensity, limiting scalability in practical implementations. While Large Language Models (LLMs) have demonstrated transformative potential in automating engineering tasks with their excellent natural language understanding, their application to assertion-based network verification remains underexplored. In this study, we propose leveraging LLMs for assertion-based verification of programmable networks. We introduce AssertGPT, an LLMdriven automated assertion generation framework designed to streamline the verification workflow. Firstly, we construct a highquality dataset in the field of assertion-based network verification, and further develop a domain-specific LLM tailored for automated assertion generation in network verification scenarios by synergizing custom instruction refinement with the Retrieval Augmented Generation (RAG) technique. To mitigate hallucinations, we develop a multi-stage assertion checker with a dynamic feedback loop, enabling iterative refinement of AssertGPT’s performance through continuous assertion regeneration guided by syntactic and semantic verification outcomes. An evaluation of diverse network verification requirements and multiple assertion languages (e.g., those in AssertP4, DBVal) underscores AssertGPT’s efficacy in automated assertion generation for assertionbased network verification.},
  month = {sep},
}

@inproceedings{abeywardana_enhancing_2025,
  title = {Enhancing {Automated},
  author = {Abeywardana, Thisuri and Nandadewa, Nethmini and Wickramasinghe, Vimansa and Rohanadheera, Sudam and Nadungodage, Thilini and Hewagamage, Priyantha},
  year = {2025},
  doi = {10.1109/ICARC64760.2025.10962827},
  url = {https://doi.org/10.1109/icarc64760.2025.10962827},
  booktitle = {2025 5th {International},
  pages = {1--6},
  keywords = {Accuracy, AES, Analytical models, Cognition, Computational modeling, Context modeling, Education, Grading, Knowledge engineering, Large language models, LLM, Prompt engineering, RAG, source: IEEE},
  abstract = {This research explores the potential of Large Language Models (LLMs) to automate the grading process in education by harnessing LLMs’ sophisticated understanding of language and instructions following nature. We explore the effectiveness of providing subject knowledge and utilizing prompt engineering techniques to grade the students’ written answers for different question types and various theoretical subjects. A grading rubric was employed to ensure consistency and fairness in the assessment process. The study results highlighted the importance of providing external knowledge within the prompt to enhance the automated student answer grading utilizing LLMs. Including grading rubrics, model answers, and course content significantly enhanced the accuracy of scores assigned by the LLM, reducing deviations from human evaluator scores. Providing course content or model answers also helped define the expected answer scope and guide the LLM in determining other possible correct answers. Using prompt engineering techniques within the prompt failed to outperform the basic prompt, suggesting the need for further exploration and refinement in prompt design strategies.},
  month = {feb},
}

@inproceedings{sunde_evaluation_2025,
  title = {On the evaluation of {StartupGPT},
  author = {Sunde, Helene Fønstelien and Lovise Ahlgren, Thea and Jaccheri, Letizia and Nguyen-Duc, Anh},
  year = {2025},
  doi = {10.1109/IWSiB66663.2025.00009},
  url = {https://doi.org/10.1109/iwsib66663.2025.00009},
  booktitle = {2025 {IEEE},
  pages = {25--32},
  keywords = {Business, Chatbots, Industries, Iterative methods, Knowledge based systems, large language models, Large language models, Online Mentor, Prototypes, Reliability theory, Retrieval augmented generation, Retrieval-Augmented Generation, Software, Startup, Startup Mentor, source: IEEE},
  abstract = {[Introduction] The study addresses the challenge of transferring research knowledge to the industry, with a particular focus on small businesses and startups, which often lack access to empirical insights. Large Language Models (LLMs), particularly those using Retrieval-Augmented Generation (RAG), offer potential for embedding knowledge from startup research into an interactive chatbot to support startup mentorship. However, empirical work exploring this application is limited. [Objective] The primary objective of this research is to design and evaluate a version of "StartupGPT," an AI-driven chatbot that uses LLMs and RAG to provide advice for software startups by leveraging a knowledge base rooted in software startup research. [Methodology] The study follows the Design Science Research Methodology (DSRM) and spans three iterative cycles, with this paper focusing on Cycle 3. The prototype was tested with 11 startup founders, who provided both qualitative and quantitative feedback on the chatbot’s usefulness and satisfaction. [Results] The findings from user tests indicate that StartupGPT was generally perceived as relevant, reliable, and helpful. However, limitations were noted in its responses, which users found overly theoretical, lacking in concrete examples, and insufficiently personalized for specific startup contexts. [Conclusion] Future LLM-based interaction designed for startups should focus on improving interactivity, incorporating more context-aware and specific advice, and leveraging advanced AI techniques, such as fine-tuning, to better align the chatbot’s responses with the unique needs of individual startups.},
  month = {may},
}

@inproceedings{serhan_vulnerabilities_2025,
  title = {Vulnerabilities of a {Medical},
  author = {Serhan, Jean H. and Nehme, Bechara F.},
  year = {2025},
  doi = {10.1109/ACTEA66485.2025.11189974},
  url = {https://doi.org/10.1109/actea66485.2025.11189974},
  booktitle = {2025 {Sixth},
  journal = {2025 Sixth International Conference …},
  pages = {1--6},
  note = {ISSN: 2993-3765},
  keywords = {fine-tuning, Large language models, LLM, medical systems, RAG, scientific systems, Semantic search, Software, vulnerabilities, source: IEEE, source: Google Scholar},
  abstract = {When dealing with content like Medical or scientific, things become more complex. Nowadays, Software engineers are increasingly adding semantic search capabilities to applications using RAG. A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) to extract the right answer using an LLM. RAG systems aim to reduce the problem of LLM hallucination and provide appropriate links to sources or references for the generated responses and remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherited from the quality of retrieved information and from LLMs limited input context window to name a few. In this paper, we present an experience report on the limitations of RAG systems from multiple case studies from separate domains: medical and scientific. We share the lessons learned and show how to prevent those limitations to consider when designing a RAG system. We show that an enhancement of up to 19\% could be added on RAG systems when implementing those suggestions.},
  month = {sep},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{sheng_talk2traffic_2025,
  title = {{Talk2Traffic},
  author = {Sheng, Zihao and Huang, Zilin and Qu, Yansong and Leng, Yue and Chen, Sikai},
  year = {2025},
  doi = {10.1109/CVPRW67362.2025.00364},
  url = {https://doi.org/10.1109/cvprw67362.2025.00364},
  booktitle = {2025 {IEEE},
  pages = {3788--3797},
  note = {ISSN: 2160-7516},
  keywords = {autonomous driving, Autonomous vehicles, Codes, Large language models, multimodal large language models, Retrieval augmented generation, Safety, Scenario generation, Semantics, Syntactics, Testing, traffic scenario generation, Translation, source: IEEE},
  abstract = {Deploying autonomous vehicles (AVs) requires testing in diverse and challenging scenarios to ensure safety and reliability, yet collecting real-world data remains prohibitively expensive. While simulation-based approaches offer costeffective alternatives, most existing methods lack sufficient support for intuitive, interactive editing of generated scenarios. This paper presents Talk2Traffic, a novel framework that leverages multimodal large language models (MLLMs) to enable interactive and editable traffic scenario generation. Talk2Traffic allows human users to generate various traffic scenarios through multimodal inputs (text, speech, and sketches). Our approach first employs an MLLM-based interpreter to extract structured representations from these inputs. These representations are then translated into executable Scenic code using a retrieval-augmented generation mechanism to reduce hallucinations and ensure syntactic correctness. Furthermore, a human feedback guidance module enables iterative refinement and editing of scenarios through natural language instructions. Experiments demonstrate that Talk2Traffic outperforms state-of-the-art methods in generating challenging scenarios. Qualitative evaluations further illustrate the framework can handle diverse input modalities and support scenario editing.},
  month = {jun},
}

@inproceedings{murata_effectiveness_2025,
  title = {On the {Effectiveness},
  author = {Murata, Hiroaki and Handa, Hisashi},
  year = {2025},
  doi = {10.1109/ICBIR65229.2025.11163304},
  url = {https://doi.org/10.1109/icbir65229.2025.11163304},
  booktitle = {2025 10th {International},
  pages = {472--475},
  keywords = {Business, Chatbots, ChatGPT, fine-tuning, question-answer system, RAG, Servers, Slack, source: IEEE},
  abstract = {QABot is an app on Slack designed for universities. When students ask questions on the app, they receive responses from teachers and TAs, but the system is designed so that students do not know who has responded. In other words, all messages, such as questions and answers, between students and teachers are forwarded to the local server where the app is running. By using the app, students no longer directly ask specific teachers or TAs via DM, and the workload can be distributed. However, there are cases where similar questions arise from various students. In this paper, we have given QABot a mechanism for creating answer drafts using ChatGPT. Three models of ChatGPT are examined: 4o mini, 4o mini using fine-tuning, and 4o mini using Simple RAG, which can refer to past questions and answers. These models are evaluated by using BertScore. 4o mini using fine-tuning demonstrated good results with statistical significance.},
  month = {may},
}

@inproceedings{tamanna_navigating_2024,
  title = {Navigating the {Roadblocks},
  author = {{Tamanna},
  year = {2024},
  doi = {10.1109/AIC61668.2024.10731053},
  url = {https://doi.org/10.1109/aic61668.2024.10731053},
  booktitle = {2024 {IEEE},
  pages = {618--626},
  keywords = {source: IEEE},
  abstract = {Large Language Model (LLM) Retrieval-Augmented Generation (RAG) chatbots hold immense potential for enhancing user engagement and information access. However, bringing such a system to life on a real-world platform presents unique challenges. This article explores the design of the chatbot, and the hurdles encountered in productionizing an LLM-RAG chatbot for a gardening society website. We delve into specific roadblocks faced during development, including•Domain-Specific Knowledge Acquisition: Training the LLM on a comprehensive gardening knowledge base proved crucial to ensure accurate and relevant responses.•Balancing Open-Endedness with Focus: While LLMs excel at open-ended conversation, enabling the chatbot to answer gardening queries effectively required focused training strategies.•Integration with Website Infrastructure: Seamless integration of the chatbot into the society's website involved addressing technical considerations and ensuring a user-friendly experience.We present the solutions implemented to overcome these challenges, offering valuable insights for those considering deploying similar LLM-RAG chatbots for niche online communities. The article concludes by discussing the lessons learned and the potential impact of such chatbots on the future of online user interactions.},
  month = {jul},
}

@article{krishnamurthy_yours_2024,
  title = {Yours {Truly},
  author = {Krishnamurthy, Vallidevi and Balaji, Varshini},
  year = {2024},
  doi = {10.1109/ACCESS.2024.3520187},
  url = {https://doi.org/10.1109/access.2024.3520187},
  journal = {IEEE Access},
  volume = {12},
  pages = {195152--195173},
  keywords = {COVID-19, Databases, Fake news, Fake news detection, few-shot learning, large language models, Pandemics, Real-time systems, retrieval augmented generation, Social networking (online), Surveys, Transformers, Vaccines, vector search, Voting, source: IEEE},
  abstract = {In an era where social media portrays subjective realities, discerning truth from propaganda has become increasingly challenging. The proposed system addresses this issue with an end-to-end credibility framework to make fact-checking effortless and intuitive. Recognizing the subjective nature of claims, the system provides a robust method to assess the veracity of social media claims. Twitter, a key platform for public opinion exchange, influences cultural beliefs, political affiliations, and crisis responses. This work offers a pragmatic solution to navigate manipulative claims, reducing the cognitive effort to distinguish fact from fabrication and breaking misinformation chains sooner. Yours Truly uses FactStore, an extensive real-time database of fact-checked claims from Indian and International fact-checking initiatives. This database powers the search function, enabling time-sensitive searches with increased coverage to retrieve relevant context. The system breaks down compound sentences into atomic claims, verifying each with iterative context retrieval. Each claim is further fact-checked with multiple articles using text and semantic search. These matched articles are reranked for relevance using a technique called query-based committee selector. The top-ranked results provide context to an instruction fine-tuned Large Language Model, which infers the truth value of input claims. This approach tackles claims’ ambiguity and complexity and returns an interpretable credibility report explaining the inferred truth value. Yours Truly achieves an impressive F1 Score of 94\% The framework is easily extensible to verify contents from other social media platforms, as it only relies on text without metadata and effectively handles long-form texts by atomizing compound statements. Yours Truly outperforms contemporary fact-checking systems on multiple misinformation baselines. It generalizes well across various text forms and information domains, demonstrating a high level of automation.},
  issn = {2169-3536},
}

@inproceedings{yadav_enhancing_2024,
  title = {Enhancing {Response},
  author = {Yadav, Divyanshi and Para, Hitesh and Sandhu, Komal and Selvakumar, Prakash},
  year = {2024},
  doi = {10.1109/ICECET61485.2024.10698632},
  url = {https://doi.org/10.1109/icecet61485.2024.10698632},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Business, Decision making, Decision Making, Email Handling, Faces, Focusing, Generative AI, Information Retrieval, Knowledge graphs, Knowledge Graphs, Large Language Model, Large language models, Natural Language Processing, Natural Language Understanding, Pathways parsing, Response Generation, Retrieval Augmented Generation, Scalability, Standards, Vectors, Word Embeddings, source: IEEE},
  abstract = {In the contemporary business environment, it is crucial to have an efficient and precise response generation system to build client trust, optimize operations, and provide customized solutions. Current response management systems often face challenges such as insufficient depth, scalability issues, and inconsistencies. There is an urgent requirement for a comprehensive system that can retrieve information for generating high-quality insights and convert it to a reply. To address these needs, an approach has been explored that integrates a domain-oriented Knowledge Graph (KG), vector embeddings, Large Language Model (LLM) and utilize the pathway parsing technique that allows for in-depth multi-hop analysis within the KG, resulting in detailed and contextually rich data retrieval. This combination enhances the performance and precision of handling inquiries, streamlines entity extraction and step identification, leverages KG for Standard Operating Procedure (SOP) guidance, and offers superior recommendations or strategies for informed decision making. The concept will be illustrated through a business study focusing on the collections department use case, which involves customer correspondence. This approach ensures a more efficient, and accurate responses, leading to reduced human intervention and latency, along with that customer satisfaction is improved and business processes are streamlined. By adopting this method, businesses can enhance their communication, make data-driven decisions, and ultimately achieve better results in the competitive market. The efficacy is evident in the practical instances, owing to its profound grasp of context.},
  month = {jul},
}

@inproceedings{bernardi_report_2024,
  title = {Report {Generation},
  author = {Bernardi, Mario Luca and Cimitile, Marta},
  year = {2024},
  doi = {10.1109/IJCNN60899.2024.10650332},
  url = {https://doi.org/10.1109/ijcnn60899.2024.10650332},
  booktitle = {2024 {International},
  pages = {1--8},
  note = {ISSN: 2161-4407},
  keywords = {Accuracy, Deep Learning, Image-Text Match, Large language models, Large Language Models (LLMs), Measurement, Radiology, Refining, Report Generation, Semantic segmentation, Statistical analysis, X-Ray imaging, source: IEEE},
  abstract = {Creating radiology reports is a vital but time-intensive task that involves analyzing images, consulting documents, and evaluating data. This process, heavily reliant on human effort, is prone to errors that can vary with the radiologists experience. Consequently, automating the generation of radiology reports is a key research goal due to its potential impact on medical procedures and patient care.This work proposes a multimodal approach specifically designed for generating radiological reports from chest X-rays (CXRs). Our method integrates a LLaMa large language model with Retrieval Augmented Generation (RAG), enhanced by a modified ALBEF embedding model that exploits efficient organ semantic segmentation and triple contrastive loss (called EALBEF). The combination of these two components allows radiological report generation that surpasses current state-of-the-art methods in terms of quality and accuracy. Our approach demonstrates a significant enhancement in the radiologist-specific metrics (e.g., RadCliQ), as well as across various generic lexical-based metrics (e.g., GLEU). Quantitative analyses of the models outputs reveal a notable increase in fluency and accuracy, with a marked reduction in issues such as hallucinations and source-reference divergences in the generated reports.},
  month = {jun},
}

@inproceedings{bustamante_raccoon_2025,
  title = {{RACCOON},
  author = {Bustamante, Samuel and Knauer, Markus and Thun, Jeremias and Schneyer, Stefan and Albu-Schäffer, Alin and Weber, Bernhard and Stulp, Freek},
  year = {2025},
  doi = {10.1109/ICRA55743.2025.11127843},
  url = {https://doi.org/10.1109/icra55743.2025.11127843},
  booktitle = {2025 {IEEE},
  pages = {4322--4329},
  keywords = {Adaptation models, Cameras, Foundation models, Grounding, Retrieval augmented generation, Robot vision systems, Robots, source: IEEE},
  abstract = {Explainability is vital for establishing user trust, also in robotics. Recently, foundation models (e.g. vision-language models, VLMs) fostered a wave of embodied agents that answer arbitrary queries about their environment and their interactions with it. However, naively prompting VLMs to answer queries based on camera images does not take into account existing robot architectures which represent the robot's tasks, skills, and beliefs about the state of the world. To overcome this limitation, we propose RACCOON, a framework that combines foundation models' responses with a robot's internal knowledge. Inspired by Retrieval-Augmented Generation (RAG), RACCOON selects relevant context, retrieves information from the robot's state, and utilizes it to refine prompts for an LLM to answer questions accurately. This bridges the gap between the model's adaptability and the robot's domain expertise.},
  month = {may},
}

@article{wang_hierarchical_2025-1,
  title = {Hierarchical {Index},
  author = {Wang, Jingyu and Guo, Lingqi and Wu, Jianyu and Yan, Caijun and Sun, Haifeng and Zhang, Lei and Zhuang, Zirui and Qi, Qi and Liao, Jianxin},
  year = {2025},
  doi = {10.1109/TMC.2025.3564937},
  url = {https://doi.org/10.1109/tmc.2025.3564937},
  journal = {IEEE Transactions on Mobile Computing},
  volume = {24},
  number = {10},
  pages = {9837--9851},
  keywords = {Adaptation models, Computational modeling, Hardware, Indexes, intent translation, Intent-based networking, large language model, Manuals, Natural language processing, Optimization, retrieval-augmented generation, Training, Translation, Wireless networks, source: IEEE},
  abstract = {Intent-Based Networking (IBN) represents an emerging network management concept that is designed to fulfill user service requirements through automation. At its core, IBN is capable of translating user intent into network policies, thereby enabling automated configuration and management. However, the application of IBN has been limited by challenges associated with automation and intelligence. The recent widespread adoption of Large Language Model (LLM) has partially mitigated these issues. Nonetheless, hardware heterogeneity and high dynamic networks remain significant challenges for IBN: (i) Devices from different vendors are challenging to manage uniformly; (ii) Aligning service demands with rapidly changing network status is difficult. To address these challenges, we propose LIT, a framework of LLM-empowered Intent Translation with manual guidance. LIT incorporates Retrieval-Augmented Generation (RAG) to reference hardware manuals and enhance the generation results of LLMs. To reduce noise from retrieval results, we optimized the general RAG process. Additionally, LIT introduces MoE (Mixture of Experts) to adjust parameter values according to network status by synthesizing results from multiple expert models. Experiments demonstrate that LIT alleviates the challenges faced by IBN, achieving a 57.5\% improvement in F1 score compared to the baseline.},
  issn = {1558-0660},
  month = {oct},
}

@inproceedings{bin_zaman_chowdhury_durghotona_2024,
  title = {Durghotona {GPT},
  author = {Bin Zaman Chowdhury, Md Thamed and Islam, Md. Ridwanul and Hossain, Moazzem},
  year = {2024},
  doi = {10.1109/ICCIT64611.2024.11021969},
  url = {https://doi.org/10.1109/iccit64611.2024.11021969},
  booktitle = {2024 27th {International},
  pages = {50--55},
  note = {ISSN: 2474-9656},
  keywords = {Accuracy, artificial intelligence, automation, data analytics, data mining, Data mining, large language models, Large language models, machine learning, Machine learning, newspaper analysis, Prompt engineering, Public healthcare, Retrieval augmented generation, road accident, Road accidents, Safety, Urban planning, web scraping, source: IEEE},
  abstract = {Road accidents pose significant concerns globally. It leads to large financial losses, injuries, disabilities and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named ‘Durghotona GPT’ that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers— Prothom Alo, Dhaka Tribune and The Daily Star leveraging web scraping techniques. The collected news was then processed using the newest available LLMs. These LLMs are: GPT-4, GPT-3.5 and Llama-3. The framework efficiently extracts relevant information, categorizes reports and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors and communication gaps. The authors’ evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 96\% accuracy in the authors’ evaluation. So, it can be considered as a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning and public health. The authors also developed an interface for ‘Durghotona GPT’ for easy use as a part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.},
  month = {dec},
}

@inproceedings{farooqui_large_2025,
  title = {Large {Language},
  author = {Farooqui, Maaz Ahmad and Nehra, Vibha and Sinha, Ayushi},
  year = {2025},
  doi = {10.1109/CONIT65521.2025.11167888},
  url = {https://doi.org/10.1109/conit65521.2025.11167888},
  booktitle = {2025 5th {International},
  pages = {1--8},
  keywords = {BART, BERTScore, BLEU, Coherence, Context modeling, Employee welfare, Explainable AI, Fine-Tuning, Large language models, Large Language Models, Mental health, Mental Health Counselling, Perplexity, Prevention and mitigation, Retrieval augmented generation, ROUGE, Semantics, Sensitivity, T5-Base, T5-Small, source: IEEE},
  abstract = {Mental health challenges are becoming increasingly prevalent worldwide, there is a growing demand for scalable and accessible support systems. This paper explores the application of Large Language Models (LLMs) in providing personalized counselling for mental health. This study evaluates the performance of three pre-trained models—T5-Small, T5-Base and BART-large on a dataset of mental health counselling queries. Metrics such as BLEU, ROUGE, and perplexity were used to assess the quality of model-generated responses before and after fine-tuning. Further, we evaluated the finetuned models using BERTScore. Results reveal that fine-tuning significantly improves model coherence and relevance, with noticeable reductions in perplexity. The findings highlight the potential of LLMs in assisting mental health professionals by providing empathetic and contextually relevant responses.},
  month = {jun},
}

@inproceedings{zhou_research_2024-1,
  title = {Research on {AI},
  author = {Zhou, Liang and Lv, Zhengyu and Zhang, Xinhui and Hu, Junpeng and Pan, Longfei and Pang, Xing and Shi, Jianyong},
  year = {2024},
  doi = {10.1049/icp.2024.4307},
  url = {https://doi.org/10.1049/icp.2024.4307},
  booktitle = {6th {International},
  volume = {2024},
  pages = {728--734},
  keywords = {source: IEEE},
  abstract = {In the digital age, in order to solve the problems of illusion, timeliness, data security, etc. of large language model technology (LLM), retrieval-augmented generation (RAG) has emerged as a feasible solution. Therefore, the architecture concept of LLM+RAG was proposed, in order to assist the design work as an efficient AI tool. Therefore, this paper adopts a layered design, and realizes the end-to-end process from professional knowledge acquisition to intelligent suggestion generation through six core functional layers: knowledge base construction layer, retrieval enhancement layer, context construction layer, LLM interaction layer, response optimization layer and interaction integration layer. In the design process, various language processing, retrieval, context model generation and other technologies are combined to complete a feasible RAG+LLM AI- assisted design architecture. Finally, through the real-time updated professional knowledge base, multi-way recall mechanism and chain reasoning method, the tool can provide reliable design suggestions for design work and improve the efficiency and quality of substation design. This study provides a design idea of a reliable RAG+LLM architecture, and finally completes the construction of an AI-assisted design tool using this architecture.},
  month = {oct},
}

@article{sun_taming_2025,
  title = {Taming {Unleashed},
  author = {Sun, Lianshan and Liu, Diandong and Wang, Maoxue and Han, Yongyi and Zhang, Yanqing and Zhou, Biwei and Ren, Yi and zhu, Peng},
  year = {2025},
  doi = {10.1109/JBHI.2025.3528526},
  url = {https://doi.org/10.1109/jbhi.2025.3528526},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {29},
  number = {6},
  pages = {4498--4511},
  keywords = {source: IEEE},
  abstract = {The digital health field's pursuit of massive, personalized healthcare continuously faces constraints from doctors' resources and capacity limitations. Recently, the emergence of large language models (LLMs), with their remarkable comprehension and processing abilities, has revolutionized digital health and enhanced massive, personalized healthcare. Although these LLMs have achieved significant advancements, they have also introduced inevitable hallucinations, which impact patient safety when used in massive applications. To address these challenges, this study proposes a digital hospital for a massive, personalized, reliable healthcare service named the Chat Chain-Brain-based Doctor (CHATCBD). In addition, this study transforms the LLM-based diagnostic process into a digital hospital architecture, designs a controllable AI agents framework, and develops a self-audit mechanism to enhance their reliability. The proposed CHATCBD uses blockchain technology to decentralize external regulation of the LLMs' personalized diagnoses. It introduces a blockchain-based personalized routing management mechanism to improve patient-centered decision-making and designs a blockchain-based audit framework based on a proposed mathematical model that ensures both the professionalism and honesty of audits, serving as a safety net for addressing LLM hallucinations. The results of extensive experiments conducted on 13 datasets from multiple perspectives demonstrate that the proposed CHATCBD system can significantly enhance the capabilities of LLMs in personalized healthcare.},
  month = {jun},
  issn = {2168-2208},
}

@inproceedings{pandini_exploratory_2025,
  title = {An {Exploratory},
  author = {Pandini, Gabriele and Martini, Antonio and Videsjorden, Adela Nedisan and Fontana, Francesca Arcelli},
  year = {2025},
  doi = {10.1109/ICSA-C65153.2025.00070},
  url = {https://doi.org/10.1109/icsa-c65153.2025.00070},
  booktitle = {2025 {IEEE},
  pages = {462--471},
  note = {ISSN: 2768-4288},
  keywords = {source: IEEE},
  abstract = {Architectural smells are abundant in codebases and regularly hinder the development of stable and maintainable code. Understanding and removing these elements can consume a huge amount of developers' time, who often need to prioritize implementing new features. This causes a substantial increase in Technical Debt, compromising the scalability and maintainability of the codebases, at time bringing the development to a standstill. Meanwhile, the use of Large Language Models for small error correction is constantly growing, bringing the attention of an ever-wider audience to these technologies. This study explores a first approach to use Large Language Models to suggest refactoring for architectural smells, with a focus on Cyclic Dependencies smells. We study the use of detailed prompt and Retrieval-Augmented Generation (RAG) to enhance LLMs, and we study local vs cloud LLMs. The results are promising, also validated with a series of interviews with students and developers, and highlight how additional and precise context is key to enhance the use of LLMs to propose refactoring suggestions. A multi-agent approach seems to be more suited when increasing the complexity of the smells.},
  month = {mar},
}

@inproceedings{russo_scaling_2024,
  title = {Scaling {LLM},
  author = {Russo, Diego and Orlando, Gian Marco and Romano, Antonio and Riccio, Giuseppe and Gatta, Valerio La and Postiglione, Marco and Moscato, Vincenzo},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825937},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825937},
  booktitle = {2024 {IEEE},
  pages = {3494--3497},
  note = {ISSN: 2573-2978},
  keywords = {Data mining, Engines, Feature extraction, International relations, Knowledge Graph (KG), Knowledge graphs, Large language models, Large Language Models (LLMs), Named Entity Recognition (NER), Prevention and mitigation, Real-time systems, Relation Extraction (RE), Reliability, Retrieval-Augmented Generation (RAG), Scalability, source: IEEE},
  abstract = {Geopolitical news provides vast amounts of information essential for understanding international relations and political events. However, organizing this information into a coherent, structured format poses challenges due to the complexity and dynamic nature of the domain. This paper introduces a scalable system leveraging Large Language Models to build continuously updated Knowledge Graphs from Italian geopolitical news. The system features a modular architecture, including a Collector Node for scalable article extraction, a Redis-based reliable queue to manage large-scale data ingestion, and a Named Entity Recognition/Relation Extraction Engine to standardize entity-relation triples. The framework addresses key challenges, such as continuous updating and hallucination mitigation, ensuring the reliability of the graph. Our evaluations demonstrate significant improvements in scalability, uniformity of extracted triples, and graph accuracy, making this architecture particularly suitable for real-time geopolitical analysis.},
  month = {dec},
}

@inproceedings{du_sc-telco_2024,
  title = {{SC},
  author = {Du, Junjie and Ding, Jianbing and Li, Jingyi and Wang, Xidong and Ye, Xiaozhou and Ouyang, Ye},
  year = {2024},
  doi = {10.1109/GCWkshp64532.2024.11100862},
  url = {https://doi.org/10.1109/gcwkshp64532.2024.11100862},
  booktitle = {2024 {IEEE},
  pages = {1--6},
  note = {ISSN: 2166-0077},
  keywords = {5G mobile communication, Accuracy, Adaptation models, Context modeling, Data models, Domain-specific question-answering, Fine-tuning, Industries, ITU, Large language models, Large Language Models, Retrievalaugmented generation, Standards, Telecommunications, source: IEEE},
  abstract = {Large Language Models (LLMs) have demonstrated exceptional language understanding and generation capabilities in the field of knowledge question-answering, with applications spanning across various industries. However, when it comes to the telecommunications domain, the complexity of the standards and specifications poses a significant challenge for LLMs. The sheer volume of standards, coupled with the frequent use of specialized terminology and concepts, necessitates a higher level of comprehension and accuracy in responses from the LLMs. This paper introduces the SC-Telco Retrieval-Augmented Generation (RAG) framework, a specialized approach designed to navigate these challenges effectively. By integrating structured contexts into the retrieval process and employing a two-stage fine-tuning technique, this framework significantly enhances LLMs’ question-answering accuracy within the telecommunications domain. The innovative methodology has been validated through notable success in "specializing Large Language Models for Telecom Networks by ITU AI/ML in 5G Challenge", achieving an accuracy of 80.75\% and demonstrating the framework’s practical impact and domain-specific effectiveness.},
  month = {dec},
}

@inproceedings{jabarulla_medblock-bot_2025,
  title = {{MedBlock},
  author = {Jabarulla, Mohamed Yaseen and Oeltze-Jafra, Steffen and Beerbaum, Philipp and Uden, Theodor},
  year = {2025},
  doi = {10.1109/CBMS65348.2025.00172},
  url = {https://doi.org/10.1109/cbms65348.2025.00172},
  booktitle = {2025 {IEEE},
  pages = {845--850},
  note = {ISSN: 2372-9198},
  keywords = {Accuracy, Blockchain, Blockchains, Clinical care, Computational modeling, Guidelines, Large language models, Medical language models, Pediatric cardiology, RAG, Reliability, Retrieval augmented generation, Reviews, Scalability, Smart contracts, source: IEEE},
  abstract = {Accessing reliable clinical knowledge quickly is an everyday challenge for clinicians. Large Language Models (LLMs) can assist healthcare professionals by providing this knowledge, but their responses often deviate from expert consensus or are not up to date necessitating reliable validation and possible correction. To address this, we introduce MedBlock-Bot, an interactive Streamlit-based system integrating a blockchain-enabled Retrieval-Augmented Generation (RAG) framework for expert-driven assessment and immutable feedback storage within a permissioned consortium network. Unlike traditional feedback mechanisms that may be altered or lost, MedBlock-Bot employs smart contracts to securely store and verify any feedback, ensuring transparency and auditability. We evaluated the system using three opensource LLMs-BioMistral, HippoMistral, and LLaMa 3.1-on clinical guideline interpretation for neonates with hypoplastic left heart syndrome. Human experts assessed model responses based on accuracy and relevance, revealing variations in adherence to the guideline knowledge. Additionally, deploying the blockchain component in a local permissioned environment (Ganache) ensured efficient transaction processing and tamperproof feedback retrieval without gas cost concerns. Our results demonstrate the integration of blockchain for LLM feedback review enhancing trust, accountability, and structured knowledge retention. Clinicians can access past expert assessments for validation, while developers can leverage this feedback for potential model refinement. Taking the long-term impact into account this approach targets towards a reliable and dynamic representation of clinical knowledge and consensus. Open-Source Code: https://github.com/yaseen28/MedBlock-Bot},
  month = {jun},
}

@inproceedings{houssel_towards_2024,
  title = {Towards {Explainable},
  author = {Houssel, Paul R. B. and Singh, Priyanka and Layeghy, Siamak and Portmann, Marius},
  year = {2024},
  doi = {10.1109/BDCAT63179.2024.00021},
  url = {https://doi.org/10.1109/bdcat63179.2024.00021},
  booktitle = {2024 {IEEE},
  pages = {67--72},
  keywords = {Computational modeling, Computer architecture, Decision making, Explainability, Large Language Model, Large language models, Natural language processing, Network intrusion detection, Network Intrusion Detection System, Retrieval augmented generation, Threat assessment, Time complexity, Transformers, source: IEEE},
  abstract = {Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents. However, their applicability to threat detection problems remains unclear. This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explain-ability. Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS. Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments. Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge. Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS. Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.},
  month = {dec},
}

@inproceedings{wang_research_2025,
  title = {Research on {Intelligent},
  author = {Wang, Yang},
  year = {2025},
  doi = {10.1109/ICAACE65325.2025.11020300},
  url = {https://doi.org/10.1109/icaace65325.2025.11020300},
  booktitle = {2025 8th {International},
  pages = {2586--2590},
  keywords = {Accuracy, Adaptation models, Collaboration, Data models, Human-machine systems, intelligent auditing, large language models, Large language models, multitask fine-tuning, Recommender systems, Regulation, regulatory recommendations, retrieval-enhanced generation, Terminology, Training, source: IEEE},
  abstract = {The accuracy and efficiency of audit judgment heavily depend on the intelligence level of regulatory recommendation systems. Traditional methods face severe challenges due to issues such as dynamic updates of regulations and complex cross-domain associations. This paper proposes an intelligent regulatory recommendation framework that integrates multitask fine-tuning with retrieval-enhanced generation (RAG). By constructing a three-level task instruction set covering concept understanding, problem classification, and regulation matching (27,600 data points), and combining lightweight fine-tuning techniques (LoRA) to optimize the domain adaptability of the Qwen-7B model, experiments show that the fine-tuned model significantly outperforms baseline models in tasks such as audit issue summarization (ROUGE-L increased by 201\%) and regulation classification (F1 reached 0.9977). After integrating RAG technology, the recommendation accuracy improves to 92.6\%, and hallucination problems decrease by 64\%. This study provides theoretical paradigms and practical tools for designing intelligent audit systems, promoting the transition of audit processes from “human-led” to “human-machine collaboration.”.},
  month = {mar},
}

@inproceedings{zhu_enhancing_2024,
  title = {Enhancing {Supply},
  author = {Zhu, Beilei and Vuppalapati, Chandrasekar},
  year = {2024},
  doi = {10.1109/BigDataService62917.2024.00025},
  url = {https://doi.org/10.1109/bigdataservice62917.2024.00025},
  booktitle = {2024 {IEEE},
  pages = {117--121},
  note = {ISSN: 2690-828X},
  keywords = {source: IEEE},
  abstract = {This paper delves into the fascinating integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) for optimizing supply chain management operations. RAG combines the robust retrieval capabilities of information retrieval systems with the generative prowess of neural language models to create a powerful tool that bolsters data protection while expanding the knowledge base to capture supply chain intricacies. This innovative methodology revolves around a dual-component system that employs a retrieval module to pinpoint relevant information from a knowledge base, while a generation module crafts contextualized responses using large language models. Through iterative retrieval strategies and tailored chunk optimization techniques, RAG enables contextualized analysis, predictive insights, and data-driven decision-making that streamlines processes from demand forecasting to inventory optimization. An experimental setup mimicking enterprise data classification assesses RAG's efficacy, employing recursive retrieval, multi-hop querying, and integration of generative and retrieval processes. Results showcase RAG's potential to revolutionize supply chain logistics, enhancing operational agility, minimizing disruptions, and fortifying data security. The impacts span improved forecasting accuracy, inventory level optimization, supplier risk assessment, and comprehensive supply chain reporting. However, RAG necessitates stringent ethical considerations and robust countermeasures against exploitation. Future work centers on system scalability, advanced evaluation metrics, and interdisciplinary collaboration between machine learning, retrieval systems, and supply chain domains. Overall, this paper presents a groundbreaking approach to optimizing supply chain management operations that could significantly impact the industry's future.},
  month = {jul},
}

@article{setyawan_soekamto_queries_2025,
  title = {From {Queries},
  author = {Setyawan Soekamto, Yosua and Christopher Limanjaya, Leonard and Kaleb Purwanto, Yoshua and Kang, Dae-Ki},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3535618},
  url = {https://doi.org/10.1109/access.2025.3535618},
  journal = {IEEE Access},
  volume = {13},
  pages = {21434--21455},
  note = {Type: Article},
  keywords = {Accuracy, Computational modeling, Context modeling, Data models, educational technology, human-centric design, large language models, Mathematical models, personalized learning path, Retrieval augmented generation, Semantics, Training data, Transformers, source: IEEE, source: Scopus},
  abstract = {Large Language Models (LLMs) hold immense potential for transforming education by automating the generation of personalized learning paths. However, traditional LLMs often suffer from hallucinations and content irrelevance. To address these challenges, we propose SKYRAG, a Separated Keyword Retrieval Augmentation Generation system that enhances the learning path generation process by integrating advanced retrieval mechanisms with LLMs. SKYRAG retrieves relevant course materials from Massive Open Online Course (MOOC) platforms, aligning them with individual learner profiles to provide personalized and coherent learning paths. Compared with Naïve RAG, SKYRAG demonstrates superior performance in terms of accuracy, relevance, and user satisfaction, as confirmed by human evaluations across four domains. By improving retrieval precision and addressing the limitations of traditional methods, SKYRAG represents a significant advancement in educational technology. This study contributes to the growing body of research on AI-driven learning systems and highlights SKYRAG’s potential for widespread adoption in dynamic educational environments.},
  issn = {2169-3536},
  annote = {Cited by: 3; All Open Access; Gold Open Access},
}

@article{toprani_llm_2025,
  title = {{LLM},
  author = {Toprani, Dheer and Madisetti, Vijay K.},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3560911},
  url = {https://doi.org/10.1109/access.2025.3560911},
  journal = {IEEE Access},
  volume = {13},
  pages = {69175--69181},
  keywords = {Best practices, CI/CD, Cognition, Infrastructure-as-code, large language models, Large language models, LLM workflows, Organizations, Retrieval augmented generation, Runtime, Scalability, Security, security automation, Static analysis, Vectors, vulnerability detection, source: IEEE},
  abstract = {This paper presents a multi-agent, AI-driven strategy employing Large Language Models (LLMs), retrieval-augmented generation, and a continuously updated knowledge base for the detection and remediation of security vulnerabilities win cloud frameworks. By examining Infrastructure as Code (IaC) templates alongside pertinent best-practice snippets, the system discerns context-specific misconfigurations commonly overlooked by static tools, achieving a detection rate of 85\% with some occurrences of false positives. Automated remediation guidance, anchored in current security standards, provides actionable solutions that seamlessly integrate into standard continuous integration/continuous development (CI/CD) workflows. Experimental results indicate the solution’s efficacy and scalability, heralding a proactive, context-aware approach to IaC security.},
  issn = {2169-3536},
}

@inproceedings{feng_intelligent_2025,
  title = {Intelligent {Automation},
  author = {Feng, Guocong and Pan, Yuan and Zhang, Chunmei and Huang, Kaitian},
  year = {2025},
  doi = {10.1109/CISAT66811.2025.11181948},
  url = {https://doi.org/10.1109/cisat66811.2025.11181948},
  booktitle = {2025 8th {International},
  pages = {220--224},
  keywords = {Inspection, Intelligent automation, Intention-driven agents, large language models, Large language models, Model-Context Protocol, Natural languages, Network security, Network topology, Protocols, Prototypes, Retrieval augmented generation, retrieval-augmented generation, security-operations automation, Topology, source: IEEE},
  abstract = {As modern networks grow in scale and complexity, traditional manual security operations struggle to maintain efficiency, consistency, and adaptability. This paper proposes a novel framework that leverages large language models (LLMs), intention understanding, and autonomous agents to achieve end-to-end automation of network-security operations. By combining the Model-Context Protocol (MCP) with retrieval-augmented generation (RAG), the system interprets human intents expressed in natural language and dynamically maps them to executable security workflows. Key operational functions including configuration inspection, vulnerability hardening, topology mapping, fault diagnosis, and compliance patrol are abstracted into intention-driven tasks executed by agents. A prototype deployed in an enterprise-scale environment shows mean task-completion-time reductions of 82.37\% and audit-accuracy gains of 6–15\% over expert baselines. These results mark a step toward fully autonomous, intention-aware network-security management.},
  month = {jul},
}

@article{guo_intent-based_2025,
  title = {Intent-{Based},
  author = {Guo, Lingqi and Zhang, Lei and Wang, Jingyu and Wu, Jianyu and Yan, Yuhang and Sun, Haifeng and He, Bo and Qi, Qi and Liao, Jianxin},
  year = {2025},
  doi = {10.1109/TASE.2025.3610906},
  url = {https://doi.org/10.1109/tase.2025.3610906},
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {22},
  pages = {22185--22197},
  keywords = {Analytical models, Autonomous network, Autonomous networks, Computer architecture, Feedback loop, intent translation, large language model, Large language models, Manuals, resource allocation, Resource management, Training, Translation, Videoconferences, source: IEEE},
  abstract = {With the rapid development of next-generation networks, the highly heterogeneous and dynamic nature of networks poses significant challenges for automated network management. Autonomous Network (AN), as a new network paradigm, aims to provide customers with a zero-wait, zero-touch, and zero-fault experience. AN facilitates network management through intent-driven interactions and provides on-demand resource orchestration and service scheduling. However, accurately translating user intents into commands and allocating resources on demand for services remain significant challenges for AN. Therefore, this paper proposes IAN, an intent-based AN framework guided by the Large Language Model (LLM). In the intent translation phase, IAN introduces RAG to enhance command generation quality by retrieving from manuals. In the resource allocation phase, the method utilizes LLM to analyze service characteristics, thereby guiding the training and inference of the resource allocation model to effectively distribute resources uniformly across emerging services. Experimental results demonstrate that IAN improves performance by 52.66\% in intent translation tasks and increases overall gain by 33.57\% in resource allocation tasks compared to other models. Note to Practitioners—In this study, the goal is to design an intent-driven autonomous framework. This framework translates user-input natural language intents into network configurations and continuously updates these configurations based on computations of downstream network states to achieve on-demand resource allocation. To accomplish this, we designed two main modules: the Intent Translator and the Resource Allocator. In the Intent Translator, we propose an LLM-empowered intent translation approach. The introduction of RAG enhances the performance of LLMs in intent translation tasks, effectively mitigating hallucinations and generating higher-quality, deployable commands. In the Resource Allocator, we present an LLM-guided resource allocation method. Leveraging their strong contextual understanding, LLMs can better analyze the attributes of emerging services, thereby guiding the training and inference of resource allocation models. Through this approach, the proposed framework achieves high-quality intent translation and efficient resource allocation, enabling intent-driven network autonomy.},
  issn = {1558-3783},
}

@inproceedings{le_framework_2024,
  title = {A {Framework},
  author = {Le, Thang V. Q. and Vu, Dinh-Hong and Pham, Van-Huy and Le, Anh-Cuong and Nguyen, Nguyen P.},
  year = {2024},
  doi = {10.1109/DSC63484.2024.00108},
  url = {https://doi.org/10.1109/dsc63484.2024.00108},
  booktitle = {2024 {IEEE},
  pages = {726--731},
  keywords = {Cyberspace, Data models, Data science, Focusing, Large language models, Large Language Models, Law, question-answering, RAG, Refining, Reinforcement learning, Retrieval augmented generation, Search engines, source: IEEE},
  abstract = {The popularity of building question-answering systems using Large Language Models (LLMs) has surged. Many projects leverage the Retrieval Augmented Generated (RAG) technique, involving two basic steps: retrieval and reading. In this research, we introduce an enhanced approach, termed CRRR (Classifier - Rewrite - Retriever - Reader), tailored for the Vietnamese legal domain. Our framework begins with a classifier to discern whether a given question pertains to law. Rather than solely focusing on refining LLM or embedding models for better responses, we prioritize enhancing the process of rewriting input questions. These rewritten queries are then utilized by a search engine to gather external information, aiding the reader in generating answers. The rewriter component is trainable using reinforcement learning, incorporating feedback from both human and AI sources.},
  month = {aug},
}

@inproceedings{tamanna_chatgpt_2025,
  title = {Chatgpt {Inaccuracy},
  author = {Tamanna, Salma Begum and Uddin, Gias and Wang, Song and Xia, Lan and Zhang, Longyu},
  year = {2025},
  doi = {10.1109/ICSE55347.2025.00145},
  url = {https://doi.org/10.1109/icse55347.2025.00145},
  booktitle = {2025 {IEEE},
  pages = {2290--2302},
  note = {ISSN: 1558-1225},
  keywords = {Benchmark testing, Chatbots, ChatGPT, Codes, Computer bugs, Hallucination, Prevention and mitigation, Software, Software engineering, Software Issue Reports, Surveys, Technological innovation, Terminology, source: IEEE},
  abstract = {Hallucinations, the tendency to produce irrelevant/incorrect responses, are prevalent concerns in generative AIbased tools like ChatGPT. Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms. We surveyed 47 software engineers and produced a benchmark of 412 Q\&A pairs from the bug reports of two OSS projects. We find that a RAG-based ChatGPT (i.e., ChatGPT tuned with the benchmark issue reports) is 36.4 \% correct when producing answers to the questions, due to two reasons 1) limitations to understand complex technical contents in code snippets like stack traces, and 2) limitations to integrate contexts denoted in the technical terms and texts. We present CHIME (ChatGPT Inaccuracy Mitigation Engine) whose underlying principle is that if we can preprocess the technical reports better and guide the query validation process in ChatGPT, we can address the observed limitations. CHIME uses context-free grammar (CFG) to parse stack traces in technical reports. CHIME then verifies and fixes ChatGPT responses by applying metamorphic testing and query transformation. In our benchmark, CHIME shows 30.3\% more correction over ChatGPT responses. In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME.},
  month = {apr},
}

@article{salek_large_2025,
  title = {A {Large},
  author = {Salek, M Sabbir and Chowdhury, Mashrur and Munir, Muhaimin Bin and Cai, Yuchen and Hasan, Mohammad Imtiaz and Tine, Jean-Michel and Khan, Latifur and Rahman, Mizanur},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3603580},
  url = {https://doi.org/10.1109/access.2025.3603580},
  journal = {IEEE Access},
  volume = {13},
  pages = {163046--163070},
  keywords = {Analytical models, Computer security, Cyberattack, cybersecurity, Data models, intelligent transportation systems, large language model, Large language models, Mathematical models, Prevention and mitigation, Resilience, Threat modeling, Transportation, transportation cyber-physical systems, source: IEEE},
  abstract = {Increased reliance on automation and connectivity exposes transportation cyber-physical systems (CPS) to many cyber vulnerabilities. Existing threat modeling frameworks are often narrow in scope, labor-intensive, and require substantial cybersecurity expertise. To this end, we introduce the Transportation Cybersecurity and Resiliency Threat Modeling Framework (TraCR-TMF), a large language model (LLM)-based threat modeling framework for transportation CPS that requires limited cybersecurity expert intervention. TraCR-TMF identifies threats, potential attack techniques (i.e., methods to exploit vulnerabilities), and relevant countermeasures (e.g., attack detection and mitigation strategies) for transportation CPS. Three LLM-based approaches support these identifications: (i) a retrieval-augmented generation approach requiring no cybersecurity expert intervention, (ii) an in-context learning approach with low intervention from cybersecurity experts, and (iii) a supervised fine-tuning approach with moderate cybersecurity expert intervention. TraCR-TMF offers LLM-based attack path identification for critical assets based on vulnerabilities across different transportation CPS entities. Additionally, it incorporates the Common Vulnerability Scoring System (CVSS) scores of previously exploited vulnerabilities to prioritize threat mitigations. The framework was evaluated through two use cases. First, the framework identified relevant attack techniques for various transportation CPS applications, about 73\% of which were validated by cybersecurity experts as correct. Second, the framework was used to identify attack paths for a target asset in a real-world cyberattack incident. TraCR-TMF successfully predicted exploitations, like lateral movement of adversaries, data exfiltration, and data encryption for ransomware, as reported in the incident. These findings demonstrate TraCR-TMF’s efficacy in transportation CPS threat modeling while reducing the need for extensive involvement of cybersecurity experts. To facilitate real-world adoption, all our codes are shared via an open-source repository.},
  issn = {2169-3536},
}

@inproceedings{li_mccoder_2025,
  title = {{MCCoder},
  author = {Li, Yin and Wang, Liangwei and Piao, Shiyuan and Yang, Boo-Ho and Li, Ziyue and Zeng, Wei and Tsung, Fugee},
  year = {2025},
  doi = {10.1109/CASE58245.2025.11163835},
  url = {https://doi.org/10.1109/case58245.2025.11163835},
  booktitle = {2025 {IEEE},
  pages = {1597--1603},
  note = {ISSN: 2161-8089},
  keywords = {Codes, Complexity theory, Libraries, Motion control, Programming, Prompt engineering, Retrieval augmented generation, Safety, Three-dimensional displays, Trajectory, source: IEEE},
  abstract = {Large Language Models (LLMs) have demonstrated significant potential in code generation. However, in the factory automation sector—particularly motion control—manual programming, alongside inefficient and unsafe debugging practices, remains prevalent. This stems from the complex interplay of mechanical and electrical systems and stringent safety requirements. Moreover, most current AI-assisted motion control programming efforts focus on PLCs, with little attention given to high-level languages and function libraries. To address these challenges, we introduce MCCoder, an LLM-powered system tailored for generating motion control code, integrated with a soft-motion controller. MCCoder improves code generation through a structured workflow that combines multitask decomposition, hybrid retrieval-augmented generation (RAG), and iterative self-correction, utilizing a well-established motion library. Additionally, it integrates a 3D simulator for intuitive motion validation and logs of full motion trajectories for data verification, significantly enhancing accuracy and safety. In the absence of benchmark datasets and metrics tailored for evaluating motion control code generation, we propose MCEVAL, a dataset spanning motion tasks of varying complexity. Experiments show that MCCoder outperforms baseline models using Advanced RAG, achieving an overall performance gain of 33.09\% and a 131.77\% improvement on complex tasks in the MCEVAL dataset. MCCoder is publicly available at https://github.com/MCCodeAI/MCCoder.},
  month = {aug},
}

@inproceedings{sandaruwan_integrating_2025,
  title = {Integrating {Large},
  author = {Sandaruwan, M. Tharuka and Wijayanayake, Janaka and Senanayake, Janaka},
  year = {2025},
  doi = {10.1109/SCSE65633.2025.11031059},
  url = {https://doi.org/10.1109/scse65633.2025.11031059},
  booktitle = {2025 {International},
  pages = {1--7},
  note = {ISSN: 2997-7363},
  keywords = {Accuracy, Automation, Computer architecture, Computer security, cybersecurity, exploiting, Grounding, Large language models, llm, Manuals, Retrieval augmented generation, scanning, Systematic literature review, Systems engineering and theory, vulnerability, source: IEEE},
  abstract = {This research explores integrating Large Language Models (LLMs) like GPT-4 and Claude 3.5 into cybersecurity vulnerability scanning to enhance automation and effectiveness. Current tools’ reliance on manual updates and human expertise is highlighted. A literature review identified effective modular architectures and Retrieval-Augmented Generation (RAG) systems for grounding LLMs with cybersecurity knowledge.A Proof of Concept (PoC) tool, developed in Python and tested on the Metasploitable system, evaluated three LLM implementations: GPT-4 Omni, GPT-4 Omni with RAG, and Claude 3.5 Sonnet. The results showed GPT-4 Omni outperformed Claude 3.5, with RAG significantly improving performance. The tool achieved 80\% accuracy in identifying and resolving vulnerabilities.The study underscores the potential of LLMs to revolutionize vulnerability scanning, making advanced cybersecurity more accessible and effective. Future work should address limitations, enable interactive sessions, create new exploits, and tackle more complex challenges.},
  month = {apr},
}

@inproceedings{hang_trumorgpt_2024,
  title = {{TrumorGPT},
  author = {Hang, Ching Nam and Yu, Pei-Duo and Tan, Chee Wei},
  year = {2024},
  doi = {10.1109/CISS59072.2024.10480162},
  url = {https://doi.org/10.1109/ciss59072.2024.10480162},
  booktitle = {2024 58th {Annual},
  pages = {1--6},
  note = {ISSN: 2837-178X},
  keywords = {Cognition, Fact-checking, knowledge graph, Knowledge graphs, large language models, Query processing, retrieval-augmented generation, semantic reasoning, Semantics, Social networking (online), Training data, Voting, source: IEEE},
  abstract = {In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for automated fact-checking. TrumorGPT aims to distinguish "trumors", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework merges machine learning with natural language processing techniques, leveraging a large language model (LLM) with few-shot learning for knowledge graph construction and semantic reasoning. TrumorGPT addresses the "hallucination" issue common in LLMs and the limitations of static training data by incorporating retrieval-augmented generation. This approach involves accessing and utilizing information from regularly updated knowledge graphs that consist of the latest news and information, ensuring that fact-checking of TrumorGPT is based on the most recent data. Accessing updated knowledge graphs greatly enhances the proficiency of TrumorGPT in delivering accurate and reliable information promptly. Evaluating with extensive datasets, TrumorGPT demonstrates superior performance in automated fact-checking. Its ability to effectively conduct automated fact-checking across various platforms marks a critical step forward in the fight against misinformation, enhancing trust and accuracy in the digital information age.},
  month = {mar},
}

@inproceedings{ginting_saycomply_2025,
  title = {{SayComply},
  author = {Ginting, Muhammad Fadhil and Kim, Dong-Ki and Kim, Sung-Kyun and Krishna, Bandi Jai and Kochenderfer, Mykel J. and Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar},
  year = {2025},
  doi = {10.1109/ICRA55743.2025.11128684},
  url = {https://doi.org/10.1109/icra55743.2025.11128684},
  booktitle = {2025 {IEEE},
  pages = {13730--13736},
  keywords = {Databases, Grounding, Hardware, Manuals, Planning, Protocols, Retrieval augmented generation, Robot sensing systems, Robots, Service robots, source: IEEE},
  abstract = {This paper addresses the problem of task planning for robots that must comply with operational manuals in real-world settings. Task planning under these constraints is essential for enabling autonomous robot operation in domains that require adherence to domain-specific knowledge. Current methods for generating robot goals and plans rely on common sense knowledge encoded in large language models. However, these models lack grounding of robot plans to domain-specific knowledge and are not easily transferable between multiple sites or customers with different compliance needs. In this work, we present SayComply, which enables grounding robotic task planning with operational compliance using retrievalbased language models. We design a hierarchical database of operational, environment, and robot embodiment manuals and procedures to enable efficient retrieval of the relevant context under the limited context length of the LLMs. We then design a task planner using a tree-based retrieval augmented generation (RAG) technique to generate robot tasks that follow user instructions while simultaneously complying with the domain knowledge in the database. We demonstrate the benefits of our approach through simulations and hardware experiments in real-world scenarios that require precise context retrieval across various types of context, outperforming the standard RAG method. Our approach bridges the gap in deploying robots that consistently adhere to operational protocols, offering a scalable and edge-deployable solution for ensuring compliance across varied and complex real-world environments. Project website: saycomply.github.io.},
  month = {may},
}

@article{mozo_adapting_2025,
  title = {Adapting {LLMs},
  author = {Mozo, Alejandro and Gálvez, Sergio and Christou, Ioannis T. and Vogiatzis, Dimitrios and Navarro, Tomás and Valverde, Francisco L.},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3605022},
  url = {https://doi.org/10.1109/access.2025.3605022},
  journal = {IEEE Access},
  volume = {13},
  pages = {155675--155696},
  keywords = {Accuracy, Adaptation models, Artificial intelligence, Biological system modeling, Computational modeling, Data models, European Space Agency, evaluation models, fine tuning LLMs, large language models, Portable document format, preprocessing for LLMs, satellite communications, Satellite communications, Satellites, Training, source: IEEE},
  abstract = {The application of large language models (LLMs) to specialized fields, such as Satellite Communications (SatCom), presents unique challenges due to the extensive and cutting-edge knowledge required. SatCom encompasses a wide range of technical details, protocols, and operational guidelines that must be addressed to produce effective and accurate models for practical use. This paper presents a fine-tuning approach for adapting 7-billion-parameter instructed LLMs (Llama-3v and Mistral) to SatCom, using a proprietary corpus sourced from the European Space Agency (ESA) consisting of domain-specific PDF documents. The confidential nature of this corpus imposes constraints on both model training and evaluation, demanding a sensible text extraction pipeline capable of handling complex structures, such as tables, to preserve critical information. Our fine-tuning methodology employs a carefully configured process, followed by an automatic evaluation framework using a curated Q\&A set tailored to SatCom. Models were created in both non-quantified and 8-bit quantized formats, ensuring feasibility for desktop-level inference. The fine-tuned models demonstrated a 6,6\% improvement over the baseline LLM, as well as significant gains when compared to retrieval-augmented generation (RAG) methods. These results indicate a promising advancement in the development of LLMs for domain-specific applications within the SatCom field.},
  issn = {2169-3536},
}

@inproceedings{boppana_open-source_2024,
  title = {An {Open},
  author = {Boppana, Lakshmi and Bhadoria, Manav and Kodali, Ravi Kishore},
  year = {2024},
  doi = {10.1109/TENCON61640.2024.10903064},
  url = {https://doi.org/10.1109/tencon61640.2024.10903064},
  booktitle = {{TENCON},
  pages = {43--46},
  note = {ISSN: 2159-3450},
  keywords = {Accuracy, Data models, IEEE Regions, Knowledge based systems, Large language models, machine learning, natural language processing, Natural language processing, recovered generation, Semantics, Supply chain management, Vectors, Web services, source: IEEE},
  abstract = {Accurate product classification in e-Commerce and supply chain management is essential to smooth operations and enhance the customer experience. While Large Language Models (LLMs) perform exceptionally in natural language processing, they encounter issues like model hallucination and dependence on outdated information. Furthermore, LLMs often rely on outdated data. This paper introduces an open source cloud-based RAG model, using Amazon Web Services (AWS) and vector databases to address these issues. The RAG architecture combines retrieval-based and generation-based methods, allowing them to supplement responses with up-to-date information from external sources, thus reducing the risk of model hallucination. The project employs a Vector DB deployed in EC2 to improve contextual understanding and retrieval capabilities of these large language models. Through comprehensive experimentation and AWS deployment, the RAG system improved contextual comprehension and increased the accuracy of the generated output. Semantic similarity search results significantly improve retrieval performance.},
  month = {dec},
}

@inproceedings{painter_enhancing_2023,
  title = {Enhancing {Drug},
  author = {Painter, Jeffery L. and Mahaux, Olivia and Vanini, Marco and Kara, Vijay and Roshan, Christie and Karwowski, Marcin and Chalamalasetti, Venkateswara Rao and Bate, Andrew},
  year = {2023},
  doi = {10.1109/CSCI62032.2023.00015},
  url = {https://doi.org/10.1109/csci62032.2023.00015},
  booktitle = {2023 {International},
  pages = {49--56},
  note = {ISSN: 2769-5654},
  keywords = {Data privacy, Documentation, drug safety, Drugs, Information retrieval, large language models, Large language models, LLM, pharmacovigilance, retrieval-augmented generation, Uncertainty, User experience, source: IEEE},
  abstract = {Integrating Large Language Models (LLMs) to enhance complex business document retrieval represents an emerging field known as retrieval-augmented generation (RAG). In highly regulated domains like drug safety (pharmacovigilance), its application has remained largely unexplored. This technology brings numerous advantages, including expedited staff on-boarding, enhanced comprehension of contextual queries, and swift information retrieval through natural language inquiries, surpassing conventional keyword searches. This study delves into various operational tasks, such as locating regulatory process guidance, navigating intricate scenarios for advice, and ensuring the LLM's competence in recognizing uncertainties to prevent misinformation. LLMs empower users to engage with documentation using natural language, markedly improving search efficiency. The case study underscores LLM's effectiveness in delivering prompt guidance within pharmacovigilance and adverse event processing and reporting, offering a user-centric solution that streamlines the search for intricate business documentation.},
  month = {dec},
}

@inproceedings{moutaoukkil_queries_2025,
  title = {From {Queries},
  author = {Moutaoukkil, Assmaa and Mezouary, Ali E L},
  year = {2025},
  doi = {10.1109/IRASET64571.2025.11008326},
  url = {https://doi.org/10.1109/iraset64571.2025.11008326},
  booktitle = {2025 5th {International},
  pages = {1--7},
  keywords = {Accuracy, Buildings, ethical considerations, Ethics, information retrieval, large language models, Large language models, Next generation networking, Retrieval augmented generation, Retrieval-Augmented Generation, search engine, Search engines, Shape, Technological innovation, User experience, source: IEEE},
  abstract = {Imagine typing a question into a search engine and getting not just a list of blue links, but a thoughtful, accurate answer that understands exactly what you need. From queries to understanding, the way we access and interpret information has been transformed. This paper explores the evolution of intelligent information retrieval systems (IRS), particularly in the context of large language models (LLMs) and their implications for future IR research and applications. We aim to address critical questions regarding the strengths and weaknesses of LLMs in enhancing IR systems. We discuss traditional IR methods, their limitations, and the rise of LLMs, emphasizing the need for a balanced approach that combines retrieval and generation techniques to improve user experience and satisfaction. The paper proposes a framework for future research that leverages LLMs to create intelligent, transparent, and responsible information retrieval system (IRS).},
  month = {may},
}

@inproceedings{sahin_large_2024,
  title = {Large {Language},
  author = {Şahin, Gürkan and Varol, Karya and Pak, Burcu Kuleli},
  year = {2024},
  doi = {10.1109/ASYU62119.2024.10757102},
  url = {https://doi.org/10.1109/asyu62119.2024.10757102},
  booktitle = {2024 {Innovations},
  pages = {1--6},
  note = {ISSN: 2770-7946},
  keywords = {Accuracy, Companies, generative artificial intelligence, gpt, Information security, Intelligent systems, large language models, Large language models, Libraries, natural language processing, question answering, Question answering (information retrieval), rag, Technological innovation, User interfaces, Vectors, source: IEEE},
  abstract = {Large language models are widely used in many natural language processing applications today. Automatic question answering is one of the areas where language models are frequently used. In this study, a question answering system was developed that allows Adesso Türkiye employees to access internal company information quickly and accurately. A Retrieval Augmented Generation (RAG)-based question answering structure was created by giving content prepared by experts in the field of human resources and information security as input to the large language model. As a result of the experiments, high accuracy results were obtained on datasets. In future studies, it is aimed to increase performance by using different language models and to make the developed system available to employees by integrating it into MS Teams.},
  month = {oct},
}

@inproceedings{samarajeewa_causal_2024,
  title = {Causal {Reasoning},
  author = {Samarajeewa, Chamod and De Silva, Daswin and Osipov, Evgeny and Alahakoon, Damminda and Manic, Milos},
  year = {2024},
  doi = {10.1109/HSI61632.2024.10613566},
  url = {https://doi.org/10.1109/hsi61632.2024.10613566},
  booktitle = {2024 16th {International},
  pages = {1--6},
  note = {ISSN: 2158-2254},
  keywords = {Benchmark testing, Cognition, Large language models, Limiting, Measurement, Semantics, Vectors, source: IEEE},
  abstract = {Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.},
  month = {jul},
}

@inproceedings{taylor_self-directed_2024,
  title = {Self-{Directed},
  author = {Taylor, Amelia and Magwira, Macphail and Chamangwana, Chimwemwe and Chapuma, Evelyn and Liwewe, Thokozani and Kankhwali, Chisomo},
  year = {2024},
  doi = {10.1109/ICHI61247.2024.00092},
  url = {https://doi.org/10.1109/ichi61247.2024.00092},
  booktitle = {2024 {IEEE},
  pages = {574--579},
  note = {ISSN: 2575-2634},
  keywords = {answers dataset, community health workers, feedback loop, Generative AI, GPT-4, LLMs, Medical services, prompting, Public healthcare, questions, RAG, self-directed learning, Surveillance, Time factors, Training, Urban areas, source: IEEE},
  abstract = {In many lower and middle-income countries, a lack of resources affects the availability and quality of education and training. In the healthcare domain, access to knowledge can make the difference between life and death. Timely access to technical and clinical guidelines to support decisions is crucial. Healthcare workers need access to up-to-date guidelines on case definitions for surveillance, treatment protocols, and relevant clinical and medical knowledge. However, guidelines documents tend to be bulky and complex and may change over time in response to health priorities, research, or public health emergencies. Generative AI has proven to be a disruptive technology in health care, but its limitations and applicability are subject to experimentation. We present evidence that Large Language Models (LLMs) can be leveraged to facilitate needs-driven and self-directed learning regarding guidelines for healthcare professionals in Malawi. We developed an application called IntelSurv that uses GPT -4 to achieve a ‘chat’ -like functionality where users ask questions about priority diseases, seek clarification on the use of case identification forms, and have access to technical guidelines published by the Ministry of Health. IntelSurv is both a web app and a mobile app and can run either online or offline. Healthcare professionals engaged in disease surveillance and community health in two major cities in Malawi tested the tool and gave positive feedback on its impact. We report on the development of the tool, and its use of GPT -4. We discuss choices of features and functionalities in response to testing and feedback from users.},
  month = {jun},
}

@inproceedings{ji_rela_2024,
  title = {{RELA},
  author = {Ji, Xin and Chen, Ruibo and Chen, Yikai and Xiang, Nan and Zhang, Kui},
  year = {2024},
  doi = {10.1109/ICARCE63054.2024.00087},
  url = {https://doi.org/10.1109/icarce63054.2024.00087},
  booktitle = {2024 3rd {International},
  pages = {430--434},
  keywords = {Accuracy, AIOps, Anomaly detection, Automation, Focusing, Large language models, LLMs, Log analysis, Online-Offline, Retrieval augmented generation, Robots, Solids, source: IEEE},
  abstract = {Recently, Large Language Models (LLMs) have played a pivotal role in the field of Artificial Intelligence Operations (AIOps), particularly in the analysis of log data. However, the use of open-source LLMs for automated log analysis often falls short of expectations in offline environments in practice. To address these challenges, this paper introduces an innovative Online-Offline framework named RELA to enhance the performance of LLMs in log analysis. In the online phase of our framework, GPT-4 meticulously analyzes logs to determine the presence of anomalies and deduces rules based on these determinations, which are then compiled into a comprehensive rules base. In the offline phase, open-source LLMs leverage this rules base to significantly enhance the effectiveness of log analysis by retrieving the most applicable rules. To validate the effectiveness of our proposed framework, we meticulously annotated a dataset specifically designed for a distinct offline scenario, focusing on two primary tasks: the detection of log anomalies and the explanation of these anomalies. Our experimental results demonstrate that the implementation of the rules base leads to a 3\% to 25.3\% increase in the accuracy of anomaly detection. Additionally, assessments conducted by domain experts validate the improvements, highlighting a substantial enhancement in the credibility of the explanations provided by LLMs within this framework. These findings not only underscore the practicality but also the efficacy of our approach, establishing a solid foundation for further research and application in AIOps.},
  month = {dec},
}

@inproceedings{manias_semantic_2024,
  title = {Semantic {Routing},
  author = {Manias, Dimitrios Michael and Chouman, Ali and Shami, Abdallah},
  year = {2024},
  doi = {10.1109/GLOBECOM52923.2024.10901065},
  url = {https://doi.org/10.1109/globecom52923.2024.10901065},
  booktitle = {{GLOBECOM},
  pages = {2924--2929},
  note = {ISSN: 2576-6813},
  keywords = {5G Core Networks, 5G mobile communication, End-to-End Network Management, Intent-Based Networking, Large language models, Large Language Models, Linguistics, Next-Generation Networks, Quantization (signal), Reliability, Retrieval augmented generation, Routing, Semantic Routing, Semantics, System performance, Translation, source: IEEE},
  abstract = {Large language models (LLMs) are rapidly emerging in Artificial Intelligence (AI) applications, especially in the fields of natural language processing and generative AI. Not limited to text generation applications, these models inherently possess the opportunity to leverage prompt engineering, where the inputs of such models can be appropriately structured to articulate a model’s purpose explicitly. A prominent example of this is intent-based networking, an emerging approach for automating and maintaining network operations and management. This paper presents semantic routing to achieve enhanced performance in LLM-assisted intent-based management and orchestration of 5G core networks. This work establishes an end-to-end intent extraction framework and presents a diverse dataset of sample user intents accompanied by a thorough analysis of the effects of encoders and quantization on overall system performance. The results show that using a semantic router improves the accuracy and efficiency of the LLM deployment compared to stand-alone LLMs with prompting architectures.},
  month = {dec},
}

@inproceedings{xu_grasp_2024,
  title = {{GRASP},
  author = {Xu, Jerry and Wang, Justin and Leung, Joley and Gu, Jasmine},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825975},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825975},
  booktitle = {2024 {IEEE},
  pages = {7438--7442},
  note = {ISSN: 2573-2978},
  keywords = {Accuracy, Chatbots, Knowledge engineering, LLMs, Local government, Municipal Documents, Prompt engineering, Prompt Engineering, RAG, ReAct Agent, Reliability, Solids, Testing, Urban areas, Web search, source: IEEE},
  abstract = {There are a growing number of AI applications, but none tailored specifically to help residents answer their questions about municipal budget, a topic most are interested in but few have a solid comprehension of. In this research paper, we propose GRASP, a custom AI chatbot framework which stands for Generation with Retrieval and Action System for Prompts. GRASP provides more truthful and grounded responses to user budget queries than traditional information retrieval systems like general Large Language Models (LLMs) or web searches. These improvements come from the novel combination of a Retrieval-Augmented Generation (RAG) framework ("Generation with Retrieval") and an agentic workflow ("Action System"), as well as prompt engineering techniques, the incorporation of municipal budget domain knowledge, and collaboration with local town officials to ensure response truthfulness. During testing, we found that our GRASP chatbot provided precise and accurate responses for local municipal budget queries 78\% percent of the time, while GPT-4o and Gemini were only accurate 60\% and 35\% of the time, respectively. GRASP chatbots greatly reduce the time and effort needed for the general public to get an intuitive and correct understanding of their town’s budget, thus fostering greater communal discourse, improving government transparency, and allowing citizens to make more informed decisions.},
  month = {dec},
}

@inproceedings{rangana_educational_2025,
  title = {Educational {Material},
  author = {Rangana, K Sachith and Dias, Gihan},
  year = {2025},
  doi = {10.1109/IALP68296.2024.11156564},
  url = {https://doi.org/10.1109/ialp68296.2024.11156564},
  booktitle = {2025 {International},
  pages = {54--59},
  note = {ISSN: 2159-1970},
  keywords = {Active learning, Active Learning in Education, Digital transformation, Educational technology, Generative AI, GraphRAG, Grounding, Knowledge graphs, Knowledge Graphs, Low-Resource Languages, Multilingual, Question generation, Retrieval augmented generation, Systems architecture, source: IEEE},
  abstract = {This work addresses the critical challenge of transforming static educational content into interactive learning experiences in secondary education, with a specific focus on multilingual environments in Sri Lanka. We propose LK-Active- Learner - an AI-driven platform that leverages knowledge graphs and Retrieval-Augmented Generation (GraphRAG) to create contextually relevant, personalized assessment materials in both Sinhala and English. By combining document AI for text extraction, knowledge graph construction, and question generation techniques, our system aims to enhance student engagement and comprehension while supporting educators through automated examination material development. This paper presents the theoretical framework, system architecture, and evaluation methodology for the proposed platform. This research contributes to educaional technology advancement in low-resource languages and offers a scalable framework for supporting active learning strategies in diverse linguistic contexts.},
  month = {aug},
}

@article{neumann_llm-driven_2025,
  title = {An {LLM},
  author = {Neumann, Alexander Tobias and Yin, Yue and Sowe, Sulayman and Decker, Stefan and Jarke, Matthias},
  year = {2025},
  doi = {10.1109/TE.2024.3467912},
  url = {https://doi.org/10.1109/te.2024.3467912},
  journal = {IEEE Transactions on Education},
  volume = {68},
  number = {1},
  pages = {103--116},
  keywords = {Accuracy, Adaptation models, Chatbots, Computer science, Databases, Education, higher education, Information systems, Information technology, large language model (LLM), Mentoring, moodle, moodlebot, Vectors, source: IEEE},
  abstract = {Contribution: This research explores the benefits and challenges of developing, deploying, and evaluating a large language model (LLM) chatbot, MoodleBot, in computer science classroom settings. It highlights the potential of integrating LLMs into LMSs like Moodle to support self-regulated learning (SRL) and help-seeking behavior. Background: Computer science educators face immense challenges incorporating novel tools into LMSs to create a supportive and engaging learning environment. MoodleBot addresses this challenge by offering an interactive platform for both students and teachers. Research Questions: Despite issues like bias, hallucinations, and teachers’ and educators’ resistance to embracing new (AI) technologies, this research investigates two questions: (RQ1) To what extent do students accept MoodleBot as a valuable tool for learning support? (RQ2) How accurately does MoodleBot churn out responses, and how congruent are these with the established course content? Methodology: This study reviews pedagogical literature on AI-driven chatbots and adopts the retrieval-augmented generation (RAG) approach for MoodleBot’s design and data processing. The technology acceptance model (TAM) evaluates user acceptance through constructs like perceived usefulness (PU) and Ease of Use. Forty-six students participated, with 30 completing the TAM questionnaire. Findings: LLM-based chatbots like MoodleBot can significantly improve the teaching and learning process. This study revealed a high accuracy rate (88\%) in providing course-related assistance. Positive responses from students attest to the efficacy and applicability of AI-driven educational tools. These findings indicate that educational chatbots are suitable for integration into courses to improve personalized learning and reduce teacher administrative burden, although improvements in automated fact-checking are needed.},
  issn = {1557-9638},
  month = {feb},
}

@inproceedings{yilmaz_grounded_2025,
  title = {Grounded {Answer},
  author = {Yılmaz, Rabia Eda and Taysi, Mehmet Anıl and Özmen, Ayşe İrem and İnce, Gökhan},
  year = {2025},
  doi = {10.1109/UBMK67458.2025.11206956},
  url = {https://doi.org/10.1109/ubmk67458.2025.11206956},
  booktitle = {2025 10th {International},
  pages = {160--165},
  note = {ISSN: 2521-1641},
  keywords = {AI-Assisted Document Management, Benchmark testing, Computer architecture, Embedding, Large Language Models, Multilingual, Multimodal Retrieval, Optical character recognition, Pipelines, Question answering (information retrieval), Reliability engineering, Retrieval augmented generation, Semantic search, Semantic Search, Visualization, source: IEEE},
  abstract = {The automated interpretation of unstructured financial records, including receipts and invoices, has become increasingly critical for intelligent document understanding. A Retrieval-Augmented Generation (RAG) framework is presented to address question answering over multilingual financial documents characterized by noisy OCR output, variable layouts, and visually embedded information. In the proposed approach, textual content is encoded using multilingual sentence encoders, while visual information is processed through multimodal language models; both representations are stored in a unified semantic index. At inference time, retrieval scores from each modality are fused to guide evidence selection, which is then provided to an instruction-tuned generator. The factuality of generated answers is subsequently validated using a lightweight verifier Large Language Model(LLM) Judge that classifies answers as grounded, partially grounded, or hallucinated. The system is trained and evaluated on a real-world dataset of 2,536 financial documents in Turkish and English, achieving 86.7\% grounded answer accuracy and reducing hallucination rates by more than 50\% compared to text-only retrieval. The contributions are: (i) an end-to-end multimodal RAG architecture for financial question answering,(ii)a curated multilingual benchmark dataset of real financial documents, and (iii) an efficient groundedness verification method based on LLM judgment, establishing a reproducible baseline for multimodal document understanding.},
  month = {sep},
}

@inproceedings{sun_trustnavgpt_2024,
  title = {{TrustNavGPT},
  author = {Sun, Xingpeng and Zhang, Yiran and Tang, Xindi and Bedi, Amrit Singh and Bera, Aniket},
  year = {2024},
  doi = {10.1109/IROS58592.2024.10801932},
  url = {https://doi.org/10.1109/iros58592.2024.10801932},
  booktitle = {2024 {IEEE},
  pages = {8794--8801},
  note = {ISSN: 2153-0866},
  keywords = {Computational efficiency, Large language models, Navigation, Planning, Resilience, Retrieval augmented generation, Robots, Semantics, Social robots, Uncertainty, source: IEEE},
  abstract = {Large language models (LLMs) exhibit a wide range of promising capabilities – from step-by-step planning to commonsense reasoning –that provide utility for robot navigation. However, as humans communicate with robots in the real world, ambiguity and uncertainty may be embedded inside spoken instructions. While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present TrustNavGPT, an LLM-based audio-guided navigation agent that uses affective cues in spoken communication—elements such as tone and inflection that convey meaning beyond words—allowing it to assess the trustworthiness of human commands and make effective, safe decisions. Experiments across a variety of simulation and real-world setups show a 70.46\% success rate in catching command uncertainty and an 80\% success rate in finding the target, 48.30\%, and 55\% outperform existing LLM-based navigation methods, respectively. Additionally, TrustNavGPT shows remarkable resilience against adversarial attacks, highlighted by a 22\%+ less decrease ratio than the existing LLM navigation method in success rate. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation. For more information, visit the TrustNav project page.},
  month = {oct},
}

@inproceedings{chaudhary_developing_2024,
  title = {Developing a {Llama},
  author = {Chaudhary, Daksh and Vadlamani, Sri Lakshmi and Thomas, Dimple and Nejati, Shiva and Sabetzadeh, Mehrdad},
  year = {2024},
  doi = {10.1109/ICSME58944.2024.00075},
  url = {https://doi.org/10.1109/icsme58944.2024.00075},
  booktitle = {2024 {IEEE},
  pages = {707--718},
  note = {ISSN: 2576-3148},
  keywords = {source: IEEE},
  abstract = {This paper presents our experience developing a Llama-based chatbot for question answering about continuous integration and continuous delivery (CI/CD) at Ericsson, a multinational telecommunications company. Our chatbot is designed to handle the specificities of CI/CD documents at Ericsson, employing a retrieval-augmented generation (RAG) model to enhance accuracy and relevance. Our empirical evaluation of the chatbot on industrial CI/CD-related questions indicates that an ensemble retriever, combining BM25 and embedding retrievers, yields the best performance. When evaluated against a ground truth of 72 CI/CD questions and answers at Ericsson, our most accurate chatbot configuration provides fully correct answers for 61.11\% of the questions, partially correct answers for 26.39\%, and incorrect answers for 12.50\%. Through an error analysis of the partially correct and incorrect answers, we discuss the underlying causes of inaccuracies and provide insights for further refinement. We also reflect on lessons learned and suggest future directions for further improving our chatbot's accuracy.},
  month = {oct},
}

@inproceedings{saad_closed_2024,
  title = {Closed {Domain},
  author = {Saad, Matthew and Qawaqneh, Zakariya},
  year = {2024},
  doi = {10.1109/ICECCME62383.2024.10796881},
  url = {https://doi.org/10.1109/iceccme62383.2024.10796881},
  booktitle = {2024 4th {International},
  pages = {1--8},
  keywords = {Accuracy, Chatbots, Closed-domain question answering, Computational modeling, Context modeling, Data models, institutional chatbot, Large Language Models (LLMs), Question answering (information retrieval), Retrieval augmented generation, Solid modeling, Training, Tuning, source: IEEE},
  abstract = {This paper introduces BrockportGPT, a specialized chatbot designed for SUNY Brockport that addresses the unique challenges of institutional question answering that general purpose Large Language Models (LLMs) face. Our approach leverages a combination of vanilla sequence-to-sequence modeling, fine tuning LLaMA-2, and Retrieval Augmented Generation (RAG). The methodology involves extensive data collection through web scraping, synthetic question generation, and a comprehensive look at information retrieval strategies, including the implementation of a question-topic classification system. Comparative analysis of the three approaches shows that finetuning and RAG have competitive performance, with RAG providing the most accurate and contextually relevant results, and finetuning having the superior dialog coherence. Through these results, BrockportGPT offers a model for developing an institutional chatbot, highlighting the potential for AI-driven education tools to improve information accessibility and dissemination processes.},
  month = {nov},
}

@inproceedings{s_rag-based_2024,
  title = {A {RAG},
  author = {S, Stewart Kirubakaran and G, Jasper Wilsie Kathrine and E, Grace Mary Kanaga and J, Mahimai Raja and Singh A, Ruban Gino and E, Yuvaraajan},
  year = {2024},
  doi = {10.1109/ICICT60155.2024.10544639},
  url = {https://doi.org/10.1109/icict60155.2024.10544639},
  booktitle = {2024 {International},
  pages = {1128--1133},
  note = {ISSN: 2767-7788},
  keywords = {Artificial intelligence, chatbot, Chatbots, COVID-19, Databases, Infectious diseases, knowledge graph, large language model, natural language processing, Recording, Reliability, retrieval augmented generation, source: IEEE},
  abstract = {Infectious diseases like COVID-19 have gained international attention recently. Furthermore, there are significantly fewer doctors per capita in densely populated nations like India, which hurts those in need. Under such circumstances, natural language processing techniques might make it feasible to create an intelligent and engaging chatbot system. The primary objective of the effort is to develop an interactive solution that is entirely open source and can be easily installed on a local computer using the most recent data. Even though there are numerous chatbots on the market, proposed solutions highlight the need to provide individualized and sympathetic responses. Getting Back While the data is stored in the graph database as nodes and relationships, and the knowledge graph is constructed on top of it, augmented generation is utilized to extract the pertinent content from the data. To improve the generator’s context, pertinent sections are collected during the question-answering process. This reduces hallucinations and increases the correctness of abstractions by providing external knowledge streams. Furthermore, the research study employs a text-to-speech model that was replicated from a physician’s voice recording to narrate the produced responses, thereby augmenting user confidence and interaction. Academic institutions and healthcare organizations can benefit from this work by better understanding the value and effectiveness of applying NLP techniques to infectious disease research.},
  month = {apr},
}

@article{abdulnazar_large_2024,
  title = {Large {Language},
  author = {Abdulnazar, Akhila and Roller, Roland and Schulz, Stefan and Kreuzthaler, Markus},
  year = {2024},
  doi = {10.1109/ACCESS.2024.3472500},
  url = {https://doi.org/10.1109/access.2024.3472500},
  journal = {IEEE Access},
  volume = {12},
  pages = {147981--147990},
  keywords = {Accuracy, Biological system modeling, Chatbots, ChatGPT, Clinical diagnosis, Codes, Data integrity, Large language models, medical concept normalization, Medical services, Natural language processing, retrieval augmented generation, Text analysis, text cleansing, Training, Unified modeling language, source: IEEE},
  abstract = {Most clinical information is only available as free text. Large language models (LLMs) are increasingly applied to clinical data to streamline communication, enhance the accuracy of clinical documentation, and ultimately improve healthcare delivery. This study focuses on a corpus of anonymized clinical narratives in German. On the one hand it evaluates the use of ChatGPT for text cleansing, i.e., the automatic rephrasing of raw text into a more readable and standardized form, and on the other hand for retrieval-augmented generation (RAG). In both tasks, the final goal was medical concept normalization (MCN), i.e., the annotation of text segments with codes from a controlled vocabulary using natural language processing. We found that ChatGPT (GPT-4) significantly improves precision and recall compared to simple dictionary matching. For all scenarios, the importance of the underlying terminological basis was also demonstrated. Maximum F1 scores of 0.607, 0.735 and 0.754 (i.e, for top 1, 5 and 10 matches) were achieved through a pipeline including document cleansing, bi-encoder-based term matching based on a large domain dictionary linked to SNOMED CT, and finally re-ranking using RAG.},
  issn = {2169-3536},
}

@inproceedings{ou_cygpt_2024,
  title = {{CyGPT},
  author = {Ou, Lu and Ni, Xiaoya and Wu, Wei and Tian, Zhihong},
  year = {2024},
  doi = {10.1109/DSC63484.2024.00036},
  url = {https://doi.org/10.1109/dsc63484.2024.00036},
  booktitle = {2024 {IEEE},
  pages = {216--223},
  keywords = {Cognition, Computer security, Cybersecurity, Data models, Knowledge based systems, Knowledge engineering, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Natural language processing, Network security, Retrieval-Augmented Generation, Systematics, source: IEEE},
  abstract = {Large Language Models (LLMs) excel in numerous Natural Language Processing (NLP) tasks but encounter significant challenges in practical applications, including hallucinations, outdated information, and a lack of domain-specific external knowledge. This study proposes a collaborative, training-free reasoning approach, leveraging close cooperation between Knowledge Graphs (KG) and LLMs for cybersecurity applications. Our approach employs the ‘Joint Reasoning Chain,’ which dynamically integrates information from network security-specific knowledge graphs, serving as an external knowledge base to enhance the domain-specific external knowledge of LLMs. This cooperative method not only improves reliable knowledge-based reasoning but also enhances the traceability of decision-making processes. Furthermore, we introduce a novel GPT-based technique to evaluate answer quality and have performed systematic experiments on a purpose-built test set. The results confirm that our method significantly boosts GPT’s performance in network security knowledge, demonstrating the potential of knowledge graphs to augment LLMs’ reasoning abilities and their applicability in specialized fields.},
  month = {aug},
}

@inproceedings{tundik_building_2024,
  title = {Building {Domain},
  author = {Tündik, Máté Ákos and Kovács, Ferenc and Blaskó, Márk},
  year = {2024},
  doi = {10.1109/CINTI63048.2024.10830834},
  url = {https://doi.org/10.1109/cinti63048.2024.10830834},
  booktitle = {2024 {IEEE},
  pages = {135--140},
  note = {ISSN: 2471-9269},
  keywords = {6G, Buildings, Chatbots, Computational intelligence, Computational modeling, domain specific chatbot, generative AI, Generative AI, Informatics, large language models, multimodal chatbot, retrieval augmented generation, telecommunication, source: IEEE},
  abstract = {The generative AI and LLMs are getting commonly used solution to ease the daily work. There are special domains where the out of box solution cannot provide sufficient outcomes as the domains have their own specialties, jargons or rules. In this paper we investigate telco related specialties and how they can be handled to tailor a general purpose chatbot.},
  month = {nov},
}

@inproceedings{mahendru_venn_2024,
  title = {Venn {Diagram},
  author = {Mahendru, Sakshi and Pandit, Tejul},
  year = {2024},
  doi = {10.1109/WSAI62426.2024.10828919},
  url = {https://doi.org/10.1109/wsai62426.2024.10828919},
  booktitle = {2024 6th {World},
  pages = {39--48},
  keywords = {source: IEEE},
  abstract = {We introduce Venn Diagram (VD) Prompting, an innovative prompting technique which allows Large Language Models (LLMs) to combine and synthesize information across complex, diverse and long-context documents in knowledge-intensive question-answering tasks. Generating answers from multiple documents involves numerous steps to extract relevant and unique information and amalgamate it into a cohesive response. To improve the quality of the final answer, multiple LLM calls or pretrained models are used to perform different tasks such as summarization, reorganization and customization. The approach covered in the paper focuses on replacing the multi-step strategy via a single LLM call using VD prompting. Our proposed technique also aims to eliminate the inherent position bias in the LLMs, enhancing consistency in answers by removing sensitivity to the sequence of input information. It overcomes the challenge of inconsistency traditionally associated with varying input sequences. We also explore the practical applications of the VD prompt based on our examination of the prompt's outcomes. In the experiments performed on four public benchmark question-answering datasets, VD prompting continually matches or surpasses the performance of a meticulously crafted instruction prompt which adheres to optimal guidelines and practices.},
  month = {jun},
}

@inproceedings{lan_lamlad_2025,
  title = {{LAMLAD},
  author = {Lan, Tianwei and Nait-Abdesselam, Farid},
  year = {2025},
  doi = {10.1109/CNS66487.2025.11195008},
  url = {https://doi.org/10.1109/cns66487.2025.11195008},
  booktitle = {2025 {IEEE},
  pages = {1--9},
  note = {ISSN: 2994-5895},
  keywords = {Cognition, Context awareness, Detectors, Machine learning, Malware, Manipulators, Perturbation methods, Retrieval augmented generation, Solid modeling, Threat assessment, source: IEEE},
  abstract = {The increasing sophistication and volume of Android malware have driven the adoption of Machine Learning (ML) models for scalable and accurate threat detection. However, these models remain vulnerable to adversarial attacks that subtly manipulate input features to evade classification. In this paper, we introduce LAMLAD, a novel adversarial attack framework that exploits the generative and reasoning capabilities of Large Language Models (LLMs) to bypass ML-based malware detectors. LAMLAD employs a two-agent architecture composed of an LLM manipulator, which crafts realistic feature-level perturbations without altering core malicious behaviors, and an LLM analyzer, which guides the modification process to ensure successful evasion. To improve efficiency and context awareness, LAMLAD integrates Retrieval-Augmented Generation (RAG) into the LLM workflow. By targeting Drebin features, LAMLAD enables stealthy and high-confidence attacks against widely used Android malware classifiers. We evaluate LAMLAD against three representative ML-based Android malware detectors and compare it with two state-of-the-art adversarial attack techniques. Results demonstrate that LAMLAD achieves an attack success rate of 97\% with an average of 3 attempts per adversarial example, highlighting its potency, efficiency, and adaptability in real-world adversarial scenarios.},
  month = {sep},
}

@inproceedings{leung_rag_2024,
  title = {{RAG},
  author = {Leung, Chun-hung Jonas and Yi, Yicheng and Kuai, Le and Li, Zongxi and Yeung, Siu-kei Au and Lee, Kwok-wah John and Ho, Ka-him Kelvin and Hung, Kevin},
  year = {2024},
  doi = {10.1109/BESC64747.2024.10780718},
  url = {https://doi.org/10.1109/besc64747.2024.10780718},
  booktitle = {2024 11th {International},
  journal = {… on Behavioural and …},
  pages = {1--6},
  note = {ISSN: 2689-8284},
  keywords = {Accuracy, Adaptation models, Computational modeling, Context modeling, Knowledge based systems, Semantics, Social computing, Solids, Testing, Training, source: IEEE, source: Google Scholar},
  abstract = {Although Large language models (LLMs) are well-known due to their superior capacity for text generation and logical inference, they are found to be inaccurate in domain-specific question-answering tasks. The powerful generator still tends to generate content even when the LLM does not have sufficient knowledge at all, which is known as the hallucination problem. We find there is a research void in applying LLMs in the vocal training industry, which requires intensive expert knowledge in any chatbot or intelligent tutor services. This paper details employing Retrieval-Augmented Generation (RAG) technology to develop a domain-specific language model, addressing inherent challenges such as hallucination, where large models generate plausible but inaccurate content, and lack of domain specificity. By segmenting the knowledge base and establishing semantic similarities between user queries and knowledge data, the project lays a solid foundation for integrating RAG, significantly improving response accuracy and contextual relevance. The report highlights the successful implementation of RAG, enhancing system intelligence and personalization for user-specific needs, discusses challenges and solutions during the implementation process, and outlines future directions to expand RAG capabilities and improve user experiences.},
  month = {aug},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{cheng_intelligent_2025,
  title = {Intelligent {Retrieval},
  author = {Cheng, You-Wei and Hsu, Chia-Yu and Lan, Yu-Ying},
  year = {2025},
  doi = {10.1109/CASE58245.2025.11164142},
  url = {https://doi.org/10.1109/case58245.2025.11164142},
  booktitle = {2025 {IEEE},
  pages = {1675--1679},
  note = {ISSN: 2161-8089},
  keywords = {Accuracy, Data mining, Generative AI, Manuals, Manufacturing, Production, Real-time systems, Retrieval augmented generation, Standards, Systems operation, source: IEEE},
  abstract = {In the field of industrial production and manufacturing, various management systems such as Manufacturing Execution Systems (MES), Enterprise Resource Planning (ERP), and Advanced Planning Systems (APS) play a crucial role in production monitoring, performance evaluation, and decision support. These systems enable managers to obtain real-time production data, analyze process anomalies, and formulate appropriate adjustment strategies to ensure production efficiency and quality consistency. However, when users are unfamiliar with system operations, they often need to search through extensive Standard Operating Procedure (SOP) manuals to find relevant operational guidance, which is both time-consuming and detrimental to operational efficiency. In recent years, the integration of Generative Artificial Intelligence (Generative AI) with Retrieval-Augmented Generation (RAG) technology has enabled systems to automatically retrieve and synthesize relevant knowledge content in response to user queries, providing real-time assistance. However, when handling unstructured or semi-structured data, traditional text segmentation methods may disrupt the original hierarchical structure and contextual relationships of documents, leading to retrieval results that fail to accurately match user queries. Moreover, if text segments lack complete source attribution, RAG models may struggle to clearly indicate the origin of the information in their responses, thereby affecting the credibility of the generated content. To address these challenges, this study proposes a method for extracting semi-structured data to enhance the quality of document parsing. By implementing this approach, the final output of the RAG system can generate high-quality responses, precisely annotate interface locations and procedural steps, and assist field personnel in utilizing management systems more efficiently.},
  month = {aug},
}

@inproceedings{chen_autokg_2023,
  title = {{AutoKG},
  author = {Chen, Bohan and Bertozzi, Andrea L.},
  year = {2023},
  doi = {10.1109/BigData59044.2023.10386454},
  url = {https://doi.org/10.1109/bigdata59044.2023.10386454},
  booktitle = {2023 {IEEE},
  pages = {3117--3126},
  keywords = {Big Data, Computer architecture, Data visualization, Graph Learning, Knowledge based systems, Knowledge Graph, Knowledge graphs, Language model, Retrieval-augmented Generation, Semantics, Training, source: IEEE},
  abstract = {Traditional methods of linking large language models (LLMs) to knowledge bases via the semantic similarity search often fall short of capturing complex relational dynamics. To address these limitations, we introduce AutoKG, a lightweight and efficient approach for automated knowledge graph (KG) construction. For a given knowledge base consisting of text blocks, AutoKG first extracts keywords using a LLM and then evaluates the relationship weight between each pair of keywords using graph Laplace learning. We employ a hybrid search scheme combining vector similarity and graph-based associations to enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a more comprehensive and interconnected knowledge retrieval mechanism compared to the semantic similarity search, thereby enhancing the capabilities of LLMs in generating more insightful and relevant outputs.},
  month = {dec},
}

@article{mudassar_yamin_applications_2024,
  title = {Applications of {LLMs},
  author = {Mudassar Yamin, Muhammad and Hashmi, Ehtesham and Ullah, Mohib and Katt, Basel},
  year = {2024},
  doi = {10.1109/ACCESS.2024.3468914},
  url = {https://doi.org/10.1109/access.2024.3468914},
  journal = {IEEE Access},
  volume = {12},
  pages = {143806--143822},
  keywords = {bounded rationality, Computer crime, Computer security, Cyber security exercise scenarios, Data models, Ethics, generative configurations, Halluciation in LLMs, large language models, Manuals, Organizations, Security, Testing, Training, Transformers, source: IEEE},
  abstract = {This study proposes a novel approach leveraging Large Language Models (LLMs) to generate dynamic and complex adaptable cybersecurity exercise scenarios. Motivated by Turing’s seminal exploration into machine cognition, which questions the ability of machines to mimic human thought and intelligence. By exploiting the generative potential of LLMs, our methodology simulates a wide range of cyber threats, both known and novel, thereby enhancing cybersecurity training and awareness. This approach transforms the potential for ‘hallucination’ inherent in LLMs into a potential advantage, enabling the creation of complex exercise scenarios that push the boundaries of traditional cybersecurity training. The innovation lies in the sophisticated application of AI, aiming to advance the preparedness of security professionals against diverse cyber threats. The scenarios generated through this method were subject to meticulous testing and a rigorous evaluation process involving (Generated Pre-Trained Transformer) GPT models and expert review to ensure their realism and applicability. In this paper, we introduce ‘CyExec,’ a novel approach leveraging GPT to dynamically generate cybersecurity training scenarios. Furthermore, the prompts provided to the LLMs were meticulously designed to adopt a Retrieval-Augmented Generation (RAG) approach, enriching the complexity and relevance of the scenarios. This incorporation of RAG, alongside the inspiration drawn from Turing’s exploration of machine intelligence, showcases an advanced application of AI in cybersecurity training, reflecting a deep understanding of how machines can augment our capabilities to anticipate and mitigate cyber threats.},
  issn = {2169-3536},
}

@inproceedings{egersdoerfer_ioagent_2025,
  title = {{IOAgent},
  author = {Egersdoerfer, Chris and Sareen, Arnav and Bez, Jean Luca and Byna, Suren and Xu, Dongkuan DK and Dai, Dong},
  year = {2025},
  doi = {10.1109/IPDPS64566.2025.00036},
  url = {https://doi.org/10.1109/ipdps64566.2025.00036},
  booktitle = {2025 {IEEE},
  pages = {322--334},
  note = {ISSN: 1530-2075},
  keywords = {Accuracy, Complexity theory, Corporate acquisitions, Distributed processing, Faces, hpc, i/o, large language model, Large language models, Navigation, parallel file system, Systematics, source: IEEE},
  abstract = {As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for dataintensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. The recent rapid progress in large language models (LLMs) opens the door to creating an automated tool that democratizes trustworthy I/O performance diagnosis capabilities to domain scientists. However, LLMs face significant challenges in this task, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions. In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates various new designs, including a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to continue asking questions about the diagnosis. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Based on this test suite, extensive evaluations were conducted, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.},
  month = {jun},
}

@inproceedings{sarcevic_enhancing_2024,
  title = {Enhancing {Programming},
  author = {Šarčević, Antonia and Tomičić, Ivan and Merlin, Andrija and Horvat, Marko},
  year = {2024},
  doi = {10.1109/MIPRO60963.2024.10569736},
  url = {https://doi.org/10.1109/mipro60963.2024.10569736},
  booktitle = {2024 47th {MIPRO},
  pages = {2051--2056},
  note = {ISSN: 2623-8764},
  keywords = {Accuracy, chatbots, Chatbots, Computational modeling, digital learning, education, Education, Electric potential, Generative AI, generative models, large language models, natural language processing, Temperature, source: IEEE},
  abstract = {This paper describes the development of an Open-Source Generative AI Chatbot, utilizing free Large Language Models (LLM) to enrich the student learning experience for a university course in “Introduction to Programming”. The article aims to provide a step-by-step guide for selecting, fine-tuning, and evaluating available models. As a first step in choosing the appropriate LLM, which provides the most accurate responses while not requiring excessive computing power, the article will cover a discussion of the advantages and disadvantages of local vs. cloud-available models. After selecting a few promising models, the next stage includes fine-tuning LLMs to answer domain-specific questions using a dataset containing essential rules, guidelines, and explanatory content regarding the subject. The crucial aspect of selecting a model was evaluating answers, and in this context, both human and automatic evaluation techniques will be presented. Finally, it is possible to enhance the model performance and accuracy by incorporating Retrieval-Augmented Generation (RAG) techniques and exploring the influence of various factors, such as different vector databases, model temperatures, maximum token lengths, prompt templates, embeddings, repetition penalties, and chunking sizes. Our results show that chatbots have significant potential to improve academic support and learning efficiency, as well as personalized education in general.},
  month = {may},
}

@inproceedings{huang_driverp_2024,
  title = {{DriveRP},
  author = {Huang, Jun and Ma, Hao and Zhang, Tengchao and Lin, Fei and Ma, Siji and Wang, Xiao and Wang, Fei-Yue},
  year = {2024},
  doi = {10.1109/DTPI61353.2024.10778684},
  url = {https://doi.org/10.1109/dtpi61353.2024.10778684},
  booktitle = {2024 {IEEE},
  pages = {547--553},
  keywords = {Accuracy, Autonomous vehicles, Autonomous Vehicles, Decision making, Digital Twin, Digital twins, Intelligent Transportation Systems, Large Language Models, Metaverse, Motion control, Parallel Driving, Prompt engineering, Retrieval-Augmented Generation, Safety, Trajectory planning, Transforms, Transportation, source: IEEE},
  abstract = {In recent years, numerous technological advancements in Artificial Generative Intelligences (AGIs) have demonstrated significant potential to transform the intelligence acquisition mechanisms in connected autonomous vehicles (CAVs). Integrating technologies like ChatGPT into CAVs can enhance human-machine interactions. However, the emergence of such new traffic entities may introduce unforeseen hallucinations and complex risks that surpass our current understanding. To address these challenges, Retrieval-Augmented Generation (RAG) and prompt engineering technologies are being explored to enhance the reliability and safety of autonomous driving systems. RAG retrieves relevant contextual information, such as driving experiences and real-time road network status, from external databases to ensure that foundation models have access to accurate and timely data for informed decision-making. Prompt engineering optimizes the performance of large language models in autonomous driving systems by designing and refining prompts that guide the models’ responses, thereby improving their relevance and accuracy in various driving scenarios. Together, these technologies enhance the robustness and trustworthiness of autonomous driving systems. This paper proposes DriveRP, a framework that integrates RAG and prompt engineering within the Descriptive-Predictive-Prescriptive Intelligence framework of Parallel Driving theory. DriveRP aims to enhance the safety and interpretability of autonomous vehicle trajectory planning, decision-making, and motion control, ultimately achieving the "6S" goals. Grounded in Digital Twins and Metaverse-embodied parallel driving theory, DriveRP provides the infrastructure and foundational intelligence for parallel driving with Multi-modal Large Lange Models(MLLMs). Additionally, the paper discusses future trends and potential research directions, focusing on the "6S" goals of parallel driving: Smart, Safe, Secure, Sensitive, Sustainable, and Serviceable.},
  month = {oct},
}

@article{lin_pe-gpt_2025,
  title = {{PE},
  author = {Lin, Fanfan and Li, Xinze and Lei, Weihao and Rodriguez-Andina, Juan J. and Guerrero, Josep M. and Wen, Changyun and Zhang, Xin and Ma, Hao},
  year = {2025},
  doi = {10.1109/TIE.2024.3454408},
  url = {https://doi.org/10.1109/tie.2024.3454408},
  journal = {IEEE Transactions on Industrial Electronics},
  volume = {72},
  number = {4},
  pages = {3778--3791},
  keywords = {Adaptation models, Analytical models, Data models, Integrated circuit modeling, Knowledge based systems, Large language model (LLM), Metaheuristics, Modulation, multimodal AI, Physics, physics-informed AI, power converter design, Power electronics, power electronics (PE) design, Vectors, source: IEEE},
  abstract = {Large language models (LLMs) have shown exciting potential in powering the growth of many industries, yet their adoption in the power electronics (PE) sector is hindered by a lack of specialized PE technical expertise and challenges in processing PE-specific data. This study presents a pioneering approach to establish a multimodal LLM tailored for PE design applications, named PE-GPT. The methodology involves enhancing PE-GPT with retrieval augmented generation from a PE knowledge base, and proposes a hybrid framework that integrates an LLM agent with metaheuristic algorithms, Model Zoo, and Simulation Repository. This enhances its multimodal processing capabilities and enables integration into the existing design workflow. The PE-GPT methodology is demonstrated with two case studies: modulation design of the dual-active bridge (DAB) converter and circuit parameter design of the buck converter. PE-GPT demonstrates a 22.2\% increase in correctness compared to human experts. Against other leading LLMs, PE-GPT shows a 35.6\% improvement in correctness and a 15.4\% enhancement in consistency, reducing hallucination. Hardware experiments validate PE-GPT’s multimodal capabilities in optimizing a five-degree-of-freedom modulation strategy for the DAB converter. The generalization of PE-GPT to other PE design applications and associated AI ethical considerations are also discussed. This research concludes by outlining inspiring future research directions, encouraging researchers to expand the boundaries of the PE industry and advance toward a more intelligent era.},
  issn = {1557-9948},
  month = {apr},
}

@inproceedings{weerathunge_optimizing_2025,
  title = {Optimizing {Response},
  author = {Weerathunge, Tharindu and Jayalal, Shantha and Wijayasiriwardhane, Keerthi},
  year = {2025},
  doi = {10.1109/ICARC64760.2025.10963313},
  url = {https://doi.org/10.1109/icarc64760.2025.10963313},
  booktitle = {2025 5th {International},
  pages = {1--6},
  keywords = {Accuracy, Large language models, Large Language Models, Medical Education, Medical services, Prompt engineering, Prompt Engineering, Reliability engineering, Response Accuracy, Response Consistency, Retrieval augmented generation, Training, source: IEEE},
  abstract = {This research focuses on optimizing the response consistency of Large Language Models (LLMs) in medical education through advanced prompt engineering techniques. LLMs often give different answers to the same question, making self-consistency a critical parameter for assessing their performance. Addressing this inconsistency is essential in high-stakes fields like healthcare, where reliable and accurate information is important. The study employed custom prompt engineering strategies, including zero-shot, few-shot, and Chain-of-Thought (CoT) prompting, to improve LLM output consistency and accuracy. We implemented a retrieval-augmented generation (RAG) framework to use external knowledge from trusted medical resources, keeping the responses accurate and contextually appropriate. Responses were scored on several dimensions: content relevance, completeness, and clinical correctness, and assessed for consistency by asking repeated queries. The results showed significant enhancement in the consistency and accuracy of the responses, proving the effectiveness of the presented method. This work outlines suggestions for the use of LLMs in a way that can be incorporated into medical education while considering the limitations. It underscores the need for further exploration of prompt engineering to improve LLM performance and establishes these tools as reliable resources for training healthcare professionals.},
  month = {feb},
}

@article{panagoulias_lyricel_2025,
  title = {{LYRICEL},
  author = {Panagoulias, Dimitrios P. and Tsichrintzi, Evangelia-Aikaterini and Sotiropoulos, Dionisios N. and Chrysafiadi, Konstantina and Sakkopoulos, Evangelos and Tsihrintzis, George A. and Virvou, Maria},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3597213},
  url = {https://doi.org/10.1109/access.2025.3597213},
  journal = {IEEE Access},
  volume = {13},
  pages = {141985--142006},
  keywords = {Accuracy, AI-based poem analysis, artificial intelligence, Artificial intelligence, Chatbots, ChatGPT, Cultural differences, cultural heritage, e-learning, Education, educational software, generative AI, Integrated circuit modeling, intelligent tutoring systems, knowledge graphs, Knowledge graphs, LLMs, machine learning, natural language processing, Reliability, Semantics, Transformers, source: IEEE},
  abstract = {This paper presents LYRICEL, a framework integrating Knowledge Graph (KG) representation learning, Large Language Models (LLMs), and machine learning for reliable, explainable, and validatable cross-cultural lyric analysis. The core component, Sequential Language Model Integration (SLMI), enhances the interpretability and reliability of transformer-based LLMs by addressing explainability and validation challenges through Retrieval-Augmented Generation (RAG), hybrid search, and rule-based evaluation. An important feature of LYRICEL is its use of KG visualizations, which serve as dynamic links to improve interpretability and validatability by structuring data relationships and sources. These visualizations are central to advancements in four areas: KG representation learning, knowledge acquisition, temporal KGs, and knowledge-aware applications. Tested on Greek folk music with models like GPT-4o and BERT, LYRICEL’s trustworthiness is assessed using the VIRTSI model, which quantifies cognitive trust in human-computer interactions. The framework shows strong potential for cross-cultural applications, particularly in languages such as Modern Greek which encompasses a rich cultural heritage spanning centuries of history and traditions resulting in a complex study. The outcomes of GPT-enabled LYRICEL are compared to ChatGPT alone and show a significant improvement in the reliability and efficiency of interactions that can reach a global audience, enhancing the accessibility and understanding of diverse cultural heritages.},
  issn = {2169-3536},
}

@inproceedings{haryanto_contextualized_2024,
  title = {Contextualized {AI},
  author = {Haryanto, Christoforus Yoga and Elvira, Anne Maria and Nguyen, Trung Duc and Vu, Minh Hieu and Hartanto, Yoshiano and Lomempow, Emily and Arakala, Arathi},
  year = {2024},
  doi = {10.1109/SIN63213.2024.10871242},
  url = {https://doi.org/10.1109/sin63213.2024.10871242},
  booktitle = {2024 17th {International},
  pages = {1--8},
  keywords = {artificial intelligence, Artificial intelligence, Chatbots, Computer crime, cyber defense strategy, cyber security, Filtering, meta-analysis, retrieval augmented generation, Robustness, Security, Surveys, source: IEEE},
  abstract = {This paper surveys the potential of contextualized AI in enhancing cyber defense capabilities, revealing significant research growth from 2015 to 2024. We identify a focus on robustness, reliability, and integration methods, while noting gaps in organizational trust and governance frameworks. Our study employs two LLM-assisted literature survey methodologies: (A) ChatGPT 4 for exploration, and (B) Gemma 2:9b for filtering with Claude 3.5 Sonnet for full-text analysis. We discuss the effectiveness and challenges of using LLMs in academic research, providing insights for future researchers.},
  month = {dec},
}

@inproceedings{p_ai-based_2025,
  title = {{AI},
  author = {P, Jayaprakash K and K, Somasekhar and P, Santhya and Menon, Rajalakshmi and Amrutesh, Aniverthy},
  year = {2025},
  doi = {10.1109/SPACE65882.2025.11170869},
  url = {https://doi.org/10.1109/space65882.2025.11170869},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  keywords = {Accuracy, AI-based troubleshooting, Aircraft, Aircraft maintenance, Complex systems, Fault Isolation manuals, Generative AI, Generative AI techniques, GPT-2, Large Language Models (LLMs), Maintenance, Manuals, Real-time systems, System of systems, Training, Vehicle dynamics, source: IEEE},
  abstract = {The current day automotive systems are complex in nature and comprise systems of systems. The automotive domain spans ground vehicles to aircraft and spacecraft. Due to the system-of-systems nature, the maintenance of such systems is very challenging and time-consuming. The maintenance logs/records for handling the troubleshooting of such systems are available for several automotive systems. To handle the challenges of troubleshooting such complex systems and leveraging the huge troubleshooting records available in the design and maintenance houses, it is proposed to develop an AI-based application which act as an aid for troubleshooting such complex systems. The program incorporates cutting-edge AI technologies and must be user-friendly for the maintenance team.This paper focuses on aircraft systems, presenting two distinct approaches to troubleshooting. The first approach employs a stored data concept where a web-based application, supported by a comprehensive database, allows users to select the system, subsystem, and problem description, and retrieve the corresponding Fault Isolation manual. The second approach uses generative AI approaches, training several LLMs, including BERT along with GPT-2, as well as RAG methodology using Mistral-Nemo-Instruct-2407, on the dataset.},
  month = {jul},
}

@inproceedings{purohit_graphaide_2024,
  title = {{GraphAide},
  author = {Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825705},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825705},
  booktitle = {2024 {IEEE},
  pages = {3485--3493},
  note = {ISSN: 2573-2978},
  keywords = {Accuracy, Cognition, Knowledge graphs, Large language models, Pattern matching, Retrieval augmented generation, Scalability, Semantic Web, Semantics, Usability, source: IEEE},
  abstract = {Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
  month = {dec},
}

@inproceedings{liu_siqa_2025,
  title = {{SiQA},
  author = {Liu, Jiawang and Tao, Ye and Wang, Fei and Li, Hui and Qin, Xiugong},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10888359},
  url = {https://doi.org/10.1109/icassp49660.2025.10888359},
  booktitle = {{ICASSP},
  pages = {1--5},
  note = {ISSN: 2379-190X},
  keywords = {Encoding, Faces, Flowcharts, Knowledge graphs, multimodal, QA, Question answering (information retrieval), RAG, Semantics, Signal processing, Signal processing algorithms, Speech processing, structured images, Visualization, source: IEEE},
  abstract = {Existing Large Multimodal Models (LMMs) demonstrate excellent performance in handling visual tasks in everyday scenarios. However, they still face challenges in understanding structured images, such as flowcharts and organizational charts, which are characterized by text-rich and complex hierarchical components. In this paper, we propose SiQA, a knowledge construction and Retrieval-Augmented Generation(RAG)-based multimodal Question-Answering model designed for Structured Images. SiQA operates in three stages: Knowledge Graph (KG) generation, retrieval-augmented, and answer generation. First, a KG representing the semantics of the structured images is generated through component analysis. We then performed similarity retrieval between the KG and queries, using a node-first algorithm to construct the most relevant subgraph. Finally, after performing an encoding alignment on the multimodal information, it is fed into the LLM to generate the answer. Additionally, we introduce a new dataset, OCQA1, which includes 5,112 questions derived from 1,000 Organizational Charts. We evaluated SiQA’s structured image detection and question-answering capabilities on the FD-DETR (a flowchart dataset) and SCQA, and verified its effectiveness and strong generalization ability through comparisons with existing state-of-the-art (SOTA) methods.},
  month = {apr},
}

@inproceedings{chi_rtlexplain_2025,
  title = {{RTLExplain},
  author = {Chi, Ting-Hsun and Mackin, Charles and Shi, Luyao and Vijayaraghavan, Prashanth and Tsai, Hsinyu and Degan, Ehsan},
  year = {2025},
  doi = {10.1109/MLCAD65511.2025.11189167},
  url = {https://doi.org/10.1109/mlcad65511.2025.11189167},
  booktitle = {2025 {ACM},
  pages = {1--7},
  keywords = {Accuracy, Codes, data dependency, documentation, Documentation, Hardware, hardware assistant, Knowledge based systems, Large language models, Large Language Models, Question answering (information retrieval), RAG, Register transfer level, Semantics, summarization, Training, Verilog, source: IEEE},
  abstract = {Large Language Models (LLMs) show promise in assisting with Register Transfer Level (RTL) design tasks, including code summarization, documentation, and question answering. However, directly applying LLMs to entire RTL codebases often leads to low accuracy in these tasks. This is primarily because LLMs are less exposed to RTL code during pretraining, limiting their ability to understand RTL-specific semantics and structural dependencies. To overcome this challenge, we propose RTLExplain, which builds project-specific knowledge bases to enhance LLM performance on RTL design tasks. Our method is entirely offline and requires no additional training or fine-tuning. Experiments on code summarization using the generated knowledge bases demonstrate consistent improvements across various medium-to-large RTL projects, even when variable names are obfuscated. Furthermore, we use these knowledge bases to support Retrieval-Augmented Generation (RAG) for question answering tasks. Results show that our enhanced knowledge bases, when combined with RAG, improve question-answering accuracy by 37\% compared to naïve prompting and 27\% compared to conventional RAG.},
  month = {sep},
}

@article{tang_automatic_2025,
  title = {Automatic {Retrieval},
  author = {Tang, Yun and Guo, Weisi},
  year = {2025},
  doi = {10.1109/MCOM.002.2400280},
  url = {https://doi.org/10.1109/mcom.002.2400280},
  journal = {IEEE Communications Magazine},
  volume = {63},
  number = {4},
  pages = {95--102},
  keywords = {6G mobile communication, Databases, Knowledge engineering, Low latency communication, Open RAN, Real-time systems, Reliability, Sensors, Technological innovation, User experience, source: IEEE},
  abstract = {6G open radio access networks (O-RAN) promises to open data interfaces to enable plug-and-play service apps, many of which are consumer and business-facing. Opening up 6G access lowers the barrier to innovation but raises the challenge of required communication specifications that are not fully known to all service designers. As such, business innovators must either be familiar with 6G standards, or consult with experts. Enabling consistent, unbiased, rapid, and low-cost requirement assessment and specification generation is crucial to the O-RAN innovation ecosystem. Here, we discuss our initiative to bridge service specification gaps between network service providers and business innovators leveraging large language models (LLMs). We first review the state-of-the-art and motivation in 6G plug-and-play services, capabilities, potential use cases, and LLMs. We identify an ample innovation space for hybrid use cases that may require diverse and variational wireless functionalities across its operating time. We show that the network specification can be automated, and present the first automatic retrieval-augmented network service specification framework for 6G use cases. To enable public acceptance and feedback, a website interface is published for the research and industrial communities to experiment with the framework. We hope this review highlights the need for emerging foundation models for this area and motivates researcher engagement and contribution to the community through our framework.},
  issn = {1558-1896},
  month = {apr},
}

@inproceedings{bei_manufacturing_2024,
  title = {Manufacturing {Domain},
  author = {Bei, Yijun and Fang, Zhibin and Mao, Shenyu and Yu, Shuyi and Jiang, Yan and Tong, Yining and Cai, Weimin},
  year = {2024},
  doi = {10.1109/IJCNN60899.2024.10649905},
  url = {https://doi.org/10.1109/ijcnn60899.2024.10649905},
  booktitle = {2024 {International},
  pages = {1--8},
  note = {ISSN: 2161-4407},
  keywords = {Accuracy, Large language models, Measurement, Neural networks, Noise, Semantics, Training, source: IEEE},
  abstract = {Large Language Models (LLMs) have demonstrated powerful capabilities, yet LLMs face issues like hallucination in certain domain-specific areas. Consequently, an increasing number of domain-specific models are emerging. The current paradigm for domain-specific models involves training with domain data, followed by the employment of Retrieval-Augmented Generation (RAG) to mitigate hallucination issues. However, in precision-critical domains such as manufacturing, if the knowledge documents are of low quality or contain noise, the context retrieved through simple semantic matching by RAG may not necessarily benefit model output. Additionally, there can be issues like getting "lost in the middle" due to irrelevant or excessive context. To overcome this, we introduce the Integrated Term Enhancement Methodology (ITEM). Inspired by Chinese educational methods focused on key term elucidation, ITEM extracts and explains critical terms precisely from knowledge documents to form a comprehensive Term Dictionary for retrieving terms and explanations to enhance query capabilities. This methodology refines query responses by providing more accurate and contextually relevant information. To assess ITEM's effectiveness, we utilize the Chinese Mould Manufacturing Dataset (CMMD) and Contextualized Adaptive Response Assessment (CARA) metric method. Our experiment demonstrates that ITEM significantly outperforms existing retrieval enhancement Dense Retrievers by over 17.0\% in accuracy while requiring only 80\% of their token length. Moreover, the accuracy of our method exceeded that of GPT-4 by 5.0\%. This advancement represents a significant leap in context-specific retrieval in LLMs, especially beneficial for specialized domains. The results underscore ITEM's potential as a transformative method in the field, offering new perspectives on integrating domain-specific knowledge into LLMs.},
  month = {jun},
}

@article{xu_large_2024,
  title = {Large {Multi},
  author = {Xu, Shengzhe and Kurisummoottil Thomas, Christo and Hashash, Omar and Muralidhar, Nikhil and Saad, Walid and Ramakrishnan, Naren},
  year = {2024},
  doi = {10.1109/MNET.2024.3427313},
  url = {https://doi.org/10.1109/mnet.2024.3427313},
  journal = {IEEE Network},
  volume = {38},
  number = {5},
  pages = {10--20},
  keywords = {6G mobile communication, AI-native, Alignment, Artificial intelligence, Cognition, Grounding, Instructibility, Large language models, Large multi-modal models, Mathematical models, Natural language processing, Next generation networking, Sensors, Symbols, Universal foundation model, Wireless communication, Wireless sensor networks, source: IEEE},
  abstract = {Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.},
  issn = {1558-156X},
  month = {sep},
}

@inproceedings{zhu_enhancing_2024-1,
  title = {Enhancing {Large},
  author = {Zhu, Zhui and Qi, Guangpeng and Shang, Guangyong and He, Qingfeng and Zhang, Weichen and Li, Ningbo and Chen, Yunzhi and Hu, Lijun and Zhang, Wenqiang and Dang, Fan},
  year = {2024},
  doi = {10.1109/ICPADS63350.2024.00042},
  url = {https://doi.org/10.1109/icpads63350.2024.00042},
  booktitle = {2024 {IEEE},
  pages = {262--269},
  note = {ISSN: 2690-5965},
  keywords = {Accuracy, Artificial Intelligence, Cognition, Data models, Distributed databases, Faces, Knowledge Graph, Knowledge graphs, Large Language Model, Large language models, Question answering (information retrieval), Reliability, Vectors, source: IEEE},
  abstract = {In recent years, large language models (LLMs) have shown rapid development, becoming one of the most popular topics in the field of artificial intelligence. LLMs have demonstrated powerful generalization and learning capabilities, and their performance on various language tasks has been remarkable. Despite their successes, LLMs face significant challenges, particularly in domain-specific tasks that require structured knowledge, often leading to issues such as hallucinations. To mitigate these challenges, we propose a novel system, SynaptiQA, which integrates LLMs with Knowledge Graphs (KGs) to answer more questions about knowledge. Our approach leverages the generative capabilities of LLMs to create and optimize KG queries, thereby improving the accuracy and contextual relevance of responses. Experimental results in an industrial data set demonstrate that SynaptiQA outperforms baseline models and naive retrieval-augmented generation (RAG) systems, demonstrating improved accuracy and reduced hallucinations. This integration of KGs with LLMs paves the way for more reliable and interpretable domain-specific question answering systems.},
  month = {oct},
}

@inproceedings{tsai_ai-enhanced_2024,
  title = {{AI},
  author = {Tsai, Hsin-Chun and Chen, Meng-Wei and Wang, Jhing-Fa},
  year = {2024},
  doi = {10.1109/ICOT64290.2024.10936938},
  url = {https://doi.org/10.1109/icot64290.2024.10936938},
  booktitle = {2024 {International},
  pages = {1--5},
  keywords = {source: IEEE},
  abstract = {Traditional patient education relies on static brochures, lacking interactivity and increasing healthcare providers' workload. This study introduces a virtual nurse dialogue system using ASR and a fine-tuned LLama3 8B LLM trained on Traditional Chinese medical texts. A RAG framework with a structured knowledge base enhances accuracy and minimizes misinformation. The system features an RWD-based interface with 3D animations and synchronized voice outputs for an immersive experience. Experimental results show 95\% accuracy and responses under 5 seconds, with 90\% user satisfaction, demonstrating its potential to enhance patient education and reduce healthcare burdens.},
  month = {dec},
}

@inproceedings{zhang_llm-assisted_2025,
  title = {{LLM},
  author = {Zhang, Weiyan and Hassan, Muhammad and Drechsler, Rolf},
  year = {2025},
  doi = {10.1109/DDECS63720.2025.11006767},
  url = {https://doi.org/10.1109/ddecs63720.2025.11006767},
  booktitle = {2025 {IEEE},
  pages = {7--12},
  note = {ISSN: 2473-2117},
  keywords = {Accuracy, Benchmark testing, Embedded software, embedded system, Embedded systems, large language model, Large language models, machine learning, Machine learning, performance estimation, Predictive models, Program processors, Register transfer level, RISC-V, Source coding, source: IEEE},
  abstract = {In this paper, we present a methodology that combines a Large Language Model (LLM) with a traditional Machine Learning (ML) approach to estimate the performance of embedded software on RISC-V processors across different microarchitectures. In particular, we employ a Retrieval-Augmented Generation (RAG)-based LLM to extract performance-related information from processor specifications and source code. Additionally, we leverage the predictive capabilities of ML models to create Predictive Models (PMs) for RISC-V processors. To demonstrate the effectiveness of our hybrid approach, we present results on the performance estimation of open-source benchmarks using the generated PMs, with open-source RISC-V-based Register Transfer Level (RTL) implementations as reference models. Our results demonstrate that our proposed LLM-assisted methodology provides highly accurate predictions, with Mean Absolute Percentage Errors (MAPEs) of only 2.50\% for SweRV core and 11.90\% for RSD core, respectively. In comparison with the state-of-the-art methodology, our approach achieves significant improvements, reducing the MAPE by 61.54\% for SweRV and 36.02\% for RSD.},
  month = {may},
}

@inproceedings{singla_hicon_2024,
  title = {{HICON},
  author = {Singla, Arjun Dev and Tripathi, Shashank and Victoria, A Helen},
  year = {2024},
  doi = {10.1109/ICPCSN62568.2024.00131},
  url = {https://doi.org/10.1109/icpcsn62568.2024.00131},
  booktitle = {2024 4th {International},
  pages = {779--784},
  keywords = {Accuracy, Automation, Categorizer, Education, Employee welfare, HICON AI, LLMs, Machine learning, Machine Learning, Natural Language Processing, Pervasive computing, Resume Screener, Retrieval Augmented Generation, Social networking (online), text to Speech, source: IEEE},
  abstract = {As more and more students are seeking counseling services to help them pursue higher education opportunities abroad, it has become apparent that there is a lack of automation and a monopolization of counseling agencies, which presents a challenge for these students. To address this issue, this study has developed HICON AI: Higher Education Counselor. HI-CON made from Higher - Counseling is an innovative application that offers personalized college selection and preparation recommendations to students. By asking a set of defined questions, the bot gets to know the user and provides tailored guidance based on the information provided, utilizing refined Machine Learning models and Retrieval Augmented Generation. This study has specifically used Llama 2, LLM by meta for the considered use case, because of its high performance and financial viability. The developed new product ensures the highest level of accuracy and reliability.},
  month = {may},
}

@inproceedings{patel_large_2025,
  title = {Large {Language},
  author = {Patel, Sanskruti and Dholakiya, Radhika Ashish},
  year = {2025},
  doi = {10.1109/ICSCSA66339.2025.11170884},
  url = {https://doi.org/10.1109/icscsa66339.2025.11170884},
  booktitle = {2025 5th {International},
  pages = {1922--1929},
  keywords = {Adaptation models, AI Applications, Computational modeling, Computer architecture, Ethical AI, Few shot learning, Industries, Large language models, Large Language Models, Medical services, Natural language processing, Natural Language Processing, Retrieval augmented generation, Transformers, source: IEEE},
  abstract = {Large Language Models (LLMs) have revolutionized artificial intelligence and natural language processing since the advent of transformer architectures in last decade. Trained on vast amounts of data with parameter counts ranging from billions to trillions, models such as GPT-4, Claude, Grok, and Llama demonstrate remarkable proficiency in language comprehension, generation, and manipulation, including access to additional domains such as conversational systems, code generation, scientific writing, and creative content creation. This survey summarizes the evolution of LLMs, from early statistical methods and neural networks to modern transformer-based frameworks, emphasizing milestones like GPT-3’s few-shot learning and GPT-4o’s multimodal advancements. We present a comparative analysis of prominent LLMs, evaluating their architectures, parameter scales, and performance on benchmarks such as MMLU and MATH, with particular attention to Grok’s real-time knowledge integration. The study highlights LLMs’ transformative applications in academia, healthcare, education, and creative industries while addressing critical challenges, including biases, misinformation, computational demands, and interpretability limitations. We explore mitigation strategies such as retrieval-augmented generation and parameter-efficient fine-tuning. Further- more, we outline future directions, including multimodal LLMs, extended context processing, and ethical AI development, providing a comprehensive guide for researchers and practitioners shaping the responsible advancement of LLMs.},
  month = {aug},
}

@inproceedings{dong_survey_2024,
  title = {A {Survey},
  author = {Dong, Xiaofei and Zhang, Xueqiang and Bu, Weixin and Zhang, Dan and Cao, Feng},
  year = {2024},
  doi = {10.1109/AIoTC63215.2024.10748304},
  url = {https://doi.org/10.1109/aiotc63215.2024.10748304},
  booktitle = {2024 3rd {International},
  pages = {407--413},
  keywords = {Applications, Artificial general intelligence, Benchmark testing, Cloud computing, Computers, Internet of Things, Key Technologies, Large language models, LLM-based Agent, Planning, Reviews, Solids, Suggestions, Surveys, Theory Foundation, source: IEEE},
  abstract = {AI Agent has presented potential towards Artificial General Intelligence (AGI), which is expected to autonomously perceive the environments, make decisions and take actions. However, most of existing AI agents tend to train in confined environments with limited knowledge, yielding sub-optimal performance. Benefiting from the remarkable progress of large language models (LLMs), diverse LLM-based agents emerge. These agents employ LLM as the central brain to perceive, plan, and memorize, etc, which exhibit human-level intelligence across multifarious applications and obtain satisfactory performance. In this paper, we propose a survey of LLM-based agents from the perspective of theories, technologies, applications and suggestions, respectively. Specifically, we first deliver a recapitulative review of the theory foundation, which includes Large Language Models, Chain of Thought and AI Alignment, Retrieval-Augmented Generation, Embodied AI, etc; With this, we then present the key technologies, comprising four critical components: Perception, Planning, Memory and Action; Subsequently, we briefly explore some domain-related and evaluation applications; Finally, we provide pertinent suggestions based on the observations of significant challenges for LLM-based agents.},
  month = {sep},
}

@inproceedings{garima_harnessing_2024,
  title = {Harnessing the {Power},
  author = {Garima, Sogani and Swapnil, Morande and Shashank, Shah},
  year = {2024},
  doi = {10.23919/ITUK62727.2024.10772761},
  url = {https://doi.org/10.23919/ituk62727.2024.10772761},
  booktitle = {2024 {ITU},
  pages = {1--8},
  keywords = {Electronic healthcare, ethical AI, Ethics, generative AI, Generative AI, ITU, Knowledge based systems, knowledge retrieval, language models, Large language models, personalized healthcare, Privacy, Robustness, Safety, Technological innovation, source: IEEE},
  abstract = {This research proposes a novel framework that integrates state-of-the-art large language models (LLMs) with curated medical knowledge bases to enable personalized, reliable, and user-centric digital health services. The architecture combines advanced generative models, retrieval-augmented generation, and domain adaptation strategies to ensure the safety and ethical alignment of AI-driven health recommendations. Empirical evaluations, including automated benchmarks and user studies, demonstrate the framework’s ability to provide accurate, relevant, and personalized health information that resonates with patients and providers. The results highlight the potential of this approach to bridge the gap between general-purpose LLMs and domain-specific healthcare applications. However, the work also underscores the challenges in responsibly developing and deploying generative AI for healthcare, such as safety, robustness, fairness, privacy, and interpretability. The research advocates for multidisciplinary collaboration to address these challenges and realize the potential of AI in enhancing health and well-being worldwide. By prioritizing patient agency, clinical validity, and ethical practices, this work contributes to the growing body of knowledge at the intersection of AI and healthcare, laying the foundation for future research and innovation in personalized, equitable, and trustworthy AI health services.},
  month = {oct},
}

@article{ge_innovative_2025,
  title = {An {Innovative},
  author = {Ge, Shijun and Sun, Yuanbo and Cui, Yin and Wei, Dapeng},
  year = {2025},
  doi = {10.1109/ACCESS.2024.3494054},
  url = {https://doi.org/10.1109/access.2024.3494054},
  journal = {IEEE Access},
  volume = {13},
  pages = {10499--10512},
  keywords = {Brain modeling, chain-of-thought fine-tuning, Cognitive load, Collaboration, Computational modeling, concept generation method, Creativity, Design methodology, EDA, Heuristic algorithms, human-agent collaboration, human-human collaboration, Knowledge based systems, Large language models, LLM-based agent, Particle swarm optimization, Training, source: IEEE},
  abstract = {To enhance the application capabilities of large language models (LLMs) in conceptual design, this study explores how to achieve deep integration between LLM-based agents and concept generation methods using the chain-of-thought (CoT) technique and evaluates its feasibility. Using GPT-4 as a case study, we designed two agents: IntelliStorm (based on the unstructured brainstorming method) and EvoluTRIZ (based on the structured TRIZ method). Thirty participants were recruited, and through two experimental phases spaced one month apart, a comparative analysis of the effects of collaboration groups (human-agent vs. human-human) and concept generation methods (brainstorming vs. TRIZ) on participants’ physiological activation and creative thinking performance were conducted. The results show that the involvement of LLM-based agents can effectively reduce participants’ electrodermal activity(EDA) response levels, indicating a reduction in cognitive load. Moreover, participants maintained their distinct physiological patterns and performance advantages across different concept generation methods. For example, IntelliStorm, like brainstorming, evokes stronger responses to information stimuli, demonstrating superior thinking fluency; EvoluTRIZ, like the TRIZ, exhibits a higher frequency of information responses, showcasing enhanced thinking elaboration. However, originality tends to favor human-human collaboration. The findings confirm that integrating LLMs with traditional concept generation methods is an effective strategy made possible by combining CoT and retrieval-augmented generation (RAG) technologies. In the future, LLM-based agents are expected to achieve broader application in the design field by incorporating additional concept generation methods.},
  issn = {2169-3536},
}

@inproceedings{anh-khoa_gvec_2024,
  title = {{GVEC},
  author = {Anh-Khoa, Ngo-Ho and Anh-Khoi, Ngo-Ho and Khuong-Duy, Vo},
  year = {2024},
  doi = {10.1109/MAPR63514.2024.10660907},
  url = {https://doi.org/10.1109/mapr63514.2024.10660907},
  booktitle = {2024 {International},
  pages = {1--6},
  note = {ISSN: 2770-6850},
  keywords = {automatic question answering, Benchmark testing, chatbot, Chatbots, Economics, generative AI, Generative AI, Multimedia databases, Question answering (information retrieval), Vietnamese language, Web services, source: IEEE},
  abstract = {Currently, the problem of automatic question answering (Q/A) with applications in chatbot services has become essential, addressing various issues in daily life. There are many approaches to solving this problem, but each presents its own challenges. The generative AI approach has recently gained widespread attention due to its ability to provide smooth, human-like responses. However, it has the drawback of producing coherent-sounding but inaccurate or fabricated content, known as "hallucinations". In this study, we aim to find a solution for the question-answering task in the context of the Vietnamese economy. We propose the Generative Vietnamese Economy Chatbot (GVEC), based on the Vietnam Economy Information Database (VEID) retrieved from VnEconomy systems. Our proposition is to apply Retrieval-Augmented Generation (RAG) to various LLM systems and test them on our specially designed benchmark, called the Vietnamese Numeric Economy Information Question/Answer Datasets (VNEIQAD), generated by our Question/Answers Generating with Numerical Information (QAGwNI) algorithm. This will help better evaluate the solution in the economic context.},
  month = {aug},
}

@inproceedings{anaissi_fine-tuning_2024,
  title = {Fine-{Tuning},
  author = {Anaissi, Ali and Braytee, Ali and Akram, Junaid},
  year = {2024},
  doi = {10.1109/ICDMW65004.2024.00026},
  url = {https://doi.org/10.1109/icdmw65004.2024.00026},
  booktitle = {2024 {IEEE},
  pages = {146--153},
  note = {ISSN: 2375-9259},
  keywords = {Accuracy, Information services, Large language models, Large Language Models, LLaMA-2, Matrix decomposition, medical question-answering, Medical services, Mistral, Noise, Overfitting, Reliability engineering, Retrieval augmented generation, retrieval on demand, Stability analysis, source: IEEE},
  abstract = {We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.},
  month = {dec},
}

@inproceedings{mohan_enhancing_2025,
  title = {Enhancing {Agricultural},
  author = {Mohan, G. Bharathi and Adhitya, C.M. Jayanth and Mithilesh, A.},
  year = {2025},
  doi = {10.1109/INCIP64058.2025.11019716},
  url = {https://doi.org/10.1109/incip64058.2025.11019716},
  booktitle = {2025 {International},
  pages = {550--556},
  keywords = {Agricultural Information Systems, Agriculture, Artificial Intelligence (AI), Biological system modeling, Farmer Inquiries, Food security, Information processing, Large language models, Large Language Models (LLMs), Metadata, Multilingual, Natural language processing, Natural Language Processing (NLP), Precision agriculture, Precision Agriculture, Solids, Transformers, source: IEEE},
  abstract = {Agriculture, being a vital component of global food security, necessitates the development of innovative solutions to address the knowledge gap experienced by numerous farmers, particularly in developing countries where access to expert advice is scarce. In these areas, agricultural producers often depend on helplines for essential guidance, but the exorbitant costs and limited accessibility of these services create substantial obstacles. By automating responses to agricultural inquiries, the burden on traditional helpline systems can be alleviated, enabling farmers to access prompt and precise information. The combination of artificial intelligence and agriculture offers a chance to tackle these challenges, with advanced language models, especially transformers, demonstrating significant potential in comprehending intricate agricultural queries and delivering appropriate responses. This paper investigates how large language models (llms) can simplify the process of finding answers for farmers by utilizing their advanced language processing abilities. By analyzing a vast collection of over four million farmer inquiries from tamil nadu, india, encompassing diverse agricultural scenarios, this study showcases the efficacy of llms in bridging information gaps and equipping farmers with immediate access to crucial knowledge.},
  month = {jan},
}

@inproceedings{alharthi_llm-powered_2025,
  title = {{LLM},
  author = {Alharthi, Dalal and Yasaei, Rozhin},
  year = {2025},
  doi = {10.1109/CLOUD67622.2025.00012},
  url = {https://doi.org/10.1109/cloud67622.2025.00012},
  booktitle = {2025 {IEEE},
  pages = {12--22},
  note = {ISSN: 2159-6190},
  keywords = {Accuracy, Adaptation models, Adaptive Prompt Engineering, Automation, Biological system modeling, Cloud computing security, Cloud Forensics, Cloud Security, Forensic Intelligence, Forensics, Large language models, Large Language Models (LLMs), Log Prioritization, Manuals, Robustness, Threat assessment, Threat Detection, source: IEEE},
  abstract = {Cloud forensics is a crucial yet challenging field, as traditional forensic techniques struggle to handle the large-scale, dynamic nature of cloud environments. Manual forensic analysis is time-consuming, error-prone, and often fails to detect evolving cyber threats. This paper presents a novel tool leveraging Large Language Models (LLMs) to fully automate cloud forensic investigations. Our approach utilizes few-shot learning to classify log data, extract forensic intelligence, and reconstruct attack timelines. We evaluate LLM-based automation against traditional machine learning models, including Random Forest, XGBoost, and Gradient Boosting, using cloud forensic log datasets. Experimental results demonstrate that LLMs improve forensic accuracy, precision, and recall while reducing the need for extensive feature engineering. However, challenges such as hallucination risks, adversarial manipulation, and forensic explainability must be addressed to ensure the reliability of AI-driven investigations. To mitigate these risks, we explore Retrieval-Augmented Generation (RAG) for context-aware forensic intelligence and propose hybrid AI models integrating rule-based forensic validation. Our findings highlight the potential of LLM-driven forensic automation to enhance cloud security operations while outlining key areas for future research, including adversarial robustness, forensic transparency, and multi-cloud scalability.},
  month = {jul},
}

@inproceedings{shrestha_fairrag_2024,
  title = {{FairRAG},
  author = {Shrestha, Robik and Zou, Yang and Chen, Qiuyu and Li, Zhiheng and Xie, Yusheng and Deng, Siqi},
  year = {2024},
  doi = {10.1109/CVPR52733.2024.01140},
  url = {https://doi.org/10.1109/cvpr52733.2024.01140},
  booktitle = {2024 {IEEE},
  pages = {11996--12005},
  note = {ISSN: 2575-7075},
  keywords = {bias, Computational modeling, Computer vision, diffusion, fairness, generative-ai, Image databases, Image synthesis, RAG, stable-diffusion, Text to image, Training data, Visualization, source: IEEE},
  abstract = {Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outper-forms existing methods in terms of demographic diversity, image-text alignment and image fidelity while incurring minimal computational overhead during inference.},
  month = {jun},
}

@inproceedings{abdullah_rails_2025,
  title = {{RAILS},
  author = {Abdullah, Wali Mohammad and Islam, Md. Morshedul and Parmar, Devraj and Patel, Happy Hasmukhbhai and Prabhakaran, Sindhuja and Saha, Baidya},
  year = {2025},
  doi = {10.1109/HPEC67600.2025.11196549},
  url = {https://doi.org/10.1109/hpec67600.2025.11196549},
  booktitle = {2025 {IEEE},
  pages = {1--6},
  note = {ISSN: 2643-1971},
  keywords = {Code Repair Automation, Codes, Compilation Feedback, Graphical user interfaces, Java, Java Import Resolution, Large language models, Large Language Models, Libraries, Maintenance engineering, Rails, Retrieval-Augmented Generation, Software development management, Software reliability, Standards, source: IEEE},
  abstract = {Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to assist software development, yet they often produce incomplete code or incorrect imports, especially when lacking access to external or project-specific documentation. We introduce RAILS (Retrieval-Augmented Intelligence for Learning Software Development), a framework that augments LLM prompts with semantically retrieved context from curated Java resources using FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop guided by compiler feedback to refine suggestions. We evaluated RAILS on 78 real-world Java import error cases spanning standard libraries, GUI APIs, external tools, and custom utilities. Despite using the same LLM, RAILS outperforms baseline prompting by preserving intent, avoiding hallucinations, and surfacing correct imports even when libraries are unavailable locally. Future work will integrate symbolic filtering via PostgreSQL and extend support to other languages and IDEs.},
  month = {sep},
}

@article{mundlamuri_evolution_2025,
  title = {The {Evolution},
  author = {Mundlamuri, Rahul and Gunnam, Ganesh Reddy and Mysari, Nikhil Kumar and Pujuri, Jayakanth},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3621344},
  url = {https://doi.org/10.1109/access.2025.3621344},
  journal = {IEEE Access},
  volume = {13},
  pages = {178302--178341},
  keywords = {Artificial intelligence, deep learning, Deep learning, Encoding, Expert systems, Hidden Markov models, large language models, Large language models, machine learning, Machine learning, neural networks, Neural networks, reinforcement learning, Training, transformers, Transformers, source: IEEE},
  abstract = {This paper provides a comprehensive review of the evolution of artificial intelligence from early symbolic, rule-based systems to modern large language models (LLMs) and retrieval-augmented generation (RAG) architectures. Early AI was dominated by symbolic reasoning and expert systems using hand-crafted rules, before a paradigm shift toward data-driven learning occurred with the advent of machine learning, notably the backpropagation algorithm that enabled training of multi-layer neural networks to learn complex tasks. This breakthrough ushered in the deep learning era: convolutional neural networks (CNNs) achieved landmark results in computer vision, recurrent neural networks (RNNs such as LSTM) excelled at sequential data processing, and deep reinforcement learning (RL) systems mastered challenging decision-making tasks. In natural language processing (NLP), distributed word embedding models like Word2Vec and GloVe replaced sparse representations, capturing semantic relationships in vector space. To handle rare and out-of-vocabulary words, subword tokenization techniques such as Byte Pair Encoding (BPE) and WordPiece were introduced. These advances culminated in the Transformer architecture, based on self-attention mechanisms, which now underpins most state-of-the-art LLMs. Transformer-based LLMs, pre-trained on large text corpora, demonstrate advanced capabilities in language generation and understanding. Beyond model architectures, the review highlights how LLMs are augmented with external knowledge via RAG. Retrieval-augmented generation combines an LLM with a vector database of text embeddings, allowing the model to retrieve relevant external information to ground its outputs. This approach significantly improves factual accuracy and relevance, mitigating issues like hallucination, though ensuring complete truthfulness remains challenging. Additional obstacles include the limited context windows of LLMs (on the order of a few thousand tokens) and the computational complexity and latency of inference. The review also examines societal implications of advanced AI, such as bias amplification from training data and other ethical considerations in deployment. Finally, it outlines future directions, including extended context lengths, improved interpretability and alignment, greater efficiency, and neuro-symbolic hybrid approaches, aiming to develop more robust and trustworthy AI systems.},
  issn = {2169-3536},
}

@inproceedings{sahin_adaptation_2025,
  title = {Adaptation and {Use},
  author = {Şahin, Emir Esad and Kurt, Beril and Dehkharghani, Rahim and Öper, Merve and Kaçar, Saygın},
  year = {2025},
  doi = {10.1109/UBMK67458.2025.11207004},
  url = {https://doi.org/10.1109/ubmk67458.2025.11207004},
  booktitle = {2025 10th {International},
  pages = {137--142},
  note = {ISSN: 2521-1641},
  keywords = {Adaptation models, ai using by human, Atmospheric modeling, Chatbots, Fourth Industrial Revolution, Furnaces, Galvanizing, Industry 4.0, Interpretive Artificial Intelligence, Knowledge based systems, LLM, Natural languages, RAG, Transformers, Vibrations, source: IEEE},
  abstract = {In recent years, several researchers have been attracted to analyzing the performance of mechanical and electronic machines using large language models (LLMs). LLMs can take the user query in formal or informal language, convert it to structured language, and make inferences on the knowledge base that contains information about machine performance. In other words, LLMs can be trained using this knowledge base as a training set and answer user queries as a chatbot that understands and generates natural language. This chatbot can facilitate and speed up the monitoring task for industrial machines. The current work analyzes the vibration data of the Radiant Tube Furnace (RTF) Combustion Air Fan used in the Borçelik Group’s galvanizing lines and makes it easily understandable with the help of a chatbot. Our data set for fine-tuning a pre-trained LLM consists of approximately 870,000 entries categorized with performance levels to analyze machine performance and detect anomalies. The chatbot, supported by the LLMs, aims to provide users with easy access to operational details, such as the last time the machine crashed or the performance level of the machine in the last week, last month, etc. The designed and implemented chatbot was tested on manually generated queries, which resulted in 75\% precision. The proposed model is now available on the Hugging Face platform.},
  month = {sep},
}

@inproceedings{sahin_adaptation_2025-1,
  title = {Adaptation and {Use},
  author = {Şahin, Emir Esad and Kurt, Beril and Dehkharghani, Rahim and Öper, Merve and Kaçar, Saygın},
  year = {2025},
  doi = {10.1109/UBMK67458.2025.11206830},
  url = {https://doi.org/10.1109/ubmk67458.2025.11206830},
  booktitle = {2025 10th {International},
  pages = {137--142},
  note = {ISSN: 2521-1641},
  keywords = {Adaptation models, ai using by human, Atmospheric modeling, Chatbots, Fourth Industrial Revolution, Furnaces, Galvanizing, Industry 4.0, Interpretive Artificial Intelligence, Knowledge based systems, LLM, Natural languages, RAG, Transformers, Vibrations, source: IEEE},
  abstract = {In recent years, several researchers have been attracted to analyzing the performance of mechanical and electronic machines using large language models (LLMs). LLMs can take the user query in formal or informal language, convert it to structured language, and make inferences on the knowledge base that contains information about machine performance. In other words, LLMs can be trained using this knowledge base as a training set and answer user queries as a chatbot that understands and generates natural language. This chatbot can facilitate and speed up the monitoring task for industrial machines. The current work analyzes the vibration data of the Radiant Tube Furnace (RTF) Combustion Air Fan used in the Borçelik Group’s galvanizing lines and makes it easily understandable with the help of a chatbot. Our data set for fine-tuning a pre-trained LLM consists of approximately 870,000 entries categorized with performance levels to analyze machine performance and detect anomalies. The chatbot, supported by the LLMs, aims to provide users with easy access to operational details, such as the last time the machine crashed or the performance level of the machine in the last week, last month, etc. The designed and implemented chatbot was tested on manually generated queries, which resulted in 75 \% precision. The proposed model is now available on the Hugging Face platform.},
  month = {sep},
}

@inproceedings{xu_automated_2024-1,
  title = {Automated {C},
  author = {Xu, Kangwei and Zhang, Grace Li and Yin, Xunzhao and Zhuo, Cheng and Schlichtmann, Ulf and Li, Bing},
  year = {2024},
  doi = {10.1109/MLCAD62225.2024.10740262},
  url = {https://doi.org/10.1109/mlcad62225.2024.10740262},
  booktitle = {2024 {ACM},
  pages = {1--9},
  keywords = {Codes, Costs, Iterative methods, Large language models, Machine learning, Maintenance engineering, Manuals, Optimization, Software, Training, source: IEEE},
  abstract = {In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular {\textbackslash},
  month = {sep},
}

@inproceedings{dong_maxmind_2024,
  title = {{MaxMind},
  author = {Dong, Yuchen and Fang, Xiaoxiang and Hu, Yuchen and Jiang, Renshuang and Jiang, Zhe},
  year = {2024},
  doi = {10.1109/SWC62898.2024.00239},
  url = {https://doi.org/10.1109/swc62898.2024.00239},
  booktitle = {2024 {IEEE},
  pages = {1562--1567},
  note = {ISSN: 2993-396X},
  keywords = {Knowledge engineering, Large language models, Large Language Models, MaxMind, Memory Networks, Philosophical considerations, Productivity, RAG, Real-time systems, Recycling, Software, Software Productivity, Tool Generation, source: IEEE},
  abstract = {Large language models can help facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity. Current research often overlooks the significance of converting real-time task experiences into system memory and fails to recognize the pivotal role of differentiating the knowledge value for future reference. This paper provides a novel system model, MaxMind, to address these issues through two novel designs: (1) evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing, and (2) enhancing a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation. To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system that aligns with the MaxMind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in the task success rate, with an improvement rate of approximately 3\%-6\% per round in this implementation example. Note that as the memories continue to accumulate, this cumulative improvement may become substantial. These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.},
  month = {dec},
}

@inproceedings{liu_chatchisel_2024,
  title = {{ChatChisel},
  author = {Liu, Tianyang and Tian, Qi and Ye, Jianmin and Fu, LikTung and Su, Shengchu and Li, Junyan and Wan, Gwok-Waa and Zhang, Layton and Wong, Sam-Zaak and Wang, Xi and Yang, Jun},
  year = {2024},
  doi = {10.1109/ISEDA62518.2024.10618053},
  url = {https://doi.org/10.1109/iseda62518.2024.10618053},
  booktitle = {2024 2nd {International},
  pages = {710--716},
  keywords = {source: IEEE},
  abstract = {With the increasing complexity of integrated circuits, agile hardware design methodologies are crucial. Modern HDLs like Chisel enhance design quality, but manual implementations remain error-prone and time-consuming. Large language models (LLMs) offer potential for design automation through natural language but face challenges in generating large circuits using Verilog. We evaluate LLM capabilities for Chisel and Verilog generation, demonstrating superior Chisel generation ability. We introduce ChatChisel, the first language-based agile hardware design workflow that generates Chisel from language specifications. ChatChisel utilizes four LLM-based modules for decomposing, generating, error-correcting, and composing hardware designs. Techniques like LLM collaboration and RAG enhance ChatChisel's performance. Using GPT-3.5-turbo, we implemented an RV32I RISC-V CPU with 5-stage pipeline and dynamic branch prediction. We also validate our approach with extensive evaluations. Our experimental results reveal that ChatChisel can outperform LLM-based hardware design with Verilog by an average of 31.86\%, implying a significant design capability enhancement and design process acceleration with ChatChisel.},
  month = {may},
}

@inproceedings{iyenghar_development_2025,
  title = {On the {Development},
  author = {Iyenghar, Padma},
  year = {2025},
  doi = {10.1109/WFCS63373.2025.11077569},
  url = {https://doi.org/10.1109/wfcs63373.2025.11077569},
  booktitle = {2025 {IEEE},
  pages = {1--8},
  note = {ISSN: 2835-8414},
  keywords = {Accuracy, Artificial intelligence, Benchmark testing, dataset, hazards, Hazards, ISO Standards, LLM, Machinery, machinery safety, performance level, RAG, Reliability, Risk management, safety function, Scenario generation, Software development management, source: IEEE},
  abstract = {This paper introduces and makes available the first structured dataset for data-driven risk assessment in industrial machinery functional safety. The dataset comprises 7800 hazard scenarios generated using a rigorous, ISO 12100 and ISO 138491 grounded methodology, including plausibility assessment. The dataset spans ten hazard types, incorporating contextual parameters and corresponding required performance level (PLr) for each hazard scenario. The utility of the dataset and its application as a benchmark is demonstrated by comparing LLM-only and RAG-enhanced hazard classification as one example application. Experiments show that that RAG significantly improves LLM accuracy in classifying hazard scenarios and predicting PLr by providing access to critical contextual information (e.g. guidance from ISO 13849-1 for determination of PLr), resulting in a 36.4\% average accuracy increase (72.8\% accuracy) compared to the LLM-only baseline (36.4\% accuracy). Furthermore, RAG reduced misclassification rates across all hazard types, with the highest misclassification dropping from over 70\% in the LLM-only case to a maximum of 29\% with RAG indicating significantly enhanced classification confidence confirmed by statistical tests. These findings demonstrate the dataset’s ability to address the gap in structured data for evaluating LLMs in functional safety tasks, potentially leading to more reliable and efficient automated risk assessments. This work provides a valuable, open-source resource (GitHub repositry) to facilitate collaborative research and adoption of AI in routine functional safety workflows.},
  month = {jun},
}

@inproceedings{rajesh_learn_2024,
  title = {Learn {Like},
  author = {Rajesh, Akshaya and Khan, Sumbul},
  year = {2024},
  doi = {10.1109/TALE62452.2024.10834370},
  url = {https://doi.org/10.1109/tale62452.2024.10834370},
  booktitle = {2024 {IEEE},
  pages = {1--8},
  keywords = {Active learning, Active Learning, AI Learning bots, Chatbots, Cognition, Context modeling, Education, Feynman Technique, Large Language Model, Large language models, Prompt engineering, Self-regulated learning, Surveys, Testing, Usability, source: IEEE},
  abstract = {The Feynman learning technique is an active learning strategy that helps learners simplify complex information through student-led teaching and discussion. In this paper, we present the development and usability testing of the Feynman Bot, which uses the Feynman technique to assist self-regulated learners who lack peer or instructor support. The Bot embodies the Feynman learning technique by encouraging learners to discuss their lecture material in a question-answer-driven discussion format. The Feynman Bot was developed using a large language model with Langchain in a Retrieval-Augmented-Generation framework to leverage the reasoning capability required to generate effective discussion-oriented questions. To test the Feynman bot, a controlled experiment was conducted over three days with fourteen participants. Formative and summative assessments were conducted, followed by a self-efficacy survey. We found that participants who used the Feynman Bot experienced higher learning gains than the Passive Learners' group. Moreover, Feynman Bot Learners' had a higher level of comfort with the subject after using the bot. We also found typing to be the preferred input modality method over speech, when interacting with the bot. The high learning gains and improved confidence with study material brought about by the Feynman Bot makes it a promising tool for self-regulated learners.},
  month = {dec},
}

@inproceedings{zhou_construction_2024,
  title = {Construction of a {Multimodal},
  author = {Zhou, Xiaofa and Shi, Jianyong and Dong, Lei and Zhang, You and Pan, Jin and Huang, Hao},
  year = {2024},
  doi = {10.1109/NPSPE62515.2024.00013},
  url = {https://doi.org/10.1109/npspe62515.2024.00013},
  booktitle = {2024 {International},
  pages = {21--28},
  keywords = {Accuracy, Data mining, Data models, Information filters, Knowledge extraction, Knowledge graph, Knowledge graphs, Large language model, Large language models, Ontologies, Ontology, Power electronics, Power grids, Safety management, source: IEEE},
  abstract = {In response to the difficulties of safety management and the complexity of information at power grid construction sites, a multimodal knowledge graph construction method based on large language models is proposed. Data from the construction site is collected and filtered, and an ontology for safety management at the construction site is constructed. The ontology is then used as retrieval augmented generation(RAG) for assistance, enabling multimodal large model image extraction, resulting in structured data in the power grid safety field. Finally, the extracted results are displayed using a graph database, completing the construction of the multimodal knowledge graph. The constructed knowledge graph includes multimodal data from the construction site, allowing for quick querying of on-site safety incidents, providing safety managers with a valuable tool for site management.},
  month = {aug},
}

@inproceedings{safaei_kglm-qa_2024,
  title = {{KGLM},
  author = {Safaei, Alireza Akhavan and Saboori, Pegah and Ramezani, Reza and Nematbakhsh, Mohammadali},
  year = {2024},
  doi = {10.1109/IKT65497.2024.10892783},
  url = {https://doi.org/10.1109/ikt65497.2024.10892783},
  booktitle = {2024 15th {International},
  pages = {234--240},
  note = {ISSN: 2476-2180},
  keywords = {Accuracy, Adaptation models, Data mining, Heavily-tailed distribution, Knowledge graph, Knowledge graphs, Large language models, Large Language Models, Navigation, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Robustness, Semantics, source: IEEE},
  abstract = {Large language models excel in various natural language processing tasks but often struggle with knowledge-intensive queries, particularly those involve rare entities or require precise factual information. This paper presents a novel framework that enhances capabilities of an LLM-based question answering system by incorporating structured knowledge from knowledge graphs. Our approach employs entity extraction, semantic similarity scoring, and adaptive graph exploration to efficiently navigate and extract relevant information from knowledge graphs. The core of the presented solution is a knowledge graph-enhanced language model process that iteratively refines subgraph exploration and answer generation, complemented by a fallback mechanism for robustness across diverse question types. Experiments on location-based questions from the Entity Questions dataset demonstrate significant improvements in the quality of responses. Using the Gemini 1.5 Flash model, our system achieved an accuracy increase from 36\% to 71\% for partially correct answers and from 22\% to 69\% for exactly correct answers, as evaluated by human assessors. This approach offers a promising direction for developing more reliable and accurate question answering systems, particularly for queries involving long-tail entities or specific factual knowledge.},
  month = {dec},
}

@inproceedings{fichtenkamm_towards_2024,
  title = {Towards an {FA},
  author = {Fichtenkamm, Maik and Kofler, Markus and Schekotihin, Konstantin and Burmer, Christian},
  year = {2024},
  doi = {10.1109/IPFA61654.2024.10691083},
  url = {https://doi.org/10.1109/ipfa61654.2024.10691083},
  booktitle = {2024 {IEEE},
  pages = {1--8},
  note = {ISSN: 1946-1550},
  keywords = {AI, Analytical models, Databases, Failure analysis, generative models, Knowledge engineering, Large language models, Measurement, Pipelines, retrieval-augmented generation, source: IEEE},
  abstract = {Failure Analysis (FA) data storages, like databases or file shares, host a lot of textual data comprising important information about products, best practices, or past cases. FA engineers require this information at their fingertips to accomplish their tasks efficiently. However, common keyword search interfaces provided by most information systems and databases are insufficient as they force an engineer to formulate a correct query, read returned documents, and manually extract required knowledge from them. In this paper, we suggest a cost-effective approach that augments retrieval systems with the capabilities of modern Large Language Models (LLMs) to provide straight-to-point answers to engineers’ questions. Preliminary evaluation shows that the suggested system can generate high-quality responses to a set of simple benchmark queries but also lacks complex reasoning capabilities for answering complex queries.},
  month = {jul},
}

@inproceedings{pang_remed_2024,
  title = {{REMED},
  author = {Pang, Tianqi and Tan, Kehui and Yao, Yujun and Liu, Xiangyang and Meng, Fanlong and Fan, Chenyou and Zhang, Xiaofan},
  year = {2024},
  doi = {10.1109/IJCNN60899.2024.10651011},
  url = {https://doi.org/10.1109/ijcnn60899.2024.10651011},
  booktitle = {2024 {International},
  pages = {1--8},
  note = {ISSN: 2161-4407},
  keywords = {Accuracy, Contrastive Learning, Data privacy, Large language models, Large Language Models, Medical Dataset, Medical Document Retrieval, Medical services, Neural networks, Refining, Vectors, source: IEEE},
  abstract = {While advanced Large Language Models (LLMs) exhibit considerable promise, their tendency to generate unreliable information poses significant challenges, particularly in high-risk domains like healthcare. However, the advent of Retrieval-Augmented Generation (RAG) offers a novel solution tailored for the medical realm. This study further enhances retrieval accuracy by introducing REMED, a specialized medical document retrieval framework designed to address the hallucination problem prevalent in LLMs. The REMED framework integrates dataset construction, an efficient embedding fine-tuning EM-FT model, retrieval-augmented generation, and human evaluation of LLM responses. The EM-FT model can end-to-end fine-tune the medical sentence representations in large pre-trained models through an efficient embedding fine-tuning method, thereby enhancing the performance of medical retrieval. We adopt contrastive learning as the loss function to optimize the performance of the EM-FT model, enabling it to accurately capture the similarity between query and relevant documents. This approach not only improves the retrieval accuracy of positively related contents but also effectively reduces the matching with negatively related contents. Compared to direct dense vector retrieval, fine-tuning query and content vectors first and then performing dense retrieval tasks significantly improved the performance. Through validation on two datasets, we demonstrate that our EM-FT method improves recall and precision on MMD by 3.2\%-6.0\% and on MPD by 14.4\%-42.6\% compared to using the embedding model directly for retrieval. Furthermore, through human evaluation on the PULSE-7Bv5 model, we further confirm the effectiveness of our retrieval results in improving the quality of generated text.},
  month = {jun},
}

@inproceedings{wei_unleashing_2025,
  title = {Unleashing the {Power},
  author = {Wei, Haiyang and Chen, Ligeng and Du, Zhengjie and Wu, Yuhan and Huang, Haohui and Liu, Yue and Cheng, Guang and Xu, Fengyuan and Wang, Linzhang and Mao, Bing},
  year = {2025},
  doi = {10.1109/IWQoS65803.2025.11143461},
  url = {https://doi.org/10.1109/iwqos65803.2025.11143461},
  booktitle = {2025 {IEEE},
  pages = {1--10},
  note = {ISSN: 2766-8568},
  keywords = {Codes, Large language models, Large Language Models, Protocol Reverse Engineering, Protocols, Reverse engineering, Security, Semantics, Software, Software Security, Source coding, State Machine, Static analysis, Syntactics, source: IEEE},
  abstract = {State machines are essential for enhancing protocol analysis to identify vulnerabilities. However, inferring state machines from network protocol implementations is challenging due to complex code syntax and semantics. Traditional dynamic analysis methods often miss critical state transitions due to limited coverage, while static analysis faces path explosion issues. To overcome these challenges, we introduce a novel state machine inference approach utilizing Large Language Models (LLMs), named ProtocolGPT. This method employs retrieval augmented generation technology to enhance a pre-trained model with specific knowledge from protocol implementations. Through effective prompt engineering, we accurately identify and infer state machines. To the best of our knowledge, our approach represents the first state machine inference that leverages the source code of protocol implementations. Our evaluation of six protocol implementations shows that our method achieves a precision of over 90 \%, outperforming the baselines by more than 30 \%. Furthermore, integrating our approach with protocol fuzzing improves coverage by more than 20 \% and uncovers two 0-day vulnerabilities compared to baseline methods.},
  month = {jul},
}

@inproceedings{hao_rap_2025,
  title = {{RAP},
  author = {Hao, Haoran and Han, Jiaming and Li, Changsheng and Li, Yu-Feng and Yue, Xiangyu},
  year = {2025},
  doi = {10.1109/CVPR52734.2025.01355},
  url = {https://doi.org/10.1109/cvpr52734.2025.01355},
  booktitle = {2025 {IEEE},
  pages = {14538--14548},
  note = {ISSN: 2575-7075},
  keywords = {Databases, Image recognition, Large language models, multimodal large language models, Oral communication, Pattern recognition, personalization, Pipelines, Question answering (information retrieval), Real-time systems, retrieval-augmented generation, Training, Visualization, source: IEEE},
  abstract = {The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human’s daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs’ personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user’s name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts’ information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/.},
  month = {jun},
}

@inproceedings{shiri_decompose_2024,
  title = {Decompose, {Enrich},
  author = {Shiri, Fatemeh and Moghimifar, Farhad and Haffari, Reza and Li, Yuan-Fang and Nguyen, Van and Yoo, John},
  year = {2024},
  doi = {10.23919/FUSION59988.2024.10706385},
  url = {https://doi.org/10.23919/fusion59988.2024.10706385},
  booktitle = {2024 27th {International},
  pages = {1--8},
  keywords = {Accuracy, Benchmark testing, Cognition, Data mining, Decision making, event argument extraction, Event detection, event extraction, information extraction, Knowledge graphs, Large language models, Natural languages, Reliability, source: IEEE},
  abstract = {Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method’s superior performance compared to baseline approaches.},
  month = {jul},
}

@inproceedings{bakharia_shaping_2024,
  title = {Shaping {Programming},
  author = {Bakharia, Aneesha and Abdi, Solmaz},
  year = {2024},
  doi = {10.1109/ICALT61570.2024.00040},
  url = {https://doi.org/10.1109/icalt61570.2024.00040},
  booktitle = {2024 {IEEE},
  pages = {116--120},
  note = {ISSN: 2161-377X},
  keywords = {AI-assisted programming, Data science, Data science education, Debugging, Education, Encoding, Ethics, Market research, Programming curriculum development, Programming education, Reviews, source: IEEE},
  abstract = {As GenAI technologies, particularly Large Language Models (LLMs), continue to revolutionize programming and data science, it is increasingly vital for educators to adapt computer science curricula. This paper presents a review of recent technical books on AI-Assisted programming and utilizes the findings to guide curriculum changes in higher education. Our analysis underscores the necessity for novel teaching strategies, emphasizing skills like problem decomposition, top-down design, and advanced debugging. Furthermore, it emphasizes the crucial expansion of curricula to encompass courses on developing applications based on LLMs, utilizing libraries such as LangChain and incorporating Retrieval Augmented Generation functionality. Our analysis reveals a significant gap in technical literature regarding the ethical and societal impacts of GenAI, highlighting the urgent need for programming curricula to evolve and equip students with the skills required to ethically develop AI-enhanced software products. This paper advocates for curriculum development that not only aligns with the latest industry trends but also contributes to research on AI-assisted coding and its future impact.},
  month = {jul},
}

@inproceedings{paket_it_2024,
  title = {{IT},
  author = {Paket, Ezgi and Şenerkek, Göksu and Akyol, Fatma Betül and Salman, Furkan},
  year = {2024},
  doi = {10.1109/UBMK63289.2024.10773473},
  url = {https://doi.org/10.1109/ubmk63289.2024.10773473},
  booktitle = {2024 9th {International},
  pages = {135--140},
  note = {ISSN: 2521-1641},
  keywords = {Companies, Computational modeling, Computer science, Large language models, large language models (LLMs), multi-class classification, Next generation networking, service desk, ticket classification, source: IEEE},
  abstract = {Service desk systems are utilized in companies to enable employees to forward their issues or requests to IT support operators. These tickets are assigned by support operators to the appropriate categories based on their personal experiences and the manually prepared keyword-matching catalogs. The dependency on individual experiences and the difficulty of maintaining and updating manually prepared catalogs increase the misclassification rate. This study aims to address a challenging multi-class classification problem in the Turkish language, which includes 172 classes, by utilizing large language models (LLMs). For this purpose, next-generation LLMs such as Titan, Llama, and Mistral are compared with BERT-based models using zero-shot, fine-tuning, and RAG methods within this st udy. This research shows that BERT-based classification models outperform the LLMs, which are not specifically trained for classification tasks with several classes.},
  month = {oct},
}

@inproceedings{wang_construction_2025,
  title = {Construction and {Application},
  author = {Wang, Junyao and Duan, Tongle and Yao, Sen and Wang, Yasong},
  year = {2025},
  doi = {10.1109/CAIBDA65784.2025.11182762},
  url = {https://doi.org/10.1109/caibda65784.2025.11182762},
  booktitle = {2025 5th {International},
  pages = {399--405},
  keywords = {source: IEEE},
  abstract = {The rapid expansion of multimodal content on social media brings both prospects and challenges to public opinion analysis. While such data provides deep insights into collective sentiment and societal dynamics, it also heightens the risks of misinformation spread and cognitive overload for analytical models. To tackle these issues, this paper presents a new approach to constructing a multimodal knowledge graph enabled by large language models, aiming to boost reasoning, traceability, and verifiability in social media opinion analysis. The proposed architecture incorporates LLM-enhanced multimodal semantic alignment, event-centric temporal relation extraction, and cross-modal confidence calibration to construct dynamic knowledge graphs for evolving public discourse. Experiments on a curated dataset of real-world events, which includes adversarial samples with inserted logical inconsistencies, demonstrate that our method outperforms traditional RAG approaches. Specifically, it achieves a 23.9 \% improvement in F1 score for information tracing and an 8.0 \% increase in accuracy for misinformation detection. These results validate the framework's capacity to reduce information overload and identify latent logical inconsistencies. However, challenges remain in real-time adaptation and resolving deep semantic contradictions, indicating future paths in combining dynamic network structures and domain-specific knowledge for early misinformation containment.},
  month = {jun},
}

@inproceedings{barros_anchor_2024,
  title = {Anchor {Your},
  author = {Barros, Ramiro N. and Arguello, Kristen K. and Wehrmann, Jônatas},
  year = {2024},
  doi = {10.1109/IJCNN60899.2024.10650518},
  url = {https://doi.org/10.1109/ijcnn60899.2024.10650518},
  booktitle = {2024 {International},
  pages = {1--8},
  note = {ISSN: 2161-4407},
  keywords = {anchor embeddings, large language models, Large language models, Neural networks, Pipelines, RAG, retrieval, Robustness, Scalability, Semantics, Storms, source: IEEE},
  abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing with their remarkable ability to generate coherent responses. Despite their impressive capabilities, LLMs grapple with the challenges of learning from private data while ensuring the relevance and timeliness of the information they provide. To overcome this challenge, retrieval-based strategies for generation have become essential, though they require clean data and a complex indexing pipeline. In this paper, we introduce Anchor Embeddings, a novel embedding enhancing technique designed to mitigate the instance-to-document semantic gap that often hinders the retrieval process. Our method proposes the usage of an anchor embedding to serve as a semantic beacon, adding more context to smaller text segments extracted from the same source. This extra embedding acts as a holistic representation regarding the original document that is merged into the instance embeddings. It can be derived based on a diversity of strategies that involve semantic representation and localization cues. Our empirical analysis on the MS MARCO dataset reveals that Anchor Embeddings can significantly outperform traditional retrieval methods, boasting up to an 14\% performance improvement. The elegance of our approach lies in its simplicity and robustness, providing more specific context while maintaining the same time complexity for retrieval of the baseline approach.},
  month = {jun},
}

@inproceedings{s_emotionsyncai-driven_2025,
  title = {{EmotionSync},
  author = {s, Atchaya and L, Sharmila and B, Pavithra and S, Nisha A},
  year = {2025},
  doi = {10.1109/ICAECA63854.2025.11012546},
  url = {https://doi.org/10.1109/icaeca63854.2025.11012546},
  booktitle = {2025 3rd {International},
  pages = {1--6},
  keywords = {Adaptation models, Artificial intelligence, Context Aware AI, Context modeling, Emotion Recognition, MAB, Mental Well-Being, Productivity, RAG, Recommender systems, Schedules, Speech recognition, Telecommunication computing, Transformer Models, Transformers, Videos, source: IEEE},
  abstract = {What if AI has the ability to sense your emotions in addition to hearing what you say? Emotions are the language of the body, thoughts are the language of the mind, and words are the link between them; they convey sentiment, meaning, and purpose. AI, however, has trouble understanding this synergy; this paper presents Emotion Syn, an AI-powered Emotion-Aware Recommendation System that turns speech into tailored insights. It uses OpenAI Whisper to transcribe speech inputs, Named Entity Recognition (NER) to extract context, and a refined EmoRoBERTa model (93\% accuracy) to identify emotions. A RAG based LLM enhances personalization by retrieving relevant knowledge from books, past interactions, and external sources. Transformer models (T5) refine suggestions and Multi-Armed Bandit (MAB) algorithms produce adaptive task and well-being recommendations, dynamically prioritizing schedules through interaction with Google Calendar. The system is a fluid, emotion-aware AI companion that comprehends not just what people say but also how they feel thanks to an ongoing feedback loop that improves ideas. Emotion Syn converts emotional intelligence into action, bridging the gap between wellbeing and productivity.},
  month = {apr},
}

@article{shao_wirelessllm_2024,
  title = {{WirelessLLM},
  author = {Shao, Jiawei and Tong, Jingwen and Wu, Qiong and Guo, Wei and Li, Zijian and Lin, Zehong and Zhang, Jun},
  year = {2024},
  doi = {10.23919/JCIN.2024.10582827},
  url = {https://doi.org/10.23919/jcin.2024.10582827},
  journal = {Journal of Communications and Information Networks},
  volume = {9},
  number = {2},
  pages = {99--112},
  keywords = {Adaptation models, Communication system security, Data models, large language models, multi-modal models, power allocation, protocol understanding, Sensors, spectrum sensing, Task analysis, Wireless communication, wireless communications, Wireless sensor networks, source: IEEE},
  abstract = {The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in large language models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.},
  issn = {2509-3312},
  month = {jun},
}

@inproceedings{kurniawan_ai_2024,
  title = {{AI},
  author = {Kurniawan, Donny and Hiererra, Siti Elda},
  year = {2024},
  doi = {10.1109/ICISS62896.2024.10751371},
  url = {https://doi.org/10.1109/iciss62896.2024.10751371},
  booktitle = {2024 {International},
  pages = {1--6},
  keywords = {Accuracy, artificial intelligence, Artificial intelligence, Chatbots, Complexity theory, Interviews, Law, legal assistance, legal literacy, Privacy, Reviews, Technological innovation, User experience, source: IEEE},
  abstract = {The purpose of this paper is to discuss the importance of integrating artificial intelligence (AI) into the legal sector to address the challenge of legal complexity and initiating the development of customized AI Legal Companion as the solution to improve access to justice, enhance legal literacy among the public and introduce new innovations to the legal field. Qualitative research is used during the research and an interview is conducted with the legal expert to get deeper information from the legal perspective in utilizing AI in legal sectors. To improve the AI knowledge base, the utilization of advanced AI techniques, Retrieval Augmented Generation (RAG) is used to provide the AI technologies, particularly ChatGPT models to provide quick, more accurate, and easily understandable legal information to the users and help people with lack of legal information with legal assistance and education. The paper focuses on the potential of AI being implemented in the legal domain and emphasizes the importance of collaboration between legal experts and technologies. The result of this paper shows that AI can provide easy access to justice and legal literacy for the public, but the legal sector is enormous therefore, human legal assistance is still important to address the misinformation provided by the AI.},
  month = {sep},
}

@article{liu_toward_2025,
  title = {Toward {Autonomous},
  author = {Liu, Teng and Huang, Xiaohong and Xie, Kun},
  year = {2025},
  doi = {10.1109/MNET.2025.3583298},
  url = {https://doi.org/10.1109/mnet.2025.3583298},
  journal = {IEEE Network},
  volume = {39},
  number = {5},
  pages = {21--29},
  keywords = {Accuracy, Agent, Automation, Codes, Collaboration, Intent recognition, Knowledge based systems, Large language models, Large Language Models, NETCONF, Network Management, Network topology, Security, Semantic communication, source: IEEE},
  abstract = {The increasing scale and complexity of network infrastructures demand efficient and secure automated configuration solutions. Existing approaches often face challenges such as incompatibility with devices from different manufacturers, limited flexibility in network resource allocation, high demands for personnel expertise, and insufficient security in configuration content. To address these issues, we propose an innovative agent-based framework that leverages large language models (LLMs) to achieve secure and intelligent network configuration. The framework integrates a multi-agent system, which includes: an intent recognition and rewriting agent that uses the Chain of Thought (CoT) method to accurately interpret user intent; a self-reflection retrieval agent that combines hybrid retrieval methods with self-reflection mechanisms to optimize knowledge retrieval; a security agent specialized in network security verification; and a content integration agent that generates final configuration files. Experimental results on a custom dataset demonstrate transformative improvements in accuracy, efficiency, and security. Our framework substantially outperforms both original large language models and strong Retrieval-Augmented Generation (RAG) baselines, with metric scores such as CodeBLEU and BLEU increasing by over 44\% compared to the RAG baseline.},
  issn = {1558-156X},
  month = {sep},
}

@inproceedings{ghassel_are_2024,
  title = {Are {Large},
  author = {Ghassel, Abdellah and Zhu, Xiaodan and Thomas, Stephen W.},
  year = {2024},
  doi = {10.1109/CCECE59415.2024.10667232},
  url = {https://doi.org/10.1109/ccece59415.2024.10667232},
  booktitle = {2024 {IEEE},
  pages = {674--679},
  note = {ISSN: 2576-7046},
  keywords = {Companies, Computational modeling, conversational artificial intelligence, dialogue breakdown, Electric breakdown, Error analysis, Industries, large language models, Large language models, Oral communication, source: IEEE},
  abstract = {This study addresses the challenge of dialogue breakdown—characterized as incoherence, irrelevance, or any disruption that significantly hampers the flow of the conversation. The impact of dialogue breakdowns has become critical with the adoption of large language models in various industries for companies with dialogue-based systems, such as Salesforce, Amazon, and Microsoft. Leveraging the Dialogue Breakdown Detection Challenge Dataset, we investigate the performance of generalist large language models, including ChatGPT, GPT-4, and Mistral-Medium, in identifying instances of dialogue breakdown without domain-specific fine-tuning. Through a series of experiments employing zero-shot and few-shot prompting techniques combined with chain-of-thought reasoning, this research sets a new benchmark in the field. Our findings reveal that GPT-4 outperforms both specialized models previously considered state-of-the-art and other generalist models in detecting dialogue breakdowns by over a 3\% margin, achieving an accuracy of 82.0\%. To our knowledge, this study is the first to demonstrate the enhanced capability of generalist large language models in this domain. Our experiments found that when detecting dialogue breakdowns, larger models like GPT-4 are less sensitive to how they are prompted. In contrast, smaller models like ChatGPT and Mistral-Medium can improve their performance by using prompting techniques that combine few-shot learning with the chain-of-thought method. This work proposes future research directions, including enhanced error analysis and developing an Ensemble RAG approach for improved generalization in dialogue breakdown detection.},
  month = {aug},
}

@article{chakraborty_scalable_2025,
  title = {A scalable framework for evaluating multiple language models through cross-domain generation and hallucination detection},
  author = {Chakraborty, Sorup and Chowdhury, Rajesh and Shuvo, Sourov Roy and Chatterjee, Rajdeep and Roy, Satyabrata},
  year = {2025},
  doi = {10.1038/s41598-025-15203-5},
  url = {https://doi.org/10.1038/s41598-025-15203-5},
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lahiri_alzheimerrag_2025,
  title = {{AlzheimerRAG},
  author = {Lahiri, Aritra Kumar and Hu, Qinmin Vivian},
  year = {2025},
  doi = {10.3390/make7030089},
  url = {https://doi.org/10.3390/make7030089},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {7},
  number = {3},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mohammed_aftina_2025,
  title = {Aftina: enhancing stability and preventing hallucination in {AI},
  author = {Mohammed, Marryam Yahya and Ali, Sama Ayman and Ali, Salma Khaled and Majeed, Ayad Abdul and Mohamed, Ensaf Hussein},
  year = {2025},
  doi = {10.1007/s00521-025-11229-y},
  url = {https://doi.org/10.1007/s00521-025-11229-y},
  journal = {Neural Computing and Applications},
  volume = {37},
  number = {25},
  pages = {20957 -- 20982},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ahn_guide_2025,
  title = {A guide to evade hallucinations and maintain reliability when using large language models for medical research: a narrative review},
  author = {Ahn, Sangzin},
  year = {2025},
  doi = {10.6065/apem.2448278.139},
  url = {https://doi.org/10.6065/apem.2448278.139},
  journal = {Annals of Pediatric Endocrinology and Metabolism},
  volume = {30},
  number = {3},
  pages = {115 -- 118},
  note = {Type: Review},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gokcimen_novel_2025,
  title = {A novel system for strengthening security in large language models against hallucination and injection attacks with effective strategies},
  author = {Gokcimen, Tunahan and Daş, Bihter},
  year = {2025},
  doi = {10.1016/j.aej.2025.03.030},
  url = {https://doi.org/10.1016/j.aej.2025.03.030},
  journal = {Alexandria Engineering Journal},
  volume = {123},
  pages = {71 -- 90},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{vrdoljak_review_2025,
  title = {A {Review},
  author = {Vrdoljak, Josip and Boban, Zvonimir and Vilović, Marino and Kumrić, Marko and Božić, Joško},
  year = {2025},
  doi = {10.3390/healthcare13060603},
  url = {https://doi.org/10.3390/healthcare13060603},
  journal = {Healthcare (Switzerland)},
  volume = {13},
  number = {6},
  note = {Type: Review},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ma_knowledge-graph_2025,
  title = {A knowledge-graph enhanced large language model-based fault diagnostic reasoning and maintenance decision support pipeline towards industry 5.0},
  author = {Ma, Yunfei and Zheng, Shuai and Yang, Zheng and Pan, Hongcheng and Hong, Jun},
  year = {2025},
  doi = {10.1080/00207543.2025.2472298},
  url = {https://doi.org/10.1080/00207543.2025.2472298},
  journal = {International Journal of Production Research},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bhattacharya_study_2025,
  title = {A {Study},
  author = {Bhattacharya, Kausik and Majumder, Anubhab and Chakrabarti, Amaresh},
  year = {2025},
  doi = {10.1007/978-981-96-5511-3_39},
  url = {https://doi.org/10.1007/978-981-96-5511-3_39},
  journal = {Lecture Notes in Mechanical Engineering},
  pages = {505 -- 519},
  note = {Type: Conference paper},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{malin_review_2025,
  title = {A review of faithfulness metrics for hallucination assessment in {Large},
  author = {Malin, Ben and Kalganova, Tatiana G. and Boulgouris, Nikolaos V.},
  year = {2025},
  doi = {10.1109/JSTSP.2025.3579203},
  url = {https://doi.org/10.1109/jstsp.2025.3579203},
  journal = {IEEE Journal on Selected Topics in Signal Processing},
  note = {Type: Review},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{rajapaksha_rag-based_2025,
  title = {A {RAG},
  author = {Rajapaksha, Sampath and Rani, Ruby and Karafili, Erisa},
  year = {2025},
  doi = {10.1007/978-3-031-82362-6_15},
  url = {https://doi.org/10.1007/978-3-031-82362-6_15},
  journal = {Lecture Notes in Computer Science},
  volume = {15264 LNCS},
  pages = {238 -- 256},
  note = {Type: Conference paper},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ieva_retrieval-augmented_2024,
  title = {A {Retrieval},
  author = {Ieva, Saverio and Loconte, Davide and Loseto, Giuseppe and Ruta, Michele and Scioscia, Floriano and Marche, Davide and Notarnicola, Marianna},
  year = {2024},
  doi = {10.3390/smartcities7060121},
  url = {https://doi.org/10.3390/smartcities7060121},
  journal = {Smart Cities},
  volume = {7},
  number = {6},
  pages = {3095 -- 3120},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{parmanto_reliable_2024,
  title = {A {Reliable},
  author = {Parmanto, Bambang and Aryoyudanta, Bayu and Soekinto, Timothius Wilbert and Setiawan, I. Made Agus and Wang, Yuhan and Hu, Haomin and Saptono, Andi and Choi, Yong K.},
  year = {2024},
  doi = {10.2196/54633},
  url = {https://doi.org/10.2196/54633},
  journal = {JMIR Formative Research},
  volume = {8},
  note = {Type: Article},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ghanbari_haez_retrieval-augmented_2024,
  title = {A {Retrieval},
  author = {Ghanbari Haez, Saba and Segala, Marina and Bellan, Patrizio and Magnolini, Simone and Sanna, Leonardo and Consolandi, Monica and Dragoni, Mauro},
  year = {2024},
  doi = {10.1007/978-3-031-66538-7_22},
  url = {https://doi.org/10.1007/978-3-031-66538-7_22},
  journal = {Lecture Notes in Computer Science},
  volume = {14844 LNAI},
  pages = {213 -- 223},
  note = {Type: Conference paper},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{essafi_genai_2025,
  title = {{GenAI},
  author = {Essafi, H.},
  year = {2025},
  doi = {10.1007/978-3-032-05607-8_3},
  url = {https://doi.org/10.1007/978-3-032-05607-8_3},
  journal = {International Conference on Flexible Query Answering …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Our aim is to highlight the key elements leading to the hallucination and impacting the reliability and trustworthiness of LLM-based AI. We will also discuss the mitigation solutions. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fan_survey_2024,
  title = {A survey on rag meeting llms: {Towards},
  author = {Fan, W. and Ding, Y. and Ning, L. and Wang, S. and Li, H. and Yin, D. and {...},
  year = {2024},
  doi = {10.1145/3637528.3671470},
  url = {https://doi.org/10.1145/3637528.3671470},
  journal = {Proceedings of the 30th …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… ing inherent limitations such as hallucinations and out-of-… RAG to enhance the in-context learning ability of ChatGPT for molecular discovery [68]. It is also been demonstrated that RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{amugongo_retrieval_2025,
  title = {Retrieval augmented generation for large language models in healthcare: {A},
  author = {Amugongo, L. M. and Mascheroni, P. and Brooks, S. and {...},
  year = {2025},
  doi = {10.1371/journal.pdig.0000877},
  url = {https://doi.org/10.1371/journal.pdig.0000877},
  journal = {PLOS Digital …},
  note = {Publisher: journals.plos.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… With RAG, the LLM performance is contextually bolstered without performing computationally … enhance the properties of RAG, it is not clear how this can reduce hallucination. A recent …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fink_retrieval-augmented_2025,
  title = {Retrieval-augmented generation improves precision and trust of a {GPT},
  author = {Fink, A. and Nattenmüller, J. and Rau, S. and Rau, A. and Tran, H. and {...},
  year = {2025},
  doi = {10.1007/s00330-025-11445-z},
  url = {https://doi.org/10.1007/s00330-025-11445-z},
  journal = {European …},
  note = {Publisher: Springer
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Even correct chatbot answers can be subject to inaccuracies and hallucinations [10], and the … Retrieval-augmented generation (RAG) is an approach that increases LLM performance by …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_mitigating_2024,
  title = {Mitigating {Hallucinations},
  author = {Li, A. and Shrestha, R. and Jegatheeswaran, T. and Chan, H. O. and Hong, C. and {...},
  year = {2024},
  doi = {10.1101/2024.09.27.24314506.abstract},
  url = {https://doi.org/10.1101/2024.09.27.24314506.abstract},
  journal = {medRxiv},
  note = {Publisher: medrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This study aims to assess the accuracy and effectiveness of RAG-integrated ChatGPT responses in organizing medical information. The information will be organized in a consultation …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ng_rag_2025,
  title = {{RAG},
  author = {Ng, K. K. Y. and Matsuba, I. and Zhang, P. C.},
  year = {2025},
  doi = {10.1056/AIra2400380},
  url = {https://doi.org/10.1056/aira2400380},
  journal = {Nejm Ai},
  note = {Publisher: ai.nejm.org},
  keywords = {source: Google Scholar},
  abstract = {… decisions is essential to building trust and accountability. Lastly, by grounding the LLM’s output in retrieved context, RAG can reduce the risks of hallucination and improve the accuracy …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{low_answering_2025,
  title = {Answering real-world clinical questions using large language model, retrieval-augmented generation, and agentic systems},
  author = {Low, Y. S. and Jackson, M. L. and Hyde, R. J. and Brown, R. E. and {...},
  year = {2025},
  doi = {10.1177/20552076251348850},
  url = {https://doi.org/10.1177/20552076251348850},
  journal = {Digital …},
  note = {Publisher: journals.sagepub.com},
  keywords = {source: Google Scholar},
  abstract = {… Additionally, to facilitate evaluation and understand LLM reasoning, we specifically asked the models to cite any referenced studies and respond with “I do not know the answer” when …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{klesel_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Klesel, M. and Wittmann, H. F.},
  year = {2025},
  doi = {10.1007/s12599-025-00945-3},
  url = {https://doi.org/10.1007/s12599-025-00945-3},
  journal = {Business \&Information Systems Engineering},
  note = {Publisher: Springer
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… LLM Fine-Tuning, and RAG. In the first scenario (Foundation Model), all training data results in an LLM … hallucinations in this manuscript to ensure consistency with the relevant literature. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gargari_enhancing_2025,
  title = {Enhancing medical {AI},
  author = {Gargari, O. K. and Habibi, G.},
  year = {2025},
  doi = {10.1177/20552076251337177},
  url = {https://doi.org/10.1177/20552076251337177},
  journal = {Digital health},
  note = {Publisher: journals.sagepub.com},
  keywords = {source: Google Scholar},
  abstract = {… The study evaluated Almanac against ChatGPT using a … Almanac significantly outperformed ChatGPT in factuality, with an … clinical calculation scenarios, whereas ChatGPT failed all. In …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hou_enhancing_2024,
  title = {Enhancing dietary supplement question answer via retrieval-augmented generation ({RAG},
  author = {Hou, Y. and Zhang, R.},
  year = {2024},
  doi = {10.1101/2024.09.11.24313513.abstract},
  url = {https://doi.org/10.1101/2024.09.11.24313513.abstract},
  journal = {medRxiv},
  note = {Publisher: medrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… data from multiple trusted sources, … with a RAG system, leveraging the strengths of large language models (LLMs) and a biomedical knowledge graph (BKG) to address the hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_evaluation_2025,
  title = {Evaluation of retrieval-augmented generation and large language models in clinical guidelines for degenerative spine conditions},
  author = {Su, A. Y. and Knebel, A. and Xu, A. Y. and Kaper, M. and Schmitt, P. and {...},
  year = {2025},
  doi = {10.1007/s00586-025-08994-8},
  url = {https://doi.org/10.1007/s00586-025-08994-8},
  journal = {European Spine …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… traditional LLM. Thus, this article aims to evaluate the concordance of ChatGPT-4o (a traditional LLM) and NotebookLM (a RAG-… of AI hallucinations, wherein LLMs such as ChatGPT-4o “…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{soong_improving_2024,
  title = {Improving accuracy of {GPT},
  author = {Soong, David and Sridhar, Sriram and Si, Han and Wagner, Jan Samuel and Sá, Ana Caroline Costa and Yu, Christina Y. and Karagoz, Kubra and Guan, Meijian and Kumar, Sanyam and Hamadeh, Hisham K. and Higgs, Brandon W.},
  year = {2024},
  doi = {10.1371/journal.pdig.0000568},
  url = {https://doi.org/10.1371/journal.pdig.0000568},
  journal = {PLOS Digital Health},
  volume = {3},
  number = {8},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Here we presented application of a retrieval-augmented generation (RAG) LLM, which … to minimize inclusion of non-factual information from the LLM. This generated k answers which …},
  annote = {Cited by: 11; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{wiratunga_cbr-rag_2024,
  title = {{CBR},
  author = {Wiratunga, N. and Abeyratne, R. and Jayawardena, L. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-63646-2_29},
  url = {https://doi.org/10.1007/978-3-031-63646-2_29},
  journal = {… Conference on Case …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) systems address this by presenting the LLM with factual … In this paper we have presented CBR-RAG, improving LLM output by augmenting input …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhou_ihillm-rag_2025,
  title = {{IHILLM},
  author = {Zhou, Z. and Yang, Y. and Ren, T.},
  year = {2025},
  doi = {10.1117/12.3056789.short},
  url = {https://doi.org/10.1117/12.3056789.short},
  journal = {Fourth International Computational …},
  note = {Publisher: spiedigitallibrary.org},
  keywords = {source: Google Scholar},
  abstract = {… phenomenon of hallucination. … LLM, so as to enhance the LLM’s learning and memory of the user’s own habits and improve the degree of personalization. We evaluated IHILLM-RAG on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_evaluation_2024,
  title = {Evaluation of retrieval-augmented generation: {A},
  author = {Yu, H. and Gan, A. and Zhang, K. and Tong, S. and Liu, Q. and Liu, Z.},
  year = {2024},
  doi = {10.1007/978-981-96-1024-2_8},
  url = {https://doi.org/10.1007/978-981-96-1024-2_8},
  journal = {CCF Conference on Big Data},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… ReEval [66] specifically targets hallucination evaluation by employing a cost-effective LLM-based framework that utilizes prompt chaining to create dynamic test cases. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mao_fit-rag_2025,
  title = {{FIT},
  author = {Mao, Y. and Dong, X. and Xu, W. and Gao, Y. and Wei, B. and {...},
  year = {2025},
  doi = {10.1145/3676957},
  url = {https://doi.org/10.1145/3676957},
  journal = {ACM Transactions on …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… treat a LLM as a black-box (ie, freeze the parameters of the LLM) and augment it with a retrievalaugmented generation (RAG) system, namely black-box RAG. Recently, black-box …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{debellis_integrating_2024,
  title = {Integrating ontologies and large language models to implement retrieval augmented generation},
  author = {DeBellis, M. and Dutta, N. and Gino, J. and Balaji, A.},
  year = {2024},
  doi = {10.1177/15705838241296446},
  url = {https://doi.org/10.1177/15705838241296446},
  journal = {Applied Ontology},
  note = {Publisher: journals.sagepub.com},
  keywords = {source: Google Scholar},
  abstract = {… problems is retrieval augmented generation (RAG). In a RAG architecture, the LLM is utilized to … The RAG architecture prevents hallucinations by utilizing a nearest neighbor algorithm to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yazaki_emergency_2025,
  title = {Emergency patient triage improvement through a retrieval-augmented generation enhanced large-scale language model},
  author = {Yazaki, M. and Maki, S. and Furuya, T. and Inoue, K. and {...},
  year = {2025},
  doi = {10.1080/10903127.2024.2374400},
  url = {https://doi.org/10.1080/10903127.2024.2374400},
  journal = {Prehospital …},
  note = {Publisher: Taylor \&Francis},
  keywords = {source: Google Scholar},
  abstract = {… The purpose of this study was to control the hallucinations of … a LLM enhanced with Retrieval Augmented Generation (RAG… demonstrated that the LLM integrated with RAG is effective in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{bazzi_wonders_2025,
  title = {The {Wonders},
  author = {Bazzi, W. and Gaith, M.},
  year = {2025},
  doi = {10.22541/au.174197235.57492382},
  url = {https://doi.org/10.22541/au.174197235.57492382},
  publisher = {authorea.com},
  keywords = {source: Google Scholar},
  abstract = {… RAG goes beyond merely creating a smarter ChatGPT; it enables … hallucinations by augmenting with up-to-date knowledge. In summary, these instances highlight the power of RAG and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dong_journey_2024,
  title = {The journey to a knowledgeable assistant with retrieval-augmented generation (rag)},
  author = {Dong, X. L.},
  year = {2024},
  doi = {10.1145/3626246.3655999},
  url = {https://doi.org/10.1145/3626246.3655999},
  journal = {Companion of the 2024 International Conference on …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM techniques. We start with our findings from a comprehensive set of experiments to assess LLM reliability in answering factual … our federated Retrieval-Augmented Generation (RAG) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_integrating_2025,
  title = {Integrating small language models with retrieval-augmented generation in computing education: {Key},
  author = {Yu, Z. and Liu, S. and Denny, P. and Bergen, A. and Liut, M.},
  year = {2025},
  doi = {10.1145/3641554.3701844},
  url = {https://doi.org/10.1145/3641554.3701844},
  journal = {Proceedings of the 56th ACM …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… and hallucination reduction, as well as data privacy maintenance. We address the “Impossible Triangle" in RAG … While combining a LLM with RAG (LLM + RAG) techniques can also …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pradeep_ragnarok_2025,
  title = {Ragnarök: {A},
  author = {Pradeep, R. and Thakur, N. and Sharifymoghaddam, S. and {...},
  year = {2025},
  doi = {10.1007/978-3-031-88708-6_9},
  url = {https://doi.org/10.1007/978-3-031-88708-6_9},
  journal = {… on Information Retrieval},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Next, answer, provides the LLM-generated RAG answer to the user topic, presented as a top-to-bottom list of sentence-level texts with corresponding segment citations. All …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bhayana_retrieval-augmented_2024,
  title = {Retrieval-augmented generation for large language models in radiology: another leap forward in board examination performance},
  author = {Bhayana, R. and Fawzy, A. and Deng, Y. and Bleakney, R. R. and Krishna, S.},
  year = {2024},
  doi = {10.1148/radiol.241489},
  url = {https://doi.org/10.1148/radiol.241489},
  journal = {Radiology},
  note = {Publisher: pubs.rsna.org},
  keywords = {source: Google Scholar},
  abstract = {… Thus, RAG is a powerful technique for enriching LLMs with domain-specific … factual responses are critical, like answering radiology questions. Using RAG to reduce LLM hallucinations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_mitigating_2024,
  title = {Mitigating entity-level hallucination in large language models},
  author = {Su, W. and Tang, Y. and Ai, Q. and Wang, C. and Wu, Z. and Liu, Y.},
  year = {2024},
  doi = {10.1145/3673791.3698403},
  url = {https://doi.org/10.1145/3673791.3698403},
  journal = {Proceedings of the 2024 …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM hallucination doesn’t exist. To address these concerns, we evaluate existing dynamic RAG … detection of hallucinations in LLM’s outputs, utilizing an entity confidence metric derived …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_potential_2024,
  title = {Potential for {GPT},
  author = {Wang, C. and Ong, J. and Wang, C. and Ong, H. and Cheng, R. and {...},
  year = {2024},
  doi = {10.1007/s10439-023-03327-6},
  url = {https://doi.org/10.1007/s10439-023-03327-6},
  journal = {Annals of biomedical …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Via RAG, future ChatGPT engines would automatically retrieve this … retrieval-augmented generation. Clinical guidelines released after ChatGPT’s cutoff of September 2021 and trusted …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{arslan_business_2024,
  title = {Business insights using {RAG},
  author = {Arslan, M. and Munawar, S. and Cruz, C.},
  year = {2024},
  doi = {10.1080/12460125.2024.2410040},
  url = {https://doi.org/10.1080/12460125.2024.2410040},
  journal = {Journal of Decision Systems},
  note = {Publisher: Taylor \&Francis},
  keywords = {source: Google Scholar},
  abstract = {… These hallucinations can arise from an overload of data, a lack of contextual relevance, or … This comprehensive review illustrates how RAG–LLM integration has revolutionised …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kamath_retrieval-augmented_2024,
  title = {Retrieval-augmented generation},
  author = {Kamath, U. and Keenan, K. and Somers, G. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-65647-7_7},
  url = {https://doi.org/10.1007/978-3-031-65647-7_7},
  journal = {Large Language Models: A …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… This is the correct answer, which ChatGPT has now been able to report due to our RAG … , RAG systems may be as likely to hallucinate as normal LLM calls; to this end, RAG systems …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jayawardena_context_2025,
  title = {Context {Driven},
  author = {Jayawardena, L. and Liret, A. and Wiratunga, N. and Nkisi-Orji, I. and {...},
  year = {2025},
  doi = {10.1007/978-3-031-96559-3_8},
  url = {https://doi.org/10.1007/978-3-031-96559-3_8},
  journal = {… Conference on Case …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Explainable Artificial Intelligence (XAI) plays a key role in ensuring that we can place trust in AI decisions when they impact crucial areas, such as predicting the presence of a tumour [2]…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{maharana_retrieval_2025,
  title = {Retrieval augmented generation for building datasets from scientiﬁc literature.},
  author = {Maharana, P. R. and Verma, A. and Joshi, K.},
  year = {2025},
  doi = {10.1088/2515-7639/ade1fa},
  url = {https://doi.org/10.1088/2515-7639/ade1fa},
  journal = {Journal of Physics: Materials},
  note = {Publisher: iopscience.iop.org},
  keywords = {source: Google Scholar},
  abstract = {… within this article, their full citation and copyright line may not be … once published for full citation and copyright details, as … In this work we implement a RAG-LLM based approach to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bianchini_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Bianchini, F.},
  year = {2025},
  doi = {10.1007/978-3-031-92285-5_7},
  url = {https://doi.org/10.1007/978-3-031-92285-5_7},
  journal = {Engineering Information Systems with Large Language …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… to cross-check factual accuracy. When an LLM powered by RAG receives a prompt or … Instead of generating an answer solely from its pre-trained language patterns, the LLM uses …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_dynamic_2025,
  title = {Dynamic and parametric retrieval-augmented generation},
  author = {Su, W. and Ai, Q. and Zhan, J. and Dong, Q. and Liu, Y.},
  year = {2025},
  doi = {10.1145/3726302.3731692},
  url = {https://doi.org/10.1145/3726302.3731692},
  journal = {… of the 48th International ACM SIGIR …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… Dynamic RAG adaptively determines when and what to retrieve during the LLM’s … and even larger LLMs lacking retrieval capabilities, especially on tasks that require precise factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhou_gastrobot_2024,
  title = {{GastroBot},
  author = {Zhou, Q. and Liu, C. and Duan, Y. and Sun, K. and Li, Y. and Kan, H. and Gu, Z. and {...},
  year = {2024},
  doi = {10.3389/fmed.2024.1392555},
  url = {https://doi.org/10.3389/fmed.2024.1392555},
  journal = {Frontiers in …},
  note = {Publisher: frontiersin.org},
  keywords = {source: Google Scholar},
  abstract = {… To gauge credibility, we initially employ LLM to extract a set of statements S a s q . If the … Particularly, our evaluation of answer relevance does not account for factual accuracy but …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{maklad_retrieval_2025,
  title = {Retrieval augmented generation based llm evaluation for protocol state machine inference with chain-of-thought reasoning},
  author = {Maklad, Y. and Wael, F. and Elsersy, W. and Hamdi, A.},
  year = {2025},
  doi = {10.1007/978-981-96-6441-2_27},
  url = {https://doi.org/10.1007/978-981-96-6441-2_27},
  journal = {International Congress on …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… of a RAG-based agentic Large Language Model (LLM) … Our method leverages RAG and text embeddings in two stages… of such approach, improving LLM-based protocol fuzzing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{thus_exploring_2024,
  title = {Exploring generative {AI},
  author = {Thüs, D. and Malone, S. and Brünken, R.},
  year = {2024},
  doi = {10.3389/fpsyg.2024.1474892},
  url = {https://doi.org/10.3389/fpsyg.2024.1474892},
  journal = {Frontiers in Psychology},
  note = {Publisher: frontiersin.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… learning to an LLM is retrieval augmented generation (RAG), where hallucinations can be … A RAG system involves searching and retrieving documents that semantically match a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cherumanal_walert_2024,
  title = {Walert: putting conversational information seeking knowledge into action by building and evaluating a large language model-powered chatbot},
  author = {Cherumanal, S. Pathiyan and Tian, L. and {...},
  year = {2024},
  doi = {10.1145/3627508.3638309},
  url = {https://doi.org/10.1145/3627508.3638309},
  journal = {Proceedings of the …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… of an open-source LLM; (ii) monitoring the problem of hallucinations (ie, the introduction of … of LLM-based conversational systems [18]. Finally, we plan to deploy RAG approaches to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tippins_domain-specific_2024,
  title = {Domain-specific retrieval-augmented generation through token factorization: {An},
  author = {Tippins, O. and Alvarez, T. and Novak, J. and Martinez, R. and {...},
  year = {2024},
  doi = {10.36227/techrxiv.172902551.15233738},
  url = {https://doi.org/10.36227/techrxiv.172902551.15233738},
  journal = {Authorea …},
  note = {Publisher: techrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG framework, leveraging Llama, an open-source LLM, to explore the effects of this combination. Llama, as an open-source LLM, … Mancinni, “Detecting llm hallucinations using monte …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xiong_improving_2024,
  title = {Improving retrieval-augmented generation in medicine with iterative follow-up questions},
  author = {Xiong, G. and Jin, Q. and Wang, X. and Zhang, M. and Lu, Z. and {...},
  year = {2024},
  doi = {10.1142/9789819807024_0015},
  url = {https://doi.org/10.1142/9789819807024_0015},
  journal = {… 2025: Proceedings of …},
  note = {Publisher: World Scientific},
  keywords = {source: Google Scholar},
  abstract = {… knowledge, but may still hallucinate and are inflexible in the … to combine LLM reasoning with external corpora is RAG, which … into LLM to augment its answer generation. Formally, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{adejumo__2024,
  title = {… {Care},
  author = {Adejumo, P. and Thangaraj, P. M. and Dhingra, L. S. and Biswas, D. and {...},
  year = {2024},
  doi = {10.1101/2024.09.19.24313992.abstract},
  url = {https://doi.org/10.1101/2024.09.19.24313992.abstract},
  journal = {medRxiv},
  note = {Publisher: medrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… and bleeding risk when comparing structured versus the RAG-LLM approach. For all outcomes, we calculated 95\% confidence intervals using bootstrap resampling with 1,000 iterations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_privacy-preserving_2024,
  title = {A {Privacy},
  author = {Li, Y. and Li, C. and Wang, Z. and Sui, D. and Yan, J.},
  year = {2024},
  doi = {10.1007/978-981-97-9437-9_2},
  url = {https://doi.org/10.1007/978-981-97-9437-9_2},
  journal = {CCF International Conference on Natural …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… is zero, indicating that the LLM has successfully learned to distinguish similar information, thereby reducing the occurrence of erroneous outputs and hallucinations. Additionally, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{poliakov_multi-meta-rag_2024,
  title = {Multi-meta-rag: {Improving},
  author = {Poliakov, M. and Shvai, N.},
  year = {2024},
  doi = {10.1007/978-3-031-81372-6_25},
  url = {https://doi.org/10.1007/978-3-031-81372-6_25},
  journal = {… on Information and Communication Technologies in …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant … RAG also helps mitigate generative hallucination and provides …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_preventing_2024,
  title = {Preventing and detecting misinformation generated by large language models},
  author = {Liu, A. and Sheng, Q. and Hu, X.},
  year = {2024},
  doi = {10.1145/3626772.3661377},
  url = {https://doi.org/10.1145/3626772.3661377},
  journal = {Proceedings of the 47th International ACM SIGIR …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… prompt guardrails, retrieval-augmented generation (RAG), and … and limitations of detecting LLM-generated misinformation. … about the credibility and authenticity of LLMgenerated content …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jin_ragcache_2024,
  title = {Ragcache: {Efficient},
  author = {Jin, C. and Zhang, Z. and Jiang, X. and Liu, F. and Liu, S. and Liu, X. and {...},
  year = {2024},
  doi = {10.1145/3768628},
  url = {https://doi.org/10.1145/3768628},
  journal = {ACM Transactions on …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… that copies bear this notice and the full citation on the first page. Copyrights for components … findings using a production RAG dataset from Company-X, a leading LLM service provider, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_tutorllm_2025,
  title = {{TutorLLM},
  author = {Li, Z. and Wang, J. and Gu, W. and Yazdanpanah, V. and Shi, L. and {...},
  year = {2025},
  doi = {10.1007/978-3-032-05005-2_8},
  url = {https://doi.org/10.1007/978-3-032-05005-2_8},
  journal = {IFIP Conference on …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… and accuracy of the LLM’s responses. The third component is an RAG Enhanced LLM. This … The text content provided by Scraper mitigates hallucinations by dynamically retrieving and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yusuf_rag-based_2024,
  title = {A {RAG},
  author = {Yusuf, A. and Karaarslan, E. and Aydin, O.},
  year = {2024},
  doi = {10.36227/techrxiv.170723304.41988020},
  url = {https://doi.org/10.36227/techrxiv.170723304.41988020},
  journal = {Authorea Preprints},
  note = {Publisher: techrxiv.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… LLM chatbots use NLP techniques to establish connections … false information, known as hallucination. Also, the chatbots' … -based Retrieval Augmented Generation (RAG) approach to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{azher_generating_2024,
  title = {Generating suggestive limitations from research articles using llm and graph-based approach},
  author = {Azher, I. A.},
  year = {2024},
  doi = {10.1145/3677389.3702612},
  url = {https://doi.org/10.1145/3677389.3702612},
  journal = {Proceedings of the 24th ACM/IEEE Joint Conference …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… Given that each paper may have many citations, we refine the graph by establishing … RAG with LLM: We made an additional vector database as a Retrieval Augmented Generation …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{perron_ai-enhanced_2025,
  title = {{AI},
  author = {Perron, B. E. and Hiltz, B. S. and Khang, E. M. and {...},
  year = {2025},
  doi = {10.1080/10437797.2024.2411172},
  url = {https://doi.org/10.1080/10437797.2024.2411172},
  journal = {Journal of Social Work …},
  note = {Publisher: Taylor \&Francis},
  keywords = {source: Google Scholar},
  abstract = {… the erosion of trust in generative AI technologies. Retrieval-augmented generation (RAG), is … However, when we connect the LLM to the knowledge base and use RAG, the response is …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{modran_llm_2024,
  title = {{LLM},
  author = {Modran, H. A. and Bogdan, I. C. and Ursuțiu, D. and Samoilă, C. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-83520-9_54},
  url = {https://doi.org/10.1007/978-3-031-83520-9_54},
  journal = {International Conference …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… , integrating the Retrieval Augmented Generation (RAG) approach with a custom LLM. The … enhancing RAG models with auxiliary signals for improved factual consistency and reduced …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nazary_poison-rag_2025,
  title = {Poison-rag: {Adversarial},
  author = {Nazary, F. and Deldjoo, Y. and Noia, T.},
  year = {2025},
  doi = {10.1007/978-3-031-88717-8_18},
  url = {https://doi.org/10.1007/978-3-031-88717-8_18},
  journal = {European Conference on Information …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Using item metadata generated through a large language model (LLM) and embeddings … LLMs now integrate RAG frameworks to address issues such as hallucinations and factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hohn_using_2024,
  title = {Using {Large},
  author = {Höhn, S. and Nasir, J. and Paikan, A. and Ziafati, P. and {...},
  year = {2024},
  doi = {10.1145/3640794.3665886},
  url = {https://doi.org/10.1145/3640794.3665886},
  journal = {Proceedings of the 6th …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… This article criticises hallucination-based LLM evaluations and … LLM language assessment at this stage. Positive effects of RAG as compared to the same implementation without RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chan_entagents_2025,
  title = {{ENTAgents},
  author = {Chan, T. K. and Dinh, N. D.},
  year = {2025},
  doi = {10.1101/2025.01.01.25319863.abstract},
  url = {https://doi.org/10.1101/2025.01.01.25319863.abstract},
  journal = {medRxiv},
  note = {Publisher: medrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… for LLM applications to solve the problem of hallucinations. … LLM agentic framework, ENTAgents, which is a system addressing the medical specialty in otolaryngology and utilizing RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{soto-jimenez_rag-based_2024,
  title = {{RAG},
  author = {Soto-Jiménez, F. and Martínez-Velásquez, M. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-66329-1_37},
  url = {https://doi.org/10.1007/978-3-031-66329-1_37},
  journal = {Intelligent Systems …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… This section presents the RAG-based proposal which uses a specific-domain database to reduce LLM hallucination. The scope of the proposal covers the design and development of a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yilma_telecomrag_2025,
  title = {Telecomrag: {Taming},
  author = {Yilma, G. M. and Ayala-Romero, J. A. and {...},
  year = {2025},
  doi = {10.1145/3711992.3711996},
  url = {https://doi.org/10.1145/3711992.3711996},
  journal = {ACM SIGCOMM …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… to reduce the hallucination phenomenon and provide reliable factual information, other LLM-… The LLM input is passed to the Base LLM in the response generation \&optimization module…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{reed_augmented_2025,
  title = {Augmented and {Programmatically},
  author = {Reed, S. M.},
  year = {2025},
  doi = {10.1021/acs.jcim.4c02322},
  url = {https://doi.org/10.1021/acs.jcim.4c02322},
  journal = {Journal of Chemical Information and Modeling},
  note = {Publisher: ACS Publications},
  keywords = {source: Google Scholar},
  abstract = {… for predicting TPSA that combines RAG and MIPRO using a commercially available LLM, … -mini LLM directly to 11.76 RMSE when MIPRO and RAG were employed on top of that LLM for …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{vakilzadeh_development_2025,
  title = {The {Development},
  author = {Vakilzadeh, H. and Wood, D. A.},
  year = {2025},
  doi = {10.2308/ISYS-2024-041/13751},
  url = {https://doi.org/10.2308/isys-2024-041/13751},
  journal = {Journal of Information Systems},
  note = {Publisher: publications.aaahq.org},
  keywords = {source: Google Scholar},
  abstract = {… responses that ChatGPT provided academic citations, five of … retrieval-augmented generation. Prompt construction is about how we feed everything into the large language model (LLM) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kahl_evaluating_2024,
  title = {Evaluating the {Impact},
  author = {Kahl, S. and Löffler, F. and Maciol, M. and Ridder, F. and Schmitz, M. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-93409-4_14},
  url = {https://doi.org/10.1007/978-3-031-93409-4_14},
  journal = {… Workshop on AI in …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Our findings indicate that RAG combined with prompt engineering significantly enhances model responses and produces better factual answers. In the context of education, RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wilkerson_implementing_2024,
  title = {On implementing case-based reasoning with large language models},
  author = {Wilkerson, K. and Leake, D.},
  year = {2024},
  doi = {10.1007/978-3-031-63646-2_26},
  url = {https://doi.org/10.1007/978-3-031-63646-2_26},
  journal = {International Conference on Case-Based …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… shown to improve LLM responses [3, 18]. RAG typically provides the LLM with knowledge in … We note that the ability to filter out such hallucinations follows from having the original case …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lang_automatic_2024,
  title = {Automatic question answering for the linguistic domain–an evaluation of {LLM},
  author = {Lang, C. and Schneider, R. and Tu, N. D. T.},
  year = {2024},
  doi = {10.1007/978-3-031-70242-6_16},
  url = {https://doi.org/10.1007/978-3-031-70242-6_16},
  journal = {… on Applications of Natural Language to …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… In our contribution, we evaluate how expanding an LLM with RAG improves QA quality. We … hallucinations [20]. Besides, annotators seem to find RAG-improved answers more factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{olawore_development_2024,
  title = {Development and {Evaluation},
  author = {Olawore, K. and McTear, M. and Bi, Y.},
  year = {2024},
  doi = {10.1007/978-3-031-88045-2_7},
  url = {https://doi.org/10.1007/978-3-031-88045-2_7},
  journal = {… on Chatbots and Human-Centered AI},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… In this paper we present an adaptive LLM-based chatbot that … known as Retrieval-Augmented Generation (RAG) which … data and may hallucinate information, the RAG system offers …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kim_vaiv_2024,
  title = {{VAIV},
  author = {Kim, S. and Yoon, J.},
  year = {2024},
  doi = {10.1186/s12859-024-05903-6},
  url = {https://doi.org/10.1186/s12859-024-05903-6},
  journal = {BMC bioinformatics},
  note = {Publisher: Springer
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… ChatGPT model [3] is adopted as the LLM. By combining search capabilities with the LLM, we can mitigate the hallucination … By conditioning on retrieved relevant documents, the RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_shizishangpt_2024,
  title = {{ShizishanGPT},
  author = {Yang, S. and Liu, Z. and Mayer, W. and Ding, N. and Wang, Y. and {...},
  year = {2024},
  doi = {10.1007/978-981-96-0573-6_21},
  url = {https://doi.org/10.1007/978-981-96-0573-6_21},
  journal = {… Conference on Web …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval Augmented Generation (RAG) framework and agent architecture (ShizishanGPT). The system uses a large language model to … articles, RAG improves the factual accuracy and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{iivanainen_investigating_2024,
  title = {Investigating large language model ({LLM},
  author = {Iivanainen, S. and Lagus, J. and Viertolahti, H. and Sippola, L. and {...},
  year = {2024},
  doi = {10.1200/JCO.2024.42.16_suppl.e13637},
  url = {https://doi.org/10.1200/jco.2024.42.16_suppl.e13637},
  publisher = {ascopubs.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval Augmented Generation (RAG) could improve the LLM performance and reduce hallucinations… (128k tokens) and ICL with RAG (ICL-RAG) heuristically including only the most …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_conversation_2025,
  title = {From {Conversation},
  author = {Zhang, Z. and Gupta, P. and Song, J. and Zolnoori, M. and {...},
  year = {2025},
  doi = {10.1111/jnu.70039},
  url = {https://doi.org/10.1111/jnu.70039},
  journal = {Journal of Nursing …},
  note = {Publisher: Wiley Online Library},
  keywords = {source: Google Scholar},
  abstract = {… RAG to enhance problem mapping, enabling the LLM to reference Omaha System terminology and retrieve relevant contextual data for improved accuracy and factual … , LLM- RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{savage_large_2024,
  title = {Large language model uncertainty measurement and calibration for medical diagnosis and treatment},
  author = {Savage, T. and Wang, J. and Gallo, R. and Boukil, A. and Patel, V. and {...},
  year = {2024},
  doi = {10.1101/2024.06.06.24308399.abstract},
  url = {https://doi.org/10.1101/2024.06.06.24308399.abstract},
  journal = {medRxiv},
  note = {Publisher: medrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… metrics to quantify LLM confidence when performing … because extra context provided by RAG can errantly lead the model … LLM uncertainty are important for building medical LLM-RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dehbozorgi_personalized_2024,
  title = {Personalized pedagogy through a {LLM},
  author = {Dehbozorgi, N. and Kunuku, M. T. and Pouriyeh, S.},
  year = {2024},
  doi = {10.1007/978-3-031-64312-5_8},
  url = {https://doi.org/10.1007/978-3-031-64312-5_8},
  journal = {International Conference on …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… adopt the RAG framework with LLM to enhance the recommendation result. RAG combines … We will further address the tendency of LLM to generate ’hallucinated’ content, and plan to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_remflow_2025,
  title = {{REMFLOW},
  author = {Wang, G. and Liu, Y. and Liu, S. and Zhang, L. and Yang, L.},
  year = {2025},
  doi = {10.1007/s13042-025-02570-8},
  url = {https://doi.org/10.1007/s13042-025-02570-8},
  journal = {International Journal of Machine …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… to provide prior knowledge of the LLM and enhanced the input … We propose REMFLOW, a RAG-based LLM approach for … for mitigating hallucinations in large language models [25]. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{posedaru_artificial_2024,
  title = {Artificial intelligence text processing using retrieval-augmented generation: {Applications},
  author = {Posedaru, B. S. and Pantelimon, F. V. and Dulgheru, M. N. and {...},
  year = {2024},
  doi = {10.2478/picbe-2024-0018},
  url = {https://doi.org/10.2478/picbe-2024-0018},
  journal = {Proceedings of the …},
  note = {Publisher: sciendo.com
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… , having as its central tool ChatGPT and its capabilities. The … RetrievalAugmented Generation and is highlighted the potential of this technology to enhance the interpretability and trust in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pondel_ai_2024,
  title = {{AI},
  author = {Pondel, M. and Chomiak-Orsa, I. and Sobińska, M. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-78468-2_1},
  url = {https://doi.org/10.1007/978-3-031-78468-2_1},
  journal = {… Conference on Artificial …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… -aware GPT-4 with RAG significantly enhanced the AI assistant’s utility. RAG’s ability to … In summary, IT - especially AI, LLM and RAG, offers powerful tools to support KM processes, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{basaran_beyond_2025,
  title = {Beyond traditional prognostics: integrating {RAG},
  author = {Basaran, A. E. and Güresir, A. and Knoch, H. and Vychopen, M. and {...},
  year = {2025},
  doi = {10.1007/s10143-025-03194-w},
  url = {https://doi.org/10.1007/s10143-025-03194-w},
  journal = {Neurosurgical …},
  note = {Publisher: Springer
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… putting trust in what conventional AI, like ChatGPT, predicts. Despite language model designers taking steps to prevent these consultations by making sure that language models are not …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_multimodal_2025,
  title = {Multimodal large language model with knowledge retrieval using flowchart embedding for forming follow-up recommendations for pancreatic cystic lesions},
  author = {Zhu, Z. and Liu, J. and Hong, C. W. and Houshmand, S. and {...},
  year = {2025},
  doi = {10.2214/ajr.25.32729},
  url = {https://doi.org/10.2214/ajr.25.32729},
  journal = {American Journal of …},
  note = {Publisher: ajronline.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG, a technique that allows domain-… LLM performance and reducing hallucinations (16,17) in other radiologic tasks (25,26). However, in the present analysis, the plain-text RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_knowledge_2024,
  title = {Knowledge graph-enhanced large language model for domain-specific question answering systems},
  author = {Li, D. and Li, Z. and Yang, Y. and Sun, L. and An, D. and Yang, Q.},
  year = {2024},
  doi = {10.36227/techrxiv.172963127.77889256},
  url = {https://doi.org/10.36227/techrxiv.172963127.77889256},
  journal = {Authorea Preprints},
  note = {Publisher: techrxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… KGRA, a novel RAG method that leverages knowledge graphs … the risk of model hallucinations and improving the LLM’s ability to … 2) LLM-Driven Retrieval Method: We developed an LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bianchini_enhancing_2024,
  title = {Enhancing complex linguistic tasks resolution through fine-tuning llms, rag and knowledge graphs (short paper)},
  author = {Bianchini, F. and Calamo, M. and Luzi, F. De and Macrì, M. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-61003-5_13},
  url = {https://doi.org/10.1007/978-3-031-61003-5_13},
  journal = {International Conference …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… RAG … RAG, AI systems produce outputs grounded in factual information retrieved from external and heterogeneous sources [30], thus mitigating concerns regarding the reliability of LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{scire_truth_2025,
  title = {Truth or {Mirage},
  author = {Scirè, A. and Bejgu, A. S. and Tedeschi, S. and Ghonim, K. and {...},
  year = {2025},
  doi = {10.1162/COLI.a.575/133509},
  url = {https://doi.org/10.1162/coli.a.575/133509},
  journal = {Computational …},
  note = {Publisher: direct.mit.edu},
  keywords = {source: Google Scholar},
  abstract = {… of LLM-OASIS for end-to-end factuality evaluation. We compare different models in Zero-Shot (ZS) and RAG … Figure 3: McNemar’s test p-values for all model comparisons in the RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{junior_domain-driven_2024,
  title = {Domain-driven {LLM},
  author = {Junior, JC dos Santos and Hu, R. and Song, R. and Bai, Y.},
  year = {2024},
  doi = {10.1145/3637528.3671445},
  url = {https://doi.org/10.1145/3637528.3671445},
  journal = {Proceedings of the 30th …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… is not covered by the LLM pre-training. This tutorial walks through the RAG and Fine-Tuning … best practices of adopting the methodologies for the LLM tasks and use cases. The hands-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lu_scchat_2024,
  title = {{scChat},
  author = {Lu, Y. C. and Varghese, A. and Nahar, R. and Chen, H. and Shao, K. and Bao, X. and {...},
  year = {2024},
  doi = {10.1101/2024.10.01.616063.abstract},
  url = {https://doi.org/10.1101/2024.10.01.616063.abstract},
  journal = {bioRxiv},
  note = {Publisher: biorxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , employs retrieval-augmented generation to reduce hallucination, and uses an LLM-powered … function calls, retrieval-augmented generation (RAG), and a powerful LLM-driven search …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{diaz-pace_helping_2024,
  title = {Helping novice architects to make quality design decisions using an llm-based assistant},
  author = {Díaz-Pace, J. A. and Tommasel, A. and Capilla, R.},
  year = {2024},
  doi = {10.1007/978-3-031-70797-1_21},
  url = {https://doi.org/10.1007/978-3-031-70797-1_21},
  journal = {European Conference on Software …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… is crucial to mitigate “hallucinations” in the LLM outputs. … , RAG [4] is a practical way of adding knowledge to an LLM, … First, RAG ensures that the AK being retrieved is relevant to the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_geofactory_2025,
  title = {{GeoFactory},
  author = {Chen, Z. and Wang, X. and Zhang, X. and Lin, M. and Liao, Y. and Li, J. and {...},
  year = {2025},
  doi = {10.1080/20964471.2025.2506291},
  url = {https://doi.org/10.1080/20964471.2025.2506291},
  journal = {Big Earth Data},
  note = {Publisher: Taylor \&Francis},
  keywords = {source: Google Scholar},
  abstract = {… All five RAG algorithms evaluated in this study demonstrated significant improvements in LLM performance, particularly in geoscientific factual tasks. The experimental findings suggest …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jauhiainen_generative_2025,
  title = {Generative {AI},
  author = {Jauhiainen, J. S. and Guerra, A. Garagorry},
  year = {2025},
  doi = {10.1080/14703297.2024.2422337},
  url = {https://doi.org/10.1080/14703297.2024.2422337},
  journal = {Innovations in Education and …},
  note = {Publisher: Taylor \&Francis},
  keywords = {source: Google Scholar},
  abstract = {… RAG framework ensured ChatGPT-4’s accurate recall of responses and secure alignment in the university’s evaluation criteria. ChatGPT-4’… ChatGPT-4 successfully assessed the factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tarabanis_performance_2024,
  title = {Performance of publicly available large language models on internal medicine board-style questions},
  author = {Tarabanis, C. and Zahid, S. and Mamalis, M. and Zhang, K. and {...},
  year = {2024},
  doi = {10.1371/journal.pdig.0000604},
  url = {https://doi.org/10.1371/journal.pdig.0000604},
  journal = {PLOS Digital …},
  note = {Publisher: journals.plos.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… To address this, ascribing a variable degree of confidence to each LLM … Retrieval Augmented Generation a viable technique for improving factual accuracy in medical examination LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{freire_knowledge_2024,
  title = {Knowledge sharing in manufacturing using {LLM},
  author = {Freire, S. Kernan and Wang, C. and Foosherian, M. and {...},
  year = {2024},
  doi = {10.3389/frai.2024.1293084},
  url = {https://doi.org/10.3389/frai.2024.1293084},
  journal = {Frontiers in Artificial …},
  note = {Publisher: frontiersin.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Using RAG also enables further transparency and explainability of the LLM's response. … We consider performance as the factuality, completeness, hallucinations, and conciseness …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_bioinformatics_2024,
  title = {Bioinformatics and biomedical informatics with {ChatGPT},
  author = {Wang, J. and Cheng, Z. and Yao, Q. and Liu, L. and Xu, D. and {...},
  year = {2024},
  doi = {10.1002/qub2.67},
  url = {https://doi.org/10.1002/qub2.67},
  journal = {Quantitative Biology},
  note = {Publisher: Wiley Online Library},
  keywords = {source: Google Scholar},
  abstract = {… ChatGPT in bioinformatics and biomedical informatics. We also utilized backward and forward citation … This approach, known as RAG, improves ChatGPT’s reliability by sourcing facts …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{musumeci_llm_2024,
  title = {Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain},
  author = {Musumeci, E. and Brienza, M. and Suriani, V. and Nardi, D. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-60615-1_7},
  url = {https://doi.org/10.1007/978-3-031-60615-1_7},
  journal = {… Conference on Human …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… LLM) to retrieve relevant knowledge, showing promising potential in mitigating LLM hallucinations … However, existing RAG systems are often inadequate in answering multi-hop queries, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yan_pdgpt_2024,
  title = {{PDGPT},
  author = {Yan, Z. and Liang, H. and Wang, J. and Zhang, H. and {...},
  year = {2024},
  doi = {10.1002/mgea.77},
  url = {https://doi.org/10.1002/mgea.77},
  journal = {Materials Genome …},
  note = {Publisher: Wiley Online Library},
  keywords = {source: Google Scholar},
  abstract = {… optimizes the LLM using the following two methods: SFT and RAG. The RAG approach … Even after adjusting ε, the LLM remained confident in providing the same incorrect answer. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{freund_enriching_2024,
  title = {Enriching {RDF},
  author = {Freund, M. and Dorsch, R. and Schmid, S. and Wehr, T. and {...},
  year = {2024},
  doi = {10.1007/978-3-031-81221-7_8},
  url = {https://doi.org/10.1007/978-3-031-81221-7_8},
  journal = {… Knowledge Graph and …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… additional input to the LLM. Specifically, we inject the additional factual knowledge from the … , which integrates NER, RAG-based entity linking, and LLM-based disambiguation with self-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{matsumoto_escargot_2025,
  title = {{ESCARGOT},
  author = {Matsumoto, N. and Choi, H. and Moran, J. and Hernandez, M. E. and {...},
  year = {2025},
  doi = {10.1093/bioinformatics/btaf031/61561980/btaf031},
  url = {https://doi.org/10.1093/bioinformatics/btaf031/61561980/btaf031},
  journal = {…},
  note = {Publisher: academic.oup.com
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations and struggle with integrating external knowledge effectively. While Retrieval-Augmented Generation (RAG… to the black-box nature of LLM-only or RAG-based …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{miao_how_2024,
  title = {How to improve {ChatGPT},
  author = {Miao, J. and Thongprayoon, C. and Craici, I. M. and {...},
  year = {2024},
  doi = {10.1007/s40620-024-01974-z},
  url = {https://doi.org/10.1007/s40620-024-01974-z},
  journal = {Journal of …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Such hallucinations compromise the dependability of large language model outputs, casting … impact of both chain-of-thought and retrieval-augmented generation methods on enhancing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_transforming_2025,
  title = {Transforming hematological research documentation with large language models: an approach to scientific writing and data analysis},
  author = {Yang, J. J. and Hwang, S. H.},
  year = {2025},
  doi = {10.1007/s44313-025-00062-w},
  url = {https://doi.org/10.1007/s44313-025-00062-w},
  journal = {Blood research},
  note = {Publisher: Springer
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… engineering, Retrieval-Augmented Generation (RAG) enhances LLM … RAG systems dynamically retrieve information from verified … To reduce hallucinations and ensure a reliable LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kirshner_talking_2025,
  title = {Talking terms: {Agent},
  author = {Kirshner, S. N. and Pan, Y. and Wu, J. X. and Gould, A.},
  year = {2025},
  doi = {10.1111/deci.70010},
  url = {https://doi.org/10.1111/deci.70010},
  journal = {Decision Sciences},
  note = {Publisher: Wiley Online Library},
  keywords = {source: Google Scholar},
  abstract = {… show that tailored retrieval-augmented generation (RAG) … with LLM agents, (3) highlight the effectiveness of tailoring RAG … , LLM agents could be given the ability to disclose credible or …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{miroyan_analyzing_2025,
  title = {Analyzing {Pedagogical},
  author = {Miroyan, M. and Mitra, C. and Jain, R. and Ranade, G. and {...},
  year = {2025},
  doi = {10.1145/3641554.3701965},
  url = {https://doi.org/10.1145/3641554.3701965},
  journal = {Proceedings of the 56th …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM assistant, Edison, a GPT-4-based method that leverages Optical Character Recognition (OCR), retrieval-augmented generation (RAG… responses more relevant and factual. Overall, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kang_nadine_2024-1,
  title = {Nadine: {A},
  author = {Kang, H. and Moussa, M. Ben and {...},
  year = {2024},
  doi = {10.1002/cav.2290},
  url = {https://doi.org/10.1002/cav.2290},
  journal = {Computer Animation and …},
  note = {Publisher: Wiley Online Library},
  keywords = {source: Google Scholar},
  abstract = {… This approach aims to reduce LLM hallucinations and incorporate data from external … In this section, we demonstrate the RAG system adopted in our interaction module. The RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fu_encchain_2024,
  title = {{EncChain},
  author = {Fu, Z. and Sha, M. and Li, Y. and Li, H. and Ma, Y. and Wang, S. and Li, F.},
  year = {2024},
  doi = {10.14778/3685800.3685888},
  url = {https://doi.org/10.14778/3685800.3685888},
  journal = {Proceedings of the VLDB …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… Remote Attestation for Enhanced Trust: EncChain enables … the deployed LLM, providing users with additional confidence in the … privacy attributes in LLM applications using RAG-based …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_language_2025,
  title = {Language models are few-shot graders},
  author = {Zhao, C. and Silva, M. and Poulsen, S.},
  year = {2025},
  doi = {10.1007/978-3-031-98459-4_1},
  url = {https://doi.org/10.1007/978-3-031-98459-4_1},
  journal = {International Conference on Artificial …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Our new LLM-based ASAG pipeline achieves better … We employ one of two selection strategies: Random or RAG. The … To mitigate the risk of LLM hallucination, such as inventing new …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mauliana_exploring_2025,
  title = {Exploring {LLM},
  author = {Mauliana, M. and Ashok, A. and Czernochowski, D. and {...},
  year = {2025},
  doi = {10.3389/frobt.2025.1585589},
  url = {https://doi.org/10.3389/frobt.2025.1585589},
  journal = {Frontiers in Robotics …},
  note = {Publisher: frontiersin.org},
  keywords = {source: Google Scholar},
  abstract = {… Two approaches were used to generate responses: RAG + LLM and LLM functionality. A … factual information. Consequently, we activate the RAG + LLM function; otherwise, the LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mensah_all_2024,
  title = {All you need is context: {Clinician},
  author = {Mensah, P. B. and Quao, N. S. and Dagadu, S. and {...},
  year = {2024},
  doi = {10.1101/2024.04.03.24305276.full},
  url = {https://doi.org/10.1101/2024.04.03.24305276.full},
  journal = {2024 IEEE 12th …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM framework based on OpenAI's "text-davinci-003" combined with RAG, versus ChatGPT … Almanac's answers as safer and more factual, they still preferred ChatGPT's answers [10]. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{duan_research_2025,
  title = {Research on a traditional {Chinese},
  author = {Duan, Y. and Zhou, Q. and Li, Y. and Qin, C. and Wang, Z. and Kan, H. and {...},
  year = {2025},
  doi = {10.3389/fmed.2024.1512329},
  url = {https://doi.org/10.3389/fmed.2024.1512329},
  journal = {Frontiers in …},
  note = {Publisher: frontiersin.org},
  keywords = {source: Google Scholar},
  abstract = {… factual statements by utilizing the knowledge stored in KGs. This method directly addresses the hallucination … with LLM and knowledge graphs, and a LLM without RAG technology. The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{antal_evaluating_2025,
  title = {Evaluating {Open},
  author = {Antal, M. and Buza, K.},
  year = {2025},
  doi = {10.1007/s44427-025-00006-3},
  url = {https://doi.org/10.1007/s44427-025-00006-3},
  journal = {Acta Universitatis Sapientiae, Informatica},
  note = {Publisher: Springer
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval Augmented Generation (RAG) systems have emerged as a powerful paradigm for enhancing Large Language Model (LLM) … It supports the evaluation of hallucinations, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_empirical_2025,
  title = {An empirical study on challenges for llm application developers},
  author = {Chen, X. and Gao, C. and Chen, C. and Zhang, G. and Liu, Y.},
  year = {2025},
  doi = {10.1145/3715007},
  url = {https://doi.org/10.1145/3715007},
  journal = {ACM Transactions on …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… issues where RAG fails to improve output quality76 or cause hallucinations after processing… Despite their potential to enhance LLM outputs, advanced techniques like CoT and RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{goyal_healai_2024,
  title = {Healai: {A},
  author = {Goyal, S. and Rastogi, E. and Rajagopal, S. P. and Yuan, D. and {...},
  year = {2024},
  doi = {10.1145/3616855.3635739},
  url = {https://doi.org/10.1145/3616855.3635739},
  journal = {Proceedings of the 17th …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… notice and the full citation on the first page. … Retrieval Augmented Generation (RAG) [1] [5] - solving our large context problem and which has shown to perform better than using raw LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{singh_stop_2024,
  title = {Stop {Hallucinations},
  author = {Singh, B.},
  year = {2024},
  doi = {10.1007/979-8-8688-0569-1_5},
  url = {https://doi.org/10.1007/979-8-8688-0569-1_5},
  journal = {Building Applications with Large Language Models …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… build your fundamentals about RAG, which lets you customize the LLM as per your need … LLM-based applications. You will also learn in detail about the three components of RAG and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fu_spell_2025,
  title = {{SpeLL},
  author = {Fu, J. and Liu, X. and Cai, W. and Fu, H. and Shao, X.},
  year = {2025},
  doi = {10.1021/acs.jcim.5c01236},
  url = {https://doi.org/10.1021/acs.jcim.5c01236},
  journal = {Journal of Chemical Information …},
  note = {Publisher: ACS Publications},
  keywords = {source: Google Scholar},
  abstract = {… their potential for “hallucination,” retrieval-augmented generation (RAG) technology offers an … recent advancements in LLMs and RAG technology, we developed Spectrum LLM (SpeLL), …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ang_tsgassist_2024,
  title = {Tsgassist: {An},
  author = {Ang, Y. and Bao, Y. and Huang, Q. and Tung, A. K. H. and {...},
  year = {2024},
  doi = {10.14778/3685800.3685862},
  url = {https://doi.org/10.14778/3685800.3685862},
  journal = {Proceedings of the VLDB …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… and reducing content hallucinations [9]. In … RAG-enhanced LLM in TSG Recommender to be significantly more effective in terms of relevance and faithfulness than the standalone LLM. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ying_beyond_2025,
  title = {Beyond words: evaluating large language models in transportation planning},
  author = {Ying, S. and Li, Z. and Yu, M.},
  year = {2025},
  doi = {10.1080/10095020.2025.2493073},
  url = {https://doi.org/10.1080/10095020.2025.2493073},
  journal = {Geo-spatial Information Science},
  note = {Publisher: Taylor \&Francis},
  keywords = {source: Google Scholar},
  abstract = {… for fine-tuning and retrieval-augmented generation (RAG) to enhance LLM performance in structured … in workflow structuring and susceptibility to hallucinations limit its practical utility for …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{beasley_tarragon_2025,
  title = {{TARRAGON},
  author = {Beasley, J. M. T. and Schatz, K. and Ding, E. and DeLuca, M. and Zaid, N. A. and {...},
  year = {2025},
  doi = {10.1101/2025.04.19.649662.abstract},
  url = {https://doi.org/10.1101/2025.04.19.649662.abstract},
  journal = {bioRxiv},
  note = {Publisher: biorxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… -is from LLM-generation, and any inaccurate claims or citations are the … Retrieval-Augmented Generation (RAG) for Target Feasibility Reports We generated large language model (LLM)…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mullins_enhancing_2024,
  title = {Enhancing classroom teaching with {LLMs},
  author = {Mullins, E. and Portillo, A. and Rohena, K. Ruiz and {...},
  year = {2024},
  doi = {10.1145/3686852.3687083},
  url = {https://doi.org/10.1145/3686852.3687083},
  journal = {Proceedings of the 25th …},
  note = {Publisher: dl.acm.org},
  keywords = {source: Google Scholar},
  abstract = {… and that copies bear this notice and the full citation on the first page. Copyrights for third-party … the answer correctness of the LLM. We used Llama as our LLM because of its availability. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zeng_cancer_2025,
  title = {Cancer gene identification through integrating causal prompting large language model with omics data–driven causal inference},
  author = {Zeng, H. and Yin, C. and Chai, C. and Wang, Y. and Dai, Q. and {...},
  year = {2025},
  doi = {10.1093/bib/bbaf113/62386227/bbaf113},
  url = {https://doi.org/10.1093/bib/bbaf113/62386227/bbaf113},
  journal = {Briefings in …},
  note = {Publisher: Oxford Academic
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gu_radalign_2025,
  title = {Radalign: {Advancing},
  author = {Gu, D. and Gao, Y. and Zhou, Y. and Zhou, M. and Metaxas, D.},
  year = {2025},
  doi = {10.1007/978-3-032-04981-0_46},
  url = {https://doi.org/10.1007/978-3-032-04981-0_46},
  journal = {International Conference on …},
  note = {Publisher: Springer},
  keywords = {source: Google Scholar},
  abstract = {… Our image-based RAG system addresses this by providing … diagnoses, we enable the LLM to better contextualize the … retrieval-augmented generation, RadAlign enhances factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{greengard_shining_2025,
  title = {Shining a {Light},
  author = {Greengard, S.},
  year = {2025},
  doi = {10.1145/3715691},
  url = {https://doi.org/10.1145/3715691},
  publisher = {dl.acm.org},
  note = {Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… example, RAG prompts the LLM to … LLM could check to see who now serves as the governor of a state, or the current price of gasoline, rather than relying on outdated training data. RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@misc{vrettos_accurate_2025,
  title = {Accurate and {Energy},
  author = {Vrettos, Konstantinos and Klontzas, Michail E.},
  year = {2025},
  doi = {10.48550/arXiv.2506.20009},
  url = {https://doi.org/10.48550/arxiv.2506.20009},
  publisher = {arXiv},
  note = {arXiv:2506.20009 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {Background The increasing adoption of Artificial Intelligence (AI) in healthcare has sparked growing concerns about its environmental and ethical implications. Commercial Large Language Models (LLMs), such as ChatGPT and DeepSeek, require substantial resources, while the utilization of these systems for medical purposes raises critical issues regarding patient privacy and safety. Methods We developed a customizable Retrieval-Augmented Generation (RAG) framework for medical tasks, which monitors its energy usage and CO2 emissions. This system was then used to create RAGs based on various open-source LLMs. The tested models included both general purpose models like llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs o4-mini model. A dataset of medical questions was used for the evaluation. Results Custom RAG models outperformed commercial models in accuracy and energy consumption. The RAG model built on llama3.1:8B achieved the highest accuracy (58.5\%) and was significantly better than other models, including o4-mini and DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption and CO2 footprint among all models, with a Performance per kWh of 0.52 and a total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x times more accuracy points per kWh and 172\% less electricity usage while maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs can be leveraged to develop RAGs that outperform commercial, online LLMs in medical tasks, while having a smaller environmental impact. Our modular framework promotes sustainable AI development, reducing electricity usage and aligning with the UNs Sustainable Development Goals.},
  annote = {Comment: 18 pages, 3 Figures},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XX8TX7YK\\Vrettos and Klontzas - 2025 - Accurate and Energy Efficient Local Retrieval-Augmented Generation Models Outperform Commercial Lar.pdf:application/pdf},
  month = {jun},
  shorttitle = {Accurate and {Energy},
  urldate = {2025-10-26},
}

@misc{zhang_culturesynth_2025,
  title = {{CultureSynth},
  author = {Zhang, Xinyu and Zhang, Pei and Luo, Shuang and Tang, Jialong and Wan, Yu and Yang, Baosong and Huang, Fei},
  year = {2025},
  doi = {10.48550/arXiv.2509.10886},
  url = {https://doi.org/10.48550/arxiv.2509.10886},
  publisher = {arXiv},
  note = {arXiv:2509.10886 [cs]},
  keywords = {source: Arxiv},
  abstract = {Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation{\textbackslash},
  urldate = {2025-10-26},
  month = {sep},
  annote = {Comment: Accepted as a Findings paper at EMNLP 2025},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZMQE2W6Y\\Zhang et al. - 2025 - CultureSynth A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question.pdf:application/pdf},
  shorttitle = {{CultureSynth},
}

@misc{cherukuri_large_2025,
  title = {Large {Language},
  author = {Cherukuri, Komala Subramanyam and Moses, Pranav Abishai and Sakata, Aisa and Chen, Jiangping and Chen, Haihua},
  year = {2025},
  doi = {10.48550/arXiv.2508.06729},
  url = {https://doi.org/10.48550/arxiv.2508.06729},
  publisher = {arXiv},
  note = {arXiv:2508.06729 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71\%), followed by Llama (84.99\%) and Qwen (83.72\%). For sentiment analysis, Llama slightly outperformed Qwen (82.66\%) and ChatGPT (82.29\%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XVLMBNEY\\Cherukuri et al. - 2025 - Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis.pdf:application/pdf},
  month = {aug},
  urldate = {2025-10-26},
}

@misc{boraud_reservoirchat_2025,
  title = {{ReservoirChat},
  author = {Boraud, Virgile and Bendi-Ouis, Yannis and Bernard, Paul and Hinaut, Xavier},
  year = {2025},
  doi = {10.48550/arXiv.2507.05279},
  url = {https://doi.org/10.48550/arxiv.2507.05279},
  publisher = {arXiv},
  note = {arXiv:2507.05279 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering, source: Arxiv},
  abstract = {We introduce a tool designed to improve the capabilities of Large Language Models (LLMs) in assisting with code development using the ReservoirPy library, as well as in answering complex questions in the field of Reservoir Computing. By incorporating external knowledge through Retrieval-Augmented Generation (RAG) and knowledge graphs, our approach aims to reduce hallucinations and increase the factual accuracy of generated responses. The system provides an interactive experience similar to ChatGPT, tailored specifically for ReservoirPy, enabling users to write, debug, and understand Python code while accessing reliable domain-specific insights. In our evaluation, while proprietary models such as ChatGPT-4o and NotebookLM performed slightly better on general knowledge questions, our model outperformed them on coding tasks and showed a significant improvement over its base model, Codestral-22B.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7UMP9EM2\\Boraud et al. - 2025 - ReservoirChat Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy.pdf:application/pdf},
  month = {jul},
  shorttitle = {{ReservoirChat},
  urldate = {2025-10-26},
}

@misc{gondhalekar_multifinrag_2025,
  title = {{MultiFinRAG},
  author = {Gondhalekar, Chinmay and Patel, Urjitkumar and Yeh, Fang-Chun},
  year = {2025},
  doi = {10.48550/arXiv.2506.20821},
  url = {https://doi.org/10.48550/arxiv.2506.20821},
  publisher = {arXiv},
  note = {arXiv:2506.20821 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computational Engineering, Finance, and Science, source: Arxiv},
  abstract = {Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span hundreds of pages and combine diverse modalities, including dense narrative text, structured tables, and complex figures. Answering questions over such content often requires joint reasoning across modalities, which strains traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines due to token limitations, layout loss, and fragmented cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation framework purpose-built for financial QA. MultiFinRAG first performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM, which produces both structured JSON outputs and concise textual summaries. These outputs, along with narrative text, are embedded and indexed with modality-aware similarity thresholds for precise retrieval. A tiered fallback strategy then dynamically escalates from text-only to text+table+image contexts when necessary, enabling cross-modal reasoning while reducing irrelevant context. Despite running on commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.},
  annote = {Comment: Preprint Copy},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7NVT3VX8\\Gondhalekar et al. - 2025 - MultiFinRAG An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Qu.pdf:application/pdf},
  month = {jun},
  shorttitle = {{MultiFinRAG},
  urldate = {2025-10-26},
}

@misc{masa_improving_2025,
  title = {Improving {LLMs},
  author = {Máša, Petr},
  year = {2025},
  doi = {10.48550/arXiv.2506.05560},
  url = {https://doi.org/10.48550/arxiv.2506.05560},
  publisher = {arXiv},
  note = {arXiv:2506.05560 [cs]},
  keywords = {Computer Science - Computation and Language, source: Arxiv},
  abstract = {Large language models (LLMs) are achieving significant progress almost every moment now. Many advanced techniques have been introduced and widely accepted, like retrieval-augmentation generation (RAG), agents, and tools. Tools can query the database to answer questions from structured data files or perform groupings or other statistics. This unlocks huge opportunities, such as it can answer any question, but also poses threats, such as safety, because there is no control over the commands that are created. We would like to discuss whether we can create a new method that improves answers based on dataset/database via some interpretable ML methods, namely enhanced association rules. The advantage would be if the method can be also used in some safe technique like RAG. Association rules have a sound history. Since the introduction of CN2 and aproiri, many enhancements have been made. In parallel, enhanced association rules have been introduced and evolved over the last 40 years. The general problem is typically that there are too many rules. There are some techniques for handling it, but when LLM emerged, it turned out to be the best use case for the RAG technique for LLMs. We proposed a method that generates a ruleset based on defined knowledge patterns, then converts rules into text form via a rule-to-text converter, and includes the result as an RAG into LLM. We compared this method with ChatGPT (even with using agents) and we have discovered a significant improvement in answering questions based on the dataset. We have also tried several strategies how much rules to generate. We found this improvement interesting. Moreover, it can also be improved in many ways as future work, like incorporating other patterns, the use of rule mining as an agent, and many others.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7KUS8IHG\\Máša - 2025 - Improving LLMs with a knowledge from databases.pdf:application/pdf},
  month = {jun},
  urldate = {2025-10-26},
}

@misc{junior_br-taxqa-r_2025,
  title = {{BR},
  author = {Júnior, Juvenal Domingos and Faria, Augusto and Oliveira, E. Seiti de and Brito, Erick de and Teotonio, Matheus and Assumpção, Andre and Carmo, Diedre and Lotufo, Roberto and Pereira, Jayr},
  year = {2025},
  doi = {10.48550/arXiv.2505.15916},
  url = {https://doi.org/10.48550/arxiv.2505.15916},
  publisher = {arXiv},
  note = {arXiv:2505.15916 [cs]},
  keywords = {source: Arxiv},
  abstract = {This paper presents BR-TaxQA-R, a novel dataset designed to support question answering with references in the context of Brazilian personal income tax law. The dataset contains 715 questions from the 2024 official Q{\textbackslash},
  urldate = {2025-10-26},
  month = {may},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\5K5YQE28\\Júnior et al. - 2025 - BR-TaxQA-R A Dataset for Question Answering with References for Brazilian Personal Income Tax Law,.pdf:application/pdf},
  shorttitle = {{BR},
}

@misc{melton_evaluating_2025,
  title = {Evaluating {Retrieval},
  author = {Melton, Chad and Sorokine, Alex and Peterson, Steve},
  year = {2025},
  doi = {10.48550/arXiv.2504.07022},
  url = {https://doi.org/10.48550/arxiv.2504.07022},
  publisher = {arXiv},
  note = {arXiv:2504.07022 [cs]},
  keywords = {source: Arxiv},
  abstract = {Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments.},
  urldate = {2025-10-26},
  month = {apr},
  annote = {Comment: 14 pages, 3 Figures, 3 tables},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\L2IQY7LP\\Melton et al. - 2025 - Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety.pdf:application/pdf},
}

@misc{bilal_onrl-rag_2025,
  title = {{OnRL},
  author = {Bilal, Ahsan and Lin, Beiyu},
  year = {2025},
  doi = {10.48550/arXiv.2504.02894},
  url = {https://doi.org/10.48550/arxiv.2504.02894},
  publisher = {arXiv},
  note = {arXiv:2504.02894 [cs]},
  keywords = {source: Arxiv},
  abstract = {Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.},
  urldate = {2025-10-26},
  month = {apr},
  annote = {Comment: It needs more revisions. I am currently working on it with my co-author},
  shorttitle = {{OnRL},
}

@misc{wang_scholarcopilot_2025,
  title = {{ScholarCopilot},
  author = {Wang, Yubo and Ma, Xueguang and Nie, Ping and Zeng, Huaye and Lyu, Zhiheng and Zhang, Yuxuan and Schneider, Benjamin and Lu, Yi and Yue, Xiang and Chen, Wenhu},
  year = {2025},
  doi = {10.48550/arXiv.2504.00824},
  url = {https://doi.org/10.48550/arxiv.2504.00824},
  publisher = {arXiv},
  note = {arXiv:2504.00824 [cs]},
  keywords = {Computer Science - Computation and Language, source: Arxiv},
  abstract = {Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their ability to support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], which is then used to query a citation database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to improve efficiency. Our model is built upon Qwen-2.5-7B and trained on 500K papers from arXiv. It achieves a top-1 retrieval accuracy of 40.1\% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0\%) and BM25 (9.8\%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality -- measured across relevance, coherence, academic rigor, completeness, and innovation -- significantly surpassing all existing models, including much larger ones like the Retrieval-Augmented Qwen2.5-72B-Instruct. Human studies further demonstrate that ScholarCopilot, despite being a 7B model, significantly outperforms ChatGPT, achieving 100\% preference in citation quality and over 70\% in overall usefulness.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\LEKEPJSQ\\Wang et al. - 2025 - ScholarCopilot Training Large Language Models for Academic Writing with Accurate Citations.pdf:application/pdf},
  month = {apr},
  shorttitle = {{ScholarCopilot},
  urldate = {2025-10-26},
}

@misc{tsuchida_goodevil_2025,
  title = {Good/{Evil},
  author = {Tsuchida, Rikuto and Yokoyama, Hibiki and Utsuro, Takehito},
  year = {2025},
  doi = {10.48550/arXiv.2503.14382},
  url = {https://doi.org/10.48550/arxiv.2503.14382},
  publisher = {arXiv},
  note = {arXiv:2503.14382 [cs]},
  keywords = {Computer Science - Computation and Language, source: Arxiv},
  abstract = {The purpose of this paper is to examine whether large language models (LLMs) can understand what is good and evil with respect to judging good/evil reputation of celebrities. Specifically, we first apply a large language model (namely, ChatGPT) to the task of collecting sentences that mention the target celebrity from articles about celebrities on Web pages. Next, the collected sentences are categorized based on their contents by ChatGPT, where ChatGPT assigns a category name to each of those categories. Those assigned category names are referred to as "aspects" of each celebrity. Then, by applying the framework of retrieval augmented generation (RAG), we show that the large language model is quite effective in the task of judging good/evil reputation of aspects and descriptions of each celebrity. Finally, also in terms of proving the advantages of the proposed method over existing services incorporating RAG functions, we show that the proposed method of judging good/evil of aspects/descriptions of each celebrity significantly outperform an existing service incorporating RAG functions.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\R86KE37E\\Tsuchida et al. - 2025 - GoodEvil Reputation Judgment of Celebrities by LLMs via Retrieval Augmented Generation.pdf:application/pdf},
  month = {jul},
  urldate = {2025-10-26},
}

@misc{samuel_agrollm_2025,
  title = {{AgroLLM},
  author = {Samuel, Dinesh Jackson and Skarga-Bandurova, Inna and Sikolia, David and Awais, Muhammad},
  year = {2025},
  doi = {10.48550/arXiv.2503.04788},
  url = {https://doi.org/10.48550/arxiv.2503.04788},
  publisher = {arXiv},
  note = {arXiv:2503.04788 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv},
  abstract = {AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing and education in agriculture using Large Language Models (LLMs) and a Retrieval-Augmented Generation (RAG) framework. By using a comprehensive open-source agricultural database, AgroLLM provides accurate, contextually relevant responses while reducing incorrect information retrieval. The system utilizes the FAISS vector database for efficient similarity searches, ensuring rapid access to agricultural knowledge. A comparative study of three advanced models: Gemini 1.5 Flash, ChatGPT-4o Mini, and Mistral-7B-Instruct-v0.2 was conducted to evaluate performance across four key agricultural domains: Agriculture and Life Sciences, Agricultural Management, Agriculture and Forestry, and Agriculture Business. Key evaluation metrics included embedding quality, search efficiency, and response relevance. Results indicated that ChatGPT-4o Mini with RAG achieved the highest accuracy at 93\%. Continuous feedback mechanisms enhance response quality, making AgroLLM a benchmark AI-driven educational tool for farmers, researchers, and professionals, promoting informed decision-making and improved agricultural practices.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\DN4XHPQZ\\Samuel et al. - 2025 - AgroLLM Connecting Farmers and Agricultural Practices through Large Language Models for Enhanced Kn.pdf:application/pdf},
  month = {feb},
  shorttitle = {{AgroLLM},
  urldate = {2025-10-26},
}

@misc{li_tutorllm_2025-1,
  title = {{TutorLLM},
  author = {Li, Zhaoxing and Yazdanpanah, Vahid and Wang, Jindi and Gu, Wen and Shi, Lei and Cristea, Alexandra I. and Kiden, Sarah and Stein, Sebastian},
  year = {2025},
  doi = {10.48550/arXiv.2502.15709},
  url = {https://doi.org/10.48550/arxiv.2502.15709},
  publisher = {arXiv},
  note = {arXiv:2502.15709 [cs]},
  keywords = {source: Arxiv},
  abstract = {The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10\% improvement in user satisfaction and a 5{\textbackslash},
  urldate = {2025-10-26},
  month = {apr},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\8ENV99M9\\Li et al. - 2025 - TutorLLM Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Genera.pdf:application/pdf},
  shorttitle = {{TutorLLM},
}

@misc{shang_biomedical_2025,
  title = {Biomedical {Relation},
  author = {Shang, Yufei and Guo, Yanrong and Hao, Shijie and Hong, Richang},
  year = {2025},
  doi = {10.48550/arXiv.2501.05155},
  url = {https://doi.org/10.48550/arxiv.2501.05155},
  publisher = {arXiv},
  note = {arXiv:2501.05155 [cs]},
  keywords = {source: Arxiv},
  abstract = {Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensive texts, serving as a crucial subfield of biomedical text mining. Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences. Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness. Besides, the scarcity of annotated data further hampers model training. Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE. Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data. Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities. Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts. Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works.},
  urldate = {2025-10-26},
  month = {jan},
  annote = {Comment: 13 pages, 6 figures},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\C6D4F8AC\\Shang et al. - 2025 - Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Ident.pdf:application/pdf},
}

@misc{mortaheb_rag-check_2025,
  title = {{RAG},
  author = {Mortaheb, Matin and Khojastepour, Mohammad A. Amir and Chakradhar, Srimat T. and Ulukus, Sennur},
  year = {2025},
  doi = {10.48550/arXiv.2501.03995},
  url = {https://doi.org/10.48550/arxiv.2501.03995},
  publisher = {arXiv},
  note = {arXiv:2501.03995 [cs]},
  keywords = {source: Arxiv},
  abstract = {Retrieval-augmented generation (RAG) improves large language models (LLMs) by using external knowledge to guide response generation, reducing hallucinations. However, RAG, particularly multi-modal RAG, can introduce new hallucination sources: (i) the retrieval process may select irrelevant pieces (e.g., documents, images) as raw context from the database, and (ii) retrieved images are processed into text-based context via vision-language models (VLMs) or directly used by multi-modal language models (MLLMs) like GPT-4o, which may hallucinate. To address this, we propose a novel framework to evaluate the reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response. We train RS and CS models using a ChatGPT-derived database and human evaluator samples. Results show that both models achieve {\textasciitilde},
  urldate = {2025-10-26},
  month = {jan},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\B6LTL8XC\\Mortaheb et al. - 2025 - RAG-Check Evaluating Multimodal Retrieval Augmented Generation Performance.pdf:application/pdf},
  shorttitle = {{RAG},
}

@misc{jia_enhancing_2025,
  title = {Enhancing {LLMs},
  author = {Jia, Mengshuo and Cui, Zeyu and Hug, Gabriela},
  year = {2025},
  doi = {10.48550/arXiv.2411.16707},
  url = {https://doi.org/10.48550/arxiv.2411.16707},
  publisher = {arXiv},
  note = {arXiv:2411.16707 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control, source: Arxiv},
  abstract = {The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13\% and 96.85\%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30\% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.},
  annote = {Comment: 16 pages},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\5KY4QGPR\\Jia et al. - 2025 - Enhancing LLMs for Power System Simulations A Feedback-driven Multi-agent Framework.pdf:application/pdf},
  month = {may},
  shorttitle = {Enhancing {LLMs},
  urldate = {2025-10-26},
}

@misc{munir_towards_2024,
  title = {Towards {Evaluating},
  author = {Munir, Siraj and Aldini, Alessandro},
  year = {2024},
  doi = {10.48550/arXiv.2411.08449},
  url = {https://doi.org/10.48550/arxiv.2411.08449},
  publisher = {arXiv},
  note = {arXiv:2411.08449 [cs]},
  keywords = {Computer Science - Computation and Language, Computer Science - Emerging Technologies, source: Arxiv},
  abstract = {Large Language Models (LLMs) are revolutionizing the landscape of Generative Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging rapidly. However, when applied to database technologies, specifically query generation for graph databases and Knowledge Graphs (KGs), LLMs still face significant challenges. While research on LLM-driven query generation for Structured Query Language (SQL) exists, similar systems for graph databases remain underdeveloped. This paper presents a comparative study addressing the challenge of generating Cypher queries a powerful language for interacting with graph databases using open-access LLMs. We rigorously evaluate several LLM agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT) reasoning. Our empirical analysis of query generation accuracy reveals that Claude Sonnet 3.5 outperforms its counterparts in this specific domain. Further, we highlight promising future research directions to address the identified limitations and advance LLM-driven query generation for graph databases.},
  annote = {Comment: Paper accepted and will be presented at CSCI2024 in December 2024, Later will be published at Springer LNCS},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\JCGHXKF9\\Munir and Aldini - 2024 - Towards Evaluating Large Language Models for Graph Query Generation.pdf:application/pdf},
  month = {nov},
  urldate = {2025-10-26},
}

@inproceedings{zhu_trustful_2024,
  title = {Trustful {LLMs},
  author = {Zhu, Xiaofeng and Mandivarapu, Jaya Krishna},
  year = {2024},
  doi = {10.18653/v1/2024.customnlp4u-1.13},
  url = {https://doi.org/10.18653/v1/2024.customnlp4u-1.13},
  booktitle = {Proceedings of the 1st {Workshop},
  pages = {156--166},
  note = {arXiv:2411.07870 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv, source: Scopus},
  abstract = {Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm that leverages knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\VEZBNP65\\Zhu and Mandivarapu - 2024 - Trustful LLMs Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders.pdf:application/pdf},
  shorttitle = {Trustful {LLMs},
  urldate = {2025-10-26},
  annote = {Cited by: 0},
}

@misc{wang_when_2025,
  title = {When {Machine},
  author = {Wang, Shang and Zhu, Tianqing and Ye, Dayong and Zhou, Wanlei},
  year = {2025},
  doi = {10.48550/arXiv.2410.15267},
  url = {https://doi.org/10.48550/arxiv.2410.15267},
  publisher = {arXiv},
  note = {arXiv:2410.15267 [cs]},
  keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, source: Arxiv},
  abstract = {The deployment of large language models (LLMs) like ChatGPT and Gemini has shown their powerful natural language generation capabilities. However, these models can inadvertently learn and retain sensitive information and harmful content during training, raising significant ethical and legal concerns. To address these issues, machine unlearning has been introduced as a potential solution. While existing unlearning methods take into account the specific characteristics of LLMs, they often suffer from high computational demands, limited applicability, or the risk of catastrophic forgetting. To address these limitations, we propose a lightweight behavioral unlearning framework based on Retrieval-Augmented Generation (RAG) technology. By modifying the external knowledge base of RAG, we simulate the effects of forgetting without directly interacting with the unlearned LLM. We approach the construction of unlearned knowledge as a constrained optimization problem, deriving two key components that underpin the effectiveness of RAG-based unlearning. This RAG-based approach is particularly effective for closed-source LLMs, where existing unlearning methods often fail. We evaluate our framework through extensive experiments on both open-source and closed-source models, including ChatGPT, Gemini, Llama-2-7b-chat, and PaLM 2. The results demonstrate that our approach meets five key unlearning criteria: effectiveness, universality, harmlessness, simplicity, and robustness. Meanwhile, this approach can extend to multimodal large language models and LLM-based agents.},
  annote = {Comment: 16 pages, 9 figures, 13 tables. To appear in IEEE Transactions on Dependable and Secure Computing (TDSC), 2025},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\HP6VUXMW\\Wang et al. - 2025 - When Machine Unlearning Meets Retrieval-Augmented Generation (RAG) Keep Secret or Forget Knowledge.pdf:application/pdf},
  month = {oct},
  shorttitle = {When {Machine},
  urldate = {2025-10-26},
}

@misc{peng_athena_2024,
  title = {Athena: {Retrieval},
  author = {Peng, Xiao and Chen, Liang},
  year = {2024},
  doi = {10.48550/arXiv.2410.11195},
  url = {https://doi.org/10.48550/arxiv.2410.11195},
  publisher = {arXiv},
  note = {arXiv:2410.11195 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {Recently, large language models (LLMs) like ChatGPT, LLaMA, and Claude have prevailed in countless domains, including legal scenarios. With LLMs' rapid technological progress, the development of prompt engineering (PE) as an interface between the LLMs and real-world applications has drawn the attention of all developers. Various PE methods have been proposed to overcome real-world challenges, such as few-shot prompting, chain-of-thought, and retrieval-augmented generation (RAG). However, RAG for legal judgment prediction (LJP) is still underexplored. To address this, we propose "Athena", a novel framework cultivating RAG as a core preprocess component to enhance LLMs' performance on specialized tasks. Athena constructs a knowledge base for accusations, attached with a semantic retrieval mechanism through vectorization. Our experiments show that Athena's overall performance has improved significantly, achieving state-of-the-art results on the CAIL2018 dataset. Our ablation study on the in-context window size parameter further reproduces LLMs' "lost-in-the-middle" phenomenon with a relative positional variation. And with moderate hyper-parameter-tuning, we can achieve at most 95\% of accuracy accordingly. We also study the impact of query rewriting and data distribution, providing possible directions for future research based on former analyses.},
  annote = {Comment: 13 pages, 6 figures},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XF3BYVF3\\Peng and Chen - 2024 - Athena Retrieval-augmented Legal Judgment Prediction with Large Language Models.pdf:application/pdf},
  month = {oct},
  shorttitle = {Athena},
  urldate = {2025-10-26},
}

@misc{islam_open-rag_2024,
  title = {Open-{RAG},
  author = {Islam, Shayekh Bin and Rahman, Md Asib and Hossain, K. S. M. Tozammel and Hoque, Enamul and Joty, Shafiq and Parvez, Md Rizwan},
  year = {2024},
  doi = {10.48550/arXiv.2410.01782},
  url = {https://doi.org/10.48550/arxiv.2410.01782},
  publisher = {arXiv},
  note = {arXiv:2410.01782 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv},
  abstract = {Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/},
  annote = {Comment: Accepted to EMNLP 2024 Findings. Website: https://openragmoe.github.io/. 14 pages, 7 figures, 5 tables},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\3NHY8DRA\\Islam et al. - 2024 - Open-RAG Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models.pdf:application/pdf},
  month = {oct},
  shorttitle = {Open-{RAG},
  urldate = {2025-10-26},
}

@misc{anwar_evaluating_2024,
  title = {Evaluating {ChatGPT},
  author = {Anwar, Muhammad and Costa, Mischa de and Hammad, Issam and Lau, Daniel},
  year = {2024},
  doi = {10.48550/arXiv.2409.00090},
  url = {https://doi.org/10.48550/arxiv.2409.00090},
  publisher = {arXiv},
  note = {arXiv:2409.00090 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {This paper examines the application of ChatGPT, a large language model (LLM), for question-and-answer (Q\&A) tasks in the highly specialized field of nuclear data. The primary focus is on evaluating ChatGPT's performance on a curated test dataset, comparing the outcomes of a standalone LLM with those generated through a Retrieval Augmented Generation (RAG) approach. LLMs, despite their recent advancements, are prone to generating incorrect or 'hallucinated' information, which is a significant limitation in applications requiring high accuracy and reliability. This study explores the potential of utilizing RAG in LLMs, a method that integrates external knowledge bases and sophisticated retrieval techniques to enhance the accuracy and relevance of generated outputs. In this context, the paper evaluates ChatGPT's ability to answer domain-specific questions, employing two methodologies: A) direct response from the LLM, and B) response from the LLM within a RAG framework. The effectiveness of these methods is assessed through a dual mechanism of human and LLM evaluation, scoring the responses for correctness and other metrics. The findings underscore the improvement in performance when incorporating a RAG pipeline in an LLM, particularly in generating more accurate and contextually appropriate responses for nuclear domain-specific queries. Additionally, the paper highlights alternative approaches to further refine and improve the quality of answers in such specialized domains.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PGRQDWY4\\Anwar et al. - 2024 - Evaluating ChatGPT on Nuclear Domain-Specific Data.pdf:application/pdf},
  month = {aug},
  urldate = {2025-10-26},
}

@misc{sannidhi_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Sannidhi, Geethan and Sakhinana, Sagar Srinivas and Runkana, Venkataramana},
  year = {2024},
  doi = {10.48550/arXiv.2408.13273},
  url = {https://doi.org/10.48550/arxiv.2408.13273},
  publisher = {arXiv},
  note = {arXiv:2408.13273 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv},
  abstract = {Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google Gemini face challenges such as inaccurate factual recall, hallucinations, biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting. To address these issues, we introduce sLA-tKGF (small-scale language assistant for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG) aided, custom-trained small-scale language models through a tabula rasa approach from scratch for effective tKG forecasting. Our framework constructs knowledge-infused prompts with relevant historical data from tKGs, web search results, and PLLMs-generated textual descriptions to understand historical entity relationships prior to the target time. It leverages these external knowledge-infused prompts for deeper understanding and reasoning of context-specific semantic and temporal information to zero-shot prompt small-scale language models for more accurate predictions of future events within tKGs. It reduces hallucinations and mitigates distributional shift challenges through comprehending changing trends over time. As a result, it enables more accurate and contextually grounded forecasts of future events while minimizing computational demands. Rigorous empirical studies demonstrate our framework robustness, scalability, and state-of-the-art (SOTA) performance on benchmark datasets with interpretable and trustworthy tKG forecasting.},
  annote = {Comment: Paper was accepted at ACM KDD -2024 -- Undergraduate Consortium. Please find the link: https://kdd2024.kdd.org/undergraduate-consortium/},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PDJXXRCN\\Sannidhi et al. - 2024 - Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph F.pdf:application/pdf},
  month = {aug},
  urldate = {2025-10-26},
}

@misc{hopp_free_2024,
  title = {Free to play: {UN},
  author = {Hopp, Daniel},
  year = {2024},
  doi = {10.48550/arXiv.2407.16896},
  url = {https://doi.org/10.48550/arxiv.2407.16896},
  publisher = {arXiv},
  note = {arXiv:2407.16896 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
  abstract = {Generative artificial intelligence (AI), and in particular Large Language Models (LLMs), have exploded in popularity and attention since the release to the public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in November of 2022. Due to the power of these general purpose models and their ability to communicate in natural language, they can be useful in a range of domains, including the work of official statistics and international organizations. However, with such a novel and seemingly complex technology, it can feel as if generative AI is something that happens to an organization, something that can be talked about but not understood, that can be commented on but not contributed to. Additionally, the costs of adoption and operation of proprietary solutions can be both uncertain and high, a barrier for often cost-constrained international organizations. In the face of these challenges, United Nations Trade and Development (UNCTAD), through its Global Crisis Response Group (GCRG), has explored and developed its own open-source Retrieval Augmented Generation (RAG) LLM application. RAG makes LLMs aware of and more useful for the organization's domain and work. Developing in-house solutions comes with pros and cons, with pros including cost, flexibility, and fostering institutional knowledge. Cons include time and skill investments and gaps and application polish and power. The three libraries developed to produce the app, nlp\_pipeline for document processing and statistical analysis, local\_rag\_llm for running a local RAG LLM, and streamlit\_rag for the user interface, are publicly available on PyPI and GitHub with Dockerfiles. A fourth library, local\_llm\_finetune, is also available for fine-tuning existing LLMs which can then be used in the application.},
  shorttitle = {Free to play},
  urldate = {2025-10-26},
  month = {jun},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\J4L6JZUE\\Hopp - 2024 - Free to play UN Trade and Development's experience with developing its own open-source Retrieval Au.pdf:application/pdf},
}

@misc{zhao_empirical_2024-1,
  title = {An {Empirical},
  author = {Zhao, Yuetong and Cao, Hongyu and Zhao, Xianyu and Ou, Zhijian},
  year = {2024},
  doi = {10.48550/arXiv.2407.15569},
  url = {https://doi.org/10.48550/arxiv.2407.15569},
  publisher = {arXiv},
  note = {arXiv:2407.15569 [cs]},
  keywords = {source: Arxiv},
  abstract = {Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become essential tools in daily life. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Augmented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.},
  urldate = {2025-10-26},
  month = {aug},
  annote = {Comment: Accepted by ISCSLP 2024},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\MN77XEW3\\Zhao et al. - 2024 - An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought.pdf:application/pdf},
}

@misc{ateia_can_2024,
  title = {Can {Open},
  author = {Ateia, Samy and Kruschwitz, Udo},
  year = {2024},
  doi = {10.48550/arXiv.2407.13511},
  url = {https://doi.org/10.48550/arxiv.2407.13511},
  publisher = {arXiv},
  note = {arXiv:2407.13511 [cs]},
  keywords = {Computer Science - Computation and Language, source: Arxiv},
  abstract = {Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP) benchmarks across different domains. New competing Open-Source alternatives like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while often offering higher throughput and being less costly to use. Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties. We participated in the 12th BioASQ challenge, which is a retrieval augmented generation (RAG) setting, and explored the performance of current GPT models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning (zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional relevant knowledge from Wikipedia added to the context-window of the LLM might improve their performance. Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains. Our results indicate that the performance gap between commercial and open-source models in RAG setups exists mainly in the zero-shot setting and can be closed by simply collecting few-shot examples for domain-specific use cases. The code needed to rerun these experiments is available through GitHub.},
  annote = {Comment: Version as accepted at the BioASQ Lab at CLEF 2024},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\NRDHSHH9\\Ateia and Kruschwitz - 2024 - Can Open-Source LLMs Compete with Commercial Models Exploring the Few-Shot Performance of Current G.pdf:application/pdf},
  month = {jul},
  shorttitle = {Can {Open},
  urldate = {2025-10-26},
}

@misc{roffo_exploring_2024,
  title = {Exploring {Advanced},
  author = {Roffo, Giorgio},
  year = {2024},
  doi = {10.13140/RG.2.2.11774.80963},
  url = {https://doi.org/10.13140/rg.2.2.11774.80963},
  note = {arXiv:2407.12036 [cs]},
  keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, source: Arxiv},
  abstract = {This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategies, including instruction fine-tuning, parameter-efficient methods like LoRA, and Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced Self-Training (ReST). Additionally, it provides a comprehensive survey of transformer architectures and training techniques for LLMs. The source code can be accessed by contacting the author via email for a request.},
  annote = {Comment: Keywords: Language Model Benchmarking, Pre-Trained LLM Comparison, LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite, Comprehensive LLM Evaluation Toolkit},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6P95PGA5\\Roffo - 2024 - Exploring Advanced Large Language Models with LLMsuite.pdf:application/pdf},
  month = {nov},
  urldate = {2025-10-26},
}

@misc{feldman_ragged_2024,
  title = {{RAGged},
  author = {Feldman, Philip and Foulds, James R. and Pan, Shimei},
  year = {2024},
  doi = {10.48550/arXiv.2403.01193},
  url = {https://doi.org/10.48550/arxiv.2403.01193},
  publisher = {arXiv},
  note = {arXiv:2403.01193 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss implications for the development of more trustworthy LLMs.},
  annote = {Comment: 7 Pages, 1 Figure, 1 Table},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\YI2WT287\\Feldman et al. - 2024 - RAGged Edges The Double-Edged Sword of Retrieval-Augmented Chatbots.pdf:application/pdf},
  month = {jun},
  shorttitle = {{RAGged},
  urldate = {2025-10-26},
}

@misc{xie_finben_2024,
  title = {{FinBen},
  author = {Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and Xu, Yijing and Kang, Haoqiang and Kuang, Ziyan and Yuan, Chenhan and Yang, Kailai and Luo, Zheheng and Zhang, Tianlin and Liu, Zhiwei and Xiong, Guojun and Deng, Zhiyang and Jiang, Yuechen and Yao, Zhiyuan and Li, Haohang and Yu, Yangyang and Hu, Gang and Huang, Jiajia and Liu, Xiao-Yang and Lopez-Lira, Alejandro and Wang, Benyou and Lai, Yanzhao and Wang, Hao and Peng, Min and Ananiadou, Sophia and Huang, Jimin},
  year = {2024},
  doi = {10.48550/arXiv.2402.12659},
  url = {https://doi.org/10.48550/arxiv.2402.12659},
  publisher = {arXiv},
  note = {arXiv:2402.12659 [cs]},
  keywords = {source: Arxiv},
  abstract = {LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive evaluation benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 36 datasets spanning 24 financial tasks, covering seven critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, and decision-making. FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source evaluation datasets for text summarization, question answering, and stock trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation in financial LLMs. All datasets, results, and codes are released for the research community: https://github.com/The-FinAI/PIXIU.},
  urldate = {2025-10-26},
  month = {jun},
  annote = {Comment: 26 pages, 11 figures},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\APVTPMIG\\Xie et al. - 2024 - FinBen A Holistic Financial Benchmark for Large Language Models.pdf:application/pdf},
  shorttitle = {{FinBen},
}

@misc{alan_rag-based_2025,
  title = {A {RAG},
  author = {Alan, Ahmet Yusuf and Karaarslan, Enis and Aydin, Ömer},
  year = {2025},
  doi = {10.48550/arXiv.2401.15378},
  url = {https://doi.org/10.48550/arxiv.2401.15378},
  publisher = {arXiv},
  note = {arXiv:2401.15378 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {Challenges exist in learning and understanding religions, such as the complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect for enlightenment on religion as a question-answering chatbot. However, LLMs also tend to generate false information, known as hallucination. Also, the chatbots' responses can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It must avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called "MufassirQAS". We created a database consisting of several open-access books that include Turkish context. These books contain Turkish translations and interpretations of Islam. This database is utilized to answer religion-related questions and ensure our answers are trustworthy. The relevant part of the dataset, which LLM also uses, is presented along with the answer. We have put careful effort into creating system prompts that give instructions to prevent harmful, offensive, or disrespectful responses to respect people's values and provide reliable results. The system answers and shares additional information, such as the page number from the respective book and the articles referenced for obtaining the information. MufassirQAS and ChatGPT are also tested with sensitive questions. We got better performance with our system. Study and enhancements are still in progress. Results and future works are given.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\WYSYDS3K\\Alan et al. - 2025 - A RAG-based Question Answering System Proposal for Understanding Islam MufassirQAS LLM.pdf:application/pdf},
  month = {mar},
  shorttitle = {A {RAG},
  urldate = {2025-10-26},
}

@misc{kang_prompt-rag_2024,
  title = {Prompt-{RAG},
  author = {Kang, Bongsu and Kim, Jundong and Yun, Tae-Rim and Kim, Chang-Eop},
  year = {2024},
  doi = {10.48550/arXiv.2401.11246},
  url = {https://doi.org/10.48550/arxiv.2401.11246},
  publisher = {arXiv},
  note = {arXiv:2401.11246 [cs]},
  keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, source: Arxiv},
  abstract = {We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel approach to enhance the performance of generative large language models (LLMs) in niche domains. Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain. To explore and exemplify this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine (CM) documents, finding that KM document embeddings correlated more with token overlaps and less with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from conventional RAG models, operates without the need for embedding vectors. Its performance was assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed existing models, including ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and informativeness. Despite challenges like content structuring and response latency, the advancements in LLMs are expected to encourage the use of Prompt-RAG, making it a promising tool for other domains in need of RAG methods.},
  annote = {Comment: 26 pages, 4 figures, 5 tables},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7AKNUT36\\Kang et al. - 2024 - Prompt-RAG Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exempl.pdf:application/pdf},
  month = {jan},
  shorttitle = {Prompt-{RAG},
  urldate = {2025-10-26},
}

@misc{kulkarni_reinforcement_2024,
  title = {Reinforcement {Learning},
  author = {Kulkarni, Mandar and Tangarajan, Praveen and Kim, Kyung and Trivedi, Anusua},
  year = {2024},
  doi = {10.48550/arXiv.2401.06800},
  url = {https://doi.org/10.48550/arxiv.2401.06800},
  publisher = {arXiv},
  note = {arXiv:2401.06800 [cs]},
  keywords = {source: Arxiv},
  abstract = {With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline.},
  urldate = {2025-10-26},
  month = {jan},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\BRL6S2VJ\\Kulkarni et al. - 2024 - Reinforcement Learning for Optimizing RAG for Domain Chatbots.pdf:application/pdf},
}

@misc{allu_beyond_2024,
  title = {Beyond {Extraction},
  author = {Allu, Uday and Ahmed, Biddwan and Tripathi, Vishesh},
  year = {2024},
  doi = {10.48550/arXiv.2401.02333},
  url = {https://doi.org/10.48550/arxiv.2401.02333},
  publisher = {arXiv},
  note = {arXiv:2401.02333 [cs]},
  keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv},
  abstract = {The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our approach aims to significantly improve the precision of complex table queries, offering a promising solution to a longstanding challenge in information retrieval.},
  annote = {Comment: Submitted to IEEE},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\EAVL39X4\\Allu et al. - 2024 - Beyond Extraction Contextualising Tabular Data for Efficient Summarisation by Language Models.pdf:application/pdf},
  month = {feb},
  shorttitle = {Beyond {Extraction},
  urldate = {2025-10-26},
}

@misc{zhang_iag_2023,
  title = {{IAG},
  author = {Zhang, Zhebin and Zhang, Xinyu and Ren, Yuanhang and Shi, Saijiang and Han, Meng and Wu, Yongkang and Lai, Ruofei and Cao, Zhao},
  year = {2023},
  doi = {10.48550/arXiv.2311.18397},
  url = {https://doi.org/10.48550/arxiv.2311.18397},
  publisher = {arXiv},
  note = {arXiv:2311.18397 [cs]},
  keywords = {Computer Science - Computation and Language, source: Arxiv},
  abstract = {Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\F42SRXGR\\Zhang et al. - 2023 - IAG Induction-Augmented Generation Framework for Answering Reasoning Questions.pdf:application/pdf},
  month = {nov},
  shorttitle = {{IAG},
  urldate = {2025-10-26},
}

@misc{asai_self-rag_2023,
  title = {Self-{RAG},
  author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  year = {2023},
  doi = {10.48550/arXiv.2310.11511},
  url = {https://doi.org/10.48550/arxiv.2310.11511},
  publisher = {arXiv},
  note = {arXiv:2310.11511 [cs]},
  keywords = {source: Arxiv},
  abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
  urldate = {2025-10-26},
  month = {oct},
  annote = {Comment: 30 pages, 2 figures, 12 tables},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5NLVQ3H\\Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Critique through Self-Reflection.pdf:application/pdf},
  shorttitle = {Self-{RAG},
}

@misc{low_answering_2024,
  title = {Answering real-world clinical questions using large language model based systems},
  author = {Low, Yen Sia and Jackson, Michael L. and Hyde, Rebecca J. and Brown, Robert E. and Sanghavi, Neil M. and Baldwin, Julian D. and Pike, C. William and Muralidharan, Jananee and Hui, Gavin and Alexander, Natasha and Hassan, Hadeel and Nene, Rahul V. and Pike, Morgan and Pokrzywa, Courtney J. and Vedak, Shivam and Yan, Adam Paul and Yao, Dong-han and Zipursky, Amy R. and Dinh, Christina and Ballentine, Philip and Derieg, Dan C. and Polony, Vladimir and Chawdry, Rehan N. and Davies, Jordan and Hyde, Brigham B. and Shah, Nigam H. and Gombar, Saurabh},
  year = {2024},
  doi = {10.48550/arXiv.2407.00541},
  url = {https://doi.org/10.48550/arxiv.2407.00541},
  journal = {arXiv preprint arXiv …},
  publisher = {arXiv},
  note = {arXiv:2407.00541 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, source: Arxiv, source: Google Scholar},
  abstract = {Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2\% - 10\%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24\% (OpenEvidence) to 58\% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65\% vs. 0-9\%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.},
  annote = {Comment: 28 pages (2 figures, 3 tables) inclusive of 8 pages of supplemental materials (4 supplemental figures and 4 supplemental tables)},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5S5S66W\\Low et al. - 2024 - Answering real-world clinical questions using large language model based systems.pdf:application/pdf},
  month = {jun},
  urldate = {2025-10-26},
}

@misc{jauhiainen_evaluating_2024,
  title = {Evaluating {Students},
  author = {Jauhiainen, Jussi S. and Guerra, Agustín Garagorry},
  year = {2024},
  doi = {10.48550/arXiv.2405.05444},
  url = {https://doi.org/10.48550/arxiv.2405.05444},
  publisher = {arXiv},
  note = {arXiv:2405.05444 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, source: Arxiv},
  abstract = {Evaluating open-ended written examination responses from students is an essential yet time-intensive task for educators, requiring a high degree of effort, consistency, and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators' time. In our study, we explore the effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in assessing university students' open-ended answers to questions made about reference material they have studied. Each model was instructed to evaluate 54 answers repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers. As of spring 2024, our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs. There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open-ended written responses. Further comparative research is essential to determine the accuracy and cost-effectiveness of using LLMs for educational assessments.},
  annote = {Comment: 18 pages, 6 tables, 1 figure},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GLHCI9AQ\\Jauhiainen and Guerra - 2024 - Evaluating Students' Open-ended Written Responses with LLMs Using the RAG Framework for GPT-3.5, GP.pdf:application/pdf},
  month = {may},
  shorttitle = {Evaluating {Students},
  urldate = {2025-10-26},
}

@misc{kunstmann_eventchat_2024,
  title = {{EventChat},
  author = {Kunstmann, Hannes and Ollier, Joseph and Persson, Joel and Wangenheim, Florian von},
  year = {2024},
  doi = {10.48550/arXiv.2407.04472},
  url = {https://doi.org/10.48550/arxiv.2407.04472},
  publisher = {arXiv},
  note = {arXiv:2407.04472 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, source: Arxiv},
  abstract = {Large language models (LLMs) present an enormous evolution in the strategic potential of conversational recommender systems (CRS). Yet to date, research has predominantly focused upon technical frameworks to implement LLM-driven CRS, rather than end-user evaluations or strategic implications for firms, particularly from the perspective of a small to medium enterprises (SME) that makeup the bedrock of the global economy. In the current paper, we detail the design of an LLM-driven CRS in an SME setting, and its subsequent performance in the field using both objective system metrics and subjective user evaluations. While doing so, we additionally outline a short-form revised ResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly evolving field. Our results reveal good system performance from a user experience perspective (85.5\% recommendation accuracy) but underscore latency, cost, and quality issues challenging business viability. Notably, with a median cost of \$0.04 per interaction and a latency of 5.7s, cost-effectiveness and response time emerge as crucial areas for achieving a more user-friendly and economically viable LLM-driven CRS for SME settings. One major driver of these costs is the use of an advanced LLM as a ranker within the retrieval-augmented generation (RAG) technique. Our results additionally indicate that relying solely on approaches such as Prompt-based learning with ChatGPT as the underlying LLM makes it challenging to achieve satisfying quality in a production environment. Strategic considerations for SMEs deploying an LLM-driven CRS are outlined, particularly considering trade-offs in the current technical landscape.},
  annote = {Comment: 27 pages, 3 tables, 5 figures, pre-print manuscript, updated version of manuscript due to typo (previous version, Figure 5 was incorrectly named Figure 6)},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\A4IPQYDB\\Kunstmann et al. - 2024 - EventChat Implementation and user-centric evaluation of a large language model-driven conversationa.pdf:application/pdf},
  month = {jul},
  shorttitle = {{EventChat},
  urldate = {2025-10-26},
}

@misc{zeng_federated_2024-1,
  title = {Federated {Recommendation},
  author = {Zeng, Huimin and Yue, Zhenrui and Jiang, Qian and Wang, Dong},
  year = {2024},
  doi = {10.48550/arXiv.2403.04256},
  url = {https://doi.org/10.48550/arxiv.2403.04256},
  publisher = {arXiv},
  note = {arXiv:2403.04256 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, source: Arxiv},
  abstract = {Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6LQZKNCJ\\Zeng et al. - 2024 - Federated Recommendation via Hybrid Retrieval Augmented Generation.pdf:application/pdf},
  month = {mar},
  urldate = {2025-10-26},
}

@article{lakatos_investigating_2025,
  title = {Investigating the performance of {Retrieval},
  author = {Lakatos, Robert and Pollner, Peter and Hajdu, Andras and Joo, Tamas},
  year = {2025},
  doi = {10.3390/make7010015},
  url = {https://doi.org/10.3390/make7010015},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {7},
  number = {1},
  pages = {15},
  note = {arXiv:2403.09727 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv},
  abstract = {The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16\% in terms of the ROGUE score, 15\% in the case of the BLEU score, and 53\% based on the cosine similarity. This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8\% better METEOR score of FN models indicates greater creativity compared to RAG.},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\P8GXIXWR\\Lakatos et al. - 2025 - Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development.pdf:application/pdf},
  issn = {2504-4990},
  month = {feb},
  urldate = {2025-10-26},
}

@misc{lu_large_2024,
  title = {Large {Language},
  author = {Lu, Qiuhao and Li, Rui and Wen, Andrew and Wang, Jinlian and Wang, Liwei and Liu, Hongfang},
  year = {2024},
  doi = {10.48550/arXiv.2407.00731},
  url = {https://doi.org/10.48550/arxiv.2407.00731},
  publisher = {arXiv},
  note = {arXiv:2407.00731 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv},
  abstract = {Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER. Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.},
  annote = {Comment: AMIA 2024 Annual Symposium Proceedings},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6W5HR7GU\\Lu et al. - 2024 - Large Language Models Struggle in Token-Level Clinical Named Entity Recognition.pdf:application/pdf},
  month = {aug},
  urldate = {2025-10-26},
}

@misc{guan_mfort-qa_2024-1,
  title = {{MFORT},
  author = {Guan, Che and Huang, Mengyu and Zhang, Peng},
  year = {2024},
  doi = {10.48550/arXiv.2403.19116},
  url = {https://doi.org/10.48550/arxiv.2403.19116},
  publisher = {arXiv},
  note = {arXiv:2403.19116 [cs]},
  keywords = {source: Arxiv},
  abstract = {In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.},
  urldate = {2025-10-26},
  month = {mar},
  annote = {Comment: 8 pages},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\RYANUDW6\\Guan et al. - 2024 - MFORT-QA Multi-hop Few-shot Open Rich Table Question Answering.pdf:application/pdf},
  shorttitle = {{MFORT},
}

@inproceedings{fortuna_natural_2024,
  title = {Natural {Language},
  author = {Fortuna, Carolina and Hanžel, Vid and Bertalanič, Blaž},
  year = {2024},
  doi = {10.1109/SmartGridComm60555.2024.10738062},
  url = {https://doi.org/10.1109/smartgridcomm60555.2024.10738062},
  booktitle = {2024 {IEEE},
  journal = {2024 IEEE International …},
  pages = {8--14},
  note = {arXiv:2406.06566 [cs]},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, source: Arxiv, source: Google Scholar},
  abstract = {Domain specific digital twins, representing a digital replica of various segments of the smart grid, are foreseen as able to model, simulate, and control the respective segments. At the same time, knowledge-based digital twins, coupled with AI, may also empower humans to understand aspects of the system through natural language interaction in view of planning and policy making. This paper is the first to assess and report on the potential of Retrieval Augmented Generation (RAG) question answers related to household electrical energy measurement aspects leveraging a knowledge-based energy digital twin. Relying on the recently published electricity consumption knowledge graph that actually represents a knowledge-based digital twin, we study the capabilities of ChatGPT, Gemini and Llama in answering electricity related questions. Furthermore, we compare the answers with the ones generated through a RAG techniques that leverages an existing electricity knowledge-based digital twin. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.},
  annote = {Comment: Accepted at IEEE SmartGridComm'24},
  file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GDYAVYLM\\Fortuna et al. - 2024 - Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin.pdf:application/pdf},
  month = {sep},
  urldate = {2025-10-26},
}

@article{chen_leveraging_2025,
  title = {Leveraging large language models to assist philosophical counseling: prospective techniques, value, and challenges},
  author = {Chen, Bokai and Zheng, Weiwei and Zhao, Liang and Ding, Xiaojun},
  year = {2025},
  doi = {10.1057/s41599-025-04657-7},
  url = {https://doi.org/10.1057/s41599-025-04657-7},
  journal = {Humanities and Social Sciences Communications},
  volume = {12},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{tsai_notitle_2025,
  author = {Tsai, Chungnan and Xie, Jingnan and Lai, Chun Ming and Lin, Chingsheng},
  year = {2025},
  doi = {10.1007/s10586-025-05721-2},
  url = {https://doi.org/10.1007/s10586-025-05721-2},
  journal = {Cluster Computing},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{yu_empowering_2025,
  title = {Empowering scientific discovery with explainable small domain-specific and large language models},
  author = {Yu, Hengjie and Wang, Yizhi and Cheng, Tao and Yan, Yan and Dawson, Kenneth A. and Li, Sam F.Y. and Zheng, Yefeng and Jin, Yaochu},
  year = {2025},
  doi = {10.1007/s10462-025-11365-w},
  url = {https://doi.org/10.1007/s10462-025-11365-w},
  journal = {Artificial Intelligence Review},
  volume = {58},
  number = {12},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{waterman_custom_2025,
  title = {A {Custom},
  author = {Waterman, Richard P. and Lafving, Brandon D. and Okar, Ceren and Jain, Nupur},
  year = {2025},
  doi = {10.1002/sta4.70109},
  url = {https://doi.org/10.1002/sta4.70109},
  journal = {Stat},
  volume = {14},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{wang_enhancing_2025,
  title = {Enhancing {LLM},
  author = {Wang, Meng and Shen, Yangyang and Zhao, Bingcheng and Zhou, Xuefeng and Sun, Lan and Liu, Xing},
  year = {2025},
  doi = {10.1007/s13755-025-00379-x},
  url = {https://doi.org/10.1007/s13755-025-00379-x},
  journal = {Health Information Science and Systems},
  volume = {13},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhai_rag-ler_2025,
  title = {{RAG},
  author = {Zhai, Fengwen and Tang, Wenyang and Jin, Jing},
  year = {2025},
  doi = {10.1016/j.neucom.2025.131514},
  url = {https://doi.org/10.1016/j.neucom.2025.131514},
  journal = {Neurocomputing},
  volume = {656},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wang_retrieval_2025,
  title = {A retrieval augmented generation based optimization approach for medical knowledge understanding and reasoning in large language models},
  author = {Wang, Yingshuai and Wan, Yanli and Lei, Xingyun and Chen, Qingkun and Hu, Hongpu},
  year = {2025},
  doi = {10.1016/j.array.2025.100504},
  url = {https://doi.org/10.1016/j.array.2025.100504},
  journal = {Array},
  volume = {28},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{nahar_catch_2025,
  title = {Catch me if you search: {When},
  author = {Nahar, Mahjabin and Lee, Eunju and Park, Jin-won and Lee, Dongwon},
  year = {2025},
  doi = {10.1016/j.chb.2025.108763},
  url = {https://doi.org/10.1016/j.chb.2025.108763},
  journal = {Computers in Human Behavior},
  volume = {173},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{song_graph_2025,
  title = {Graph retrieval augmented large language models for facial phenotype associated rare genetic disease},
  author = {Song, Jie and Xu, Zhichuan (Jason) and He, Mengqiao and Feng, Jinhua and Shen, Bairong},
  year = {2025},
  doi = {10.1038/s41746-025-01955-x},
  url = {https://doi.org/10.1038/s41746-025-01955-x},
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{garcia_improving_2025,
  title = {Improving automated deep phenotyping through large language models using retrieval-augmented generation},
  author = {Garcia, Brandon T. and Westerfield, Lauren E. and Yelemali, Priya and Gogate, Nikhita and Rivera-Muñoz, Edgar Andres and Du, Haowei and Dawood, Moez and Jolly, Angad and Lupski, James R. and Posey, Jennifer E.},
  year = {2025},
  doi = {10.1186/s13073-025-01521-w},
  url = {https://doi.org/10.1186/s13073-025-01521-w},
  journal = {Genome Medicine},
  volume = {17},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{islayem_using_2025,
  title = {Using large language models for enhanced fraud analysis and detection in blockchain based health insurance claims},
  author = {Islayem, Ruba and Gebreab, Senay A. and Al Khader, Walaa and Musamih, Ahmad and Salah, Khaled H. and Jayaraman, Raja and Khan, Muhammad Khurram},
  year = {2025},
  doi = {10.1038/s41598-025-15676-4},
  url = {https://doi.org/10.1038/s41598-025-15676-4},
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{sun_humanmod_2025,
  title = {{HumanMoD},
  author = {Sun, Song and Zhong, Zhijie and Yu, Nanlan and Gong, Xinrong and Yang, Kaixiang},
  year = {2025},
  doi = {10.1016/j.asoc.2025.113684},
  url = {https://doi.org/10.1016/j.asoc.2025.113684},
  journal = {Applied Soft Computing},
  volume = {184},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… This paper introduces HumanMoD, a novel LLM framework designed to emulate … bases to mitigate LLM hallucinations, ensuring that recommendations are rooted in credible medical …},
  annote = {Cited by: 0},
}

@article{barrit_specialized_2025,
  title = {Specialized {AI},
  author = {Barrit, Sami and Ranuzzi, Giovanni and Fetzer, Steffen and Al Barajraji, Mejdeddine and Hadwe, Salim El and Zanello, Marc and Otler, Martin and O’Flaherty, Julieta and Massager, Nicolas and Madsen, Joseph R. and Dibué-Adjei, Maxine and Carron, Romain},
  year = {2025},
  doi = {10.1007/s00701-025-06610-8},
  url = {https://doi.org/10.1007/s00701-025-06610-8},
  journal = {Acta Neurochirurgica},
  volume = {167},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wada_retrieval-augmented_2025,
  title = {Retrieval-augmented generation elevates local {LLM},
  author = {Wada, Akihiko and Tanaka, Yuya and Nishizawa, Mitsuo and Yamamoto, Akira and Akashi, Toshiaki and Hagiwara, Akifumi and Hayakawa, Yayoi K. and Kikuta, Junko and Shimoji, Keigo and Sano, Katsuhiro and Kamagata, Koji and Nakanishi, Atsushi and Aoki, Shigeki},
  year = {2025},
  doi = {10.1038/s41746-025-01802-z},
  url = {https://doi.org/10.1038/s41746-025-01802-z},
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Our findings reinforce RAG’s critical role in hallucination mitigation, a fundamental … The complete elimination of hallucinations (reducing incidence from 8\% to 0\%) in our RAG-enhanced …},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{edelman_valsci_2025,
  title = {Valsci: an open-source, self-hostable literature review utility for automated large-batch scientific claim verification using large language models},
  author = {Edelman, Brice and Skolnick’, Jeffrey},
  year = {2025},
  doi = {10.1186/s12859-025-06159-4},
  url = {https://doi.org/10.1186/s12859-025-06159-4},
  journal = {BMC Bioinformatics},
  volume = {26},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{xie_intelligent_2025,
  title = {An intelligent guided troubleshooting method for aircraft based on {HybirdRAG},
  author = {Xie, Xiaoyue and Tang, Xilang and Gu, Siwei and Cui, Lijie},
  year = {2025},
  doi = {10.1038/s41598-025-02643-2},
  url = {https://doi.org/10.1038/s41598-025-02643-2},
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{umer_transforming_2025,
  title = {Transforming education: tackling the two sigma problem with {AI},
  author = {Umer, Fahad and Naved, Nighat and Naseem, Azra and Mansoor, Ayesha and Kazmi, Professor Syed Murtaza Raza},
  year = {2025},
  doi = {10.1038/s41405-025-00338-4},
  url = {https://doi.org/10.1038/s41405-025-00338-4},
  journal = {BDJ Open},
  volume = {11},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) mitigates this by … of a RAG-enhanced LLM to support journal club discussions. … neutral response, citing both advantages and limitations. The …},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{zhang_leveraging_2025,
  title = {Leveraging long context in retrieval augmented language models for medical question answering},
  author = {Zhang, Gongbo and Xu, Zihan and Jin, Qiao and Chen, Fangyi and Fang, Yilu and Liu, Yi and Rousseau, Justin F. and Xu, Ziyang and Lu, Zhiyong and Weng, Chunhua and Peng, Yifan},
  year = {2025},
  doi = {10.1038/s41746-025-01651-w},
  url = {https://doi.org/10.1038/s41746-025-01651-w},
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{ke_retrieval_2025,
  title = {Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness},
  author = {Ke, Yuhe and Jin, Liyuan and Elangovan, Kabilan and Abdullah, Hairil Rizal and Liu, Nan and Sia, Alex Tiong Heng and Soh, Chai Rick and Tung, Joshua Yi Min and Ong, Jasmine Chiat Ling and Kuo, Changfu and Wu, Shaochun and Kovacheva, Vesela P. and Ting, Daniel Shu Wei},
  year = {2025},
  doi = {10.1038/s41746-025-01519-z},
  url = {https://doi.org/10.1038/s41746-025-01519-z},
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Additionally, the model exhibited an absence of hallucinations and … LLM-RAG models into healthcare workflows, such as preoperative medicine. Our findings indicate that the LLM-RAG …},
  annote = {Cited by: 14; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{upadhyay_enhancing_2025,
  title = {Enhancing {Health},
  author = {Upadhyay, Rishabh and Viviani, Marco},
  year = {2025},
  doi = {10.1007/s10791-025-09505-5},
  url = {https://doi.org/10.1007/s10791-025-09505-5},
  journal = {Discover Computing},
  volume = {28},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 9; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{goodell_large_2025,
  title = {Large language model agents can use tools to perform clinical calculations},
  author = {Goodell, Alex J. and Chu, Simon N. and Rouholiman, Dara and Chu, Lawrence Fu Nien},
  year = {2025},
  doi = {10.1038/s41746-025-01475-8},
  url = {https://doi.org/10.1038/s41746-025-01475-8},
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… questions in medicine but are prone to hallucinations and arithmetic errors. Early evidence suggests … To assess the incremental value of each additional tool (code interpreter, RAG, and …},
  annote = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{zhang_treeqa_2025,
  title = {{TreeQA},
  author = {Zhang, Xiangrui and Zhao, Fuyong and Liu, Yutian and Chen, Panfeng and Wang, Yanhao and Wang, Xiaohua and Ma, Dan and Xu, Huarong and Chen, Mei and Li, Hui},
  year = {2025},
  doi = {10.1016/j.knosys.2025.114526},
  url = {https://doi.org/10.1016/j.knosys.2025.114526},
  journal = {Knowledge-Based Systems},
  volume = {330},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{chen_knowledge_2025-1,
  title = {Knowledge graph and large language model integration with focus on educational applications: {A},
  author = {Chen, Guanyu and Song, Tao and Wang, Quanyu and Ma, Zheng and Hu, Jun and Li, Qi and Wu, Chunming},
  year = {2025},
  doi = {10.1016/j.neucom.2025.131230},
  url = {https://doi.org/10.1016/j.neucom.2025.131230},
  journal = {Neurocomputing},
  volume = {654},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{de_jesus_enhanced_2025,
  title = {Enhanced {LLM},
  author = {de Jesus, Davi Dos Reis and de Souza Júnior, Antônio Pereira and de Albergaria, Elisa Tuler and Pagano, Adriana P. and de Oliveira, Isaias José Ramos and Dias, Cristiane Dos Santos and Lage, Eura Martins and de Oliveira, Flávia Ribeiro and Oliveira, Juliana Almeida and de Carvalho Gomes, Igor and Rocha, Leonardo Chaves Dutra Da and Silveira Nogueira Reis, Zilma Silveira Nogueira},
  year = {2025},
  doi = {10.1016/j.compbiomed.2025.111135},
  url = {https://doi.org/10.1016/j.compbiomed.2025.111135},
  journal = {Computers in Biology and Medicine},
  volume = {198},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{adanza_leveraging_2025,
  title = {Leveraging generative {AI},
  author = {Adanza, Daniel and Gifre Renom, Lluis L. and Alemany, Pol and Natalino, Carlos and Monti, Paolo and Munoz, Raul and Vilalta, Ricard},
  year = {2025},
  doi = {10.1016/j.comnet.2025.111647},
  url = {https://doi.org/10.1016/j.comnet.2025.111647},
  journal = {Computer Networks},
  volume = {272},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zong_evidencemap_2025,
  title = {{EvidenceMap},
  author = {Zong, Chang and Wan, Jian and Siliang, Tang and Zhang, Lei},
  year = {2025},
  doi = {10.1016/j.artmed.2025.103246},
  url = {https://doi.org/10.1016/j.artmed.2025.103246},
  journal = {Artificial Intelligence in Medicine},
  volume = {169},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{shi_fine-tuning_2025,
  title = {Fine-tuning a large language model for automated code compliance of building regulations},
  author = {Shi, Jack Wei Lun and Solihin, Wawan and Yeoh, Justin K.W.},
  year = {2025},
  doi = {10.1016/j.aei.2025.103676},
  url = {https://doi.org/10.1016/j.aei.2025.103676},
  journal = {Advanced Engineering Informatics},
  volume = {68},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{mondillo_artificial_2025,
  title = {Artificial intelligence for solving pediatric clinical cases: {A},
  author = {Mondillo, Gianluca and Colosimo, Simone and Perrotta, Alessandra and Frattolillo, Vittoria and Masino, Mariapia and Martino, Marco and Miraglia del Giudice, Emanuele and Marzuillo, Pierluigi},
  year = {2025},
  doi = {10.1016/j.ijmedinf.2025.106027},
  url = {https://doi.org/10.1016/j.ijmedinf.2025.106027},
  journal = {International Journal of Medical Informatics},
  volume = {203},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Hybrid Gold Open Access},
}

@article{xu_development_2025,
  title = {Development and evaluation of a retrieval-augmented large language model framework for enhancing endodontic education},
  author = {Xu, Xiaowei and Liu, Siyi and Zhu, Lin and Long, Yunzi and Zeng, Yin and Lu, Xudong and Li, Jiao and Dong, Yanmei},
  year = {2025},
  doi = {10.1016/j.ijmedinf.2025.106006},
  url = {https://doi.org/10.1016/j.ijmedinf.2025.106006},
  journal = {International Journal of Medical Informatics},
  volume = {203},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… impact of LLM technologies throughout the medical domain. … generation of hallucinations, instances where LLM produce … -KB platform with the RAG framework, combining the reasoning …},
  annote = {Cited by: 1; All Open Access; Hybrid Gold Open Access},
}

@article{qu_pncd_2025,
  title = {{PNCD},
  author = {Qu, Jiayi and Liu, Jun and Liu, Xiangjun and Chen, Meihui and Li, Jinchi and Wang, Jintao},
  year = {2025},
  doi = {10.1016/j.inffus.2025.103328},
  url = {https://doi.org/10.1016/j.inffus.2025.103328},
  journal = {Information Fusion},
  volume = {123},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… ability to cope with the hallucinations of LLMs. The ability to … (PNCD) based on RAG to solve the hallucination of LLMs in … of negative examples that induce hallucinations for BaseLLMs. …},
  annote = {Cited by: 1},
}

@article{zhang_notitle_2025,
  author = {Zhang, Xiaojun and null, null and Zhao, Jinghao and null, null and Tian, Yajun},
  year = {2025},
  doi = {10.1016/j.jclepro.2025.146776},
  url = {https://doi.org/10.1016/j.jclepro.2025.146776},
  journal = {Journal of Cleaner Production},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{shunmuga_priya_gender_2025,
  title = {Gender inclusive language generation framework: {A},
  author = {Shunmuga Priya, M. C. and Drury-Grogan, Meghann L. and Chakravarthi, Bharathi Raja},
  year = {2025},
  doi = {10.1016/j.knosys.2025.114092},
  url = {https://doi.org/10.1016/j.knosys.2025.114092},
  journal = {Knowledge-Based Systems},
  volume = {328},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{ozmen_specialized_2025,
  title = {Specialized {Artificial},
  author = {Ozmen, Berk Baris and Singh, Nishant and Shah, Kavach and Berber, Ibrahim and Singh, Damanjit and Pinsky, Eugene and Sinclair, Nicholas R. and Isakov, Raymond and Schwarz, Graham S.},
  year = {2025},
  doi = {10.1093/asj/sjaf120},
  url = {https://doi.org/10.1093/asj/sjaf120},
  journal = {Aesthetic Surgery Journal},
  volume = {45},
  number = {11},
  pages = {1206 -- 1212},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{guo_leveraging_2025,
  title = {Leveraging inter-chunk interactions for enhanced retrieval in large language model-based question answering},
  author = {Guo, Tiezheng and Wang, Chen and Liu, Yanyi and Tang, Jiawei and Li, Pan and Xu, Sai and Yang, Qingwen and Gao, Xianlin and Li, Zhi and Wen, Yingyou},
  year = {2025},
  doi = {10.1016/j.neucom.2025.130931},
  url = {https://doi.org/10.1016/j.neucom.2025.130931},
  journal = {Neurocomputing},
  volume = {650},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{null_notitle_2025,
  author = {null, null and null, null and null, null and Lee, Minsu},
  year = {2025},
  doi = {10.3390/electronics14193859},
  url = {https://doi.org/10.3390/electronics14193859},
  booktitle = {{CEUR},
  journal = {Electronics (Switzerland)},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{null_notitle_2025-1,
  author = {null, null and Yu, Xuhong and null, null and Nong, Jing and Liu, Zhentao and Dai, Xinmin and Xie, Xiaoyao},
  year = {2025},
  doi = {10.3390/app151910447},
  url = {https://doi.org/10.3390/app151910447},
  journal = {Applied Sciences (Switzerland)},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wei_rtphy-chatbot_2025,
  title = {{RTPhy},
  author = {Wei, Shuoyang and Hu, Ankang and Wang, Zhiqun and Meng, Xiangyin and Yu, Lang and Yang, Bo and Qiu, Jie},
  year = {2025},
  doi = {10.1002/acm2.70263},
  url = {https://doi.org/10.1002/acm2.70263},
  journal = {Journal of Applied Clinical Medical Physics},
  volume = {26},
  number = {10},
  pages = {e70263},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{bayram_evaluating_2025,
  title = {Evaluating {AI},
  author = {Bayram, Hati̇ce Merve and Arslan, Sedat and Ozturkcan, Arda},
  year = {2025},
  doi = {10.1111/jep.70295},
  url = {https://doi.org/10.1111/jep.70295},
  journal = {Journal of Evaluation in Clinical Practice},
  volume = {31},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{cruzes_revolutionizing_2025,
  title = {Revolutionizing optical networks: {The},
  author = {Cruzes, Sergio},
  year = {2025},
  doi = {10.1016/j.osn.2025.100812},
  url = {https://doi.org/10.1016/j.osn.2025.100812},
  journal = {Optical Switching and Networking},
  volume = {57},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{niel_performance_2025,
  title = {Performance evaluation of large language models in pediatric nephrology clinical decision support: a comprehensive assessment},
  author = {Niel, Olivier R.P. and Dookhun, Dishana and Caliment, Ancuta},
  year = {2025},
  doi = {10.1007/s00467-025-06819-w},
  url = {https://doi.org/10.1007/s00467-025-06819-w},
  journal = {Pediatric Nephrology},
  volume = {40},
  number = {10},
  pages = {3211 -- 3218},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ding_enhancing_2025,
  title = {Enhancing graph multi-hop reasoning for question answering with {LLMs},
  author = {Ding, Lianhong and Ding, Na and Tao, Qi and Shi, Peng},
  year = {2025},
  doi = {10.1007/s10844-025-00945-5},
  url = {https://doi.org/10.1007/s10844-025-00945-5},
  journal = {Journal of Intelligent Information Systems},
  volume = {63},
  number = {5},
  pages = {1455 -- 1485},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{pimpale_legalmind_2025,
  title = {{LEGALMIND},
  author = {Pimpale, Harsh and Raut, Aditi and Patil, Yash Kishor and Parpol, Gaurav and Yadav, Prajwal and Sangoi, Janhavi},
  year = {2025},
  doi = {10.5935/jetia.v11i55.1925},
  url = {https://doi.org/10.5935/jetia.v11i55.1925},
  journal = {Journal of Engineering and Technology for Industrial Applications},
  volume = {11},
  number = {55},
  pages = {105 -- 117},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kang_similarity_2025,
  title = {Similarity {Evaluation},
  author = {Kang, Hanhoon},
  year = {2025},
  doi = {10.3837/tiis.2025.09.008},
  url = {https://doi.org/10.3837/tiis.2025.09.008},
  journal = {KSII Transactions on Internet and Information Systems},
  volume = {19},
  number = {9},
  pages = {2963 -- 2983},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{kaufmes_evaluating_2025,
  title = {Evaluating {Medium},
  author = {Kaufmes, Kevin and Mathes, Georg and Vladimirova, Dilyana and Berger, Stephanie and Fegeler, Christian and Sigle, Stefan},
  year = {2025},
  doi = {10.3233/SHTI251382},
  url = {https://doi.org/10.3233/shti251382},
  journal = {Studies in Health Technology and Informatics},
  volume = {331},
  pages = {81 -- 90},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{malik_evaluating_2025,
  title = {Evaluating {Artificial},
  author = {Malik, Sheza and Frey, Lewis J. and Gutman, Jason and Mushtaq, Asim and Warraich, Fatima and Qureshi, Kamran},
  year = {2025},
  doi = {10.14309/ajg.0000000000003255},
  url = {https://doi.org/10.14309/ajg.0000000000003255},
  journal = {American Journal of Gastroenterology},
  volume = {120},
  number = {9},
  pages = {2081 -- 2085},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@article{ehrlich-sommer_forestgpt_2025,
  title = {{ForestGPT},
  author = {Ehrlich-Sommer, Florian and Eberhard, Benno Richard and Holzinger, Andreas T.},
  year = {2025},
  doi = {10.3390/electronics14183583},
  url = {https://doi.org/10.3390/electronics14183583},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {18},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{feyijimi_chatgpts_2025,
  title = {{ChatGPT},
  author = {Feyijimi, Taiwo Raphael and Aliu, John and Oke, Ayodeji Emmanuel and Aghimien, Douglas Omoregie},
  year = {2025},
  doi = {10.3390/computers14090366},
  url = {https://doi.org/10.3390/computers14090366},
  journal = {Computers},
  volume = {14},
  number = {9},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{ibrahim_mera_2025,
  title = {{MERA},
  author = {Ibrahim, Ahmed and Khalili, Abdullah and Arabi, Maryam and Sattar, Aamenah and Hosseini, Abdullah and Serag, Ahmed},
  year = {2025},
  doi = {10.3390/make7030073},
  url = {https://doi.org/10.3390/make7030073},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {7},
  number = {3},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kovari_ai_2025,
  title = {{AI},
  author = {Kovari, A.},
  year = {2025},
  doi = {10.3390/computers14090367},
  url = {https://doi.org/10.3390/computers14090367},
  journal = {Computers},
  volume = {14},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{ersahin_bedside_2025,
  title = {From {Bedside},
  author = {Ersahin, Koray and Sanduleanu, Sebastian and Thulasi Seetha, Sithin and Bremm, Johannes and Abbasli, Cavid and Zimmer, Chantal and Damer, Tim and Kottlors, Jonathan and Goertz, Lukas and Bruns, Christiane Josephine and Maintz, David and Abdullayev, Nuran},
  year = {2025},
  doi = {10.3390/life15091387},
  url = {https://doi.org/10.3390/life15091387},
  journal = {Life},
  volume = {15},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{neha_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Neha, Fnu and Bhati, Deepshikha and Shukla, Deepak Kumar},
  year = {2025},
  doi = {10.3390/ai6090226},
  url = {https://doi.org/10.3390/ai6090226},
  journal = {AI (Switzerland)},
  volume = {6},
  number = {9},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{syahputri_unlocking_2025,
  title = {Unlocking the {Potential},
  author = {Syahputri, Irdina Wanda and Budiardjo, E. K. and Putra, Panca O.Hadi},
  year = {2025},
  doi = {10.3390/ai6090206},
  url = {https://doi.org/10.3390/ai6090206},
  journal = {AI (Switzerland)},
  volume = {6},
  number = {9},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{iyenghar_empirical_2025,
  title = {Empirical {Evaluation},
  author = {Iyenghar, Padma},
  year = {2025},
  doi = {10.3390/electronics14183624},
  url = {https://doi.org/10.3390/electronics14183624},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {18},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{vinayan_kozhipuram_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Vinayan Kozhipuram, Aparna and Shailendra, Samar and Kadel, Rajan},
  year = {2025},
  doi = {10.3390/info16090766},
  url = {https://doi.org/10.3390/info16090766},
  journal = {Information (Switzerland)},
  volume = {16},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{tebourbi_bpmn-based_2025,
  title = {{BPMN},
  author = {Tebourbi, Hedi and Nouzri, Sana and Mualla, Yazan and El Fatimi, Meryem and Najjar, Amro and Abbas-Turki, A. and Dridi, Mahjoub},
  year = {2025},
  doi = {10.3390/info16090809},
  url = {https://doi.org/10.3390/info16090809},
  journal = {Information (Switzerland)},
  volume = {16},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{mokashi_analysis_2025,
  title = {Analysis of {Large},
  author = {Mokashi, Abhijit and Puthuparambil, Bennet and Daniel, Chaissy and Hanne, Thomas},
  year = {2025},
  doi = {10.3390/info16090786},
  url = {https://doi.org/10.3390/info16090786},
  journal = {Information (Switzerland)},
  volume = {16},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… compare ChatGPT-4 responses before and after RAG integration, … that RAG improves the relevance and verifiability of LLM … on how RAG can mitigate biases and hallucinations inherent …},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{lee_memory-augmented_2025,
  title = {Memory-{Augmented},
  author = {Lee, Jaeseung and Rew, Jehyeok},
  year = {2025},
  doi = {10.3390/app15179775},
  url = {https://doi.org/10.3390/app15179775},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {17},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{jung_intelligent_2025,
  title = {An {Intelligent},
  author = {Jung, Tae-moon and Joe, Inwhee},
  year = {2025},
  doi = {10.3390/app15179398},
  url = {https://doi.org/10.3390/app15179398},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {17},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{liu_reapr_2025,
  title = {{ReAPR},
  author = {Liu, Zixin and Du, Xiaozhi and Liu, Hairui},
  year = {2025},
  doi = {10.1007/s11219-025-09728-1},
  url = {https://doi.org/10.1007/s11219-025-09728-1},
  journal = {Software Quality Journal},
  volume = {33},
  number = {3},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{forootani_bio-eng-llm_2025,
  title = {Bio-{Eng},
  author = {Forootani, Ali and Esmaeili Aliabadi, Danial and Thraen, Daniela},
  year = {2025},
  doi = {10.1016/j.softx.2025.102260},
  url = {https://doi.org/10.1016/j.softx.2025.102260},
  journal = {SoftwareX},
  volume = {31},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{proestel_semantic_2025,
  title = {Semantic {Search},
  author = {Proestel, Scott E. and Jeng, Linda Jo Bone and Smith, Christopher and Deady, Matthew and Amer, Omar and Ahmed, Mohamed and Rodgers, Sarah},
  year = {2025},
  doi = {10.1007/s43441-025-00798-8},
  url = {https://doi.org/10.1007/s43441-025-00798-8},
  journal = {Therapeutic Innovation and Regulatory Science},
  volume = {59},
  number = {5},
  pages = {1148 -- 1159},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{do_esac_2025,
  title = {{ESAC},
  author = {Do, Changwoo and Nagy, Gergely and Heller, William T.},
  year = {2025},
  doi = {10.1016/j.softx.2025.102191},
  url = {https://doi.org/10.1016/j.softx.2025.102191},
  journal = {SoftwareX},
  volume = {31},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{bedi_xlr-kgdd_2025,
  title = {{XLR},
  author = {Bedi, Punam and Thukral, Anjali and Dhiman, Shivani},
  year = {2025},
  doi = {10.1007/s10115-025-02465-8},
  url = {https://doi.org/10.1007/s10115-025-02465-8},
  journal = {Knowledge and Information Systems},
  volume = {67},
  number = {9},
  pages = {7451 -- 7471},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{silva_ai-assisted_2025,
  title = {{AI},
  author = {Silva, Julio C.M.C. and Gouveia, Rafael P. and Zielinski, Kallil M.C. and Oliveira, Maria Cristina F. and Amancio, Diego Raphael and Bruno, Odemir Martinez and de Oliveira Junior, Osvaldo Novais},
  year = {2025},
  doi = {10.1021/acsami.5c08837},
  url = {https://doi.org/10.1021/acsami.5c08837},
  journal = {ACS Applied Materials and Interfaces},
  volume = {17},
  number = {34},
  pages = {47795 -- 47805},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{zhang_plantgpt_2025,
  title = {{PlantGPT},
  author = {Zhang, Ruixiang and Wang, Yu and Yang, Weiyang and Wen, Jun and Liu, Weizhi and Zhi, Shipeng and Li, Guangzhou and Chai, Nan and Huang, Jiaqi and Xie, Yongyao and Xie, Xianrong and Chen, Letian and Gu, Miao and Liu, Yaoguang and Zhu, Qinlong},
  year = {2025},
  doi = {10.1002/advs.202503926},
  url = {https://doi.org/10.1002/advs.202503926},
  journal = {Advanced Science},
  volume = {12},
  number = {30},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 7; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{kuroda_cocktailai_2025,
  title = {{CocktailAI},
  author = {Kuroda, Tomohiro and Sado, Keina and Suda, Kenji and Okamoto, Kazuya and Miyake, Masahiro and Tamura, Hiroshi and Sugiyama, Osamu and Yamamoto, Goshiro},
  year = {2025},
  doi = {10.3233/SHTI250875},
  url = {https://doi.org/10.3233/shti250875},
  journal = {Studies in Health Technology and Informatics},
  volume = {329},
  pages = {426 -- 430},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{shah_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Shah, Hurmat Ali and Islam, Ashhadul and Ul Abideen Tariq, Zain and Belhaouari, Samir Brahim and Househ, Mowafa Said},
  year = {2025},
  doi = {10.3233/SHTI250929},
  url = {https://doi.org/10.3233/shti250929},
  journal = {Studies in Health Technology and Informatics},
  volume = {329},
  pages = {693 -- 697},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{segalini_tanagpt_2025,
  title = {Tanagpt: {How},
  author = {Segalini, Agustín and Castro, Cyntia and Lerner, Jacobo and Horn, Maria Clara and Simon, Mariana and Rubin, Luciana and Otero, Carlos Martin and Pedretti, Ana Soledad and Castaño, José M. and Marin, Francisco and Kloster, Pablo and Pietragalla, Patricio and Luna, Daniel Roberto and Benítez, Sonia Elizabeth},
  year = {2025},
  doi = {10.3233/SHTI250813},
  url = {https://doi.org/10.3233/shti250813},
  journal = {Studies in Health Technology and Informatics},
  volume = {329},
  pages = {118 -- 122},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{jiang_retrieval_2025,
  title = {Retrieval {And},
  author = {Jiang, Pengcheng and Ouyang, Siru and Jiao, Yizhu and Zhong, Ming and Tian, Runchu and Han, Jiawei},
  year = {2025},
  doi = {10.1145/3711896.3736557},
  url = {https://doi.org/10.1145/3711896.3736557},
  booktitle = {Proceedings of the {ACM},
  volume = {2},
  pages = {6032 -- 6042},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{huang_prompting_2025,
  title = {Prompting large language models with knowledge graphs for question answering involving long-tail facts},
  author = {Huang, Wenyu and Zhou, Guancheng and Lapata, Mirella and Vougiouklis, Pavlos and Montella, Sebastien and Pan, Jeff Z.},
  year = {2025},
  doi = {10.1016/j.knosys.2025.113648},
  url = {https://doi.org/10.1016/j.knosys.2025.113648},
  journal = {Knowledge-Based Systems},
  volume = {324},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{shahraki_big_2025,
  title = {Big {Data},
  author = {Shahraki, Hamed Shahrokhi and Babazadeh, Abbas},
  year = {2025},
  doi = {10.1007/s13177-025-00470-3},
  url = {https://doi.org/10.1007/s13177-025-00470-3},
  journal = {International Journal of Intelligent Transportation Systems Research},
  volume = {23},
  number = {2},
  pages = {629 -- 647},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{welsh_custom-tailored_2025,
  title = {Custom-{Tailored},
  author = {Welsh, Michael and Lopez-Rippe, Julian and Alkhulaifat, Dana and Khalkhali, Vahid and Wang, Xinmeng and Sinti-Ycochea, Mario and Sotardi, Susan T.},
  year = {2025},
  doi = {10.3390/inventions10040055},
  url = {https://doi.org/10.3390/inventions10040055},
  journal = {Inventions},
  volume = {10},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Our RAG system leverages a database of 167,028 radiology-related abstracts sourced from … RAG LLM and GPT-4-Consensus models were evaluated for factual accuracy (FA), citation …},
  annote = {Cited by: 0},
}

@article{hiriyanna_multi-layered_2025,
  title = {Multi-{Layered},
  author = {Hiriyanna, Sachin and Zhao, Wenbing},
  year = {2025},
  doi = {10.3390/computers14080332},
  url = {https://doi.org/10.3390/computers14080332},
  journal = {Computers},
  volume = {14},
  number = {8},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{kuo_automated_2025,
  title = {Automated {Clinical},
  author = {Kuo, Shengming and Tai, Shaokuo and Lin, Hung Yu and Chen, Rungching},
  year = {2025},
  doi = {10.3390/ai6080188},
  url = {https://doi.org/10.3390/ai6080188},
  journal = {AI (Switzerland)},
  volume = {6},
  number = {8},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… We propose and implement a multimodal Retrieval-Augmented Generation (RAG)–LLM … retrieval—namely Retrieval-Augmented Generation (RAG)—to bolster factual consistency. …},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{papageorgiou_role_2025,
  title = {The {Role},
  author = {Papageorgiou, Platon S. and Christodoulou, Rafail C. and Pitsillos, Rafael and Petrou, Vasileia and Vamvouras, Georgios and Kormentza, Eirini Vasiliki and Papagelopoulos, Panayiotis J. and Georgiou, Michalis F.},
  year = {2025},
  doi = {10.3390/app15169005},
  url = {https://doi.org/10.3390/app15169005},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {16},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{ye_ihgr-rag_2025,
  title = {{IHGR},
  author = {Ye, Zhenhao and Qi, Donglian and Liu, Hanlin and Zhang, Siqi},
  year = {2025},
  doi = {10.3390/electronics14163284},
  url = {https://doi.org/10.3390/electronics14163284},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {16},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{wu_domain-specialized_2025,
  title = {Domain-{Specialized},
  author = {Wu, Weitong and Xu, Di and Liu, Liangan and Wang, Bingqin and Zhao, Yadi and Cheng, Xuequn and Li, Xiaogang},
  year = {2025},
  doi = {10.3390/app15169226},
  url = {https://doi.org/10.3390/app15169226},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {16},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhao_mdkag_2025,
  title = {{MDKAG},
  author = {Zhao, Xu and Wang, Guozhong and Lu, Yufei},
  year = {2025},
  doi = {10.3390/app15169095},
  url = {https://doi.org/10.3390/app15169095},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {16},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{dos_santos-junior_ai-assisted_2025,
  title = {An {AI},
  author = {Dos Santos-Júnior, Julles Mitoura and de Freitas, Antonio Carlos Daltro and Pinto Mariano, Adriano Pinto},
  year = {2025},
  doi = {10.3390/pr13082508},
  url = {https://doi.org/10.3390/pr13082508},
  journal = {Processes},
  volume = {13},
  number = {8},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{liu_spatiotemporalsemantic_2025,
  title = {A {Spatiotemporal},
  author = {Liu, Huimin and Yin, Shutong and Hu, Xin and Deng, Min and Yang, Xuexi and Xu, Gang},
  year = {2025},
  doi = {10.3390/app15169012},
  url = {https://doi.org/10.3390/app15169012},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {16},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{khasanova_zafar_kizi_design_2025,
  title = {Design and {Performance},
  author = {Khasanova Zafar Kizi, Maksuda and Suh, Youngjung},
  year = {2025},
  doi = {10.3390/electronics14153095},
  url = {https://doi.org/10.3390/electronics14153095},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {15},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{smajic_large_2025,
  title = {Large {Language},
  author = {Smajić, Alma and Karlović, Ratomir and Bobanović Dasko, Mieta and Lorencin, Ivan},
  year = {2025},
  doi = {10.3390/electronics14153153},
  url = {https://doi.org/10.3390/electronics14153153},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {15},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{du_knowledge-augmented_2025,
  title = {Knowledge-{Augmented},
  author = {Du, Jianguang and Li, Bo and Chen, Zhenyu and Shen, Lian and Liu, Pufan and Ran, Zhongyang},
  year = {2025},
  doi = {10.3390/electronics14153101},
  url = {https://doi.org/10.3390/electronics14153101},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {15},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{ali_atlasgpt_2025,
  title = {{AtlasGPT},
  author = {Ali, Rohaid and Abdulrazeq, Hael F. and Patil, Advait and Cheatham, Morgan and Connolly, Ian David and Tang, Oliver Y. and Doberstein, Cody A. and Riccelli, Tori E. and Huang, Kevin T. and Shankar, Ganesh M. and Williamson, Theresa L. and Shin, John H. and Carter, Bob S. and Torabi, Radmehr and Lee, Christine K. and Cielo, Deus J. and Telfeian, Albert Edward and Gökaslan, Ziya Levent and Cohen-Gadol, Aaron Afshin and Zou, James Y. and Asaad, Wael F.},
  year = {2025},
  doi = {10.3171/2024.12.JNS241607},
  url = {https://doi.org/10.3171/2024.12.jns241607},
  journal = {Journal of Neurosurgery},
  volume = {143},
  number = {2},
  pages = {560 -- 567},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{hu_intelligent_2025,
  title = {Intelligent assistant in radiation protection based on large language model with knowledge base},
  author = {Hu, Ankang and Li, Kaiwen and Wu, Zhen and Zhang, Hui and Qiu, Rui and Li, Junli},
  year = {2025},
  doi = {10.1007/s00411-025-01124-4},
  url = {https://doi.org/10.1007/s00411-025-01124-4},
  journal = {Radiation and Environmental Biophysics},
  volume = {64},
  number = {3},
  pages = {519 -- 529},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kelly_folkrag_2025,
  title = {{FolkRAG},
  author = {Kelly, Paul and Schild, Jonathan and Jafari, Amir Hossein},
  year = {2025},
  doi = {10.1007/s00521-025-11455-4},
  url = {https://doi.org/10.1007/s00521-025-11455-4},
  journal = {Neural Computing and Applications},
  volume = {37},
  number = {24},
  pages = {20281 -- 20297},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{almeida_artificial_2025,
  title = {Artificial {Intelligence},
  author = {Almeida, Victor Fogagnoli A. and Dantas, Manoela and Duncan, Andra E.},
  year = {2025},
  doi = {10.1213/ANE.0000000000007627},
  url = {https://doi.org/10.1213/ane.0000000000007627},
  journal = {Anesthesia and Analgesia},
  volume = {141},
  number = {2},
  pages = {e33 -- e34},
  note = {Type: Letter},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{naagarajan_enhancing_2025,
  title = {Enhancing greenhouse management with interpretable {AI},
  author = {Naagarajan, Ramesh Arvind and Streif, Stefan},
  year = {2025},
  doi = {10.1016/j.atech.2025.101041},
  url = {https://doi.org/10.1016/j.atech.2025.101041},
  journal = {Smart Agricultural Technology},
  volume = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{baek_automated_2025,
  title = {Automated safety risk management guidance enhanced by retrieval-augmented large language model},
  author = {Baek, Seungwon and Park, Chan-young and Jung, Wooyong},
  year = {2025},
  doi = {10.1016/j.autcon.2025.106255},
  url = {https://doi.org/10.1016/j.autcon.2025.106255},
  journal = {Automation in Construction},
  volume = {176},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{yang_ragva_2025,
  title = {{RAGVA},
  author = {Yang, Rui and Fu, Michael and Tantithamthavorn, Kla (kla) and Arora, Chetan and Vandenhurk, Lisa and Chua, Joey},
  year = {2025},
  doi = {10.1016/j.jss.2025.112436},
  url = {https://doi.org/10.1016/j.jss.2025.112436},
  journal = {Journal of Systems and Software},
  volume = {226},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… as how to update the LLM component within RAG systems while ensuring … hallucinations in LLM responses, the same approach might be excessive for evaluating hallucinations in RAG …},
  annote = {Cited by: 2; All Open Access; Hybrid Gold Open Access},
}

@article{markey_rags_2025,
  title = {From {RAGs},
  author = {Markey, Nigel and El-Mansouri, Ilyass and Rensonnet, Gaetan and van Langen, Casper and Meier, Christoph},
  year = {2025},
  doi = {10.1177/17407745251320806},
  url = {https://doi.org/10.1177/17407745251320806},
  journal = {Clinical Trials},
  volume = {22},
  number = {5},
  pages = {626 -- 631},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{xu_logprobs_2025,
  title = {Logprobs {Know},
  author = {Xu, Mengyao and Gan, Qiaoyin and Zhu, Zhenyu and Qin, Haojun},
  year = {2025},
  doi = {10.1145/3696630.3731433},
  url = {https://doi.org/10.1145/3696630.3731433},
  booktitle = {Proceedings of the {ACM},
  pages = {1242 -- 1243},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{fu_cue_2025,
  title = {Cue {RAG},
  author = {Fu, Yuanshuang and Liu, Dan and Zhang, Bonan and Jiang, Zhuotong and Mei, Haibo and Guan, Jiajin},
  year = {2025},
  doi = {10.1016/j.neucom.2025.130235},
  url = {https://doi.org/10.1016/j.neucom.2025.130235},
  journal = {Neurocomputing},
  volume = {639},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{zhou_1st_2025,
  title = {The 1st {NIP},
  author = {Zhou, Yujia and Ji, Wei and Ge, Xuri and Ai, Qingyao and Jose, Joemon M. and Liu, Yiqun},
  year = {2025},
  doi = {10.1145/3726302.3730364},
  url = {https://doi.org/10.1145/3726302.3730364},
  pages = {4176 -- 4179},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{deng_unveiling_2025,
  title = {Unveiling {Knowledge},
  author = {Deng, Yang and Li, Moxin and Pang, Liang and Zhang, Wenxuan and Lam, Wai},
  year = {2025},
  doi = {10.1145/3726302.3731684},
  url = {https://doi.org/10.1145/3726302.3731684},
  pages = {4086 -- 4089},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{xu_generative_2025-1,
  title = {Generative artifcial intelligence-powered multi-agent paradigm for smart urban mobility: {Opportunities},
  author = {Xu, Haowen and Yuan, Jinghui and Zhou, Anye and Xu, Guanhao and Li, Wan and Ban, Xuegang(Jeff) X. and Ye, Xinyue},
  year = {2025},
  doi = {10.1201/9781003503262-14},
  url = {https://doi.org/10.1201/9781003503262-14},
  pages = {123 -- 137},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{alan_improving_2025,
  title = {Improving {LLM},
  author = {Alan, Ahmet Yusuf and Karaarslan, Enis and Aydin, Ömer},
  year = {2025},
  doi = {10.31127/tuje.1624773},
  url = {https://doi.org/10.31127/tuje.1624773},
  journal = {Turkish Journal of Engineering},
  volume = {9},
  number = {3},
  pages = {544 -- 559},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… RAG-based system designed to minimize hallucinations and misinformation in religious inquiries. To achieve this, we constructed a specialized RAG … is enhancing chatbot credibility by …},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{tayebi_arasteh_radiorag_2025,
  title = {{RadioRAG},
  author = {Tayebi Arasteh, Soroosh and Lotfinia, Mahshad and Bressem, Keno Kyrill and Siepmann, Robert Malte and Adams, Lisa Christine and Ferber, Dyke and Kuhl, Christiane K. and Kather, Jakob Nikolas and Nebelung, Sven and Truhn, Daniel},
  year = {2025},
  doi = {10.1148/ryai.240476},
  url = {https://doi.org/10.1148/ryai.240476},
  journal = {Radiology: Artificial Intelligence},
  volume = {7},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{fink_retrieval-augmented_2025-1,
  title = {Retrieval-{Augmented},
  author = {Fink, Anna and Rau, Alexander and Reisert, Marco and Bamberg, Fabian and Russe, Maximilian Frederik},
  year = {2025},
  doi = {10.1148/ryai.240790},
  url = {https://doi.org/10.1148/ryai.240790},
  journal = {Radiology: Artificial Intelligence},
  volume = {7},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{rupanetti_industry_2025,
  title = {An {Industry},
  author = {Rupanetti, Dulana and Uberecken, Corissa and King, Adam and Salamy, Hassan A. and Min, Cheol-hong and Schmidgall, Samantha},
  year = {2025},
  doi = {10.3390/a18070414},
  url = {https://doi.org/10.3390/a18070414},
  journal = {Algorithms},
  volume = {18},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{wu_adaptive_2025,
  title = {Adaptive {RAG},
  author = {Wu, Kuochen and Chew, Fattyang and Cheng, Kanglun and Shen, Wuchung and Yeh, Peichun and Kao, Chiahung and Guo, Wanyuo and Chang, Shihsheng},
  year = {2025},
  doi = {10.3390/bioengineering12070698},
  url = {https://doi.org/10.3390/bioengineering12070698},
  journal = {Bioengineering},
  volume = {12},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{lee_retrieval-augmented_2025,
  title = {A {Retrieval},
  author = {Lee, Jeong-ha and Ali, Ghazanfar and Hwang, Jaein},
  year = {2025},
  doi = {10.1002/cav.70048},
  url = {https://doi.org/10.1002/cav.70048},
  journal = {Computer Animation and Virtual Worlds},
  volume = {36},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{bessa_performance_2025,
  title = {Performance {Comparison},
  author = {Bessa, Renato Freitas and de Oliveira, Adonias Caetano and Bessa, Rafael Freitas and Sousa, Daniel Lima and Alves, Rafaela De Brito and Barbosa, Amanda and Carneiro, Alinne and Soares, Carla and Teles, Ariel Soares},
  year = {2025},
  doi = {10.3390/app15137134},
  url = {https://doi.org/10.3390/app15137134},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {13},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{dong_syract_2025,
  title = {{SyRACT},
  author = {Dong, Xin and Zhao, Di and Meng, Jiana and Guo, Bocheng and Lin, Hongfei},
  year = {2025},
  doi = {10.1093/bioinformatics/btaf356},
  url = {https://doi.org/10.1093/bioinformatics/btaf356},
  journal = {Bioinformatics},
  volume = {41},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{wang_innovative_2025,
  title = {An innovative {ESGH},
  author = {Wang, Jhing-Fa Fa and Zhang, Wenyuan and Tseng, Shihpang},
  year = {2025},
  doi = {10.1007/s11227-025-07604-0},
  url = {https://doi.org/10.1007/s11227-025-07604-0},
  journal = {Journal of Supercomputing},
  volume = {81},
  number = {10},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ashwin_fake_2025,
  title = {Fake it till you make it? {AI},
  author = {Ashwin, M. and Jha, Sukriti and Prasad, Ganga A. and Kumar, Subodh},
  year = {2025},
  doi = {10.4103/joacp.joacp_56_25},
  url = {https://doi.org/10.4103/joacp.joacp_56_25},
  journal = {Journal of Anaesthesiology Clinical Pharmacology},
  volume = {41},
  number = {3},
  pages = {381 -- 383},
  note = {Type: Editorial},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{ozmen_development_2025,
  title = {Development of a novel artificial intelligence clinical decision support tool for hand surgery: {HandRAG},
  author = {Ozmen, Berk Baris and Singh, Nishant and Shah, Kavach and Berber, Ibrahim and Singh, Damanjit and Pinsky, Eugene and Rampazzo, Antonio and Schwarz, Graham S.},
  year = {2025},
  doi = {10.1016/j.jham.2025.100293},
  url = {https://doi.org/10.1016/j.jham.2025.100293},
  journal = {Journal of Hand and Microsurgery},
  volume = {17},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Hybrid Gold Open Access},
}

@article{guofeng_multimodal_2025,
  title = {Multimodal large language model for wheat breeding: {A},
  author = {Guofeng, Yang and Li, Yu and He, Yong and Zhou, Zhenjiang and Ye, Lingzhen and Fang, Hui and Luo, Yi and Feng, Xuping},
  year = {2025},
  doi = {10.1016/j.isprsjprs.2025.03.027},
  url = {https://doi.org/10.1016/j.isprsjprs.2025.03.027},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {225},
  pages = {492 -- 513},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{reason_using_2025,
  title = {Using {Generative},
  author = {Reason, Tim and Klijn, Sven L. and Rawlinson, William and Benbow, Emma and Langham, Julia and Teitsson, Siguroli and Johannesen, Kasper Munk and Malcolm, Bill},
  year = {2025},
  doi = {10.1007/s41669-025-00580-4},
  url = {https://doi.org/10.1007/s41669-025-00580-4},
  journal = {PharmacoEconomics - Open},
  volume = {9},
  number = {4},
  pages = {501 -- 517},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{sezgin_decoypot_2025,
  title = {{DecoyPot},
  author = {Sezgin, Anıl and Boyacı, Aytuğ},
  year = {2025},
  doi = {10.1016/j.cose.2025.104458},
  url = {https://doi.org/10.1016/j.cose.2025.104458},
  journal = {Computers and Security},
  volume = {154},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… requests to create prompt-response pairs that improve a Retrieval-Augmented Generation based (RAG) large language model (LLM). DecoyPot can instantly adjust its answers to mimic …},
  annote = {Cited by: 2},
}

@article{wellawatte_chemlit-qa_2025,
  title = {{ChemLit},
  author = {Wellawatte, Geemi P. and Guo, Huixuan and Lederbauer, Magdalena and Borisova, Anna and Hart, Matthew and Brucka, Marta and Schwaller, Philippe},
  year = {2025},
  doi = {10.1088/2632-2153/adc2d6},
  url = {https://doi.org/10.1088/2632-2153/adc2d6},
  journal = {Machine Learning: Science and Technology},
  volume = {6},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{wu_mkgf_2025,
  title = {{MKGF},
  author = {Wu, Yinan and Lu, Yuming and Zhou, Yan and Ding, Yifan and Liu, Jingping and Ruan, Tong},
  year = {2025},
  doi = {10.1016/j.neucom.2025.129999},
  url = {https://doi.org/10.1016/j.neucom.2025.129999},
  journal = {Neurocomputing},
  volume = {635},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{setlur_supporting_2025,
  title = {Supporting {Human},
  author = {Setlur, Vidya},
  year = {2025},
  doi = {10.1145/3722212.3725628},
  url = {https://doi.org/10.1145/3722212.3725628},
  booktitle = {Proceedings of the {ACM},
  pages = {851 -- 854},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{hamed_knowledge_2025,
  title = {From knowledge generation to knowledge verification: examining the biomedical generative capabilities of {ChatGPT},
  author = {Hamed, Ahmed Abdeen and Crimi, Alessandro and Misiak, Magdalena M. and Lee, Byung-suk},
  year = {2025},
  doi = {10.1016/j.isci.2025.112492},
  url = {https://doi.org/10.1016/j.isci.2025.112492},
  journal = {iScience},
  volume = {28},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… , is the verification of the factuality of content generated by ChatGPT. Here, we extend our … directly or via retrieval-augmented generation (RAG) methods. RAG integrates contextual …},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{hou_geo-fub_2025,
  title = {Geo-{FuB},
  author = {Hou, Shuyang and Zhao, Anqi and Liang, Jianyuan and Shen, Zhangxiao and Wu, Huayi},
  year = {2025},
  doi = {10.1016/j.knosys.2025.113624},
  url = {https://doi.org/10.1016/j.knosys.2025.113624},
  journal = {Knowledge-Based Systems},
  volume = {319},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{nazary_stealthy_2025,
  title = {Stealthy {LLM},
  author = {Nazary, Fatemeh and Deldjoo, Yashar and Di Noia, Tommaso and Di Sciascio, Eugenio},
  year = {2025},
  doi = {10.1145/3708319.3733675},
  url = {https://doi.org/10.1145/3708319.3733675},
  pages = {98 -- 102},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{emekci_are_2025,
  title = {Are {LLMs},
  author = {Emekci, Hakan},
  year = {2025},
  doi = {10.4018/979-8-3693-8332-2.ch004},
  url = {https://doi.org/10.4018/979-8-3693-8332-2.ch004},
  pages = {87 -- 105},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ren_large_2025,
  title = {Large language model for interpreting research policy using adaptive two-stage retrieval augmented fine-tuning method},
  author = {Ren, Runtao and Ma, Jian and Zheng, Zhimin},
  year = {2025},
  doi = {10.1016/j.eswa.2025.127330},
  url = {https://doi.org/10.1016/j.eswa.2025.127330},
  journal = {Expert Systems with Applications},
  volume = {278},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… However, current large language model (LLM)-based systems often generate responses … (AT-RAFT) method, a novel LLM-based approach specifically designed for science policy …},
  annote = {Cited by: 1},
}

@article{schaffer_multimodal_2025,
  title = {Multimodal cell maps as a foundation for structural and functional genomics},
  author = {Schaffer, Leah V. and Hu, Mengzhou and Qian, Gege and Moon, Kyungmee and Pal, Abantika and Soni, Neelesh and Latham, Andrew P. and Pontano-Vaites, Laura and Tsai, Dorothy and Mattson, Nicole M. and Licon, Katherine S. and Bachelder, Robin E. and Cesnik, Anthony J. and Gaur, Ishan and Le, Trang and Leineweber, William D. and Palar, Aji and Pulido, Ernst H. and Qin, Yue and Zhao, Xiaoyu and Churas, Christopher P. and Lenkiewicz, Joanna and Chen, Jing and Ono, Keiichiro and Pratt, Dexter and Zage, Peter E. and Echeverria, Ignacia and Jurica, Jan and Harper, J. Wade and Gygi, Steven P. and Foster, Leonard J. and Huttlin, Edward L. and Lundberg, Emma K. and Ideker, Trey E.},
  year = {2025},
  doi = {10.1038/s41586-025-08878-3},
  url = {https://doi.org/10.1038/s41586-025-08878-3},
  journal = {Nature},
  volume = {642},
  number = {8066},
  pages = {222 -- 231},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{wang_improving_2025,
  title = {Improving knowledge management in building engineering with hybrid retrieval-augmented generation framework},
  author = {Wang, Zhiqi and Liu, Zhongcun and Lu, Weizhen and Jia, Lu},
  year = {2025},
  doi = {10.1016/j.jobe.2025.112189},
  url = {https://doi.org/10.1016/j.jobe.2025.112189},
  journal = {Journal of Building Engineering},
  volume = {103},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… by several factors, including hallucinations, a lack of domain-… develop an improved retrieval augmented generation (RAG) … demonstrates the potential of LLM-RAG-based solutions for …},
  annote = {Cited by: 8},
}

@article{choi_empowering_2025,
  title = {Empowering {PET},
  author = {Choi, Hongyoon and Lee, Dongjoo and Kang, Yeon-koo and Suh, Minseok},
  year = {2025},
  doi = {10.1007/s00259-025-07101-9},
  url = {https://doi.org/10.1007/s00259-025-07101-9},
  journal = {European Journal of Nuclear Medicine and Molecular Imaging},
  volume = {52},
  number = {7},
  pages = {2452 -- 2462},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{pelletier_evidence-based_2025,
  title = {Evidence-based {Knowledge},
  author = {Pelletier, Alexander R. and Ramirez, Joseph and Sankar, Baradwaj Simha and Adam, Irsyad and Yan, Yu and Steinecke, Dylan and Wang, Wei and Watson, Karol E. and Ping, Peipei},
  year = {2025},
  doi = {10.3791/67525},
  url = {https://doi.org/10.3791/67525},
  journal = {Journal of Visualized Experiments},
  volume = {2025-June},
  number = {220},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{fukataki_developing_2025,
  title = {Developing artificial intelligence tools for institutional review board pre-review: {A},
  author = {Fukataki, Yasuko and Hayashi, Wakako and Nishimoto, Naoki and Ito, Yoichi M.},
  year = {2025},
  doi = {10.1371/journal.pdig.0000695},
  url = {https://doi.org/10.1371/journal.pdig.0000695},
  journal = {PLOS Digital Health},
  volume = {4},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{genovese_data_2025,
  title = {From {Data},
  author = {Genovese, Ariana and Prabha, S. and Borna, Sahar and Gomez-Cabello, Cesar Abraham and Haider, Syed Ali and Trabilsy, Maissa and Tao, Cui and Forte, Antonio J.},
  year = {2025},
  doi = {10.3390/ebj6020028},
  url = {https://doi.org/10.3390/ebj6020028},
  journal = {European Burn Journal},
  volume = {6},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… the performance of the LLM, spanning foundational … LLM’s ability to navigate the literature effectively, simulate the demands of clinical practice, and highlight the potential of RAG …},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{montenegro_what_2025,
  title = {What {We},
  author = {Montenegro, Larissa and Gomes, Luís Mendes and Machado, José Manuel Ferreira},
  year = {2025},
  doi = {10.3390/ai6060109},
  url = {https://doi.org/10.3390/ai6060109},
  journal = {AI (Switzerland)},
  volume = {6},
  number = {6},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{gupta_pidqaquestion_2025,
  title = {{PIDQA},
  author = {Gupta, Mohit and Wei, Chialing and Czerniawski, Thomas and Eiris, Ricardo},
  year = {2025},
  doi = {10.3390/make7020039},
  url = {https://doi.org/10.3390/make7020039},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {7},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{li_combining_2025,
  title = {Combining {Lexicon},
  author = {Li, Jiabin and Wei, Tingxin and Qü, Weiguang and Li, Bin and Feng, Minxuan and Wang, Dongbo},
  year = {2025},
  doi = {10.3390/math13122023},
  url = {https://doi.org/10.3390/math13122023},
  journal = {Mathematics},
  volume = {13},
  number = {12},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… During evaluation, we observed that ChatGPT-4o and Taiyan 2.0 generate source citations only for allusion chunks. For instance, in the annotation of “大树” (referring to the historical …},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{sezgin_natural_2025,
  title = {Natural {Language},
  author = {Sezgin, Anıl},
  year = {2025},
  doi = {10.3390/drones9060444},
  url = {https://doi.org/10.3390/drones9060444},
  journal = {Drones},
  volume = {9},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{wagenpfeil_multimedia_2025,
  title = {Multimedia {Graph},
  author = {Wagenpfeil, Stefan},
  year = {2025},
  doi = {10.3390/electronics14122472},
  url = {https://doi.org/10.3390/electronics14122472},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {12},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{guo_malgta_2025,
  title = {Malgta: large language model-based guided malware tactical analysis},
  author = {Guo, Wenjie and Xue, Jingfeng and Liu, Zeyang and Han, Weijie and Hu, Jingjing},
  year = {2025},
  doi = {10.1007/s11227-025-07545-8},
  url = {https://doi.org/10.1007/s11227-025-07545-8},
  journal = {Journal of Supercomputing},
  volume = {81},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{he_using_2025,
  title = {Using {Large},
  author = {He, Rui and Zhang, Liang and Lyu, Mengyao and Lyu, Liangqing and Xue, Changbin},
  year = {2025},
  doi = {10.3390/aerospace12060498},
  url = {https://doi.org/10.3390/aerospace12060498},
  journal = {Aerospace},
  volume = {12},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{hu_combining_2025,
  title = {Combining {Retrieval},
  author = {Hu, Xinqiang and Abisado, Mideth B.},
  year = {2025},
  doi = {10.30564/fls.v7i6.9143},
  url = {https://doi.org/10.30564/fls.v7i6.9143},
  journal = {Forum for Linguistic Studies},
  volume = {7},
  number = {6},
  pages = {531 -- 553},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{bevara_prospects_2025,
  title = {Prospects of {Retrieval},
  author = {Bevara, Ravi Varma Kumar and Lund, Brady Daniel and Mannuru, Nishith Reddy and Karedla, Sai Pranathi and Mohammed, Yara and Kolapudi, Sai and Mannuru, Aashrith},
  year = {2025},
  doi = {10.5860/ital.v44i2.17361},
  url = {https://doi.org/10.5860/ital.v44i2.17361},
  journal = {Information Technology and Libraries},
  volume = {44},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{james_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {James, Antony and Trovati, Marcello and Bolton, Simon},
  year = {2025},
  doi = {10.3390/app15116247},
  url = {https://doi.org/10.3390/app15116247},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{allard_mega-gpt_2025,
  title = {{MEGA},
  author = {Allard, John B. and Kumar, Sudhir},
  year = {2025},
  doi = {10.1093/molbev/msaf101},
  url = {https://doi.org/10.1093/molbev/msaf101},
  journal = {Molecular Biology and Evolution},
  volume = {42},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{he_venous_2025,
  title = {Venous {Thrombosis},
  author = {He, Dong and Pu, Hongrui and He, Jianfeng},
  year = {2025},
  doi = {10.3390/electronics14112164},
  url = {https://doi.org/10.3390/electronics14112164},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{yang_review_2025,
  title = {A review on synergizing knowledge graphs and large language models},
  author = {Yang, Zhenyao and Yuan, Sha and Shao, Zhou and Li, Wenfa and Liu, Runzhou},
  year = {2025},
  doi = {10.1007/s00607-025-01499-8},
  url = {https://doi.org/10.1007/s00607-025-01499-8},
  journal = {Computing},
  volume = {107},
  number = {6},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{papageorgiou_hybrid_2025,
  title = {Hybrid {Multi},
  author = {Papageorgiou, George and Sarlis, Vangelis and Maragoudakis, M. and Tjortjis, Christos},
  year = {2025},
  doi = {10.3390/app15116315},
  url = {https://doi.org/10.3390/app15116315},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{roll_augmenting_2025,
  title = {Augmenting {Orbital},
  author = {Roll, Daniel S. and Kurt, Zeyneb and Li, Yulei and Woo, Wai Lok},
  year = {2025},
  doi = {10.3390/s25113352},
  url = {https://doi.org/10.3390/s25113352},
  journal = {Sensors},
  volume = {25},
  number = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{dorfner_evaluating_2025,
  title = {Evaluating the effectiveness of biomedical fine-tuning for large language models on clinical tasks},
  author = {Dorfner, Felix J. and Dada, Amin and Busch, Felix and Makowski, Marcus Richard and Han, Tianyu and Truhn, Daniel and Kleesiek, Jens and Sushil, Madhumita and Adams, Lisa Christine and Bressem, Keno Kyrill},
  year = {2025},
  doi = {10.1093/jamia/ocaf045},
  url = {https://doi.org/10.1093/jamia/ocaf045},
  journal = {Journal of the American Medical Informatics Association},
  volume = {32},
  number = {6},
  pages = {1015 -- 1024},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{li_retrieval-augmented_2025,
  title = {Retrieval-augmented generation for educational application: {A},
  author = {Li, Zongxi and Wang, Zijian and Wang, Weiming and Hung, Kevin King Fai and Xie, Haoran and Wang, Fu Lee},
  year = {2025},
  doi = {10.1016/j.caeai.2025.100417},
  url = {https://doi.org/10.1016/j.caeai.2025.100417},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  note = {Type: Review},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information … it into the LLM's generation process. This approach improves factual accuracy and enables …},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{wang_rag-leaks_2025,
  title = {{RAG},
  author = {Wang, Guangshuo and He, Jiajun and Li, Hao and Zhang, Min and Feng, Dengguo},
  year = {2025},
  doi = {10.1007/s11432-024-4441-4},
  url = {https://doi.org/10.1007/s11432-024-4441-4},
  journal = {Science China Information Sciences},
  volume = {68},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{magesh_hallucination-free_2025,
  title = {Hallucination-{Free},
  author = {Magesh, Varun and Surani, Faiz and Dahl, Matthew and Suzgun, Mirac and Manning, Christopher D. and Ho, Daniel E.},
  year = {2025},
  doi = {10.1111/jels.12413},
  url = {https://doi.org/10.1111/jels.12413},
  journal = {Journal of Empirical Legal Studies},
  volume = {22},
  number = {2},
  pages = {216 -- 242},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 6; All Open Access; Hybrid Gold Open Access},
}

@article{vaidyanathan_survival_2025,
  title = {Survival {Analysis},
  author = {Vaidyanathan, Jyothi and Gupta, Shourya and Lee, Justin and Prabhu, Srikanth and Sengupta, Saptarshi},
  year = {2025},
  doi = {10.1609/aaaiss.v5i1.35549},
  url = {https://doi.org/10.1609/aaaiss.v5i1.35549},
  volume = {5},
  number = {1},
  pages = {31 -- 36},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{fu_engineering_2025,
  title = {Engineering the {Reproducible},
  author = {Fu, Yuanxi and Schneider, Jodi A.},
  year = {2025},
  doi = {10.1609/aaaiss.v5i1.35612},
  url = {https://doi.org/10.1609/aaaiss.v5i1.35612},
  volume = {5},
  number = {1},
  pages = {360 -- 364},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{jiao_retrieval_2025,
  title = {Retrieval and {Structuring},
  author = {Jiao, Yizhu and Ouyang, Siru and Zhong, Ming and Zhang, Yunyi and Ding, Linyi and Zhou, Sizhe and Han, Jiawei},
  year = {2025},
  doi = {10.1145/3701716.3715870},
  url = {https://doi.org/10.1145/3701716.3715870},
  pages = {25 -- 28},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{mou_exploring_2025,
  title = {Exploring the {Potential},
  author = {Mou, Yongli and Siepmann, Robert Malte and Truhnn, Daniel and Sowe, Sulayman K. and Decker, Stefan},
  year = {2025},
  doi = {10.3233/SHTI250482},
  url = {https://doi.org/10.3233/shti250482},
  journal = {Studies in Health Technology and Informatics},
  volume = {327},
  pages = {863 -- 867},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{carvalho_towards_2025,
  title = {Towards the {Application},
  author = {Carvalho., Marco M. and Nembhard, Fitzroy and Mehta, Dhanish},
  year = {2025},
  doi = {10.32473/flairs.38.1.138895},
  url = {https://doi.org/10.32473/flairs.38.1.138895},
  booktitle = {Proceedings of the {International},
  volume = {38},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{plonka_comparative_2025,
  title = {A comparative evaluation of the effectiveness of document splitters for large language models in legal contexts},
  author = {Płonka, Mateusz and Kocot, Krzysztof and Hołda, Kacper and Daniec, Krzysztof and Nawrat, Aleksander M.},
  year = {2025},
  doi = {10.1016/j.eswa.2025.126711},
  url = {https://doi.org/10.1016/j.eswa.2025.126711},
  journal = {Expert Systems with Applications},
  volume = {272},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{wu_diagnosis_2025,
  title = {Diagnosis assistant for liver cancer utilizing a large language model with three types of knowledge},
  author = {Wu, Xuzhou and Li, Guangxin and Wang, Xing and Xu, Zeyu and Wang, Yingni and Lei, Shuge and Xian, Jianming and Wang, Xueyu and Zhang, Yibao and Li, Gong and Yuan, Kehong},
  year = {2025},
  doi = {10.1088/1361-6560/adcb17},
  url = {https://doi.org/10.1088/1361-6560/adcb17},
  journal = {Physics in Medicine and Biology},
  volume = {70},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{loevenich_design_2025,
  title = {Design and evaluation of an {Autonomous},
  author = {Loevenich, Johannes F. and Adler, Erik and Hürten, Tobias and Rigolin Ferreira Lopes, Roberto Rigolin Ferreira},
  year = {2025},
  doi = {10.1016/j.comnet.2025.111162},
  url = {https://doi.org/10.1016/j.comnet.2025.111162},
  journal = {Computer Networks},
  volume = {262},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 6},
}

@article{weinert_enhancing_2025,
  title = {Enhancing {Large},
  author = {Weinert, Dane A. and Rauschecker, Andreas M.},
  year = {2025},
  doi = {10.1148/ryai.240313},
  url = {https://doi.org/10.1148/ryai.240313},
  journal = {Radiology: Artificial Intelligence},
  volume = {7},
  number = {3},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@article{hao_advancing_2025,
  title = {Advancing {Large},
  author = {Hao, Jinxing and Chen, Lei and Meng, Luyao},
  year = {2025},
  doi = {10.3390/drones9050361},
  url = {https://doi.org/10.3390/drones9050361},
  journal = {Drones},
  volume = {9},
  number = {5},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{wang_artificial_2025,
  title = {Artificial {General},
  author = {Wang, Jiulong and Luo, Xiaotian and Zhang, Xuhui and Du, Shuyi},
  year = {2025},
  doi = {10.3390/pr13051413},
  url = {https://doi.org/10.3390/pr13051413},
  journal = {Processes},
  volume = {13},
  number = {5},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{yao_ai-powered_2025,
  title = {{AI},
  author = {Yao, Yao and González–Vélez, Horacio},
  year = {2025},
  doi = {10.3390/app15094989},
  url = {https://doi.org/10.3390/app15094989},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{pickard_automatic_2025,
  title = {Automatic biomarker discovery and enrichment with {BRAD},
  author = {Pickard, Joshua and Prakash, Ram and Choi, Marc Andrew and Oliven, Natalie and Stansbury, Cooper M. and Cwycyshyn, Jillian and Galioto, Nicholas and Gorodetsky, Alex Arkady and Velasquez, Alvaro and Rajapakse, Indika},
  year = {2025},
  doi = {10.1093/bioinformatics/btaf159},
  url = {https://doi.org/10.1093/bioinformatics/btaf159},
  journal = {Bioinformatics},
  volume = {41},
  number = {5},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{ozmen_evidence-based_2025,
  title = {Evidence-based artificial intelligence: {Implementing},
  author = {Ozmen, Berk Baris and Mathur, Piyush},
  year = {2025},
  doi = {10.1016/j.bjps.2025.03.053},
  url = {https://doi.org/10.1016/j.bjps.2025.03.053},
  journal = {Journal of Plastic, Reconstructive and Aesthetic Surgery},
  volume = {104},
  pages = {414 -- 416},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… In conclusion, RAG models represent a significant advancement in overcoming traditional LLM … may incorrectly describe microsurgical anastomosis techniques due to hallucinations, …},
  annote = {Cited by: 4},
}

@inproceedings{ben_amor_instruct--sparql_2025,
  title = {Instruct-to-{SPARQL},
  author = {Ben Amor, Mehdi and Strappazzon, Alexis and Granitzer, Michael and Egyed-Zsigmond, Elöd D. and Mitrović, Jelena},
  year = {2025},
  doi = {10.1145/3698204.3716476},
  url = {https://doi.org/10.1145/3698204.3716476},
  pages = {390 -- 395},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{novin_tool_2025,
  title = {A {Tool},
  author = {Novin, Alamir and Towne, Georgia},
  year = {2025},
  doi = {10.1145/3698204.3716477},
  url = {https://doi.org/10.1145/3698204.3716477},
  pages = {396 -- 404},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{hanafi_creating_2025,
  title = {Creating {Conversational},
  author = {Hanafi, Maeda F. and Fadnis, Kshitij P. and Danilevsky, Marina and Rosenthal, Sara and Katsis, Yannis},
  year = {2025},
  doi = {10.1145/3706599.3719962},
  url = {https://doi.org/10.1145/3706599.3719962},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{ko_legolas_2025,
  title = {{LEGOLAS},
  author = {Ko, Kangbeen and Oh, Minwoo and Seong, Minwoo and Kim, Seungjun},
  year = {2025},
  doi = {10.1145/3706599.3720141},
  url = {https://doi.org/10.1145/3706599.3720141},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{kruk_banglassist_2025,
  title = {{BanglAssist},
  author = {Kruk, Francesco and Herath, Savindu and Choudhury, Prithwiraj},
  year = {2025},
  doi = {10.1145/3706599.3720226},
  url = {https://doi.org/10.1145/3706599.3720226},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{reinhard_fact_2025,
  title = {Fact or {Fiction},
  author = {Reinhard, Philipp and Li, Mahei Manhei and Fina, Matteo and Leimeister, Jan Marco},
  year = {2025},
  doi = {10.1145/3706599.3720249},
  url = {https://doi.org/10.1145/3706599.3720249},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{tadaga_gangadhar_comparative_2025,
  title = {Comparative {Analysis},
  author = {Tadaga Gangadhar, Adithya and Pranav Rao, Harsh and Aranha, Alston Richard},
  year = {2025},
  doi = {10.1145/3711542.3711590},
  url = {https://doi.org/10.1145/3711542.3711590},
  pages = {5 -- 11},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{liang_fast_2025,
  title = {Fast {Think},
  author = {Liang, Xujian and Gu, Zhaoquan},
  year = {2025},
  doi = {10.1609/aaai.v39i23.34635},
  url = {https://doi.org/10.1609/aaai.v39i23.34635},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {24558 -- 24566},
  note = {Issue: 23
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{long_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Long, Xinwei and Ma, Zhiyuan and Hua, Ermo and Zhang, Kaiyan and Qi, Biqing and Zhou, Bowen},
  year = {2025},
  doi = {10.1609/aaai.v39i23.34653},
  url = {https://doi.org/10.1609/aaai.v39i23.34653},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {24723 -- 24731},
  note = {Issue: 23
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@inproceedings{lin_explore_2025,
  title = {Explore {What},
  author = {Lin, Xin and Huang, Zhenya and Zhang, Zhiqiang and Zhou, Jun and Chen, Enhong},
  year = {2025},
  doi = {10.1609/aaai.v39i23.34638},
  url = {https://doi.org/10.1609/aaai.v39i23.34638},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {24585 -- 24594},
  note = {Issue: 23
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@inproceedings{deng_cram_2025,
  title = {{CrAM},
  author = {Deng, Boyi and Wang, Wenjie and Zhu, Fengbin and Wang, Qifan and Feng, Fuli},
  year = {2025},
  doi = {10.1609/aaai.v39i22.34547},
  url = {https://doi.org/10.1609/aaai.v39i22.34547},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {23760 -- 23768},
  note = {Issue: 22
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{zhang_knowpo_2025,
  title = {{KnowPO},
  author = {Zhang, Ruizhe and Xu, Yongxin and Xiao, Yuzhen and Zhu, Runchuan and Jiang, Xinke and Chu, Xu and Zhao, Junfeng and Wang, Yasha},
  year = {2025},
  doi = {10.1609/aaai.v39i24.34783},
  url = {https://doi.org/10.1609/aaai.v39i24.34783},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {25895 -- 25903},
  note = {Issue: 24
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{zhu_radio_2025,
  title = {{RaDIO},
  author = {Zhu, Jia and Guo, Hanghui and Shi, Weijie and Chen, Zhangze and de Meo, Pasquale},
  year = {2025},
  doi = {10.1609/aaai.v39i24.34809},
  url = {https://doi.org/10.1609/aaai.v39i24.34809},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {26129 -- 26137},
  note = {Issue: 24
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{wang_maferw_2025,
  title = {{MaFeRw},
  author = {Wang, Yujing and Zhang, Hainan and Pang, Liang and Guo, Binghui and Zheng, Hongwei and Zheng, Zhiming},
  year = {2025},
  doi = {10.1609/aaai.v39i24.34732},
  url = {https://doi.org/10.1609/aaai.v39i24.34732},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {25434 -- 25442},
  note = {Issue: 24
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@inproceedings{tabarsi_developing_2025,
  title = {Developing {LLM},
  author = {Tabarsi, Benyamin T.},
  year = {2025},
  doi = {10.1609/aaai.v39i28.35228},
  url = {https://doi.org/10.1609/aaai.v39i28.35228},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {29301 -- 29302},
  note = {Issue: 28
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{zhang_ratt_2025,
  title = {{RATT},
  author = {Zhang, Jinghan and Wang, Xiting and Ren, Weijieying and Jiang, Lu and Wang, Dongjie and Liu, Kunpeng},
  year = {2025},
  doi = {10.1609/aaai.v39i25.34876},
  url = {https://doi.org/10.1609/aaai.v39i25.34876},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {26733 -- 26741},
  note = {Issue: 25
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@inproceedings{zhou_clinicalrag_2025,
  title = {{ClinicalRAG},
  author = {Zhou, Qiaohui and Zhou, Zhongliang and Johnson, Michael and Ngo, Michelle and Ferrari, Federico and Ma, Junshui},
  year = {2025},
  doi = {10.1609/aaai.v39i28.35384},
  url = {https://doi.org/10.1609/aaai.v39i28.35384},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {29736 -- 29738},
  note = {Issue: 28
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{tabarsi_merryquery_2025,
  title = {{MerryQuery},
  author = {Tabarsi, Benyamin T. and Basarkar, Aditya and Liu, Xukun and Xu, Dongkuan D.K. and Barnes, Tiffany Michelle},
  year = {2025},
  doi = {10.1609/aaai.v39i28.35372},
  url = {https://doi.org/10.1609/aaai.v39i28.35372},
  booktitle = {Proceedings of the {AAAI},
  volume = {39},
  pages = {29700 -- 29702},
  note = {Issue: 28
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{eskenazi_evaluating_2025,
  title = {Evaluating retrieval augmented generation and {ChatGPT},
  author = {Eskenazi, Jordan A. and Krishnan, Varun and Konarzewski, Maximilian and Constantinescu, David S. and Lobaton, Gilberto O. and Dodds, Seth Detchon},
  year = {2025},
  doi = {10.21037/aoj-24-49},
  url = {https://doi.org/10.21037/aoj-24-49},
  journal = {Annals of Joint},
  volume = {10},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… hypothesized that ChatGPT-4 with RAG would outperform ChatGPT versions without RAG, … Second, ChatGPT-4 + RAG can cite the sources it uses when answering questions. Tracing …},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{hong_innovative_2025,
  title = {{INNOVATIVE},
  author = {Hong, Youngpyo and Kim, Dongsoo},
  year = {2025},
  doi = {10.24507/ijicic.21.02.481},
  url = {https://doi.org/10.24507/ijicic.21.02.481},
  journal = {International Journal of Innovative Computing, Information and Control},
  volume = {21},
  number = {2},
  pages = {481 -- 490},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… ) into business operations requires addressing hallucination issues, incorporating proprietary … challenge involve implementing a Retrieval-Augmented Generation (RAG) methodology, …},
  annote = {Cited by: 0},
}

@article{soman_human_2025,
  title = {Human guided empathetic {AI},
  author = {Soman, Gayathri and Judy, M. V. and Abou, Aadhil Muhammad},
  year = {2025},
  doi = {10.1016/j.cogsys.2025.101337},
  url = {https://doi.org/10.1016/j.cogsys.2025.101337},
  journal = {Cognitive Systems Research},
  volume = {90},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… LLM-based conversational agent that relies on the integration of Retrieval Augmented Generation (RAG… , steady training dynamics, decreased hallucination rates with responses having …},
  annote = {Cited by: 3},
}

@article{tozuka_application_2025,
  title = {Application of {NotebookLM},
  author = {Tozuka, Ryota and Johno, Hisashi and Amakawa, Akitomo and Sato, Junichi and Muto, Mizuki and Seki, Shoichiro and Komaba, Atsushi and Onishi, Hiroshi},
  year = {2025},
  doi = {10.1007/s11604-024-01705-1},
  url = {https://doi.org/10.1007/s11604-024-01705-1},
  journal = {Japanese Journal of Radiology},
  volume = {43},
  number = {4},
  pages = {706 -- 712},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 16},
}

@article{kim_llm-based_2025,
  title = {{LLM},
  author = {Kim, Junseo and Kim, Seok-jun and Ahn, Junseok and Lee, Suehyun},
  year = {2025},
  doi = {10.4258/hir.2025.31.2.136},
  url = {https://doi.org/10.4258/hir.2025.31.2.136},
  journal = {Healthcare Informatics Research},
  volume = {31},
  number = {2},
  pages = {136 -- 145},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{swacha_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Swacha, Jakub and Gracel, Michał},
  year = {2025},
  doi = {10.3390/app15084234},
  url = {https://doi.org/10.3390/app15084234},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {8},
  note = {Type: Review},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) overcomes the main barrier for the adoption of LLM-based chatbots in education: hallucinations. The uncomplicated architecture of RAG …},
  annote = {Cited by: 11; All Open Access; Gold Open Access},
}

@article{farias_chatbot_2025,
  title = {Chatbot {Based},
  author = {Farias, H. A. and González-Aroca, Joaquin and Ortiz, Daniel},
  year = {2025},
  doi = {10.3390/technologies13040140},
  url = {https://doi.org/10.3390/technologies13040140},
  journal = {Technologies},
  volume = {13},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… In this work, we analyze three main types of RAG in the healthcare domain: RAG, Corrective … SELF-RAG benefits from the CoT structure by reducing hallucinations through the creation …},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{liu_improving_2025-2,
  title = {Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines},
  author = {Liu, Siru and McCoy, Allison B. and Wright, Adam T.},
  year = {2025},
  doi = {10.1093/jamia/ocaf008},
  url = {https://doi.org/10.1093/jamia/ocaf008},
  journal = {Journal of the American Medical Informatics Association},
  volume = {32},
  number = {4},
  pages = {605 -- 615},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{lee_developing_2025,
  title = {Developing a computer-based tutor utilizing {Generative},
  author = {Lee, Youngjin},
  year = {2025},
  doi = {10.1007/s10639-024-13129-5},
  url = {https://doi.org/10.1007/s10639-024-13129-5},
  journal = {Education and Information Technologies},
  volume = {30},
  number = {6},
  pages = {7841 -- 7862},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@article{son_advancing_2025,
  title = {Advancing {Multimodal},
  author = {Son, Minjun and Lee, Sungjin},
  year = {2025},
  doi = {10.3390/app15073992},
  url = {https://doi.org/10.3390/app15073992},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{shin_thyro-genai_2025,
  title = {Thyro-{GenAI},
  author = {Shin, Minjeong and Song, Junho and Kim, Myung-gwan and Yu, Hyeongwon and Choe, Eun-kyung and Chai, Young Jun},
  year = {2025},
  doi = {10.3390/jcm14072450},
  url = {https://doi.org/10.3390/jcm14072450},
  journal = {Journal of Clinical Medicine},
  volume = {14},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… , ChatGPT-4o [27] generates a final answer with citations to the … RAG has been shown to significantly enhance LLM … RAG-based LLMs have demonstrated improved accuracy and …},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{mitchell_using_2025,
  title = {Using artificial intelligence tools to automate data extraction for living evidence syntheses},
  author = {Mitchell, Evan and Are, Elisha B. and Colijn, Caroline and Earn, David J.D.},
  year = {2025},
  doi = {10.1371/journal.pone.0320151},
  url = {https://doi.org/10.1371/journal.pone.0320151},
  journal = {PLOS ONE},
  volume = {20},
  number = {4 April},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{ilyas_enhancing_2025,
  title = {Enhancing the {RAG},
  author = {Ilyas, Qazi Mudassar and Aziz, Sadia},
  year = {2025},
  doi = {10.4018/979-8-3693-6255-6.ch003},
  url = {https://doi.org/10.4018/979-8-3693-6255-6.ch003},
  pages = {59 -- 80},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{mukhopadhyay_designing_2025,
  title = {Designing {Conversational},
  author = {Mukhopadhyay, Parthasarathi},
  year = {2025},
  doi = {10.14429/djlit.45.2.20206},
  url = {https://doi.org/10.14429/djlit.45.2.20206},
  journal = {DESIDOC Journal of Library and Information Technology},
  volume = {45},
  number = {2},
  pages = {109 -- 115},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… hallucinated, outdated, or out of context. By examining the feasibility of an open-source RAG … objectives of this study are: 1) To select a suitable opensource LLM and a RAG framework …},
  annote = {Cited by: 0},
}

@article{williams_ai_2025,
  title = {{AI},
  author = {Williams, Adele},
  year = {2025},
  doi = {10.1002/vetr.5339},
  url = {https://doi.org/10.1002/vetr.5339},
  journal = {Veterinary Record},
  volume = {196},
  number = {6},
  pages = {219},
  note = {Type: Note},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{susnjak_automating_2025,
  title = {Automating {Research},
  author = {Sušnjak, Teo and Hwang, Peter and Reyes, Napoleon H. and Barczak, Andre L.C. and McIntosh, Timothy R. and Ranathunga, Surangika},
  year = {2025},
  doi = {10.1145/3715964},
  url = {https://doi.org/10.1145/3715964},
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {19},
  number = {3},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 9},
}

@inproceedings{bordino_uniask_2025,
  title = {{UniAsk},
  author = {Bordino, Ilaria and Di Iorio, Francesco and Galliani, A. and Rosatelli, Alessio and Severini, Lorenzo},
  year = {2025},
  doi = {10.48786/edbt.2025.89},
  url = {https://doi.org/10.48786/edbt.2025.89},
  booktitle = {Advances in {Database},
  volume = {28},
  pages = {1057 -- 1065},
  note = {Issue: 3
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{rastogi_specialtyscribe_2025,
  title = {{SpecialtyScribe},
  author = {Rastogi, Eti and Goyal, Sagar and Zhao, Fen and Yuan, Dong},
  year = {2025},
  doi = {10.1145/3701551.3706131},
  url = {https://doi.org/10.1145/3701551.3706131},
  pages = {1098 -- 1099},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhao_deriving_2025,
  title = {Deriving insights from enhanced accuracy: {Leveraging},
  author = {Zhao, Quantong and Wang, Haiyan and Wang, Ran and Cao, Hongshi},
  year = {2025},
  doi = {10.1016/j.nepr.2025.104284},
  url = {https://doi.org/10.1016/j.nepr.2025.104284},
  journal = {Nurse Education in Practice},
  volume = {84},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3; All Open Access; Hybrid Gold Open Access},
}

@article{kim_development_2025,
  title = {Development of an {Automated},
  author = {Kim, Eu Wang and Shin, Yeon-ju and Kim, Kyong-ju and Kwon, Sehoon},
  year = {2025},
  doi = {10.3390/buildings15060923},
  url = {https://doi.org/10.3390/buildings15060923},
  journal = {Buildings},
  volume = {15},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{sharma_forensicllm_2025,
  title = {{ForensicLLM},
  author = {Sharma, Binaya and Ghawaly, James M. and McCleary, Kyle and Webb, Andrew M. and Baggili, Ibrahim M.},
  year = {2025},
  doi = {10.1016/j.fsidi.2025.301872},
  url = {https://doi.org/10.1016/j.fsidi.2025.301872},
  journal = {Forensic Science International: Digital Investigation},
  volume = {52},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… , and response hallucinations could compromise their … and RAG model over the base model. ForensicLLM showed strength in “correctness” and “relevance” metrics, while the RAG …},
  annote = {Cited by: 2},
}

@article{perkins_retrieval-augmented_2025,
  title = {Retrieval-augmented generation salvages poor performance from large language models in answering microbiology-specific multiple-choice questions},
  author = {Perkins, Grace and Anderson, Neil W. and Spies, Nicholas C.},
  year = {2025},
  doi = {10.1128/jcm.01624-24},
  url = {https://doi.org/10.1128/jcm.01624-24},
  journal = {Journal of Clinical Microbiology},
  volume = {63},
  number = {3},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{zhang_hallucination_2025,
  title = {Hallucination {Mitigation},
  author = {Zhang, Wan and Zhang, Jing},
  year = {2025},
  doi = {10.3390/math13050856},
  url = {https://doi.org/10.3390/math13050856},
  journal = {Mathematics},
  volume = {13},
  number = {5},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 8; All Open Access; Gold Open Access},
}

@inproceedings{hou_construction_2025,
  title = {Construction and {Application},
  author = {Hou, Yingqi and Shao, Yichang and Han, Zhongyi and Ye, Zhirui},
  year = {2025},
  doi = {10.4271/2025-01-7139},
  url = {https://doi.org/10.4271/2025-01-7139},
  booktitle = {{SAE},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{ma_sdd-lawllm_2025,
  title = {{SDD},
  author = {Ma, Hanjie and Lu, Yuhang and Xiao, Zhengdong and Feng, Jie and Zhang, Haixiang and Yu, Jian},
  year = {2025},
  doi = {10.3390/electronics14040742},
  url = {https://doi.org/10.3390/electronics14040742},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{lakehal_cores_2025,
  title = {{CORES},
  author = {Lakehal, Abderrahim and Alti, Adel and Annane, Boubakeur},
  year = {2025},
  doi = {10.3390/fi17020094},
  url = {https://doi.org/10.3390/fi17020094},
  journal = {Future Internet},
  volume = {17},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{vallayil_carag_2025,
  title = {{CARAG},
  author = {Vallayil, Manju and Nand, Parma and Yan, Weiqi and Allende-Cid, Héctor and Vamathevan, Thamilini},
  year = {2025},
  doi = {10.3390/app15041970},
  url = {https://doi.org/10.3390/app15041970},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{son_optimizing_2025,
  title = {Optimizing {Large},
  author = {Son, Minjun and Won, Yun-jae and Lee, Sungjin},
  year = {2025},
  doi = {10.3390/app15031430},
  url = {https://doi.org/10.3390/app15031430},
  journal = {Applied Sciences (Switzerland)},
  volume = {15},
  number = {3},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 7; All Open Access; Gold Open Access},
}

@article{matsumoto_escargot_2025-1,
  title = {{ESCARGOT},
  author = {Matsumoto, Nicholas and Choi, Hyunjun and Moran, Jay and Hernandez, Miguel E. and Venkatesan, Mythreye and Li, Xi and Chang, Juihsuan and Wang, Paul Zhiping and Moore, Jason H.},
  year = {2025},
  doi = {10.1093/bioinformatics/btaf031},
  url = {https://doi.org/10.1093/bioinformatics/btaf031},
  journal = {Bioinformatics},
  volume = {41},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{zhang_aligning_2025,
  title = {Aligning {Large},
  author = {Zhang, Yingbo and Ren, Shumin and Wang, Jiao and Lu, Junyu and Wu, Cong and He, Mengqiao and Liu, Xingyun and Wu, Rongrong and Zhao, Jing and Zhan, Chaoying and Du, Dan and Zhan, Zhajun and Singla, Rajeev K. and Shen, Bairong},
  year = {2025},
  doi = {10.1007/s40265-024-02124-2},
  url = {https://doi.org/10.1007/s40265-024-02124-2},
  journal = {Drugs},
  volume = {85},
  number = {2},
  pages = {231 -- 254},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{fuellen_validation_2025,
  title = {Validation requirements for {AI},
  author = {Fuellen, Georg F. and Kulaga, Anton Y. and Lobentanzer, Sebastian and Unfried, Maximilian and Avelar, Roberto A. and Palmer, Daniel H. and Kennedy, Brian Keith},
  year = {2025},
  doi = {10.1016/j.arr.2024.102617},
  url = {https://doi.org/10.1016/j.arr.2024.102617},
  journal = {Ageing Research Reviews},
  volume = {104},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Hybrid Gold Open Access},
}

@article{uhm_effectiveness_2025,
  title = {Effectiveness of retrieval augmented generation-based large language models for generating construction safety information},
  author = {Uhm, Miyoung and Kim, Jaehee and Ahn, Seungjun and Jeong, Hoyoung and Kim, Hongjo},
  year = {2025},
  doi = {10.1016/j.autcon.2024.105926},
  url = {https://doi.org/10.1016/j.autcon.2024.105926},
  journal = {Automation in Construction},
  volume = {170},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 13},
}

@article{chen_using_2025,
  title = {Using {Large},
  author = {Chen, Cai and Li, Shule and So, Anthony D. and Xu, Yaoyang and Guo, Zhaofeng and Wang, Xinbing and Graham, David W. and Zhu, Yongguan},
  year = {2025},
  doi = {10.1021/acs.est.4c07842},
  url = {https://doi.org/10.1021/acs.est.4c07842},
  journal = {Environmental Science and Technology},
  volume = {59},
  number = {2},
  pages = {1243 -- 1252},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{rosenthal_clapnq_2025,
  title = {{CLAPnq},
  author = {Rosenthal, Sara and Sil, Avirup and Florian, Radu and Roukos, Salim},
  year = {2025},
  doi = {10.1162/tacl_a_00729},
  url = {https://doi.org/10.1162/tacl_a_00729},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {13},
  pages = {53 -- 72},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{diers-lawson_hallucination_2025,
  title = {Hallucination or vision? {Establishing},
  author = {Diers-Lawson, Audra R. and Lawson, Stuart J.},
  year = {2025},
  doi = {10.1108/CCIJ-08-2024-0139},
  url = {https://doi.org/10.1108/ccij-08-2024-0139},
  journal = {Corporate Communications},
  volume = {30},
  number = {1},
  pages = {162 -- 183},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{sastre_derivation_2025,
  title = {Derivation {Prompting},
  author = {Sastre, Ignacio and Moncecchi, Guillermo and Rosá, Aiala},
  year = {2025},
  doi = {10.1007/978-3-031-80366-6_34},
  url = {https://doi.org/10.1007/978-3-031-80366-6_34},
  journal = {Lecture Notes in Computer Science},
  volume = {15277 LNCS},
  pages = {412 -- 423},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{alkouz_ukrag_2025,
  title = {{UKRAG},
  author = {Alkouz, Akram and al-Saleh, Mohammed Ibrahim and Alarabeyyat, Abdulsalam and Bouchahma, Majed},
  year = {2025},
  doi = {10.1007/978-3-031-82931-4_1},
  url = {https://doi.org/10.1007/978-3-031-82931-4_1},
  journal = {Communications in Computer and Information Science},
  volume = {2381 CCIS},
  pages = {1 -- 19},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{amazou_accurate_2025,
  title = {Accurate {AI},
  author = {Amazou, Youssra and Tayalati, Faouzi and Mensouri, Houssam and Azmani, A. and Azmani, Monir},
  year = {2025},
  doi = {10.14569/IJACSA.2025.01602113},
  url = {https://doi.org/10.14569/ijacsa.2025.01602113},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {16},
  number = {2},
  pages = {1141 -- 1150},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{khalila_investigating_2025,
  title = {Investigating {Retrieval},
  author = {Khalila, Zahra and Nasution, Arbi Haza and Monika, Winda and Onan, Aytuǧ and Murakami, Yohei and Radi, Yasir Bin Ismail and Osmani, Noor Mohammad},
  year = {2025},
  doi = {10.14569/IJACSA.2025.01602134},
  url = {https://doi.org/10.14569/ijacsa.2025.01602134},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {16},
  number = {2},
  pages = {1361 -- 1371},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{anichkov_retrieval_2025,
  title = {Retrieval {Poisoning},
  author = {Anichkov, Yegor and Popov, Victor and Bolovtsov, Sergey},
  year = {2025},
  doi = {10.1007/978-3-031-80853-1_31},
  url = {https://doi.org/10.1007/978-3-031-80853-1_31},
  journal = {Lecture Notes in Computer Science},
  volume = {15460 LNCS},
  pages = {417 -- 429},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{berman_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Berman, Eliza and Malek, Holly Sundberg and Bitzer, Michael and Malek, Nisar Peter and Eickhoff, Carsten},
  year = {2025},
  doi = {10.2196/64364},
  url = {https://doi.org/10.2196/64364},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{qian_capacity_2025,
  title = {On the {Capacity},
  author = {Qian, Haosheng and Fan, Yixing and Zhang, Ruqing and Guo, Jiafeng},
  year = {2025},
  doi = {10.1007/978-981-96-1710-4_9},
  url = {https://doi.org/10.1007/978-981-96-1710-4_9},
  journal = {Lecture Notes in Computer Science},
  volume = {15418 LNCS},
  pages = {109 -- 123},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{poretsky_assessing_2025,
  title = {Assessing the performance of generative artificial intelligence in retrieving information against manually curated genetic and genomic data},
  author = {Poretsky, Elly and Blake, Victoria Carollo and Andorf, Carson M. and Sen, Taner Z.},
  year = {2025},
  doi = {10.1093/database/baaf011},
  url = {https://doi.org/10.1093/database/baaf011},
  journal = {Database : the journal of biological databases and curation},
  volume = {2025},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{elfayoumi_knowledge_2025,
  title = {Knowledge augmented significant language model-based chatbot for explainable diabetes mellitus prediction},
  author = {Elfayoumi, Mazen and AbouElazm, Mohamed and Mohamed, Omar and AbuHmed, Tamer and El–Sappagh, Shaker H.Ali},
  year = {2025},
  doi = {10.1109/IMCOM64595.2025.10857525},
  url = {https://doi.org/10.1109/imcom64595.2025.10857525},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{cinquin_steering_2025,
  title = {Steering veridical large language model analyses by correcting and enriching generated database queries: first steps toward {ChatGPT},
  author = {Cinquin, Olivier},
  year = {2025},
  doi = {10.1093/bib/bbaf045},
  url = {https://doi.org/10.1093/bib/bbaf045},
  journal = {Briefings in Bioinformatics},
  volume = {26},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… hallucination lead ChatGPT to err, as do incorrect sequence manipulations. To address this, we propose a system basing LLM … It may gain from explicit control of the RAG process, for …},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{lane_mitigating_2025,
  title = {Mitigating risks, embracing potential: a framework for integrating generative artificial intelligence in geographical and environmental education},
  author = {Lane, Rod},
  year = {2025},
  doi = {10.1080/10382046.2025.2458561},
  url = {https://doi.org/10.1080/10382046.2025.2458561},
  journal = {International Research in Geographical and Environmental Education},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{ai_foundations_2025,
  title = {Foundations of {Generative},
  author = {Ai, Qingyao and Zhan, Jingtao and Liu, Yiqun},
  year = {2025},
  doi = {10.1007/978-3-031-73147-1_2},
  url = {https://doi.org/10.1007/978-3-031-73147-1_2},
  journal = {Information Retrieval Series},
  volume = {51},
  pages = {15 -- 45},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{meng_using_2025,
  title = {Using the {Retrieval},
  author = {Meng, Wenjun and Li, Yuzhe and Chen, Lili L. and Dong, Zhaomin},
  year = {2025},
  doi = {10.3390/electronics14020386},
  url = {https://doi.org/10.3390/electronics14020386},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… For factual accuracy (F c ), using the LLM, we can split the generated answer (A) and the … This study has demonstrated that retrieval-augmented generation can improve the LLM’s …},
  annote = {Cited by: 3; All Open Access; Gold Open Access},
}

@article{thetbanthad_application_2025,
  title = {Application of {Generative},
  author = {Thetbanthad, Parinya and Sathanarugsawait, Benjaporn and Praneetpolgrang, Prasong},
  year = {2025},
  doi = {10.3390/jimaging11010011},
  url = {https://doi.org/10.3390/jimaging11010011},
  journal = {Journal of Imaging},
  volume = {11},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{dayarathne_comparing_2025,
  title = {Comparing the {Performance},
  author = {Dayarathne, Ranul Navojith and Ranaweera, Uvini and Ganegoda, Gamage Upeksha},
  year = {2025},
  doi = {10.1007/978-981-97-9255-9_26},
  url = {https://doi.org/10.1007/978-981-97-9255-9_26},
  journal = {Lecture Notes on Data Engineering and Communications Technologies},
  volume = {228},
  pages = {387 -- 403},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{das_two-layer_2025,
  title = {Two-{Layer},
  author = {Das, Sudeshna and Ge, Yao and Guo, Yuting and Rajwal, Swati and Hairston, Ja Mor and Powell, Jeanne M. and Walker, Drew and Peddireddy, Snigdha R. and Lakamana, Sahithi and Bozkurt, Selen and Reyna, Matthew A. and Sameni, Reza and Xiao, Yunyu and Kim, Sangmi and Chandler, Rasheeta D. and Hernandez, Natalie D. and Mowery, Danielle Lee and Wightman, Rachel Sarah and Love, Jennifer S. and Spadaro, Anthony V. and Perrone, Jeanmarie and Sarker, Abeed},
  year = {2025},
  doi = {10.2196/66220},
  url = {https://doi.org/10.2196/66220},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{kehan_crp-rag_2025,
  title = {{CRP},
  author = {Kehan, Xu and Kun, Zhang and Jingyuan, Li and Wei, Huang},
  year = {2025},
  doi = {10.3390/electronics14010047},
  url = {https://doi.org/10.3390/electronics14010047},
  journal = {Electronics (Switzerland)},
  volume = {14},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 7; All Open Access; Gold Open Access},
}

@article{labib_leveraging_2025,
  title = {Leveraging {Multimodal},
  author = {Labib, Rania},
  year = {2025},
  doi = {10.1007/978-981-97-8313-7_83},
  url = {https://doi.org/10.1007/978-981-97-8313-7_83},
  journal = {Lecture Notes in Civil Engineering},
  volume = {554 LNCE},
  pages = {611 -- 618},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{guan__2025,
  title = {: {A},
  author = {Guan, Wenbo and Li, Xiaoqian and Lu, Jiyu and Zhou, Jun},
  year = {2025},
  doi = {10.1007/978-3-031-78119-3_9},
  url = {https://doi.org/10.1007/978-3-031-78119-3_9},
  journal = {Lecture Notes in Computer Science},
  volume = {15331 LNCS},
  pages = {124 -- 138},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{agater_slango_2025,
  title = {{SLANGO},
  author = {Agater, Jérôme and Memari, Ammar},
  year = {2025},
  doi = {10.1007/978-3-031-77918-3_15},
  url = {https://doi.org/10.1007/978-3-031-77918-3_15},
  journal = {Lecture Notes in Computer Science},
  volume = {15447 LNAI},
  pages = {208 -- 221},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{gupta_refine_2025,
  title = {{REFINE},
  author = {Gupta, Ambuje and Rawat, Mrinal and Stolçke, Andreas and Pieraccini, Roberto},
  year = {2025},
  doi = {10.1007/978-981-96-0348-0_6},
  url = {https://doi.org/10.1007/978-981-96-0348-0_6},
  journal = {Lecture Notes in Computer Science},
  volume = {15442 LNAI},
  pages = {73 -- 85},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{nouzri_beyond_2025,
  title = {Beyond {Chatbots},
  author = {Nouzri, Sana and El Fatimi, Meryem and Guerin, Titouan and Othmane, Mahfoud and Najjar, Amro},
  year = {2025},
  doi = {10.1007/978-3-031-77367-9_29},
  url = {https://doi.org/10.1007/978-3-031-77367-9_29},
  journal = {Lecture Notes in Computer Science},
  volume = {15395 LNAI},
  pages = {385 -- 401},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@article{song_goal-oriented_2025,
  title = {A goal-oriented document-grounded dialogue based on evidence generation},
  author = {Song, Yong and Fan, Hongjie and Liu, Junfei and Liu, Yunxin and Ye, Xiaozhou and Ouyang, Ye},
  year = {2025},
  doi = {10.1016/j.datak.2024.102378},
  url = {https://doi.org/10.1016/j.datak.2024.102378},
  journal = {Data and Knowledge Engineering},
  volume = {155},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{liu_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Liu, Xuemin and Liu, Jie},
  year = {2025},
  doi = {10.1007/978-981-97-9434-8_36},
  url = {https://doi.org/10.1007/978-981-97-9434-8_36},
  journal = {Lecture Notes in Computer Science},
  volume = {15360 LNAI},
  pages = {462 -- 473},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{njeh_enhancing_2025,
  title = {Enhancing {RAG},
  author = {Njeh, Chaima and Nakouri, Haïfa and Jaafar, Fehmi},
  year = {2025},
  doi = {10.1007/978-3-031-74186-9_17},
  url = {https://doi.org/10.1007/978-3-031-74186-9_17},
  journal = {Lecture Notes in Computer Science},
  volume = {14858 LNAI},
  pages = {201 -- 213},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 6},
}

@article{kharitonova_incorporating_2025,
  title = {Incorporating evidence into mental health {Q},
  author = {Kharitonova, Ksenia and Pérez-Férnandez, David and Gutiérrez-Hernando, Javier and Gutiérrez-Fandiño, Asier and Callejas, Zoraida and Griol, David},
  year = {2025},
  doi = {10.1080/0144929X.2024.2321959},
  url = {https://doi.org/10.1080/0144929x.2024.2321959},
  journal = {Behaviour and Information Technology},
  volume = {44},
  number = {10},
  pages = {2333 -- 2350},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{papadopoulos_notitle_2025,
  author = {Papadopoulos, Theodoros and Alexopoulos, Charalampos and Charalabidis, Yannis K.},
  year = {2025},
  doi = {10.3389/fpos.2025.1601440},
  url = {https://doi.org/10.3389/fpos.2025.1601440},
  journal = {Frontiers in Political Science},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{null_notitle_2025-3,
  author = {null, null and Wijaya, Rifki and Kosala, Gamma},
  year = {2025},
  doi = {10.1109/ICoAILO66760.2025.11155930},
  url = {https://doi.org/10.1109/icoailo66760.2025.11155930},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wu_notitle_2025,
  author = {Wu, Chenghan and Liu, Ren-Shiou},
  year = {2025},
  doi = {10.1109/ICoAILO66760.2025.11156060},
  url = {https://doi.org/10.1109/icoailo66760.2025.11156060},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jin_risk_2025,
  title = {Risk {Identification},
  author = {Jin, Peng and Wang, Zongwei and Wang, Xiuchun and Zhao, Guoyi and Su, Yuan and Bu, Xiaoyang and Jiang, Dong},
  year = {2025},
  doi = {10.1007/978-3-032-06372-4_22},
  url = {https://doi.org/10.1007/978-3-032-06372-4_22},
  journal = {Smart Innovation, Systems and Technologies},
  volume = {451 SIST},
  pages = {254 -- 264},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhao_steprag_2025,
  title = {{StepRAG},
  author = {Zhao, Niannian and Wei, Xiao},
  year = {2025},
  doi = {10.18293/SEKE2025-058},
  url = {https://doi.org/10.18293/seke2025-058},
  booktitle = {Proceedings of the {International},
  pages = {56 -- 59},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wang_multi-agent_2025,
  title = {Multi-{Agent},
  author = {Wang, Qian and Duan, Wuxuan and Wang, Haiyan and Luan, Zhirong and Wang, Yu and Wang, Yulu},
  year = {2025},
  doi = {10.1109/ISAEECE66033.2025.11159949},
  url = {https://doi.org/10.1109/isaeece66033.2025.11159949},
  pages = {675 -- 680},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wang_leveraging_2025,
  title = {Leveraging {Electronic},
  author = {Wang, Jhing Fa and Wei, Mingjun and Chiang, Teming and Yeh, Tzu Chun and Cheng, Eric and Lee, Yuan Teh and Chen, Hong I.},
  year = {2025},
  doi = {10.1055/a-2707-2862},
  url = {https://doi.org/10.1055/a-2707-2862},
  journal = {Methods of Information in Medicine},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{yahia_empathetic_2025,
  title = {Empathetic {AI},
  author = {Yahia, Ahmed and Bakry, Mohamed and Radwan, Nada and Yossef, Nourhan and Amin, Ahmed and Ahmed, Ibrahim and Eldin Saad, Noha Gamal},
  year = {2025},
  doi = {10.1109/IMSA65733.2025.11166690},
  url = {https://doi.org/10.1109/imsa65733.2025.11166690},
  pages = {93 -- 98},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{guo_integrating_2025,
  title = {Integrating {Retrieval},
  author = {Guo, Yonghe and Yan, Longchuan and Niu, Jianing and Gao, Dequan and Yuan, Xiaoyu},
  year = {2025},
  doi = {10.1109/IAEAC65194.2025.11166604},
  url = {https://doi.org/10.1109/iaeac65194.2025.11166604},
  booktitle = {{IEEE},
  pages = {651 -- 657},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{neeser_wireless_2025,
  title = {Wireless {Knowledge},
  author = {Neeser, Andrew and Kurisummoottil Thomas, Christo Kurisummoottil and Xu, Shengzhe and Ramakrishnan, Naren and Saad, Walid M.},
  year = {2025},
  doi = {10.1109/ICC52391.2025.11160863},
  url = {https://doi.org/10.1109/icc52391.2025.11160863},
  booktitle = {Conference {Record},
  pages = {1572 -- 1577},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sharma_mitigating_2025,
  title = {Mitigating {Hallucination},
  author = {Sharma, Anantha and John, Sheeba Elizabeth and Nikroo, Fatemeh Rezapoor and Bhatt, Krupali and Zambre, Mrunal and Wikhe, Aditi},
  year = {2025},
  doi = {10.1109/ACDSA65407.2025.11165971},
  url = {https://doi.org/10.1109/acdsa65407.2025.11165971},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ekin_red-teaming_2025,
  title = {Red-{Teaming},
  author = {Ekin, Tahir and Mandadapu, Krishna Sai},
  year = {2025},
  doi = {10.1109/ACDSA65407.2025.11166648},
  url = {https://doi.org/10.1109/acdsa65407.2025.11166648},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{bhagwat_nyai_2025,
  title = {{NyAI},
  author = {Bhagwat, Swaroop and Panchangam, Sraina and Sawant, Aryaan and Sharma, Prakhar and Nayak, Udaychandra},
  year = {2025},
  doi = {10.1109/CONIT65521.2025.11167568},
  url = {https://doi.org/10.1109/conit65521.2025.11167568},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jing_kg-enhanced_2025,
  title = {{KG},
  author = {Jing, Xiao and Bhanpato, Jirat and Bendarkar, Mayank V. and Mavris, Dimitri N.},
  year = {2025},
  doi = {10.2514/6.2025-3250},
  url = {https://doi.org/10.2514/6.2025-3250},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{rahman_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Rahman, Hagan Sadina and Kusumaningtyas, Entin Martiana and Hadiah Muliawati, Tri and Karlita, Tita},
  year = {2025},
  doi = {10.1109/IES67184.2025.11161634},
  url = {https://doi.org/10.1109/ies67184.2025.11161634},
  pages = {886 -- 892},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{tampubolon_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Tampubolon, Andrew Lomaksan Manuel and Anggraini, Ratih Nur Esti and Hidayati, Shintami Chusnul},
  year = {2025},
  doi = {10.1109/ICoDSA67155.2025.11157319},
  url = {https://doi.org/10.1109/icodsa67155.2025.11157319},
  pages = {333 -- 338},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{mohsin_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Mohsin, Muhammad Ahmed and Bilal, Ahsan and Bhattacharya, Sagnik and Cioffi, John Matthew},
  year = {2025},
  doi = {10.1109/ICCWorkshops67674.2025.11162457},
  url = {https://doi.org/10.1109/iccworkshops67674.2025.11162457},
  pages = {184 -- 189},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{hamayat_seebot_2025,
  title = {{SEEBot},
  author = {Hamayat, Faizan and Ejaz, Luqman and Danish, Muhammad and Nazir, Ahsen and Ahadian, Pegah and Ahmad, Rana Fayyaz},
  year = {2025},
  doi = {10.1109/ICAIE64856.2025.11158323},
  url = {https://doi.org/10.1109/icaie64856.2025.11158323},
  pages = {147 -- 151},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kukreja_performance_2025,
  title = {Performance {Enhancement},
  author = {Kukreja, Sanjay and Kumar, Tarun and Bharate, Vishal D. and Gadwe, Sweta and Dasgupta, Abhijit and Guha, Debashis},
  year = {2025},
  doi = {10.1109/ICAIE64856.2025.11158296},
  url = {https://doi.org/10.1109/icaie64856.2025.11158296},
  pages = {465 -- 469},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{al_atraqchi_large_2025,
  title = {Large {Language},
  author = {Al Atraqchi, Osama Mohammed Ahmed and Abdulmuhsin, Amir A. and Yaseen, Saad G. and Alkhwaldi, Abeer F. and Rehman, Shafique Ur},
  year = {2025},
  doi = {10.1007/978-3-031-90271-0_15},
  url = {https://doi.org/10.1007/978-3-031-90271-0_15},
  journal = {Studies in Systems, Decision and Control},
  volume = {597},
  pages = {197 -- 218},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ding_sc-rag-cot_2025,
  title = {{SC},
  author = {Ding, Chen and Li, Cheng and Wu, Guiling and Qian, Liang and Liu, Ruifeng and Cai, Xinhao},
  year = {2025},
  doi = {10.1109/MRAI65197.2025.11135574},
  url = {https://doi.org/10.1109/mrai65197.2025.11135574},
  pages = {403 -- 407},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{mahmoud_o-ran_2025,
  title = {O-{RAN},
  author = {Mahmoud, Haitham Hassan M. and Zhang, Yunsheng and Lu, Chen and Odedra, Vishalji and Ismail, Khalid N. and Mi, De and He, Ziming and Chen, Feng and Liu, Haoting},
  year = {2025},
  doi = {10.1109/INFOCOMWKSHPS65812.2025.11152865},
  url = {https://doi.org/10.1109/infocomwkshps65812.2025.11152865},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{parameswaran_evaluating_2025,
  title = {Evaluating {Large},
  author = {Parameswaran, Vijaya and Bernard, Jenna and Bernard, Alec and Deo, Neil and Tsung, Sean and Lyytinen, Kalle J. and Sharp, Christopher Demuth and Rodriguez, Fatima and Maron, David J. and Dash, Rajesh},
  year = {2025},
  doi = {10.2196/78625},
  url = {https://doi.org/10.2196/78625},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{lee_llm-based_2025,
  title = {{LLM},
  author = {Lee, Po Han and Lin, Yu Cheng and Ku, Chantung and Hsu, Chan and Huang, Pei Cing and Wu, Pinghsun and Kang, Yihuang},
  year = {2025},
  doi = {10.1109/IRI66576.2025.00055},
  url = {https://doi.org/10.1109/iri66576.2025.00055},
  pages = {265 -- 270},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{nishisako_reducing_2025,
  title = {Reducing {Hallucinations},
  author = {Nishisako, Sota and Higashi, Takahiro and Wakao, Fumihiko},
  year = {2025},
  doi = {10.2196/70176},
  url = {https://doi.org/10.2196/70176},
  journal = {JMIR Cancer},
  volume = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{alenjareghi_llm-driven_2025,
  title = {{LLM},
  author = {Alenjareghi, Morteza Jalali and Keivanpour, Samira and Chinniah, Yuvin A. and Jocelyn, Sabrina},
  year = {2025},
  doi = {10.1109/ICHMS65439.2025.11154327},
  url = {https://doi.org/10.1109/ichms65439.2025.11154327},
  pages = {295 -- 301},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{zhao_knowledge_2025,
  title = {Knowledge {Retrieval},
  author = {Zhao, Shuai and Feng, Jun and Shen, Xiaojun and Yan, Huaguang and Chen, Zhenyu and Du, Jianguang and Luo, Pengxin and Wang, Zhen},
  year = {2025},
  doi = {10.1109/PSGEC66102.2025.11151081},
  url = {https://doi.org/10.1109/psgec66102.2025.11151081},
  pages = {421 -- 427},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wang_ms-rag_2025,
  title = {{MS},
  author = {Wang, Hongda and Qi, Miao and Wang, Zheng and Yang, Qiliang},
  year = {2025},
  doi = {10.1109/ICETIS66286.2025.11144098},
  url = {https://doi.org/10.1109/icetis66286.2025.11144098},
  pages = {307 -- 313},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{pham_vigpt_2025,
  title = {{ViGPT},
  author = {Pham, Thi Thu Trang and Pham, Phuc H. and Pham, Van Hoang and Tran, Hieu Minh and Duong, Tan Nghia},
  year = {2025},
  doi = {10.1109/ATIGB66719.2025.11142107},
  url = {https://doi.org/10.1109/atigb66719.2025.11142107},
  pages = {199 -- 204},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{majumder_development_2025,
  title = {Development and {Evaluation},
  author = {Majumder, Anubhab and Bhattacharya, Kausik and Chakrabarti, Amaresh},
  year = {2025},
  doi = {10.1007/978-981-96-5511-3_38},
  url = {https://doi.org/10.1007/978-981-96-5511-3_38},
  journal = {Lecture Notes in Mechanical Engineering},
  pages = {489 -- 504},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{somasundharam_llm_2025,
  title = {{LLM},
  author = {Somasundharam, Lalitha and Vardhan, C. H.Harsha and Varma, Sagi Bhimeswara Akshay and Swamy, Koyya Manjunadha},
  year = {2025},
  doi = {10.1109/ICCMC65190.2025.11140911},
  url = {https://doi.org/10.1109/iccmc65190.2025.11140911},
  pages = {675 -- 682},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{rajbhar_aiced-prep_2025,
  title = {{AIced},
  author = {Rajbhar, Snehal and Shelke, Siddhi and Singh, Ayush and Singh, Yash and Shinde, Pravin},
  year = {2025},
  doi = {10.1109/ICDICI66477.2025.11135157},
  url = {https://doi.org/10.1109/icdici66477.2025.11135157},
  pages = {1840 -- 1843},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{lee_less_2025,
  title = {Less is {Better},
  author = {Lee, Jaekun and Lee, G.},
  year = {2025},
  doi = {10.22260/ISARC2025/0143},
  url = {https://doi.org/10.22260/isarc2025/0143},
  booktitle = {Proceedings of the {International},
  pages = {1105 -- 1112},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{mikulic_integrating_2025,
  title = {Integrating {External},
  author = {Mikulic, Ivan and Vlaic, Marin and Delač, Goran and Šilić, Marin and Vladimir, Klemo},
  year = {2025},
  doi = {10.1109/MIPRO65660.2025.11131735},
  url = {https://doi.org/10.1109/mipro65660.2025.11131735},
  pages = {93 -- 98},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jiao_deepvulhunter_2025,
  title = {{DeepVulHunter},
  author = {Jiao, Yutong and Han, Jiaxuan and Huang, Cheng},
  year = {2025},
  doi = {10.1007/s10844-025-00982-0},
  url = {https://doi.org/10.1007/s10844-025-00982-0},
  journal = {Journal of Intelligent Information Systems},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sonawane_implementation_2025,
  title = {Implementation of an {Interactive},
  author = {Sonawane, Vedant and Sambare, Santosh S. and Ambala, Srinivas and Kadam, Ganesh},
  year = {2025},
  doi = {10.1109/ICOCT64433.2025.11118878},
  url = {https://doi.org/10.1109/icoct64433.2025.11118878},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sambare_survey_2025,
  title = {Survey of {Interactive},
  author = {Sambare, Santosh S. and Ambala, Srinivas and Kadam, Ganesh and Sonawane, Vedant},
  year = {2025},
  doi = {10.1109/ICOCT64433.2025.11118753},
  url = {https://doi.org/10.1109/icoct64433.2025.11118753},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{qi_deepsem_2025,
  title = {{DeepSem},
  author = {Qi, Yujia and Gu, Shijia and Ma, Wenxian and Wang, Nan and Wang, Qian},
  year = {2025},
  doi = {10.1109/COMPSAC65507.2025.00174},
  url = {https://doi.org/10.1109/compsac65507.2025.00174},
  pages = {1388 -- 1393},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{anjum_halo_2025,
  title = {{HALO},
  author = {Anjum, Sumera and Zhang, Hanzhi and Zhou, Wenjun and Paek, Eun Jin and Zhao, Xiaopeng and Feng, Yunhe},
  year = {2025},
  doi = {10.1145/3721201.3721385},
  url = {https://doi.org/10.1145/3721201.3721385},
  pages = {187 -- 198},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{vonderhaar_surveying_2025,
  title = {Surveying the {RAG},
  author = {Vonderhaar, Lynn and MacHado, Daniel A. and Ochoa, Omar},
  year = {2025},
  doi = {10.1109/AITest66680.2025.00015},
  url = {https://doi.org/10.1109/aitest66680.2025.00015},
  pages = {69 -- 76},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{yu_engineering_2025,
  title = {Engineering {Critical},
  author = {Yu, Hongqing and Scanlon, Brian and Stephan Reiff-Marganiec, Stephan},
  year = {2025},
  doi = {10.1109/SOSE67019.2025.00005},
  url = {https://doi.org/10.1109/sose67019.2025.00005},
  pages = {1 -- 7},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{mashnoor_timelyhls_2025,
  title = {{TimelyHLS},
  author = {Mashnoor, Nowfel and Akyash, Mohammad and Kamali, Hadi Mardani and Azar, Kimia Zamiri},
  year = {2025},
  doi = {10.1109/COINS65080.2025.11125726},
  url = {https://doi.org/10.1109/coins65080.2025.11125726},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{pinna_ai-driven_2025,
  title = {{AI},
  author = {Pinna, Simone and Massa, Silvia Maria and Riboni, Daniele},
  year = {2025},
  doi = {10.1109/IE64880.2025.11130125},
  url = {https://doi.org/10.1109/ie64880.2025.11130125},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{garcia_df-rag_2025,
  title = {{DF},
  author = {Garcia, Julian and Hahn, Andrew and Zajac, Michal and Gong, Jiaqi},
  year = {2025},
  doi = {10.1145/3721201.3725426},
  url = {https://doi.org/10.1145/3721201.3725426},
  booktitle = {2025 {IEEE},
  pages = {418 -- 423},
  note = {Type: Conference paper},
  keywords = {source: Scopus, Accuracy, Collaboration, Decision making, federated learning, knowledge graphs, large language models, Medical diagnostic imaging, Privacy, privacy-preserving, Real-time systems, Retrieval augmented generation, retrieval-augmented generation, Scalability, Training, source: IEEE},
  abstract = {This paper introduces Dual Federated Retrieval-Augmented Generation (DF-RAG), a framework addressing privacy, interpretability, and reliability challenges in AI-driven healthcare. Large Language Models (LLMs) typically depend on centralized training and static data, causing inaccuracies ("hallucinations") and privacy risks. DF-RAG mitigates these issues through Federated Fine-Tuning (FFT) and Federated Knowledge Graphs (FKGs). FFT enables secure collaborative model refinement across institutions using encrypted updates, ensuring compliance with HIPAA and GDPR. Concurrently, FKGs offer real-time, validated medical knowledge retrieval, reducing inaccuracies and enhancing interpretability. Initial evaluations suggest DF-RAG improves diagnostic accuracy, scalability, and patient privacy, promising significant advances in clinical decision-making and personalized medicine.},
  annote = {Cited by: 0},
  month = {jun},
}

@article{min_verification_2025,
  title = {Verification and {Validation},
  author = {Min, Ziran and Budnik, Christof J.},
  year = {2025},
  doi = {10.1109/AITest66680.2025.00012},
  url = {https://doi.org/10.1109/aitest66680.2025.00012},
  pages = {50 -- 53},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{yang_chain--summary_2025,
  title = {Chain-of-{Summary},
  author = {Yang, Chongchong and Zhu, Wenhao and Yang, Zhiqiang and Liu, Chaoqian and Hong, Jianfeng},
  year = {2025},
  doi = {10.1109/COMPSAC65507.2025.00090},
  url = {https://doi.org/10.1109/compsac65507.2025.00090},
  pages = {673 -- 682},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jain_mitigating_2025,
  title = {On {Mitigating},
  author = {Jain, Nihal and Kwiatkowski, Robert and Ray, Baishakhi and Ramanathan, Murali Krishna and Kumar, Varun},
  year = {2025},
  doi = {10.1109/ICSE-SEIP66354.2025.00027},
  url = {https://doi.org/10.1109/icse-seip66354.2025.00027},
  journal = {IEEE/ACM International Conference on Software Engineering - Software Engineering in Practice},
  number = {2025},
  pages = {237 -- 248},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{vungarala_tpu-gen_2025,
  title = {{TPU},
  author = {Vungarala, Deepak and Elbtity, Mohammed E. and Pandit, Kartik and Syed, Sumiya and Alam, Sakila and Ghosh, Arnab and Zand, Ramtin Mohammadi and Angizi, Shaahin},
  year = {2025},
  doi = {10.1109/ICLAD65226.2025.00010},
  url = {https://doi.org/10.1109/iclad65226.2025.00010},
  pages = {1 -- 8},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{tang_hivegen_2025,
  title = {{HiVeGen},
  author = {Tang, Jinwei and Qin, Jiayin and Thorat, Kiran and Zhu-Tian, Chen and Cao, Yu and Zhao, Yang Katie and Ding, Caiwen},
  year = {2025},
  doi = {10.1109/ICLAD65226.2025.00031},
  url = {https://doi.org/10.1109/iclad65226.2025.00031},
  pages = {30 -- 36},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ping_hdlcore_2025,
  title = {{HDLCoRe},
  author = {Ping, Heng and Li, Shixuan and Zhang, Peiyu and Cheng, Anzhe and Duan, Shukai and Kanakaris, Nikos and Xiao, Xiongye and Yang, Wei and Nazarian, Shahin and Irimia, Andrei and Bogdan, Paul},
  year = {2025},
  doi = {10.1109/ICLAD65226.2025.00034},
  url = {https://doi.org/10.1109/iclad65226.2025.00034},
  pages = {108 -- 116},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{khan_towards_2025,
  title = {Towards {Efficient},
  author = {Khan, Umar Ali and Khan, Fiza and Khan, Ekram and Hasnain, Mohd Areeb and Moinuddin, Athar Ali},
  year = {2025},
  doi = {10.1109/ETCC65847.2025.11108323},
  url = {https://doi.org/10.1109/etcc65847.2025.11108323},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{wan_facilitating_2025,
  title = {Facilitating {Design},
  author = {Wan, Yuwei and Liu, Ying and Zammit, Joseph Paul and Chen, Zheyuan and Li, Li and Francalanza, Emmanuel},
  year = {2025},
  doi = {10.1109/ICE/ITMC65658.2025.11106597},
  url = {https://doi.org/10.1109/ice/itmc65658.2025.11106597},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ugur_guided_2025,
  title = {Guided {Decoding},
  author = {Ugur, Ozgur and Yilmaz, Musa and Savirdi, Esra and Ezerceli, Özay and El Huseyni, Mahmut and Tas, Selva and Bayraktar, Reyhan},
  year = {2025},
  doi = {10.1109/SIU66497.2025.11111950},
  url = {https://doi.org/10.1109/siu66497.2025.11111950},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{liu_enhancing_2025-1,
  title = {Enhancing {Large},
  author = {Liu, Jiaxiang and Zhou, Tong and Chen, Yubo and Zhao, Jun and Liu, Kang},
  year = {2025},
  doi = {10.1109/ICDEW67478.2025.00019},
  url = {https://doi.org/10.1109/icdew67478.2025.00019},
  pages = {97 -- 106},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{cheung_hallucination_2025,
  title = {Hallucination {Detection},
  author = {Cheung, Pak Ming},
  year = {2025},
  doi = {10.1109/ICDEW67478.2025.00033},
  url = {https://doi.org/10.1109/icdew67478.2025.00033},
  journal = {arXiv preprint arXiv:2506.22486},
  pages = {230 -- 238},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… may not exist within the LLM, necessitating retrieval-augmented generation (RAG) [4] to enhance its capacity by providing knowledge that does not exist in the LLM. Related context can …},
  annote = {Cited by: 0},
}

@article{chen_adaptive_2025,
  title = {Adaptive {Retrieval},
  author = {Chen, Mengmeng and Bai, Yuanyuan and Fang, Xiaoyong and Yi, Pengfei},
  year = {2025},
  doi = {10.1109/SEAI65851.2025.11108876},
  url = {https://doi.org/10.1109/seai65851.2025.11108876},
  pages = {230 -- 234},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{donvir_leveraging_2025,
  title = {Leveraging {RAG},
  author = {Donvir, Anujkumarsinh and Yadav, Priti and Panyam, Sriram and Joshi, Ram},
  year = {2025},
  doi = {10.1109/AIIoT65859.2025.11105349},
  url = {https://doi.org/10.1109/aiiot65859.2025.11105349},
  pages = {655 -- 661},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sha_leveraging_2025,
  title = {Leveraging {Retrieval},
  author = {Sha, Hangyu and Gong, Fan and Liu, Bo and Liu, Runfeng and Wang, Haofen and Wu, Tianxing},
  year = {2025},
  doi = {10.2196/75279},
  url = {https://doi.org/10.2196/75279},
  journal = {JMIR Medical Informatics},
  volume = {13},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{hedi_artificial_2025,
  title = {Artificial {Intelligence},
  author = {Hedi, Tebourbi and Nouzri, Sana and Mualla, Yazan and Abbas-Turki, A.},
  year = {2025},
  doi = {10.1016/j.procs.2025.07.179},
  url = {https://doi.org/10.1016/j.procs.2025.07.179},
  booktitle = {Procedia {Computer},
  volume = {265},
  pages = {252 -- 259},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{vach_evaluating_2025,
  title = {Evaluating {Retrieval},
  author = {Vach, Marius and Gliem, Michael and Weiss, Daniel and Ivan, Vivien Lorena and Hauke, Frederik and Boschenriedter, Christian and Rubbert, Christian and Caspers, Julian},
  year = {2025},
  doi = {10.1007/s00062-025-01562-z},
  url = {https://doi.org/10.1007/s00062-025-01562-z},
  journal = {Clinical Neuroradiology},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{vieira-vieira_data_2025,
  title = {From data silos to insights: the {PRINCE},
  author = {Vieira-Vieira, Carlos Henrique and Kulkarni, Sarang Sanjay and Zalewski, Adam and Löffler, Jobst and Münch, Jonas and Kreuchwig, Annika},
  year = {2025},
  doi = {10.3389/frai.2025.1636809},
  url = {https://doi.org/10.3389/frai.2025.1636809},
  journal = {Frontiers in Artificial Intelligence},
  volume = {8},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{kumar_leveraging_2025,
  title = {Leveraging {RAG},
  author = {Kumar, Rajiv},
  year = {2025},
  doi = {10.1007/978-981-96-6297-5_30},
  url = {https://doi.org/10.1007/978-981-96-6297-5_30},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1408 LNNS},
  pages = {389 -- 399},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{yildirim_rag-based_2025,
  title = {A {RAG},
  author = {Yıldırım, Şenda and Şamli, Rüya},
  year = {2025},
  doi = {10.55549/epstem.1729714},
  url = {https://doi.org/10.55549/epstem.1729714},
  booktitle = {Eurasia {Proceedings},
  volume = {33},
  pages = {20 -- 27},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{wang_hiyo-encoder_2025,
  title = {{HIYO},
  author = {Wang, Chenxu and Feng, Yijun and Wang, Tiande and Qin, Huaibin},
  year = {2025},
  doi = {10.1117/12.3068397},
  url = {https://doi.org/10.1117/12.3068397},
  booktitle = {Proceedings of {SPIE},
  volume = {13692},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{rui_chatcl_2025,
  title = {{ChaTCL},
  author = {Rui, Yibo and Li, Yuanhang and Wang, Rui and Chen, Ruiqi and Zhu, Yanxiang and Di, Zhixiong and Wang, Xi and Ling, Ming},
  year = {2025},
  doi = {10.1109/ISEDA65950.2025.11101624},
  url = {https://doi.org/10.1109/iseda65950.2025.11101624},
  pages = {736 -- 742},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wiest_large_2025,
  title = {Large language models for clinical decision support in gastroenterology and hepatology},
  author = {Wiest, Isabella Catharina and Bhat, Mamatha Pallavi and Clusmann, Jan and Schneider, Carolin Victoria and Jiang, Xiaofeng and Kather, Jakob Nikolas},
  year = {2025},
  doi = {10.1038/s41575-025-01108-1},
  url = {https://doi.org/10.1038/s41575-025-01108-1},
  journal = {Nature Reviews Gastroenterology and Hepatology},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{belkhouribchia_large_2025,
  title = {Large language models in clinical nutrition: an overview of its applications, capabilities, limitations, and potential future prospects},
  author = {Belkhouribchia, Jamal and Pen, Joeri J.},
  year = {2025},
  doi = {10.3389/fnut.2025.1635682},
  url = {https://doi.org/10.3389/fnut.2025.1635682},
  journal = {Frontiers in Nutrition},
  volume = {12},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{zheng_incorporating_2025,
  title = {Incorporating {RAG},
  author = {Zheng, Lei and Ye, Junmin and Zhao, Gang and Luo, Sheng and Nan, Mengting and Xie, Yiliang},
  year = {2025},
  doi = {10.1109/CSTE64638.2025.11092000},
  url = {https://doi.org/10.1109/cste64638.2025.11092000},
  pages = {843 -- 847},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{kamra_evaluating_2025,
  title = {Evaluating {Reinforcement},
  author = {Kamra, Vikas and Gupta, Lakshya and Arora, Dhruv and Yadav, Ashwin Kumar},
  year = {2025},
  doi = {10.1109/ICC-ROBINS64345.2025.11086322},
  url = {https://doi.org/10.1109/icc-robins64345.2025.11086322},
  pages = {254 -- 259},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{bernardi_automatic_2025,
  title = {Automatic {Generation},
  author = {Bernardi, Mario Luca and Cimitile, Marta and Panella, Giovanni and Pecori, Riccardo and Simoncelli, Giuditta},
  year = {2025},
  doi = {10.1007/s10796-025-10634-x},
  url = {https://doi.org/10.1007/s10796-025-10634-x},
  journal = {Information Systems Frontiers},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{ngandu_juro_2025,
  title = {Juro: {A},
  author = {Ngandu, Bernard and Mbale, Landry and Bagula, Antoine Bigomokero},
  year = {2025},
  doi = {10.1007/978-3-031-94439-0_15},
  url = {https://doi.org/10.1007/978-3-031-94439-0_15},
  journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
  volume = {632 LNICST},
  pages = {260 -- 274},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kim_geographic_2025,
  title = {Geographic {Limitations},
  author = {Kim, Junghwan},
  year = {2025},
  doi = {10.1007/978-3-031-87421-5_20},
  url = {https://doi.org/10.1007/978-3-031-87421-5_20},
  journal = {Springer Geography},
  volume = {Part F723},
  pages = {289 -- 296},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{blanchard_making_2025,
  title = {Making {Generative},
  author = {Blanchard, Emmanuel G. and Callahan, Jean Christophe and Akretche, Idir and Asswiel, Naif and Schmitt, Lucas and Kessemtini, Amine and Khan, Imran S.A.},
  year = {2025},
  doi = {10.1007/978-3-031-99264-3_36},
  url = {https://doi.org/10.1007/978-3-031-99264-3_36},
  journal = {Communications in Computer and Information Science},
  volume = {2591 CCIS},
  pages = {289 -- 296},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{qi_p-lrr_2025,
  title = {P-{LRR},
  author = {Qi, Gege and Yi, Ningyuan and Wang, Shuo and Chang, Wenjing and Yu, Jianjun},
  year = {2025},
  doi = {10.1007/978-981-96-9921-6_38},
  url = {https://doi.org/10.1007/978-981-96-9921-6_38},
  journal = {Lecture Notes in Computer Science},
  volume = {15857 LNCS},
  pages = {469 -- 480},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{cole_towards_2025,
  title = {Towards {AI},
  author = {Cole, Carolyn and Hajikhani, Arash and Hylkilä, Eveliina and Paronen, Essi and Pihkola, Hanna},
  year = {2025},
  doi = {10.1007/s11367-025-02508-w},
  url = {https://doi.org/10.1007/s11367-025-02508-w},
  journal = {International Journal of Life Cycle Assessment},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{wang_visualrag_2025,
  title = {{VisualRAG},
  author = {Wang, Hengchang and Liu, Li and Zhang, Huaxiang and Zhu, Lei and Chang, Xiaojun and Du, Hao},
  year = {2025},
  doi = {10.1109/TCSVT.2025.3597097},
  url = {https://doi.org/10.1109/tcsvt.2025.3597097},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhang_leveraging_2025-1,
  title = {Leveraging {Large},
  author = {Zhang, Rongqian and Xie, Guanwen and Ying, Jie and Hua, Zhongsheng},
  year = {2025},
  doi = {10.1109/JBHI.2025.3594014},
  url = {https://doi.org/10.1109/jbhi.2025.3594014},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jiang_multi_semantic_2025,
  title = {Multi\_semantic {RAG},
  author = {Jiang, Ming and Zou, Ling and Lu, Yao and Zhang, Hao and Tang, Yan and Qin, Ping},
  year = {2025},
  doi = {10.1007/978-981-96-9994-0_14},
  url = {https://doi.org/10.1007/978-981-96-9994-0_14},
  journal = {Communications in Computer and Information Science},
  volume = {2573 CCIS},
  pages = {163 -- 173},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{li_eduragrag_2025,
  title = {{EduRAG},
  author = {Li, Yue and Zheng, Luohao},
  year = {2025},
  doi = {10.1007/978-981-96-9815-8_15},
  url = {https://doi.org/10.1007/978-981-96-9815-8_15},
  journal = {Lecture Notes in Computer Science},
  volume = {15860 LNCS},
  pages = {173 -- 182},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{sonkar_dynamic_2025,
  title = {Dynamic {Query},
  author = {Sonkar, Aakash and Singh, Sumeet Pratap and Sahu, Komendra and Sahu, Aayush and Mishra, Sachin},
  year = {2025},
  doi = {10.1109/OTCON65728.2025.11070378},
  url = {https://doi.org/10.1109/otcon65728.2025.11070378},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhai_law_2025,
  title = {Law {GraphRAG},
  author = {Zhai, Haoxing},
  year = {2025},
  doi = {10.1109/AIITA65135.2025.11047851},
  url = {https://doi.org/10.1109/aiita65135.2025.11047851},
  pages = {1407 -- 1410},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhang_corag_2025,
  title = {{CoRAG},
  author = {Zhang, Jingyan and Feng, Dawei and Ding, Bo},
  year = {2025},
  doi = {10.1109/AIITA65135.2025.11047906},
  url = {https://doi.org/10.1109/aiita65135.2025.11047906},
  pages = {2116 -- 2123},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{li_grag-zre_2025,
  title = {{GRAG},
  author = {Li, Changjian and Song, Yang and Li, Aiping},
  year = {2025},
  doi = {10.1007/978-981-96-9818-9_23},
  url = {https://doi.org/10.1007/978-981-96-9818-9_23},
  journal = {Lecture Notes in Computer Science},
  volume = {15861 LNCS},
  pages = {273 -- 285},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{fan_why_2025,
  title = {Why {Did},
  author = {Fan, Zhilin and Chen, Penghe and Lu, Yu},
  year = {2025},
  doi = {10.1007/978-3-031-98459-4_23},
  url = {https://doi.org/10.1007/978-3-031-98459-4_23},
  journal = {Lecture Notes in Computer Science},
  volume = {15880 LNAI},
  pages = {321 -- 335},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{patel_agentic_2025,
  title = {{AGENTIC},
  author = {Patel, Yug and Badre, Snehlata},
  year = {2025},
  doi = {10.1049/icp.2025.1296},
  url = {https://doi.org/10.1049/icp.2025.1296},
  journal = {IET Conference Proceedings},
  volume = {2025},
  number = {7},
  pages = {208 -- 213},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kumawat_sh-rag_2025,
  title = {{SH},
  author = {Kumawat, Ayush and Jawdekar, Anand and Patsariya, Sanjay and Gupta, Vicky},
  year = {2025},
  doi = {10.1049/icp.2025.1292},
  url = {https://doi.org/10.1049/icp.2025.1292},
  journal = {IET Conference Proceedings},
  volume = {2025},
  number = {7},
  pages = {178 -- 185},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhao_slideitright_2025,
  title = {{SlideItRight},
  author = {Zhao, Chloe Qianhui and Cao, Jie and Chen, Eason and Koedinger, Kenneth R. and Lin, Jionghao},
  year = {2025},
  doi = {10.1007/978-3-031-98459-4_27},
  url = {https://doi.org/10.1007/978-3-031-98459-4_27},
  journal = {Lecture Notes in Computer Science},
  volume = {15880 LNAI},
  pages = {378 -- 392},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{shimada_leveraging_2025,
  title = {Leveraging {Lecture},
  author = {Shimada, Atsushi},
  year = {2025},
  doi = {10.1007/978-3-031-98462-4_43},
  url = {https://doi.org/10.1007/978-3-031-98462-4_43},
  journal = {Lecture Notes in Computer Science},
  volume = {15881 LNAI},
  pages = {340 -- 347},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{huang_corgpt_2025,
  title = {{CorGPT},
  author = {Huang, Baoqian and Zhang, Hongkuan and Liu, Zhaoyang and Wang, Jiwei and Zhou, Shuwang},
  year = {2025},
  doi = {10.1007/978-981-95-0027-7_24},
  url = {https://doi.org/10.1007/978-981-95-0027-7_24},
  journal = {Lecture Notes in Computer Science},
  volume = {15866 LNBI},
  pages = {272 -- 282},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{stankov_application_2025,
  title = {Application of a {Large},
  author = {Stankov, Svetlomir and Velinova, Desislava},
  year = {2025},
  doi = {10.17770/etr2025vol2.8603},
  url = {https://doi.org/10.17770/etr2025vol2.8603},
  journal = {… . Proceedings of the International Scientific and …},
  volume = {2},
  pages = {345 -- 351},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… what RAG systems are and how they help increase the credibility of the text generated by LLM models (reducing their hallucinations) … An approach for the development of a RAG system …},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{guo_root_2025,
  title = {Root {Cause},
  author = {Guo, Zhaorui and Zou, Jing and Xin, Peizhe and Zhao, Xiongfei and Hu, Tian and Zhuang, Shangyuan and Sun, Jiyan and Liu, Yinlong and Ma, Wei},
  year = {2025},
  doi = {10.1109/CSCWD64889.2025.11033346},
  url = {https://doi.org/10.1109/cscwd64889.2025.11033346},
  journal = {Proceedings of the International Conference on Computer Supported Cooperative Work in Design, CSCWD},
  number = {2025},
  pages = {624 -- 629},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{amo-filva_expressing_2025,
  title = {Expressing {Educational},
  author = {Amo-Filva, Daniel and Pikatza-Huerga, A. and Romero-Yesa, Susana and Gomez, Alvaro Sicilia and Donate, Belén and Fernandez, Eduard and Fonseca, David},
  year = {2025},
  doi = {10.1007/978-981-96-5658-5_16},
  url = {https://doi.org/10.1007/978-981-96-5658-5_16},
  journal = {Lecture Notes in Educational Technology},
  volume = {Part F642},
  pages = {155 -- 166},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{arjona-giner_multiagent_2025,
  title = {Multiagent {Systems},
  author = {Arjona-Giner, Sergio and Llorens-Largo, Faraón},
  year = {2025},
  doi = {10.1007/978-981-96-5658-5_132},
  url = {https://doi.org/10.1007/978-981-96-5658-5_132},
  journal = {Lecture Notes in Educational Technology},
  volume = {Part F642},
  pages = {1332 -- 1341},
  note = {Type: Book chapter},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{bu_rb-rag_2025,
  title = {{RB},
  author = {Bu, Kangning and Wang, Zehua and Zhu, Pengcheng and Isaev, Parviz and Chen, Wei and Xu, Tingting and Liu, Jueting},
  year = {2025},
  doi = {10.1007/978-981-96-9946-9_10},
  url = {https://doi.org/10.1007/978-981-96-9946-9_10},
  journal = {Communications in Computer and Information Science},
  volume = {2565 CCIS},
  pages = {112 -- 123},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{abtahi_augmenting_2025,
  title = {Augmenting {Large},
  author = {Abtahi, Seyed Moein and Azim, Akramul},
  year = {2025},
  doi = {10.1109/Forge66646.2025.00017},
  url = {https://doi.org/10.1109/forge66646.2025.00017},
  pages = {82 -- 92},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{agrawal_llm_2025,
  title = {{LLM},
  author = {Agrawal, Shweta and Li, He Nan Tony and Lu, Lucas},
  year = {2025},
  doi = {10.1109/CAI64502.2025.00151},
  url = {https://doi.org/10.1109/cai64502.2025.00151},
  pages = {853 -- 858},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{miyaji_empowering_2025,
  title = {Empowering {Business},
  author = {Miyaji, Renato and Moulin, Renato and Moncao, Samuel and MacHado, Leonardo},
  year = {2025},
  doi = {10.1109/CAI64502.2025.00016},
  url = {https://doi.org/10.1109/cai64502.2025.00016},
  pages = {55 -- 60},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{joseph_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Joseph, Tibakanya and Hellen, Nakayiza and Marvin, Ggaliwango},
  year = {2025},
  doi = {10.1007/978-981-96-2700-4_31},
  url = {https://doi.org/10.1007/978-981-96-2700-4_31},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1277 LNNS},
  pages = {425 -- 442},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{fink_referral_2025,
  title = {From {Referral},
  author = {Fink, Anna and Rau, Stephan and Kästingschäfer, Kai and Weib, Jakob and Bamberg, Fabian and Russe, Maximilian Frederik},
  year = {2025},
  doi = {10.1055/a-2641-3059},
  url = {https://doi.org/10.1055/a-2641-3059},
  journal = {RoFo Fortschritte auf dem Gebiet der Rontgenstrahlen und der Bildgebenden Verfahren},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{shen_large_2025-1,
  title = {Large {Language},
  author = {Shen, Ying and Zhao, Shichao and Lv, Yanfei and Chen, Fei and Fu, Li and Karimi‑Maleh, Hassan},
  year = {2025},
  doi = {10.32604/cmc.2025.067427},
  url = {https://doi.org/10.32604/cmc.2025.067427},
  journal = {Computers, Materials and Continua},
  volume = {84},
  number = {2},
  pages = {1921 -- 1950},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{matys_aggtruth_2025,
  title = {{AggTruth},
  author = {Matys, Piotr and Eliasz, Jan and Kiełczyński, Konrad and Langner, Mikołaj and Ferdinan, Teddy and Kocoń, Jan and Kazienko, Przemysław},
  year = {2025},
  doi = {10.1007/978-3-031-97570-7_18},
  url = {https://doi.org/10.1007/978-3-031-97570-7_18},
  journal = {Lecture Notes in Computer Science},
  volume = {15911 LNCS},
  pages = {227 -- 243},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{karpe_infonest_2025,
  title = {{InfoNest},
  author = {Karpe, Rudraksh and Morale, Suyash and Mir, Waseem Ahmad and Khiani, Simran R. and Kumar, Aman and Aryan, Krish and Mishra, Tushar and Jedhe, Pravin},
  year = {2025},
  doi = {10.1007/978-981-96-3102-5_6},
  url = {https://doi.org/10.1007/978-981-96-3102-5_6},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1288 LNNS},
  pages = {85 -- 98},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{tel_utilizing_2025,
  title = {Utilizing the {Structure},
  author = {Tel, Tolga and Minor, Mirjam},
  year = {2025},
  doi = {10.1007/978-3-031-96559-3_11},
  url = {https://doi.org/10.1007/978-3-031-96559-3_11},
  journal = {Lecture Notes in Computer Science},
  volume = {15662 LNAI},
  pages = {157 -- 171},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wojtasik_citeverifier_2025,
  title = {{CiteVerifier},
  author = {Wojtasik, Konrad and Dolega, Tymoteusz and Piasecki, Maciej},
  year = {2025},
  doi = {10.1007/978-3-031-97570-7_19},
  url = {https://doi.org/10.1007/978-3-031-97570-7_19},
  journal = {Lecture Notes in Computer Science},
  volume = {15911 LNCS},
  pages = {244 -- 257},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{yang_multi-agent_2025,
  title = {A {Multi},
  author = {Yang, Yangrui and Wang, Pengfei and Liu, Xuemei and Luo, Wenyu and Yang, Libo},
  year = {2025},
  doi = {10.1007/s11269-025-04312-5},
  url = {https://doi.org/10.1007/s11269-025-04312-5},
  journal = {Water Resources Management},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{garcia-carmona_leveraging_2025,
  title = {Leveraging {Large},
  author = {Garcia-Carmona, Angel Manuel and Prieto, María Lorena and Puertas, Enrique and Beunza, Juan Jose},
  year = {2025},
  doi = {10.2196/68776},
  url = {https://doi.org/10.2196/68776},
  journal = {JMIR AI},
  volume = {4},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{sudhir_leveraging_2025,
  title = {Leveraging {Agentic},
  author = {Sudhir, Pratul and Suresh, Sreya and Thontadari, C.},
  year = {2025},
  doi = {10.1007/978-981-96-4883-2_4},
  url = {https://doi.org/10.1007/978-981-96-4883-2_4},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1355 LNNS},
  pages = {31 -- 42},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{munir_evaluating_2025,
  title = {Evaluating {AI},
  author = {Munir, Bakht and Abbasi, Muhammad Zubair and Blake Wilson, W. and Colombo, Allen},
  year = {2025},
  doi = {10.1017/jli.2025.10052},
  url = {https://doi.org/10.1017/jli.2025.10052},
  journal = {International Journal of Legal Information},
  volume = {53},
  number = {2},
  pages = {103 -- 114},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{bianchini_automating_2025,
  title = {Automating {Industrial},
  author = {Bianchini, Filippo and Calamo, Marco and Marinacci, Matteo and Rossi, Jacopo and Mecella, Massimo},
  year = {2025},
  doi = {10.1007/978-3-031-96235-6_19},
  url = {https://doi.org/10.1007/978-3-031-96235-6_19},
  journal = {IFIP Advances in Information and Communication Technology},
  volume = {758 IFIPAICT},
  pages = {253 -- 266},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{seabra_enhancing_2025,
  title = {Enhancing {Explainability},
  author = {Seabra, Antony and Cavalcante, Claudio and Lifschitz, Sérgio},
  year = {2025},
  doi = {10.1007/978-3-031-96228-8_30},
  url = {https://doi.org/10.1007/978-3-031-96228-8_30},
  journal = {IFIP Advances in Information and Communication Technology},
  volume = {756 IFIPAICT},
  pages = {405 -- 418},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{patel_llm-based_2025,
  title = {{LLM},
  author = {Patel, Nikilkumar and Mouratidis, Haralambos and Zhi, Kenneth Ng Kai},
  year = {2025},
  doi = {10.1007/978-3-031-96235-6_26},
  url = {https://doi.org/10.1007/978-3-031-96235-6_26},
  journal = {IFIP Advances in Information and Communication Technology},
  volume = {758 IFIPAICT},
  pages = {360 -- 373},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{liang_research_2025,
  title = {Research on {Large},
  author = {Liang, Zhanfan and Liao, Haofan and Huang, Hai and Zhong, Xier},
  year = {2025},
  doi = {10.1109/AINIT65432.2025.11035618},
  url = {https://doi.org/10.1109/ainit65432.2025.11035618},
  pages = {2034 -- 2038},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{joy_ai-powered_2025,
  title = {{AI},
  author = {Joy, Manuel and Abraham, Alexander},
  year = {2025},
  doi = {10.1061/9780784486191.041},
  url = {https://doi.org/10.1061/9780784486191.041},
  pages = {458 -- 468},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jing_when_2025,
  title = {When {Large},
  author = {Jing, Zhi and Su, Yongye and Han, Yikun},
  year = {2025},
  doi = {10.1109/AIxMM62960.2025.00008},
  url = {https://doi.org/10.1109/aixmm62960.2025.00008},
  pages = {7 -- 13},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{de_paoli_generative_2025,
  title = {Generative {AI},
  author = {de Paoli, Federica and Berardelli, Silvia and Tudisco, Alessia and Blindu, Andrei Stefan and Parimbelli, Enea and Zucca, Susanna},
  year = {2025},
  doi = {10.1007/978-3-031-95841-0_24},
  url = {https://doi.org/10.1007/978-3-031-95841-0_24},
  journal = {Lecture Notes in Computer Science},
  volume = {15735 LNAI},
  pages = {127 -- 131},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{park_system_2025,
  title = {A {System},
  author = {Park, Jaeman and Song, Jeihyuck and Park, Jungmin and Choi, Kyoungmook and Mun, Junhouk and Song, Daesub and Park, Hyeryun and Cho, Minho Stephen},
  year = {2025},
  doi = {10.1007/978-981-96-6966-0_2},
  url = {https://doi.org/10.1007/978-981-96-6966-0_2},
  journal = {Communications in Computer and Information Science},
  volume = {2288 CCIS},
  pages = {16 -- 30},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{subbapurmath_rag_2025,
  title = {{RAG},
  author = {Subbapurmath, Nagbhushan R. and Kaur, Harkeerat},
  year = {2025},
  doi = {10.1007/978-981-96-8197-6_25},
  url = {https://doi.org/10.1007/978-981-96-8197-6_25},
  journal = {Lecture Notes in Computer Science},
  volume = {15835 LNAI},
  pages = {336 -- 348},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{yang_towards_2025,
  title = {Towards {Retrieval},
  author = {Yang, Dayu and Wang, Fuli},
  year = {2025},
  doi = {10.1007/978-981-96-8180-8_25},
  url = {https://doi.org/10.1007/978-981-96-8180-8_25},
  journal = {Lecture Notes in Computer Science},
  volume = {15872 LNAI},
  pages = {317 -- 330},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{hodges_using_2025,
  title = {Using {AI},
  author = {Hodges, Frank M. and Pullela, Sundareswar and Cohen, Guy and Mulgund, Akshay and Roach, Jared C. and Ramsey, Stephen A.},
  year = {2025},
  doi = {10.1007/978-3-031-95841-0_35},
  url = {https://doi.org/10.1007/978-3-031-95841-0_35},
  journal = {Lecture Notes in Computer Science},
  volume = {15735 LNAI},
  pages = {186 -- 191},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{nemeth_exploring_2025,
  title = {Exploring the use of retrieval-augmented generation models in higher education: {A},
  author = {Németh, Renáta and Tátrai, Annamária and Szabó, Miklós and Zaletnyik, Péter Tibor and Tamási, Árpád},
  year = {2025},
  doi = {10.1016/j.ssaho.2025.101751},
  url = {https://doi.org/10.1016/j.ssaho.2025.101751},
  journal = {Social Sciences and Humanities Open},
  volume = {12},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… intelligence tutor like ChatGPT, enhanced with retrieval-augmented generation, in a pilot … large language model. We found that retrieval-augmented generation reduces hallucinations …},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{zhu_hybrid_2025,
  title = {Hybrid {Feature},
  author = {Zhu, Yi and Liu, Xiangyang and Pang, Tianqi and Xiao, Xuncan and Zhang, Xiaofan and Fan, Chenyou},
  year = {2025},
  doi = {10.1109/ICASSP49660.2025.10888956},
  url = {https://doi.org/10.1109/icassp49660.2025.10888956},
  booktitle = {Proceedings - {ICASSP},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{chis_graphxx_2025,
  title = {{GRAPHxx},
  author = {Chis, Andrei and Ghiran, Ana Maria and Buchmann, Robert Andrei},
  year = {2025},
  doi = {10.1007/s10270-025-01297-y},
  url = {https://doi.org/10.1007/s10270-025-01297-y},
  journal = {Software and Systems Modeling},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{wulf_impact_2025,
  title = {The {Impact},
  author = {Wulf, Jochen and Meierhofer, Jürg},
  year = {2025},
  doi = {10.1016/j.procir.2025.03.071},
  url = {https://doi.org/10.1016/j.procir.2025.03.071},
  booktitle = {Procedia {CIRP},
  volume = {134},
  pages = {1089 -- 1094},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{liu_retrieval-augmented_2025,
  title = {A retrieval-augmented large language model hybrid framework for adaptive wireless resource allocation: design, simulation, and analysis},
  author = {Liu, Yu},
  year = {2025},
  doi = {10.1117/12.3073002},
  url = {https://doi.org/10.1117/12.3073002},
  booktitle = {Proceedings of {SPIE},
  volume = {13661},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{he_external_2025,
  title = {External {Retrievals},
  author = {He, Kai and Xu, Jiaxing and Lin, Qika and Wang, Wenqing and Gao, Zeyu and Wu, Jialun and Huang, Yucheng and Feng, Mengling},
  year = {2025},
  doi = {10.1109/TFUZZ.2025.3581205},
  url = {https://doi.org/10.1109/tfuzz.2025.3581205},
  journal = {IEEE Transactions on Fuzzy Systems},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{tseng_designing_2025,
  title = {Designing an {AI},
  author = {Tseng, Yuan Chi and Chen, Samuel and Mah, Kang Heng and Chen, Yuchang},
  year = {2025},
  doi = {10.1007/978-3-031-93736-1_17},
  url = {https://doi.org/10.1007/978-3-031-93736-1_17},
  journal = {Lecture Notes in Computer Science},
  volume = {15785 LNCS},
  pages = {261 -- 276},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{zhang_knowledge-enhanced_2025,
  title = {A {Knowledge},
  author = {Zhang, Chi and Yang, Hao and Liu, Xingyun and Wu, Rongrong and Zong, Hui and Wu, Erman and Zhou, Yi and Li, Jiakun and Shen, Bairong},
  year = {2025},
  doi = {10.2196/67201},
  url = {https://doi.org/10.2196/67201},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… We curated a tailored LLM framework incorporating RAG … RAG-based LLM framework using the RAGAs evaluation tool and found that RAG integration significantly boosted the factual …},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{li_enhancing_2025,
  title = {Enhancing {Pulmonary},
  author = {Li, Ronghao and Mao, Shuai and Zhu, Congmin and Yang, Yingliang and Tan, Chunting and Li, Li and Mu, Xiangdong and Liu, Honglei and Yang, Yuqing},
  year = {2025},
  doi = {10.2196/72638},
  url = {https://doi.org/10.2196/72638},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{ilagan_virtual_2025,
  title = {A {Virtual},
  author = {Ilagan, Joseph Benjamin and Yu, Wolverix Skyler and See, Samantha Mae and Rayco, Stephanie},
  year = {2025},
  doi = {10.1007/978-3-031-93415-5_15},
  url = {https://doi.org/10.1007/978-3-031-93415-5_15},
  journal = {Lecture Notes in Computer Science},
  volume = {15820 LNAI},
  pages = {253 -- 262},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{aboulela_exploring_2025,
  title = {Exploring {RAG},
  author = {AboulEla, Samar and Zabihitari, Paria and Ibrahim, Nourhan M. and Afshar, Majid and Kashef, Rasha F.},
  year = {2025},
  doi = {10.1109/SysCon64521.2025.11014810},
  url = {https://doi.org/10.1109/syscon64521.2025.11014810},
  journal = {2025 IEEE …},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Although computing complexity remains a restriction, this study shows that RAG topologies might not consistently enhance LLM reliability in practical situations. Index Terms—…},
  annote = {Cited by: 0},
}

@article{lotfi_rethinking_2025,
  title = {Rethinking {Strategic},
  author = {Lotfi, Ismail and Alabbasi, Nouf and Alhussein, Omar},
  year = {2025},
  doi = {10.1109/MIOT.2025.3576260},
  url = {https://doi.org/10.1109/miot.2025.3576260},
  journal = {IEEE Internet of Things Magazine},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{ye_intelligent_2025,
  title = {Intelligent {Tutoring},
  author = {Ye, Zhenhong},
  year = {2025},
  doi = {10.1109/ICCECE65250.2025.10984799},
  url = {https://doi.org/10.1109/iccece65250.2025.10984799},
  pages = {189 -- 194},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhang_cmedragbot_2025,
  title = {{CMedRAGBot},
  author = {Zhang, Dongfang and Du, Haoze and Wang, Xiaolei and Zhu, Mingdong and Pang, Xiaoxiao and Wei, Dongqing and Wang, Xianfang},
  year = {2025},
  doi = {10.1007/s12539-025-00715-5},
  url = {https://doi.org/10.1007/s12539-025-00715-5},
  journal = {Interdisciplinary Sciences - Computational Life Sciences},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{chen_predicting_2025,
  title = {Predicting 30-{Day},
  author = {Chen, Yinghao and Ruan, Shanq Jang and Chen, Peifu},
  year = {2025},
  doi = {10.2196/75052},
  url = {https://doi.org/10.2196/75052},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{liu_think_2025,
  title = {{THINK},
  author = {Liu, Jiaxin and Zhang, Yating and Wang, Deze and Li, Yiwei and Dong, Wei},
  year = {2025},
  doi = {10.1109/SANER64311.2025.00029},
  url = {https://doi.org/10.1109/saner64311.2025.00029},
  pages = {229 -- 240},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{hwang_efficient_2025,
  title = {Efficient {Information},
  author = {Hwang, Hyuncheon and Park, Jisu and Shon, Jin Gon},
  year = {2025},
  doi = {10.1007/978-981-96-5693-6_35},
  url = {https://doi.org/10.1007/978-981-96-5693-6_35},
  journal = {Lecture Notes in Electrical Engineering},
  volume = {1416 LNEE},
  pages = {227 -- 231},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sirisha_analytical_2025,
  title = {An {Analytical},
  author = {Sirisha, Uddagiri and Chanumolu, Kiran Kumar and Durgam, Revathi and Eswaraiah, Poluru and Nagamani, G. Muni},
  year = {2025},
  doi = {10.32604/cmc.2025.063721},
  url = {https://doi.org/10.32604/cmc.2025.063721},
  journal = {Computers, Materials and Continua},
  volume = {83},
  number = {3},
  pages = {4031 -- 4059},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{alam_towards_2025,
  title = {Towards {Interpretable},
  author = {Alam, Hasan Md Tusfiqur and Srivastav, Devansh and Kadir, Md Abdul and Sonntag, Daniel},
  year = {2025},
  doi = {10.1007/978-3-031-88714-7_18},
  url = {https://doi.org/10.1007/978-3-031-88714-7_18},
  journal = {Lecture Notes in Computer Science},
  volume = {15574 LNCS},
  pages = {201 -- 209},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wang_nas-based_2025,
  title = {A {NAS},
  author = {Wang, Chen and Guo, Tiezheng and Yang, Qingwen and Liu, Yanyi and Tang, Jiawei and Wen, Yingyou},
  year = {2025},
  doi = {10.32604/cmc.2025.063676},
  url = {https://doi.org/10.32604/cmc.2025.063676},
  journal = {Computers, Materials and Continua},
  volume = {83},
  number = {3},
  pages = {5561 -- 5574},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{varma_data_2025,
  title = {From {Data},
  author = {Varma, Sandeep and Shivam, Shivam and Natarajan, Sarun and Banerjee, Ankita and Roy, Sourodeep},
  year = {2025},
  doi = {10.1109/BigComp64353.2025.00016},
  url = {https://doi.org/10.1109/bigcomp64353.2025.00016},
  journal = {Proceedings of the IEEE International Conference on Big Data and Smart Computing, BIGCOMP},
  number = {2025},
  pages = {41 -- 48},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ateia_bioragent_2025,
  title = {{BioRAGent},
  author = {Ateia, Samy and Kruschwitz, Udo},
  year = {2025},
  doi = {10.1007/978-3-031-88720-8_1},
  url = {https://doi.org/10.1007/978-3-031-88720-8_1},
  journal = {Lecture Notes in Computer Science},
  volume = {15576 LNCS},
  pages = {1 -- 5},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{bao_mindwell_2025,
  title = {{MindWell},
  author = {Bao, Eliseo and Pérez, Anxo and Parapar, Javier},
  year = {2025},
  doi = {10.1007/978-3-031-88720-8_9},
  url = {https://doi.org/10.1007/978-3-031-88720-8_9},
  journal = {Lecture Notes in Computer Science},
  volume = {15576 LNCS},
  pages = {47 -- 52},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{abras_can_2025,
  title = {Can {Generative},
  author = {Abras, Jordan and Burnay, Corentin and Faulkner, Stéphane},
  year = {2025},
  doi = {10.1007/978-3-031-92471-2_1},
  url = {https://doi.org/10.1007/978-3-031-92471-2_1},
  journal = {Lecture Notes in Business Information Processing},
  volume = {548 LNBIP},
  pages = {3 -- 19},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{al-machot_building_2025,
  title = {Building {Trustworthy},
  author = {Al-Machot, Fadi and Horsch, Martin Thomas and Ullah, Habib},
  year = {2025},
  doi = {10.1007/978-3-031-89274-5_3},
  url = {https://doi.org/10.1007/978-3-031-89274-5_3},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1375 LNNS},
  pages = {25 -- 34},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{elyas_tailoring_2025,
  title = {Tailoring {Large},
  author = {Elyas, Odai A. and Al Hashim, Hassan W. and Williams, John R.},
  year = {2025},
  doi = {10.2118/224128-MS},
  url = {https://doi.org/10.2118/224128-ms},
  booktitle = {{SPE},
  volume = {2025-April},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{tan_llama-utp_2025,
  title = {{LLaMA},
  author = {Tan, Yutong and Wu, Bi and Cao, Jialei and Jiang, Bingying},
  year = {2025},
  doi = {10.1109/ACCESS.2025.3571502},
  url = {https://doi.org/10.1109/access.2025.3571502},
  journal = {IEEE Access},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{rubinstein_summarizing_2025,
  title = {Summarizing clinical evidence utilizing large language models for cancer treatments: a blinded comparative analysis},
  author = {Rubinstein, Samuel M. and Mohsin, Aleenah and Banerjee, Rahul and Ma, Will and Mishra, Sanjay Kumar and Kwok, Mary Lee and Yang, Peter C. and Warner, Jeremy Lyle and Cowan, Andrew John},
  year = {2025},
  doi = {10.3389/fdgth.2025.1569554},
  url = {https://doi.org/10.3389/fdgth.2025.1569554},
  journal = {Frontiers in Digital Health},
  volume = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{yan_multimodal_2025,
  title = {Multimodal {Medical},
  author = {Yan, Hanrui and Shao, Dan},
  year = {2025},
  doi = {10.12720/jait.16.4.568-581},
  url = {https://doi.org/10.12720/jait.16.4.568-581},
  journal = {Journal of Advances in Information Technology},
  volume = {16},
  number = {4},
  pages = {568 -- 581},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{rai_echo_2025,
  title = {Echo {Chamber},
  author = {Rai, Taranpreet and Tarrant, Georgina and Wells, Kevin D.},
  year = {2025},
  doi = {10.1117/12.3047380},
  url = {https://doi.org/10.1117/12.3047380},
  booktitle = {Progress in {Biomedical},
  volume = {13411},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{jervas_context-driven_2025,
  title = {Context-{Driven},
  author = {Jervas, Sandra and Jacob, Chinnu and Sundar, Sumod and Deepasree Varma, P.},
  year = {2025},
  doi = {10.1109/ETIS64005.2025.10960935},
  url = {https://doi.org/10.1109/etis64005.2025.10960935},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{hang_trumorgpt_2025,
  title = {{TrumorGPT},
  author = {Hang, Ching Nam and Yu, Peiduo and Tan, Chee Wei},
  year = {2025},
  doi = {10.1109/TAI.2025.3567369},
  url = {https://doi.org/10.1109/tai.2025.3567369},
  journal = {IEEE Transactions on Artificial Intelligence},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 9; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{kelly_effectiveness_2025,
  title = {The {Effectiveness},
  author = {Kelly, Anthony and Noctor, Eoin and Ryan, Laura and Van de Ven, Pepijn W.J.},
  year = {2025},
  doi = {10.2196/70131},
  url = {https://doi.org/10.2196/70131},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{li_use_2025,
  title = {Use of {Retrieval},
  author = {Li, Hai and Huang, Jingyi and Ji, Mengmeng and Yang, Yuyi and An, Ruopeng},
  year = {2025},
  doi = {10.2196/66098},
  url = {https://doi.org/10.2196/66098},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… By addressing the challenges of LLM inaccuracies and the high costs of traditional fact-checking methods, our RAG-enhanced approach improves the factual correctness of outputs. It …},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{iacob_are_2025,
  title = {Are {LLMs},
  author = {Iacob, Isabela and Cosmin Silaghi, Gheorghe},
  year = {2025},
  doi = {10.1007/978-981-96-0161-5_39},
  url = {https://doi.org/10.1007/978-981-96-0161-5_39},
  journal = {Smart Innovation, Systems and Technologies},
  volume = {426},
  pages = {441 -- 451},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{pampel_regaining_2025,
  title = {Regaining {Control},
  author = {Pampel, Barbara and Martin, Simon and Padó, Ulrike},
  year = {2025},
  doi = {10.5220/0013425500003932},
  url = {https://doi.org/10.5220/0013425500003932},
  booktitle = {International {Conference},
  volume = {2},
  pages = {371 -- 378},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{sarmiento_investigating_2025,
  title = {Investigating {Flavors},
  author = {Sarmiento, Christian and Lauría, Eitel J.M.},
  year = {2025},
  doi = {10.5220/0013468200003932},
  url = {https://doi.org/10.5220/0013468200003932},
  booktitle = {International {Conference},
  volume = {2},
  pages = {421 -- 428},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{kaczmarek_optimizing_2025,
  title = {Optimizing {Retrieval},
  author = {Kaczmarek, Jeremi I. and Pokrywka, Jakub and Biedalak, Krzysztof and Kurzyp, Grzegorz and Grzybowski, Łukasz},
  year = {2025},
  doi = {10.5220/0013477700003932},
  url = {https://doi.org/10.5220/0013477700003932},
  booktitle = {International {Conference},
  volume = {2},
  pages = {174 -- 186},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{hoffmann_text_2025,
  title = {From {Text},
  author = {Hoffmann, Michael Peter and Fillies, Jan and Peikert, Silvio and Paschke, Adrian},
  year = {2025},
  doi = {10.5220/0013215400003932},
  url = {https://doi.org/10.5220/0013215400003932},
  booktitle = {International {Conference},
  volume = {2},
  pages = {246 -- 256},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{berry_utilizing_2025,
  title = {Utilizing large language models for gastroenterology research: a conceptual framework},
  author = {Berry, Parul and Dhanakshirur, Rohan Raju and Khanna, Sahil},
  year = {2025},
  doi = {10.1177/17562848251328577},
  url = {https://doi.org/10.1177/17562848251328577},
  journal = {Therapeutic Advances in Gastroenterology},
  volume = {18},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{benavent_raging_2025,
  title = {{RAGing},
  author = {Benavent, Diego and Venerito, Vincenzo and Michelena, Xabier},
  year = {2025},
  doi = {10.1177/1759720X251331529},
  url = {https://doi.org/10.1177/1759720x251331529},
  journal = {Therapeutic Advances in Musculoskeletal Disease},
  volume = {17},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{temsah_authors_2025,
  title = {Authors’ {Reply},
  author = {Temsah, Mohamad Hani and Al-Eyadhy, Ayman A. and Jamal, Amr A. and Alhasan, Khalid A. and Malki, Khalid H.},
  year = {2025},
  doi = {10.2196/73698},
  url = {https://doi.org/10.2196/73698},
  journal = {JMIR Medical Education},
  volume = {11},
  note = {Type: Letter},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… In conclusion, while RAG augmented by HAT represents a potential advancement in reducing hallucinations, the … RAG-HAT: a hallucination-aware tuning pipeline for LLM in …},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{akilesh_multi-agent_2025,
  title = {Multi-{Agent},
  author = {Akilesh, S. and Sekar, Rajeev and Om Kumar, C. U. and Prakalya, D. and Suguna, M.},
  year = {2025},
  doi = {10.1109/SCEECS64059.2025.10940635},
  url = {https://doi.org/10.1109/sceecs64059.2025.10940635},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{bhiwgade_integrating_2025-1,
  title = {Integrating {Open},
  author = {Bhiwgade, Akash Wamanrao and Nagrale, Nilesh B. and Patil Bedekar, Pragati and Sheikh, Sayara Bano},
  year = {2025},
  doi = {10.1109/SCEECS64059.2025.10940324},
  url = {https://doi.org/10.1109/sceecs64059.2025.10940324},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zheng_knowledge-based_2025,
  title = {Knowledge-{Based},
  author = {Zheng, Shufeng and Li, Helin and Wang, Wenjie and Zhang, Fan and Zhao, Huadong},
  year = {2025},
  doi = {10.1109/CITSC64390.2025.00056},
  url = {https://doi.org/10.1109/citsc64390.2025.00056},
  pages = {272 -- 276},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{khan_rfsensinggpt_2025,
  title = {{RFSensingGPT},
  author = {Khan, Muhammad Zakir and Ge, Yao and Mollel, Michael S. and McCann, Julie A. and Abbasi, Qammer Hussain and Imran, Muhammad Ali},
  year = {2025},
  doi = {10.1109/TCCN.2025.3558069},
  url = {https://doi.org/10.1109/tccn.2025.3558069},
  journal = {IEEE Transactions on Cognitive Communications and Networking},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{yu_ba-gpt_2025,
  title = {{BA},
  author = {Yu, Chenyang and Mao, Zhaoyong and Wu, Yinglong and Tu, Qinhao and Shen, Junge},
  year = {2025},
  doi = {10.1007/978-981-96-3564-1_29},
  url = {https://doi.org/10.1007/978-981-96-3564-1_29},
  journal = {Lecture Notes in Electrical Engineering},
  volume = {1376 LNEE},
  pages = {300 -- 309},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{orlowski_towards_2025,
  title = {Towards {Personal},
  author = {Orlowski, Maximilian and Knauff, Emilia and Marquardt, Florian},
  year = {2025},
  doi = {10.5220/0013175600003890},
  url = {https://doi.org/10.5220/0013175600003890},
  booktitle = {International {Conference},
  volume = {3},
  pages = {695 -- 705},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{guttikonda_explainable_2025,
  title = {Explainable {AI},
  author = {Guttikonda, Devansh and Indran, Deepika and Narayanan, Lakshmi and Pasarad, Tanishka and Sandesh, Balasaraswati J.},
  year = {2025},
  doi = {10.5220/0013241300003890},
  url = {https://doi.org/10.5220/0013241300003890},
  booktitle = {International {Conference},
  volume = {3},
  pages = {948 -- 955},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{santos_using_2025,
  title = {Using {LLMs},
  author = {Santos, Diogo and Gonçalves, Filipa and Reis, Gonçalo and Santos, Miguel and Saraiva, Miguel and Durães, Pedro F. and Maximiano, Marisa and Gomes, Ricardo P. and Távora, Vitor N. and Remedios, Orlando},
  year = {2025},
  doi = {10.1016/j.procs.2025.02.126},
  url = {https://doi.org/10.1016/j.procs.2025.02.126},
  booktitle = {Procedia {Computer},
  volume = {256},
  pages = {319 -- 326},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@inproceedings{awadid_ritsa_2025,
  title = {{RITSA},
  author = {Awadid, Afef and Meyer-Vitali, André Philippe and Vereno, Dominik and Gagnant, Maxence},
  year = {2025},
  doi = {10.5220/0013443300003896},
  url = {https://doi.org/10.5220/0013443300003896},
  booktitle = {International {Conference},
  volume = {1},
  pages = {466 -- 473},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{reza_crix_2025,
  title = {{CriX},
  author = {Reza, Muhammad Ashar and Bisaria, Aaditya and Advaitha, S. and Ponnekanti, Alekhya and Arya, Arti},
  year = {2025},
  doi = {10.5220/0013316200003890},
  url = {https://doi.org/10.5220/0013316200003890},
  booktitle = {International {Conference},
  volume = {2},
  pages = {714 -- 725},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{lorusso_gollum_2025,
  title = {{GOLLUM},
  author = {Lorusso, Roberto and Maci, Antonio and Coscia, Antonio},
  year = {2025},
  doi = {10.5220/0013221900003890},
  url = {https://doi.org/10.5220/0013221900003890},
  booktitle = {International {Conference},
  volume = {1},
  pages = {489 -- 496},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{maruyama_retrieval-augmented_2025,
  title = {Retrieval-augmented generation enhances large language model performance on the {Japanese},
  author = {Maruyama, Juntaro and Maki, Satoshi and Furuya, Takeo and Nagashima, Yuki and Kitagawa, Kyota and Toki, Yasunori and Iwata, Shuhei and Yazaki, Megumi and Kitamura, Takaki and Gushiken, Sho and Noguchi, Yuji and Miura, Masataka and Inoue, Masahiro and Shiga, Yasuhiro and Inage, Kazuhidee and Orita, Sumihisa and Ohtori, Seiji},
  year = {2025},
  doi = {10.1016/j.jos.2025.03.003},
  url = {https://doi.org/10.1016/j.jos.2025.03.003},
  journal = {Journal of Orthopaedic Science},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… The integration of Retrieval-Augmented Generation (RAG) has been proposed to improve these models by reducing hallucinations and enhancing domain-specific information access. …},
  annote = {Cited by: 0},
}

@article{holvoet_approach_2025,
  title = {An {Approach},
  author = {Holvoet, Laura and van Bekkum, Michael A. and de Vries, Aijse},
  year = {2025},
  doi = {10.1007/978-3-031-86489-6_23},
  url = {https://doi.org/10.1007/978-3-031-86489-6_23},
  journal = {Lecture Notes in Mechanical Engineering},
  pages = {224 -- 233},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{barghi_qrag_2025,
  title = {{QRAG},
  author = {Barghi, Alexandria},
  year = {2025},
  doi = {10.1109/CCWC62904.2025.10903812},
  url = {https://doi.org/10.1109/ccwc62904.2025.10903812},
  pages = {561 -- 568},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{ecker_trust-informed_2025,
  title = {Trust-{Informed},
  author = {Ecker, James E. and Danette Allen, B.},
  year = {2025},
  doi = {10.2514/6.2025-1916},
  url = {https://doi.org/10.2514/6.2025-1916},
  journal = {AIAA SciTech Forum},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… of hallucinating non-existent information. While Retrieval Augmented Generation (RAG) … work seeks to explore methods to engender a LLM with an intrinsic capability to evaluate an …},
  annote = {Cited by: 0},
}

@article{hou_improving_2025,
  title = {Improving {Dietary},
  author = {Hou, Yu and Bishop, Jeffrey R. and Liu, Hongfang and Zhang, Rui},
  year = {2025},
  doi = {10.2196/67677},
  url = {https://doi.org/10.2196/67677},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{bassiouny_uji-butler_2025,
  title = {{UJI},
  author = {Bassiouny, Abdelrhman M. and Elsayed, Ahmed Hassan and Falomir, Zoe and del Pobil, Angel P.},
  year = {2025},
  doi = {10.1007/s12369-025-01234-5},
  url = {https://doi.org/10.1007/s12369-025-01234-5},
  journal = {International Journal of Social Robotics},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@inproceedings{zhou_ihillm-rag_2025-1,
  title = {{IHILLM},
  author = {Zhou, Zibo and Yang, Yi and Ren, Tianling},
  year = {2025},
  doi = {10.1117/12.3056789},
  url = {https://doi.org/10.1117/12.3056789},
  booktitle = {Proceedings of {SPIE},
  volume = {13542},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{lu_decoding_2025,
  title = {Decoding urban policies: {NLP},
  author = {Lu, Zhengyang and Wang, Weifan and Guo, Tianhao and Li, Yifan and Wang, Feng},
  year = {2025},
  doi = {10.1177/23998083251321981},
  url = {https://doi.org/10.1177/23998083251321981},
  journal = {Environment and Planning B: Urban Analytics and City Science},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kliestik_enterprise_2024,
  title = {Enterprise generative artificial intelligence technologies, internet of things and blockchain-based fintech management, and digital twin industrial metaverse in the cognitive algorithmic economy},
  author = {Kliestik, Tomas and Dragomir, Robert and Bǎluţǎ, Aurelian Virgil and Grecu, Iulia and Durana, Pavol and Karabolevski, Oana Ludmila and Kráľ, Pavol and Balica, Raluca Stefania and Suler, Petr and Busu, Oprea Valentin and Bugaj, Martin and Voinea, Dan Valeriu and Vrbka, Jaromir and Cocosatu, Mădălina and Grupac, Marian and Pera, Aurel and Gajdosikova, Dominika},
  year = {2024},
  doi = {10.24136/oc.3109},
  url = {https://doi.org/10.24136/oc.3109},
  journal = {Oeconomia Copernicana},
  volume = {15},
  number = {4},
  pages = {1183 -- 1221},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 11; All Open Access; Gold Open Access},
}

@article{zhao_embodied_2024,
  title = {Embodied {AI},
  author = {Zhao, Zhuoran and Yin, Zhizhuo and Sun, Jia and Hui, Pan},
  year = {2024},
  doi = {10.1145/3680533.3697070},
  url = {https://doi.org/10.1145/3680533.3697070},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{rajeshkumar_dermai_2024,
  title = {{DermAI},
  author = {Rajeshkumar, Pradeep and Kharche, Shubhangi Pravin and Poojari, Prithvi and Utekar, Sachet and Saini, Sahil and Bdwai, Samrridhi},
  year = {2024},
  doi = {10.52549/ijeei.v12i4.5806},
  url = {https://doi.org/10.52549/ijeei.v12i4.5806},
  journal = {Indonesian Journal of Electrical Engineering and Informatics},
  volume = {12},
  number = {4},
  pages = {1032 -- 1039},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{xu_evaluation_2024,
  title = {Evaluation of the integration of retrieval-augmented generation in large language model for breast cancer nursing care responses},
  author = {Xu, Ruiyu and Hong, Ying and Zhang, Feifei and Xu, Hongmei},
  year = {2024},
  doi = {10.1038/s41598-024-81052-3},
  url = {https://doi.org/10.1038/s41598-024-81052-3},
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… , inherent hallucinations can lead to inaccurate responses. … revealed that RAG technology could improve LLM performance … These findings provide a theoretical basis for applying RAG …},
  annote = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{bora_systematic_2024,
  title = {Systematic {Analysis},
  author = {Bora, Arunabh and Cuayáhuitl, Heriberto},
  year = {2024},
  doi = {10.3390/make6040116},
  url = {https://doi.org/10.3390/make6040116},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {6},
  number = {4},
  pages = {2355 -- 2374},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 16; All Open Access; Gold Open Access},
}

@article{lee_enhancing_2024,
  title = {Enhancing {Large},
  author = {Lee, Jaedong and Cha, Hyosoung and Hwangbo, Yul and Cheon, Wonjoong},
  year = {2024},
  doi = {10.3390/jpm14121131},
  url = {https://doi.org/10.3390/jpm14121131},
  journal = {Journal of Personalized Medicine},
  volume = {14},
  number = {12},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 7; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{zhang_generative_2024,
  title = {Generative {AI},
  author = {Zhang, Peng and Shi, Jiayu and Kamel Boulos, Maged N.},
  year = {2024},
  doi = {10.3390/fi16120462},
  url = {https://doi.org/10.3390/fi16120462},
  journal = {Future Internet},
  volume = {16},
  number = {12},
  note = {Type: Review},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… LLM hallucinations, to mitigate the high costs and time associated with a completely manual review of LLM… NVIDIA’s ‘Chat with RTX’ is a free tool for building a custom LLM using a RAG …},
  annote = {Cited by: 11; All Open Access; Gold Open Access},
}

@article{earley_powerful_2024,
  title = {Powerful tools for personalisation: {Using},
  author = {Earley, Seth and Mehta, Sanjay},
  year = {2024},
  doi = {10.69554/NMCE9908},
  url = {https://doi.org/10.69554/nmce9908},
  journal = {Applied Marketing Analytics},
  volume = {10},
  number = {3},
  pages = {271 -- 288},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{chen_enhanced_2024,
  title = {An {Enhanced},
  author = {Chen, Qi and Zhou, Weifeng and Cheng, Jian and Yang, Ji},
  year = {2024},
  doi = {10.3390/app142411529},
  url = {https://doi.org/10.3390/app142411529},
  journal = {Applied Sciences (Switzerland)},
  volume = {14},
  number = {24},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… for LLM processing was developed, named the BM-RAGAM (BM25 retrieval-augmented generation … This study addressed the problem of hallucinations and the lack of interpretability in …},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{craig_lmrac_2024,
  title = {{LmRaC},
  author = {Craig, Douglas B. and Drəghici, Sorin},
  year = {2024},
  doi = {10.1093/bioinformatics/btae679},
  url = {https://doi.org/10.1093/bioinformatics/btae679},
  journal = {Bioinformatics},
  volume = {40},
  number = {12},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… LLM for reasoning tasks. In addition to making dynamic and quantitative data available to the LLM, RAG … Answers are drawn solely from this RAG with citations to the paragraph level, …},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@article{yao_adaptive_2024,
  title = {Adaptive {Control},
  author = {Yao, Chengyuan and Fujita, Satoshi},
  year = {2024},
  doi = {10.3390/electronics13234643},
  url = {https://doi.org/10.3390/electronics13234643},
  journal = {Electronics (Switzerland)},
  volume = {13},
  number = {23},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 5; All Open Access; Gold Open Access},
}

@article{paneru_leveraging_2024,
  title = {Leveraging {AI},
  author = {Paneru, Biplov and Thapa, Bipul and Paneru, Bishwash},
  year = {2024},
  doi = {10.1016/j.teler.2024.100181},
  url = {https://doi.org/10.1016/j.teler.2024.100181},
  journal = {Telematics and Informatics Reports},
  volume = {16},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4; All Open Access; Gold Open Access},
}

@article{toro_dynamic_2024,
  title = {Dynamic {Retrieval},
  author = {Toro, Sabrina and Anagnostopoulos, Anna V. and Bello, Susan M. and Blumberg, Kai Lewis and Cameron, Rhiannon and Carmody, Leigh C. and Diehl, Alexander D. and Dooley, Damion M. and Duncan, William D. and Fey, Petra and Gaudet, Pascale and Harris, Nomi L. and Joachimiak, Marcin Pawel and Kiani, Leila and Lubiana, Tiago and Muñoz-Torres, Mónica C. and O'Neil, Shawn T. and Osumi-Sutherland, David J. and Puig-Barbe, Aleix and Reese, Justin T. and Reiser, Leonore and Robb, Sofia M.C. and Ruemping, Troy and Seager, James and Sid, Eric and Stefancsik, R. and Weber, Magalie and Wood, Valerie and Haendel, Melissa Anne and Mungall, Christopher John},
  year = {2024},
  doi = {10.1186/s13326-024-00320-3},
  url = {https://doi.org/10.1186/s13326-024-00320-3},
  journal = {Journal of Biomedical Semantics},
  volume = {15},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 13; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{gue_evaluating_2024,
  title = {Evaluating the {OpenAI},
  author = {Gue, Celeste Ci Ying and Rahim, Noorul Dharajath Abdul and Rojas‑Carabali, William and Agrawal, Rupesh V. and Rk, Palvannan and Abisheganaden, J. Arputhan and Yip, Wanfen},
  year = {2024},
  doi = {10.1186/s13643-024-02523-2},
  url = {https://doi.org/10.1186/s13643-024-02523-2},
  journal = {Systematic Reviews},
  volume = {13},
  number = {1},
  note = {Type: Letter},
  keywords = {source: Scopus},
  annote = {Cited by: 7; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{gilbert_augmented_2024,
  title = {Augmented non-hallucinating large language models as medical information curators},
  author = {Gilbert, Stephen Henry and Kather, Jakob Nikolas and Hogan, Aidan},
  year = {2024},
  doi = {10.1038/s41746-024-01081-0},
  url = {https://doi.org/10.1038/s41746-024-01081-0},
  journal = {npj Digital Medicine},
  volume = {7},
  number = {1},
  note = {Type: Note},
  keywords = {source: Scopus},
  annote = {Cited by: 30; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{kresevic_optimization_2024,
  title = {Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework},
  author = {Kresevic, Simone and Giuffrè, Mauro and Ajčević, Miloš and Accardo, P. Agostino and Crocè, Lory S. and Shung, Dennis Legen},
  year = {2024},
  doi = {10.1038/s41746-024-01091-y},
  url = {https://doi.org/10.1038/s41746-024-01091-y},
  journal = {npj Digital Medicine},
  volume = {7},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… novel LLM framework integrating clinical guidelines with RAG, … significantly outperforms the baseline LLM model in producing … When inaccurate outputs were reviewed for hallucinations, …},
  annote = {Cited by: 92; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{soh_data_2024,
  title = {Data science solutions on azure: {The},
  author = {Soh, Julian and Singh, Priyanshi},
  year = {2024},
  doi = {10.1007/979-8-8688-0914-9},
  url = {https://doi.org/10.1007/979-8-8688-0914-9},
  pages = {1 -- 289},
  note = {Type: Book},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{moric_integrating_2024,
  title = {Integrating a {Virtual},
  author = {Morić, Zlatan and Mršić, Leo and Filjak, Mario and Đambić, Goran},
  year = {2024},
  doi = {10.3390/app142210748},
  url = {https://doi.org/10.3390/app142210748},
  journal = {Applied Sciences (Switzerland)},
  volume = {14},
  number = {22},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{song_travelrag_2024,
  title = {{TravelRAG},
  author = {Song, Sihan and Yang, Chuncheng and Xu, Li and Shang, Haibin and Li, Zhuo and Chang, Yinghui},
  year = {2024},
  doi = {10.3390/ijgi13110414},
  url = {https://doi.org/10.3390/ijgi13110414},
  journal = {ISPRS International Journal of Geo-Information},
  volume = {13},
  number = {11},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 6; All Open Access; Gold Open Access},
}

@article{ge_development_2024,
  title = {Development of a liver disease-specific large language model chat interface using retrieval-augmented generation},
  author = {Ge, Jin and Sun, Steve and Owens, Joseph and Galvez, Victor and Gologorskaya, Oksana and Lai, Jennifer Cindy and Pletcher, Mark James and Lai, Ki},
  year = {2024},
  doi = {10.1097/HEP.0000000000000834},
  url = {https://doi.org/10.1097/hep.0000000000000834},
  journal = {Hepatology},
  volume = {80},
  number = {5},
  pages = {1158 -- 1168},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… the LLM by layering it on top of the LLM information retrieval and output processes. The theoretical advantages of RAG … ” for the LLM, and (2) Decreasing hallucinations by limiting the …},
  annote = {Cited by: 68},
}

@article{schulz_autorag_2024,
  title = {{AutoRAG},
  author = {Schulz, Tim and Luttermann, Malte and Möller, Ralf},
  year = {2024},
  doi = {10.1007/s13218-024-00850-z},
  url = {https://doi.org/10.1007/s13218-024-00850-z},
  journal = {KI - Kunstliche Intelligenz},
  volume = {38},
  number = {3},
  pages = {203 -- 217},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Hybrid Gold Open Access},
}

@article{le_anspre_2024,
  title = {{ANSPRE},
  author = {Le, Nguyen Khang and Nguyen, Dieu Hien and Nguyen, Minh Le},
  year = {2024},
  doi = {10.3233/FAIA240778},
  url = {https://doi.org/10.3233/faia240778},
  journal = {Frontiers in Artificial Intelligence and Applications},
  volume = {392},
  pages = {2500 -- 2507},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{chen_ontology_2024,
  title = {Ontology {Text},
  author = {Chen, Jieying and Dong, Hang and Chen, Jiaoyan and Horrocks, Ian},
  year = {2024},
  doi = {10.3233/FAIA240639},
  url = {https://doi.org/10.3233/faia240639},
  journal = {Frontiers in Artificial Intelligence and Applications},
  volume = {392},
  pages = {1389 -- 1396},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{lammert_expert-guided_2024,
  title = {Expert-{Guided},
  author = {Lammert, Jacqueline and Dreyer, Tobias F. and Mathes, Sonja and Kuligin, Leonid and Borm, Kai Joachim and Schatz, Ulrich A. and Kiechle, Marion Brigitta and Lörsch, Alisa Martina and Jung, Johannes J. and Lange, Sebastian and Pfarr, N. and Durner, Anna and Schwamborn, Kristina and Winter, Christof and Ferber, Dyke and Kather, Jakob Nikolas and Mogler, Carolin and Illert, Anna Lena and Tschochohei, Maximilian},
  year = {2024},
  doi = {10.1200/PO-24-00478},
  url = {https://doi.org/10.1200/po-24-00478},
  journal = {JCO Precision Oncology},
  volume = {8},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 14},
}

@article{iaroshev_evaluating_2024,
  title = {Evaluating {Retrieval},
  author = {Iaroshev, Ivan and Pillai, Ramalingam and Vaglietti, Leandro and Hanne, Thomas},
  year = {2024},
  doi = {10.3390/app14209318},
  url = {https://doi.org/10.3390/app14209318},
  journal = {Applied Sciences (Switzerland)},
  volume = {14},
  number = {20},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… , a selection of non-peer-reviewed papers with substantial citations and reputable authors was also integrated, acknowledging the rapid advancements in LLM and RAG research. …},
  annote = {Cited by: 8; All Open Access; Gold Open Access},
}

@article{han_automating_2024,
  title = {Automating {Systematic},
  author = {Han, Binglan and Sušnjak, Teo and Mathrani, Anuradha},
  year = {2024},
  doi = {10.3390/app14199103},
  url = {https://doi.org/10.3390/app14199103},
  journal = {Applied Sciences (Switzerland)},
  volume = {14},
  number = {19},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 18; All Open Access; Gold Open Access},
}

@article{wysocka_large_2024,
  title = {Large {Language},
  author = {Wysocka, Magdalena and Wysocki, Oskar and Delmas, Maxime and Mutel, Vincent and Freitas, André},
  year = {2024},
  doi = {10.1016/j.jbi.2024.104724},
  url = {https://doi.org/10.1016/j.jbi.2024.104724},
  journal = {Journal of Biomedical Informatics},
  volume = {158},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Hybrid Gold Open Access},
}

@article{arun_chatgpt_2024,
  title = {{ChatGPT},
  author = {Arun, Gautham and Perumal, Vivek and Urias, Francis Paul John Bato and Ler, Yan En and Tan, Bryan Wen Tao and Vallabhajosyula, Ranganath and Tan, Emmanuel and Ng, Olivia X.W. and Ng, Kian Bee and Mogali, Sreenivasulu Reddy},
  year = {2024},
  doi = {10.1002/ase.2502},
  url = {https://doi.org/10.1002/ase.2502},
  journal = {Anatomical Sciences Education},
  volume = {17},
  number = {7},
  pages = {1396 -- 1405},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 21},
}

@article{guo_knowledgenavigator_2024,
  title = {{KnowledgeNavigator},
  author = {Guo, Tiezheng and Yang, Qingwen and Wang, Chen and Liu, Yanyi and Li, Pan and Tang, Jiawei and Li, Dapeng and Wen, Yingyou},
  year = {2024},
  doi = {10.1007/s40747-024-01527-8},
  url = {https://doi.org/10.1007/s40747-024-01527-8},
  journal = {Complex and Intelligent Systems},
  volume = {10},
  number = {5},
  pages = {7063 -- 7076},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 20; All Open Access; Gold Open Access},
}

@article{ascorbe_automatic_2024,
  title = {Automatic and {Manual},
  author = {Ascorbe, Pablo and Campos, María Soledad and Dominguez, Cesar and Heras, Jónathan and Pérez, Magdalena and Terroba-Reinares, Ana Rosa},
  year = {2024},
  doi = {10.26342/2024-73-11},
  url = {https://doi.org/10.26342/2024-73-11},
  journal = {Procesamiento del Lenguaje Natural},
  number = {73},
  pages = {151 -- 164},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{soman_biomedical_2024,
  title = {Biomedical knowledge graph-optimized prompt generation for large language models},
  author = {Soman, Karthik and Rose, Peter W. W. and Morris, John H. and Akbas, Rabia E. and Smith, Brett and Peetoom, Braian and Villouta-Reyes, Catalina and Cerono, Gabriel and Shi, Yongmei and Rizk-Jackson, Angela M. and Israni, Sharat S. and Nelson, Charlotte A. and Huang, Sui and Baranzini, Sergio Enrique},
  year = {2024},
  doi = {10.1093/bioinformatics/btae560},
  url = {https://doi.org/10.1093/bioinformatics/btae560},
  journal = {Bioinformatics},
  volume = {40},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 25; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{mansurova_qa-rag_2024,
  title = {{QA},
  author = {Mansurova, Aigerim and Mansurova, Aiganym and Nugumanova, Aliya B.},
  year = {2024},
  doi = {10.3390/bdcc8090115},
  url = {https://doi.org/10.3390/bdcc8090115},
  journal = {Big Data and Cognitive Computing},
  volume = {8},
  number = {9},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… This makes it harder for users to trust and verify LLM-generated … QA-RAG for constructing question-answering systems empowered by LLMs to handle the occurrence of hallucination. …},
  annote = {Cited by: 14; All Open Access; Gold Open Access},
}

@article{byun_design_2024,
  title = {Design and {Implementation},
  author = {Byun, Jaeyeon and Kim, Bokyeong and Cha, Kyung-ae and Lee, Eunhyung},
  year = {2024},
  doi = {10.3390/app14177995},
  url = {https://doi.org/10.3390/app14177995},
  journal = {Applied Sciences (Switzerland)},
  volume = {14},
  number = {17},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… A well-established semantic space is required for employing RAG to compensate for LLM hallucinations and ensure that appropriate and accurate answers are retrieved by the LLM …},
  annote = {Cited by: 5; All Open Access; Gold Open Access},
}

@article{alonso_medexpqa_2024,
  title = {{MedExpQA},
  author = {Alonso, Iñigo and Oronoz, Maite and Agerri, Rodrigo},
  year = {2024},
  doi = {10.1016/j.artmed.2024.102938},
  url = {https://doi.org/10.1016/j.artmed.2024.102938},
  journal = {Artificial Intelligence in Medicine},
  volume = {155},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 15; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access},
}

@article{li_refai_2024,
  title = {{RefAI},
  author = {Li, Yiming and Zhao, Jeff and Li, Manqi and Dang, Yifang and Yu, Evan and Li, Jianfu and Sun, Zenan and Hussein, Usama Khamis and Wen, Jianguo and Abdelhameed, Ahmed M. and Mai, Junhua and Li, Shenduo and Yu, Yue and Hu, Xinyue and Yang, Daowei and Feng, Jingna and Li, Zehan and He, Jianping and Tao, Wei and Duan, Tiehang and Lou, Yanyan and Li, Fang and Tao, Cui},
  year = {2024},
  doi = {10.1093/jamia/ocae129},
  url = {https://doi.org/10.1093/jamia/ocae129},
  journal = {Journal of the American Medical Informatics Association},
  volume = {31},
  number = {9},
  pages = {2030 -- 2039},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  annote = {Cited by: 23; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{landschaft_implementation_2024,
  title = {Implementation and evaluation of an additional {GPT},
  author = {Landschaft, Assaf and Antweiler, Dario and Mackay, Sina and Kugler, Sabine and Rüping, Stefan J. and Wrobel, Stefan and Hoeres, Timm and Allende-Cid, Héctor},
  year = {2024},
  doi = {10.1016/j.ijmedinf.2024.105531},
  url = {https://doi.org/10.1016/j.ijmedinf.2024.105531},
  journal = {International Journal of Medical Informatics},
  volume = {189},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 15; All Open Access; Gold Open Access},
}

@article{giuffre_optimizing_2024,
  title = {Optimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes},
  author = {Giuffrè, Mauro and Kresevic, Simone and Pugliese, Nicola and You, Kisung and Shung, Dennis Legen},
  year = {2024},
  doi = {10.1111/liv.15974},
  url = {https://doi.org/10.1111/liv.15974},
  journal = {Liver International},
  volume = {44},
  number = {9},
  pages = {2114 -- 2124},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 28; All Open Access; Hybrid Gold Open Access},
}

@article{alkhalaf_applying_2024,
  title = {Applying generative {AI},
  author = {Alkhalaf, Mohammad and Yu, Ping and Yin, Mengyang and Deng, Chao},
  year = {2024},
  doi = {10.1016/j.jbi.2024.104662},
  url = {https://doi.org/10.1016/j.jbi.2024.104662},
  journal = {Journal of Biomedical Informatics},
  volume = {156},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… their own and in combination with retrieval augmented generation (RAG), for the automating … RAG approach improved the model performance and mitigated the hallucination problem. …},
  annote = {Cited by: 45; All Open Access; Hybrid Gold Open Access},
}

@article{church_emerging_2024,
  title = {Emerging trends: {A},
  author = {Church, Kenneth Ward and Sun, Jiameng and Yue, Richard and Vickers, Peter and Saba, Walid S. and Chandrasekar, Raman},
  year = {2024},
  doi = {10.1017/S1351324924000044},
  url = {https://doi.org/10.1017/s1351324924000044},
  journal = {Natural Language Engineering},
  volume = {30},
  number = {4},
  pages = {870 -- 881},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 7; All Open Access; Hybrid Gold Open Access},
}

@article{jearanaitanakij_thai_2024,
  title = {Thai {Question},
  author = {Jearanaitanakij, Kietikul and Srithongdee, Chananchida and Ketkham, Sirinoot and Ardsana, Onwanya and Kullawan, Tiwat and Yongpiyakul, Chankit},
  year = {2024},
  doi = {10.37936/ecti-cit.2024183.256043},
  url = {https://doi.org/10.37936/ecti-cit.2024183.256043},
  journal = {ECTI Transactions on Computer and Information Technology},
  volume = {18},
  number = {3},
  pages = {406 -- 416},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3; All Open Access; Gold Open Access},
}

@article{alghamdi_towards_2024,
  title = {Towards {Reliable},
  author = {Alghamdi, Hanan M. and Mostafa, Abeer},
  year = {2024},
  doi = {10.3390/info15070371},
  url = {https://doi.org/10.3390/info15070371},
  journal = {Information (Switzerland)},
  volume = {15},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 6; All Open Access; Gold Open Access},
}

@article{suresh_towards_2024,
  title = {Towards a {RAG},
  author = {Suresh, Karthik and Kackar, Neeltje and Schleck, Luke and Fanelli, Cristiano},
  year = {2024},
  doi = {10.1088/1748-0221/19/07/C07006},
  url = {https://doi.org/10.1088/1748-0221/19/07/c07006},
  journal = {Journal of Instrumentation},
  volume = {19},
  number = {7},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Hybrid Gold Open Access},
}

@article{bhattacharya_strategies_2024,
  title = {Strategies to mitigate hallucinations in large language models},
  author = {Bhattacharya, Ranjeeta},
  year = {2024},
  doi = {10.69554/nxxb8234},
  url = {https://doi.org/10.69554/nxxb8234},
  journal = {Applied Marketing Analytics},
  volume = {10},
  number = {1},
  pages = {62 -- 67},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Human involvement, in addition to grounding the LLM with external data (RAG), will allow for the verification of responses against external data sources, ensuring that generated content …},
  annote = {Cited by: 1},
}

@article{matsumoto_kragen_2024,
  title = {{KRAGEN},
  author = {Matsumoto, Nicholas and Moran, Jay and Choi, Hyunjun and Hernandez, Miguel E. and Venkatesan, Mythreye and Wang, Paul Zhiping and Moore, Jason H.},
  year = {2024},
  doi = {10.1093/bioinformatics/btae353},
  url = {https://doi.org/10.1093/bioinformatics/btae353},
  journal = {Bioinformatics},
  volume = {40},
  number = {6},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… relevant knowledge through the RAG framework, which limits the hallucinations, and finally, … , and feeding the prompt to the LLM. The LLM then uses the RAG system to retrieve relevant …},
  annote = {Cited by: 40; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{murugan_empowering_2024,
  title = {Empowering personalized pharmacogenomics with generative {AI},
  author = {Murugan, Mullai and Yuan, Bo and Venner, Eric and Ballantyne, Christie Mitchell and Robinson, Katherine M. and Coons, J. C. and Wang, Liwen and Empey, Philip E. and Gibbs, Richard A.},
  year = {2024},
  doi = {10.1093/jamia/ocae039},
  url = {https://doi.org/10.1093/jamia/ocae039},
  journal = {Journal of the American Medical Informatics Association},
  volume = {31},
  number = {6},
  pages = {1356 -- 1366},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 23; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{dernbach_glam_2024,
  title = {{GLaM},
  author = {Dernbach, Stefan and Agarwal, Khushbu and Zuniga, Alejandro Michel and Henry, Michael and Choudhury, Sutanay},
  year = {2024},
  doi = {10.1609/aaaiss.v3i1.31186},
  url = {https://doi.org/10.1609/aaaiss.v3i1.31186},
  volume = {3},
  number = {1},
  pages = {82 -- 89},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 10; All Open Access; Gold Open Access},
}

@article{toukmaji_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Toukmaji, Christopher and Tee, Allison},
  year = {2024},
  doi = {10.1609/aaaiss.v3i1.31210},
  url = {https://doi.org/10.1609/aaaiss.v3i1.31210},
  volume = {3},
  number = {1},
  pages = {273 -- 278},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 9; All Open Access; Gold Open Access},
}

@article{sharma_reliable_2024,
  title = {A reliable knowledge processing framework for combustion science using foundation models},
  author = {Sharma, Vansh and Raman, Venkat},
  year = {2024},
  doi = {10.1016/j.egyai.2024.100365},
  url = {https://doi.org/10.1016/j.egyai.2024.100365},
  journal = {Energy and AI},
  volume = {16},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 10; All Open Access; Gold Open Access},
}

@article{wu_framework_2024,
  title = {A framework enabling {LLMs},
  author = {Wu, Leihong and Xu, Joshua and Thakkar, Shraddha and Gray, Magnus and Qu, Yanyan and Li, Dongying and Tong, Weida},
  year = {2024},
  doi = {10.1016/j.yrtph.2024.105613},
  url = {https://doi.org/10.1016/j.yrtph.2024.105613},
  journal = {Regulatory Toxicology and Pharmacology},
  volume = {149},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 9; All Open Access; Hybrid Gold Open Access},
}

@inproceedings{chen_benchmarking_2024,
  title = {Benchmarking {Large},
  author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  year = {2024},
  doi = {10.1609/aaai.v38i16.29728},
  url = {https://doi.org/10.1609/aaai.v38i16.29728},
  booktitle = {Proceedings of the {AAAI},
  volume = {38},
  pages = {17754 -- 17762},
  note = {Issue: 16
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 255; All Open Access; Gold Open Access},
}

@inproceedings{odede_jaybot_2024,
  title = {{JayBot},
  author = {Odede, Julius and Frommholz, Ingo},
  year = {2024},
  doi = {10.1145/3627508.3638293},
  url = {https://doi.org/10.1145/3627508.3638293},
  pages = {391 -- 395},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 12},
}

@inproceedings{volker_chat_2024,
  title = {From {Chat},
  author = {Völker, Tom and Pfister, Jan and Koopmann, Tobias and Hotho, Andreas},
  year = {2024},
  doi = {10.1145/3627508.3638298},
  url = {https://doi.org/10.1145/3627508.3638298},
  pages = {386 -- 390},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{seas_commentary_2024,
  title = {Commentary on “{Use},
  author = {Seas, Andreas and Abd-El-Barr, Muhammad M.},
  year = {2024},
  doi = {10.14245/ns.2448248.124},
  url = {https://doi.org/10.14245/ns.2448248.124},
  journal = {Neurospine},
  volume = {21},
  number = {1},
  pages = {159 -- 161},
  note = {Type: Note},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{miao_integrating_2024,
  title = {Integrating {Retrieval},
  author = {Miao, Jing and Thongprayoon, Charat and Suppadungsuk, Supawadee and García-Valencia, Oscar Alejandro and Cheungpasitporn, Wisit},
  year = {2024},
  doi = {10.3390/medicina60030445},
  url = {https://doi.org/10.3390/medicina60030445},
  journal = {Medicina (Lithuania)},
  volume = {60},
  number = {3},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 78; All Open Access; Gold Open Access; Green Open Access},
}

@article{putri_simplification_2024,
  title = {Simplification of {Embedding},
  author = {Putri, Mindi Richia and Husodo, Ario Yudo and Irmawati, Budi},
  year = {2024},
  doi = {10.1109/COMNETSAT63286.2024.10862926},
  url = {https://doi.org/10.1109/comnetsat63286.2024.10862926},
  journal = {2024 IEEE International …},
  pages = {665 -- 670},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… In this study, a QAC model is built using a Large Language Model (LLM) and the … can mitigate hallucinations in LLM. This research employs a Retrieval Augmented Generation (RAG) …},
  annote = {Cited by: 1},
}

@article{taormina_interpretable_2024,
  title = {Interpretable {Sewer},
  author = {Taormina, Riccardo and van der Werf, Job Augustijn},
  year = {2024},
  doi = {10.3390/engproc2024069158},
  url = {https://doi.org/10.3390/engproc2024069158},
  journal = {Engineering Proceedings},
  volume = {69},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access},
}

@inproceedings{jiang_medirag_2024,
  title = {{MediRAG},
  author = {Jiang, Emily and Chen, Alice and Tenison, Irene and Kagal, Lalana},
  year = {2024},
  doi = {10.1109/BigData62323.2024.10825604},
  url = {https://doi.org/10.1109/bigdata62323.2024.10825604},
  pages = {6476 -- 6485},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{zhu_atm_2024,
  title = {{ATM},
  author = {Zhu, Junda and Yan, Lingyong and Shi, Haibo and Yin, Dawei and Sha, Lei},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.610},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.610},
  pages = {10902 -- 10919},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{pan_not_2024,
  title = {Not {All},
  author = {Pan, Ruotong and Cao, Boxi and Lin, Hongyu and Han, Xianpei and Zheng, Jia and Wang, Sirui and Cai, Xunliang and Sun, Le},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.1109},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.1109},
  pages = {19844 -- 19863},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{qi_model_2024,
  title = {Model {Internals},
  author = {Qi, Jirui and Sarti, Gabriele and Fernández, Raquel and Bisazza, Arianna},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.347},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.347},
  pages = {6037 -- 6053},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{jin_dvd_2024,
  title = {{DVD},
  author = {Jin, Jing and Wang, Houfeng and Zhang, Hao and Li, Xiaoguang and Guo, Zhijiang},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.266},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.266},
  pages = {4624 -- 4637},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{buchmann_attribute_2024,
  title = {Attribute or {Abstain},
  author = {Buchmann, Jan and Liu, Xiao and Gurevych, Iryna},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.463},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.463},
  pages = {8113 -- 8140},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{chen_controlling_2024,
  title = {Controlling {Risk},
  author = {Chen, Lu and Zhang, Ruqing and Guo, Jiafeng and Fan, Yixing and Cheng, Xueqi},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.133},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.133},
  pages = {2380 -- 2393},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{cao_tonggu_2024,
  title = {{TongGu},
  author = {Cao, Jiahuan and Peng, Dezhi and Zhang, Peirong and Shi, Yongxin and Liu, Yang and Ding, Kai and Jin, Lianwen},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.243},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.243},
  pages = {4196 -- 4210},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{thakur_knowing_2024,
  title = {“{Knowing},
  author = {Thakur, Nandan and Bonifacio, Luiz Henrique and Zhang, Xinyu and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Chen, Boxing and Rezagholizadeh, Mehdi and Lin, Jimmy},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.730},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.730},
  journal = {Findings of the …},
  pages = {12508 -- 12526},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… LLM hallucinations against first-stage retrieval errors in RAG. (… lenges in LLM robustness by often hallucinating an answer … on LLM’s generation results, and find several hallucination …},
  annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{yao_readme_2024,
  title = {{README},
  author = {Yao, Zonghai and Kantu, Nandyala Siddharth and Wei, Guanghao and Tran, Hieu and Duan, Zhangqi and Kwon, Sunjae and Yang, Zhichao and Yu, Hong},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.737},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.737},
  pages = {12609 -- 12629},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{li_refiner_2024,
  title = {Refiner: {Restructure},
  author = {Li, Zhonghao and Hu, Xuming and Liu, Aiwei and Zheng, Kening and Huang, Sirui and Xiong, Hui},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.500},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.500},
  pages = {8548 -- 8572},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{yu_llms_2024,
  title = {{LLMs},
  author = {Yu, Jiong and Wu, Sixing and Chen, Jiahao and Zhou, Wei},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.794},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.794},
  journal = {Findings of the Association for …},
  pages = {13586 -- 13612},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… of LLM itself and then suffers from the hallucination and outdated knowledge, and 2) RAG … 2, this work proposes a novel LLM-based DCRAG, which regards LLM as Collaborator in …},
  annote = {Cited by: 2},
}

@inproceedings{li_alleviating_2024,
  title = {Alleviating {Action},
  author = {Li, Kanxue and Zheng, Qi and Zhan, Yibing and Zhang, Chong and Zhang, Tianle and Lin, Xu and Qi, Chongchong and Li, Lusong and Tao, Dapeng},
  year = {2024},
  doi = {10.1109/PRAI62207.2024.10826957},
  url = {https://doi.org/10.1109/prai62207.2024.10826957},
  pages = {613 -- 621},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{nguyen_consrag_2024,
  title = {{ConsRAG},
  author = {Nguyen, Ha Thanh and Satoh, Ken},
  year = {2024},
  doi = {10.3233/FAIA241263},
  url = {https://doi.org/10.3233/faia241263},
  journal = {Frontiers in Artificial Intelligence and Applications},
  volume = {395},
  pages = {327 -- 332},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Hybrid Gold Open Access},
}

@article{anaguchi_reasoning_2024,
  title = {Reasoning and {Justification},
  author = {Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
  year = {2024},
  doi = {10.1109/CANDAR64496.2024.00010},
  url = {https://doi.org/10.1109/candar64496.2024.00010},
  pages = {11 -- 20},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{sankararaman_provenance_2024,
  title = {Provenance: {A},
  author = {Sankararaman, Hithesh and Yasin, Mohammed Nasheed and Sorensen, Tanner and Di Bari, Alessandro and Stolçke, Andreas},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-industry.97},
  url = {https://doi.org/10.18653/v1/2024.emnlp-industry.97},
  journal = {arXiv preprint arXiv …},
  pages = {1305 -- 1313},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… retrievalaugmented generation (RAG). Given a context and putative output, we compute a factuality score that can be thresholded to yield a binary decision to check the results of LLM-…},
  annote = {Cited by: 0},
}

@inproceedings{song_rag-hat_2024,
  title = {{RAG},
  author = {Song, Juntong and Wang, Xingguang and Zhu, Juno and Wu, Yuanhao and Cheng, Xuxin and Zhong, Randy and Niu, Cheng},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-industry.113},
  url = {https://doi.org/10.18653/v1/2024.emnlp-industry.113},
  journal = {Proceedings of the …},
  pages = {1548 -- 1558},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… While Tian’s work aims to enable language models to produce more factual answers, our research specifically focuses on enhancing LLM capabilities in RAG scenarios. …},
  annote = {Cited by: 4},
}

@inproceedings{zimmerman_two-tiered_2024,
  title = {Two-tiered {Encoder},
  author = {Zimmerman, Ilana and Tredup, Jadin and Selfridge, Ethan O. and Bradley, Joseph},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-industry.2},
  url = {https://doi.org/10.18653/v1/2024.emnlp-industry.2},
  pages = {8 -- 22},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhang_raglab_2024,
  title = {{RAGLAB},
  author = {Zhang, Xuanwang and Song, Yunze and Wang, Yidong and Tang, Shuyun and Li, Xinfeng and Zeng, Zhengran and Wu, Zhen and Ye, Wei and Xu, Wenyuan and Zhang, Yue and Dai, Xinyu and Zhang, Shikun and Wen, Qingsong},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-demo.43},
  url = {https://doi.org/10.18653/v1/2024.emnlp-demo.43},
  pages = {408 -- 418},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{bruzzone_generative_2024,
  title = {Generative {AI},
  author = {Bruzzone, Agostino G. and Giovannetti, Antonio and Genta, Giacomo and Cefaliello, Daniele},
  year = {2024},
  doi = {10.46354/i3m.2024.mas.021},
  url = {https://doi.org/10.46354/i3m.2024.mas.021},
  booktitle = {Proceedings of the {International},
  volume = {2024-September},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{kim_re-rag_2024,
  title = {{RE},
  author = {Kim, Kiseung and Lee, Jay Yoon},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.1236},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.1236},
  pages = {22149 -- 22161},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{chen_ipl_2024,
  title = {{IPL},
  author = {Chen, Kang and Zhang, Qingheng and Lian, Chengbao and Ji, Yixin and Liu, Xuwei and Han, Shuguang and Wu, Guoqiang and Huang, Fei and Chen, Jufeng},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-industry.52},
  url = {https://doi.org/10.18653/v1/2024.emnlp-industry.52},
  pages = {697 -- 711},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{mandal_iitk_2024,
  title = {{IITK},
  author = {Mandal, Shreyasi and Modi, Ashutosh},
  year = {2024},
  doi = {10.18653/v1/2024.semeval-1.201},
  url = {https://doi.org/10.18653/v1/2024.semeval-1.201},
  pages = {1397 -- 1404},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{he_infrrdai_2024,
  title = {Infrrd.ai at {SemEval},
  author = {He, Jianglong and Tallam, Saiteja and Nakshathri, Srirama and Amarnath, Navaneeth and Pratiba, K. R. and Kumar, Deepak},
  year = {2024},
  doi = {10.18653/v1/2024.semeval-1.136},
  url = {https://doi.org/10.18653/v1/2024.semeval-1.136},
  pages = {940 -- 951},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{knollmeyer_benchmarking_2024,
  title = {Benchmarking of {Retrieval},
  author = {Knollmeyer, Simon and Caymazer, Oğuz and Koval, Leonid and Akmal, Muhammad Uzair and Asif, Saara and Mathias, Selvine George and Grossmann, Daniel J.},
  year = {2024},
  doi = {10.5220/0013065700003838},
  url = {https://doi.org/10.5220/0013065700003838},
  booktitle = {International {Joint},
  volume = {3},
  pages = {137 -- 148},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{swapnil_automation_2024,
  title = {From {Automation},
  author = {Swapnil, Morande and Garima, Sogani and Shashank, Shah},
  year = {2024},
  doi = {10.23919/ITUK62727.2024.10772932},
  url = {https://doi.org/10.23919/ituk62727.2024.10772932},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{palaniappan_enhancing_2024,
  title = {Enhancing {Enterprise},
  author = {Palaniappan, Somasundaram and Mali, Rohit and Cuomo, Luca and Vitale, Mauro and Youssef, Ali and Madathil, Athira Puthanveetil and Murugesan, Malathi and Bettini, Alessandro and de Magistris, Giovanni and Veneri, Giacomo},
  year = {2024},
  doi = {10.2118/222032-MS},
  url = {https://doi.org/10.2118/222032-ms},
  journal = {Abu Dhabi …},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… the occurrence of hallucinations. An application built with … We have commercially available RAG+LLM systems but it is … In this technical paper we present a RAG+LLM system with a …},
  annote = {Cited by: 1},
}

@inproceedings{rai_generative_2024,
  title = {Generative {AI},
  author = {Rai, P. and Jain, A. and Anand, A.},
  year = {2024},
  doi = {10.2118/221872-MS},
  url = {https://doi.org/10.2118/221872-ms},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{singh_application_2024,
  title = {Application of {Generative},
  author = {Singh, Ajay P. and Jia, Tianxia and Nalagatla, Varun and Cunningham, Brian and Siwani, Talib},
  year = {2024},
  doi = {10.2118/222932-MS},
  url = {https://doi.org/10.2118/222932-ms},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{chang_leveraging_2024,
  title = {Leveraging {Retrieval},
  author = {Chang, Chenchi and Chang, Hanpi and Lee, Hungshin},
  year = {2024},
  doi = {10.1109/RASSE64357.2024.10773731},
  url = {https://doi.org/10.1109/rasse64357.2024.10773731},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{gummadi_enhancing_2024,
  title = {Enhancing {Communication},
  author = {Gummadi, Venkata Akhil Kumar and Udayaraju, Pamula and Sarabu, Venkata Rahul and Ravulu, Chaithanya and Seelam, Dhanunjay Reddy and Sarella, Venkata Ramana},
  year = {2024},
  doi = {10.1109/ICSES63445.2024.10763024},
  url = {https://doi.org/10.1109/icses63445.2024.10763024},
  pages = {612 -- 617},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{zhang_synthetic_2024,
  title = {Synthetic {Knowledge},
  author = {Zhang, Jiaxin and Cui, Wendi and Huang, Yiran and Das, Kamalika and Sricharan, Kumar},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.1196},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.1196},
  journal = {arXiv preprint arXiv …},
  pages = {21456 -- 21473},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… step towards enhancing the factual accuracy of LLM outputs by refining knowledge … Further evaluations were conducted on our method within RAG systems, detailed in Table 2. Inverse …},
  annote = {Cited by: 0},
}

@inproceedings{laban_summary_2024,
  title = {Summary of a {Haystack},
  author = {Laban, Philippe and Fabbri, Alexander Richard and Xiong, Caiming and Wu, Chiensheng},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.552},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.552},
  journal = {arXiv preprint arXiv:2407.01370},
  pages = {9885 -- 9903},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… trade-offs exist when choosing between a RAG pipeline and a longcontext LLM, with RAG systems typically improving citation quality, at the cost of insight coverage, (3) using advanced …},
  annote = {Cited by: 4},
}

@article{badurowicz_feasibility_2024,
  title = {{FEASIBILITY},
  author = {Badurowicz, Marcin and Skulimowski, Stanisław Piotr and Laskowski, Maciej},
  year = {2024},
  doi = {10.35784/acs-2024-46},
  url = {https://doi.org/10.35784/acs-2024-46},
  journal = {Applied Computer Science},
  volume = {20},
  number = {4},
  pages = {175 -- 191},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{abdelaziem_innovative_2024,
  title = {Innovative {Approach},
  author = {Abdelaziem, Osama Elsayed and Nasser Khafagy, A. and Yehia, Taha A.},
  year = {2024},
  doi = {10.2118/223359-MS},
  url = {https://doi.org/10.2118/223359-ms},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{he_retrieving_2024,
  title = {Retrieving, {Rethinking},
  author = {He, Bolei and Chen, Nuo and He, Xinran and Yan, Lingyong and Wei, Zhenkai and Luo, Jinchang and Ling, Zhenhua},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.607},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.607},
  pages = {10371 -- 10393},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{su_parameter_2024,
  title = {Parameter recommendation system enhanced by {RAG},
  author = {Su, Aihua and Sun, Jiawei and Dong, Kejing and Ling, Weiqing},
  year = {2024},
  doi = {10.1109/EIT63098.2024.10762220},
  url = {https://doi.org/10.1109/eit63098.2024.10762220},
  pages = {467 -- 471},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jauhiainen_evaluating_2024-1,
  title = {Evaluating {Students},
  author = {Jauhiainen, Jussi Sakari and Guerra, Agustín Garagorry},
  year = {2024},
  doi = {10.54364/AAIML.2024.44177},
  url = {https://doi.org/10.54364/aaiml.2024.44177},
  journal = {Advances in Artificial Intelligence and Machine Learning},
  volume = {4},
  number = {4},
  pages = {3097 -- 3113},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{lu_triageagent_2024,
  title = {{TRIAGEAGENT},
  author = {Lu, Meng and Ho, Brandon J. and Ren, Dennis M. and Wang, Xuan},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.329},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.329},
  pages = {5747 -- 5764},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{zhang_ask_2024,
  title = {Ask {NAEP},
  author = {Zhang, Ting and Patterson, Luke and Beiting-Parrish, Maggie and Webb, Blue and Abeysinghe, Bhashithe and Bailey, Paul and Sikali, Emmanuel},
  year = {2024},
  doi = {10.21031/epod.1548128},
  url = {https://doi.org/10.21031/epod.1548128},
  journal = {Journal of Measurement and Evaluation in Education and Psychology},
  volume = {15},
  pages = {378 -- 394},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{wang_searching_2024,
  title = {Searching for {Best},
  author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.981},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.981},
  pages = {17716 -- 17736},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 18},
}

@article{el_maazouzi_optimizing_2024,
  title = {Optimizing {Recommendation},
  author = {El Maazouzi, Qamar and Retbi, Asmaâ and Bennani, Samir},
  year = {2024},
  doi = {10.1007/978-3-031-77040-1_8},
  url = {https://doi.org/10.1007/978-3-031-77040-1_8},
  journal = {Communications in Computer and Information Science},
  volume = {2167 CCIS},
  pages = {106 -- 118},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{ghali_enhancing_2024,
  title = {Enhancing {Retrieval},
  author = {Ghali, Julien Pierre Edmond and Shima, Kosuke and Moriyama, Koichi and Mutoh, Atsuko and Inuzuka, Nobuhiro},
  year = {2024},
  doi = {10.1016/j.procs.2024.09.424},
  url = {https://doi.org/10.1016/j.procs.2024.09.424},
  booktitle = {Procedia {Computer},
  volume = {246},
  pages = {443 -- 452},
  note = {Issue: C
Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@inproceedings{conia_towards_2024,
  title = {Towards {Cross},
  author = {Conia, Simone and Lee, Daniel and Li, Min and Minhas, Umar Farooq and Potdar, Saloni and Li, Yunyao},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.914},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.914},
  pages = {16343 -- 16360},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 5; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{chen_eyegpt_2024,
  title = {{EyeGPT},
  author = {Chen, Xiaolan and Zhao, Ziwei and Zhang, Weiyi and Xu, Pusheng and Wu, Yue and Xu, Mingpu and Gao, Le and Li, Yinwen and Shang, Xianwen and Shi, Danli and He, Mingguang},
  year = {2024},
  doi = {10.2196/60063},
  url = {https://doi.org/10.2196/60063},
  journal = {Journal of Medical Internet Research},
  volume = {26},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… into a general LLM using role-play, fine-tuning, and RAG methods, resulting in … LLM responses to Ophthalmic Knowledge Assessment Program style queries. In this study, hallucination …},
  annote = {Cited by: 15; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{isbarov_enhanced_2024,
  title = {Enhanced document retrieval with topic embeddings},
  author = {Isbarov, Jafar and Huseynova, Kavsar},
  year = {2024},
  doi = {10.1109/AICT61888.2024.10740455},
  url = {https://doi.org/10.1109/aict61888.2024.10740455},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{gulden_leveraging_2024,
  title = {Leveraging {Large},
  author = {Gulden, Timothy R. and Zhang, Liang and Geist, Edward Moore and Awan, Jalal and Abdurahaman, Zara Fatima and Ahmadi, Mohammad},
  year = {2024},
  doi = {10.1007/978-3-031-64193-0_18},
  url = {https://doi.org/10.1007/978-3-031-64193-0_18},
  journal = {Springer Proceedings in Complexity},
  pages = {272 -- 283},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{tang_minicheck_2024,
  title = {{MiniCheck},
  author = {Tang, Liyan and Laban, Philippe and Durrett, Greg},
  year = {2024},
  doi = {10.18653/v1/2024.emnlp-main.499},
  url = {https://doi.org/10.18653/v1/2024.emnlp-main.499},
  pages = {8818 -- 8847},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 8},
}

@inproceedings{islam_open-rag_2024-1,
  title = {{OPEN},
  author = {Islam, Shayekh Bin and Rahman, Md Asib and Tozammel Hossain, K. S.M. and Hoque, Enamul and Joty, Shafiq Rayhan and Parvez, Md Rizwan},
  year = {2024},
  doi = {10.18653/v1/2024.findings-emnlp.831},
  url = {https://doi.org/10.18653/v1/2024.findings-emnlp.831},
  pages = {14231 -- 14244},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{firsanova_graph-based_2024,
  title = {A graph-based approach to closed-domain natural language generation; Новый графовый подход к генерации текстов узкой предметной области на естественном языке},
  author = {Firsanova, Victoria},
  year = {2024},
  doi = {10.18413/2313-8912-2024-10-3-0-7},
  url = {https://doi.org/10.18413/2313-8912-2024-10-3-0-7},
  journal = {Research Result. Theoretical and Applied Linguistics},
  volume = {10},
  number = {3},
  pages = {135 -- 167},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Gold Open Access},
}

@article{shi_ask-eda_2024,
  title = {Ask-{EDA},
  author = {Shi, Luyao and Kazda, Michael A. and Sears, Bradley and Shropshire, Nick and Puri, Ruchir},
  year = {2024},
  doi = {10.1109/LAD62341.2024.10691824},
  url = {https://doi.org/10.1109/lad62341.2024.10691824},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 13},
}

@inproceedings{ordonez-camacho_aurel_ai_2024,
  title = {Aurel\_AI: {Automating},
  author = {Ordonez-Camacho, Diego and Melgarejo-Heredia, Rafael and Abbasi, Mohsen and González-Solis, Lucía},
  year = {2024},
  doi = {10.54808/WMSCI2024.01.81},
  url = {https://doi.org/10.54808/wmsci2024.01.81},
  booktitle = {Proceedings of {World},
  volume = {2024-September},
  pages = {81 -- 84},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{cvetkovic_semantic_2024,
  title = {Semantic {Exploration},
  author = {Cvetkovic, Stevica S. and Speletic, Matija and Nikolić, Saša V.},
  year = {2024},
  doi = {10.1007/978-3-031-71419-1_25},
  url = {https://doi.org/10.1007/978-3-031-71419-1_25},
  journal = {Lecture Notes in Networks and Systems},
  volume = {860 LNNS},
  pages = {289 -- 298},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{tihanyi_cybermetric_2024,
  title = {{CyberMetric},
  author = {Tihanyi, Norbert and Ferrag, Mohamed Amine and Jain, Ridhi and Bisztray, Tamás and Debbah, Merouane},
  year = {2024},
  doi = {10.1109/CSR61664.2024.10679494},
  url = {https://doi.org/10.1109/csr61664.2024.10679494},
  pages = {296 -- 302},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 20},
}

@inproceedings{liu_ra-isf_2024,
  title = {{RA},
  author = {Liu, Yanming and Peng, Xinyue and Zhang, Xuhong and Liu, Weihao and Yin, Jianwei and Cao, Jiannan and Du, Tianyu},
  year = {2024},
  doi = {10.18653/v1/2024.findings-acl.281},
  url = {https://doi.org/10.18653/v1/2024.findings-acl.281},
  booktitle = {Proceedings of the {Annual},
  pages = {4730 -- 4749},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 7},
}

@inproceedings{hsieh_found_2024,
  title = {Found in the middle: {Calibrating},
  author = {Hsieh, Chengyu and Chuang, Yungsung and Li, Chunliang and Wang, Zifeng and Le, Long T. and Kumar, Abhishek and Glass, James R. and Ratner, Alexander Alex and Lee, Chenyu and Krishna, Ranjay and Pfister, Tomas},
  year = {2024},
  doi = {10.18653/v1/2024.findings-acl.890},
  url = {https://doi.org/10.18653/v1/2024.findings-acl.890},
  booktitle = {Proceedings of the {Annual},
  pages = {14982 -- 14995},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 6},
}

@inproceedings{naseem_grounded_2024,
  title = {A {Grounded},
  author = {Naseem, Tahira and Xu, Guangxuan and Swaminathan, Sarathkrishna and Yehudai, Asaf and Chaudhury, Subhajit and Florian, Radu and Astudillo, Ramón Fernández and Munawar, Asim},
  year = {2024},
  doi = {10.18653/v1/2024.findings-acl.10},
  url = {https://doi.org/10.18653/v1/2024.findings-acl.10},
  booktitle = {Proceedings of the {Annual},
  pages = {151 -- 162},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{xiong_benchmarking_2024,
  title = {Benchmarking {Retrieval},
  author = {Xiong, Guangzhi and Jin, Qiao and Lu, Zhiyong and Zhang, Aidong},
  year = {2024},
  doi = {10.18653/v1/2024.findings-acl.372},
  url = {https://doi.org/10.18653/v1/2024.findings-acl.372},
  booktitle = {Proceedings of the {Annual},
  pages = {6233 -- 6251},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 47},
}

@inproceedings{enouen_textgenshap_2024,
  title = {{TextGenSHAP},
  author = {Enouen, James and Nakhost, Hootan and Ebrahimi, Sayna and Arik, Sercan Ömer and Liu, Yan and Pfister, Tomas},
  year = {2024},
  doi = {10.18653/v1/2024.findings-acl.832},
  url = {https://doi.org/10.18653/v1/2024.findings-acl.832},
  booktitle = {Proceedings of the {Annual},
  pages = {13984 -- 14011},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{zhang_retrievalqa_2024,
  title = {{RetrievalQA},
  author = {Zhang, Zihan and Fang, Meng and Chen, Ling},
  year = {2024},
  doi = {10.18653/v1/2024.findings-acl.415},
  url = {https://doi.org/10.18653/v1/2024.findings-acl.415},
  booktitle = {Proceedings of the {Annual},
  pages = {6963 -- 6975},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{addad_homeopathic_2024,
  title = {Homeopathic {Poisoning},
  author = {Addad, Boussad and Kapusta, Katarzyna},
  year = {2024},
  doi = {10.1007/978-3-031-68738-9_28},
  url = {https://doi.org/10.1007/978-3-031-68738-9_28},
  journal = {Lecture Notes in Computer Science},
  volume = {14989 LNCS},
  pages = {358 -- 364},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{fang_enhancing_2024,
  title = {Enhancing {Noise},
  author = {Fang, Feiteng and Bai, Yuelin and Ni, Shiwen and Yang, Min and Chen, Xiaojun and Xu, Ruifeng},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.540},
  url = {https://doi.org/10.18653/v1/2024.acl-long.540},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {10028 -- 10039},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{jacobs_leveraging_2024,
  title = {Leveraging {Lecture},
  author = {Jacobs, Sven and Jaschke, Steffen},
  year = {2024},
  doi = {10.1109/CSEET62301.2024.10663001},
  url = {https://doi.org/10.1109/cseet62301.2024.10663001},
  booktitle = {Software {Engineering},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 7},
}

@inproceedings{agarwal_symkgqa_2024,
  title = {{SymKGQA},
  author = {Agarwal, Prerna and Kumar, Nishant and Bedathur, Srikanta J.},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.545},
  url = {https://doi.org/10.18653/v1/2024.acl-long.545},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {10119 -- 10140},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{zhang_arl2_2024,
  title = {{ARL2},
  author = {Zhang, Lingxi and Yu, Yue and Wang, Kuan and Zhang, Chao},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.203},
  url = {https://doi.org/10.18653/v1/2024.acl-long.203},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {3708 -- 3719},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{fernandez_syllabusqa_2024,
  title = {{SYLLABUSQA},
  author = {Fernandez, Nigel and Scarlatos, Alexander and Lan, Andrew S.},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.557},
  url = {https://doi.org/10.18653/v1/2024.acl-long.557},
  booktitle = {Proceedings of the {Annual},
  journal = {arXiv preprint arXiv:2403.14666},
  volume = {1},
  pages = {10344 -- 10369},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… Factuality Metrics We design a novel LLMbased (GPT-4) evaluation … RAG provides a significant boost in LLM performance. In the zero-shot setting, LLaMA-2-70B combined with RAG …},
  annote = {Cited by: 3},
}

@inproceedings{niu_ragtruth_2024,
  title = {{RAGTruth},
  author = {Niu, Cheng and Wu, Yuanhao and Zhu, Juno and Xu, Siliang and Shum, Kashun and Zhong, Randy and Song, Juntong and Zhang, Tong},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.585},
  url = {https://doi.org/10.18653/v1/2024.acl-long.585},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {10862 -- 10878},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 12},
}

@inproceedings{wan_what_2024,
  title = {What {Evidence},
  author = {Wan, Alexander and Wallace, Eric and Klein, Dan},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.403},
  url = {https://doi.org/10.18653/v1/2024.acl-long.403},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {7468 -- 7484},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{chen_vullibgen_2024,
  title = {{VulLibGen},
  author = {Chen, Tianyu and Li, Lin and Zhu, Liuchuan and Li, Zongyang and Liu, Xueqing and Liang, Guangtai and Wang, Qianxiang and Xie, Tao},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.527},
  url = {https://doi.org/10.18653/v1/2024.acl-long.527},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {9767 -- 9780},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{shi_generate-then-ground_2024,
  title = {Generate-then-{Ground},
  author = {Shi, Zhengliang and Zhang, Shuo and Sun, Weiwei and Gao, Shen and Ren, Pengjie and Chen, Zhunmin and Ren, Zhaochun},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.397},
  url = {https://doi.org/10.18653/v1/2024.acl-long.397},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {7339 -- 7353},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 10},
}

@inproceedings{li_citation-enhanced_2024,
  title = {Citation-{Enhanced},
  author = {Li, Weitao and Li, Junkai and Ma, Weizhi and Liu, Yang},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.79},
  url = {https://doi.org/10.18653/v1/2024.acl-long.79},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {1451 -- 1466},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 7},
}

@inproceedings{maharana_evaluating_2024,
  title = {Evaluating {Very},
  author = {Maharana, Adyasha and Lee, Dong-ho and Tulyakov, Sergey and Bansal, Mohit and Barbieri, Francesco and Fang, Yuwei},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.747},
  url = {https://doi.org/10.18653/v1/2024.acl-long.747},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {13851 -- 13870},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 7},
}

@article{srinivasagan_retrieval_2024,
  title = {Retrieval {Augmented},
  author = {Srinivasagan, Gokul and Georges, Munir},
  year = {2024},
  doi = {10.1007/978-3-031-70566-3_1},
  url = {https://doi.org/10.1007/978-3-031-70566-3_1},
  journal = {Lecture Notes in Computer Science},
  volume = {15049 LNAI},
  pages = {3 -- 12},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sui_enhancing_2024,
  title = {Enhancing {LLM},
  author = {Sui, Yize and Ren, Jing and Tan, Huibin and Chen, Huan and Li, Zhaoye and Wang, Ji},
  year = {2024},
  doi = {10.1007/978-3-031-70365-2_15},
  url = {https://doi.org/10.1007/978-3-031-70365-2_15},
  journal = {Lecture Notes in Computer Science},
  volume = {14946 LNAI},
  pages = {251 -- 268},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{li_role_2024,
  title = {On the {Role},
  author = {Li, Dongyang and Yan, Junbing and Zhang, Taolin and Wang, Chengyu and He, Xiaofeng and Huang, Longtao and Xue, Hui and Huang, Jun},
  year = {2024},
  doi = {10.18653/v1/2024.acl-short.12},
  url = {https://doi.org/10.18653/v1/2024.acl-short.12},
  booktitle = {Proceedings of the {Annual},
  volume = {2},
  pages = {120 -- 126},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{monea_glitch_2024,
  title = {A {Glitch},
  author = {Monea, Giovanni and Peyrard, Maxime and Josifoski, Martin and Chaudhary, Vishrav and Eisner, Jason and Kiciman, Emre and Palangi, Hamid and Patra, Barun and West, Robert},
  year = {2024},
  doi = {10.18653/v1/2024.acl-long.369},
  url = {https://doi.org/10.18653/v1/2024.acl-long.369},
  booktitle = {Proceedings of the {Annual},
  volume = {1},
  pages = {6828 -- 6844},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access},
}

@inproceedings{kuppa_manipulating_2024,
  title = {Manipulating {Prompts},
  author = {Kuppa, Aditya and Nicholls, Jack and Le-Khac, Nhien An},
  year = {2024},
  doi = {10.5220/0012803100003767},
  url = {https://doi.org/10.5220/0012803100003767},
  booktitle = {Proceedings of the {International},
  journal = {Proceedings of the 21st …},
  pages = {777 -- 785},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… We demonstrate empirically that is it possible to increase the citation score of LLM output to include erroneous or unnecessary sources of information to redirect a reader to a desired …},
  annote = {Cited by: 1},
}

@article{mamalis_large_2024,
  title = {A {Large},
  author = {Mamalis, Marios Evangelos and Kalampokis, Evangelos and Fitsilis, Fotios and Theodorakopoulos, Georgios and Tarabanis, Konstantinos A.},
  year = {2024},
  doi = {10.1007/978-3-031-70274-7_18},
  url = {https://doi.org/10.1007/978-3-031-70274-7_18},
  journal = {Lecture Notes in Computer Science},
  volume = {14841 LNCS},
  pages = {286 -- 301},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access},
}

@article{otto_enhancing_2024,
  title = {Enhancing {Software},
  author = {Otto, Wolfgang and Upadhyaya, Sharmila and Dietze, Stefan},
  year = {2024},
  doi = {10.1007/978-3-031-65794-8_21},
  url = {https://doi.org/10.1007/978-3-031-65794-8_21},
  journal = {Lecture Notes in Computer Science},
  volume = {14770 LNAI},
  pages = {289 -- 306},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0; All Open Access; Hybrid Gold Open Access},
}

@article{wang_csprd_2024,
  title = {{CSPRD},
  author = {Wang, Jinyuan and Wang, Zhong and Zhu, Zeyang and Xie, Jinhao and Yu, Yong and Fei, Yongjian and Huang, Yue and Cheng, Dawei and Zhao, Hai},
  year = {2024},
  doi = {10.1007/978-3-031-68309-1_1},
  url = {https://doi.org/10.1007/978-3-031-68309-1_1},
  journal = {Lecture Notes in Computer Science},
  volume = {14910 LNCS},
  pages = {3 -- 17},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{xu_enhancing_2024,
  title = {Enhancing {Retrieval},
  author = {Xu, Sheng and Chen, Mike and Chen, Shuwen},
  year = {2024},
  doi = {10.1007/978-981-97-5678-0_34},
  url = {https://doi.org/10.1007/978-981-97-5678-0_34},
  journal = {Lecture Notes in Computer Science},
  volume = {14880 LNAI},
  pages = {398 -- 409},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 8},
}

@article{kawashima_development_2024,
  title = {Development of {RAG},
  author = {Kawashima, Soki and Shiramatsu, Shun and Mizumoto, Takeshi},
  year = {2024},
  doi = {10.1007/978-981-97-3305-7_48},
  url = {https://doi.org/10.1007/978-981-97-3305-7_48},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1004 LNNS},
  pages = {603 -- 617},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{miladi_comparative_2024,
  title = {Comparative {Performance},
  author = {Miladi, Fatma and Psyché, Valéry and Lemire, Daniel},
  year = {2024},
  doi = {10.1007/978-3-031-65996-6_7},
  url = {https://doi.org/10.1007/978-3-031-65996-6_7},
  journal = {Communications in Computer and Information Science},
  volume = {2162 CCIS},
  pages = {81 -- 92},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{kamel_boulos_nvidias_2024,
  title = {{NVIDIA},
  author = {Kamel Boulos, Maged N. and Dellavalle, Robert Paul},
  year = {2024},
  doi = {10.2196/58396},
  url = {https://doi.org/10.2196/58396},
  journal = {JMIR Dermatology},
  volume = {7},
  note = {Type: Review},
  keywords = {source: Scopus},
  annote = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access},
}

@article{ko_enhancing_2024,
  title = {Enhancing {Python},
  author = {Ko, Hsing Tzu and Liu, Yen Ku and Tsai, Yuncheng and Suen, Summit},
  year = {2024},
  doi = {10.1007/978-3-031-65884-6_17},
  url = {https://doi.org/10.1007/978-3-031-65884-6_17},
  journal = {Lecture Notes in Computer Science},
  volume = {14786 LNCS},
  pages = {164 -- 173},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{rorseth_towards_2024,
  title = {Towards {Explainability},
  author = {Rorseth, Joel and Godfrey, Parke and Golab, Lukasz and Srivastava, Divesh and Szlichta, Jaroslaw},
  year = {2024},
  doi = {10.1109/ICDE60146.2024.00466},
  url = {https://doi.org/10.1109/icde60146.2024.00466},
  booktitle = {Proceedings - {International},
  pages = {5669 -- 5670},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{miladi_leveraging_2024,
  title = {Leveraging {GPT},
  author = {Miladi, Fatma and Psyché, Valéry and Lemire, Daniel},
  year = {2024},
  doi = {10.1007/978-3-031-64315-6_40},
  url = {https://doi.org/10.1007/978-3-031-64315-6_40},
  journal = {Communications in Computer and Information Science},
  volume = {2150 CCIS},
  pages = {427 -- 434},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 9},
}

@inproceedings{li_traq_2024,
  title = {{TRAQ},
  author = {Li, Shuo and Lee, Insup and Park, Sangdon and Bastani, Osbert},
  year = {2024},
  doi = {10.18653/v1/2024.naacl-long.210},
  url = {https://doi.org/10.18653/v1/2024.naacl-long.210},
  volume = {1},
  pages = {3799 -- 3821},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{sokolov_generative_2024,
  title = {Generative {Reader},
  author = {Sokolov, Andrey P. and Zamelin, Pavel and Kamelina, Yulia and Plastova, Polina},
  year = {2024},
  doi = {10.1109/NeuroNT62606.2024.10585446},
  url = {https://doi.org/10.1109/neuront62606.2024.10585446},
  pages = {135 -- 138},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{omeed_integrating_2024,
  title = {Integrating {Computer},
  author = {Omeed, Holan K. and Alani, Ahmed O. and Rasul, Ibrahim H. and Ashir, Abubakar Muhammad and Mohammed, Sava Ahmed},
  year = {2024},
  doi = {10.1109/SSD61670.2024.10548649},
  url = {https://doi.org/10.1109/ssd61670.2024.10548649},
  pages = {124 -- 131},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{tishechkin_improve_2024,
  title = {Improve {Geoscience},
  author = {Tishechkin, Dmitriy and Dubovik, A. S. and Koriagin, Aleksandr and Khudorozhkov, Roman L.},
  year = {2024},
  doi = {10.3997/2214-4609.202439088},
  url = {https://doi.org/10.3997/2214-4609.202439088},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{welz_enhancing_2024,
  title = {Enhancing {Large},
  author = {Welz, Laslo and Lanquillon, Carsten},
  year = {2024},
  doi = {10.1007/978-3-031-60615-1_9},
  url = {https://doi.org/10.1007/978-3-031-60615-1_9},
  journal = {Lecture Notes in Computer Science},
  volume = {14736 LNAI},
  pages = {135 -- 146},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{yang_yangqi_2024,
  title = {yangqi at {SemEval},
  author = {Yang, Qi and Zeng, Jingjie and Yang, Liang and Lin, Hongfei},
  year = {2024},
  doi = {10.18653/v1/2024.semeval-1.36},
  url = {https://doi.org/10.18653/v1/2024.semeval-1.36},
  pages = {233 -- 238},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{huang_evaluation_2024,
  title = {Evaluation of {Orca},
  author = {Huang, Donghao and Wang, Zhaoxia},
  year = {2024},
  doi = {10.1007/978-981-97-2650-9_1},
  url = {https://doi.org/10.1007/978-981-97-2650-9_1},
  journal = {Lecture Notes in Computer Science},
  volume = {14658 LNAI},
  pages = {3 -- 19},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{wijesinghe_artificial_2024,
  title = {Artificial {Intelligence},
  author = {Wijesinghe, Abhiru and Rajapakse, Chathura and Asanka, Dinesh},
  year = {2024},
  doi = {10.1109/ICARC61713.2024.10499761},
  url = {https://doi.org/10.1109/icarc61713.2024.10499761},
  pages = {97 -- 102},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{mosser_exploration_2024,
  title = {Exploration {Robot},
  author = {Mosser, Lukas J. and Aursand, P. and Brakstad, Kjetil S. and Lehre, C. and Myhre-Bakkevig, J.},
  year = {2024},
  doi = {10.2118/218439-MS},
  url = {https://doi.org/10.2118/218439-ms},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 7},
}

@article{he_quality_2024,
  title = {Quality of {Answers},
  author = {He, Zhe and Bhasuran, Balu and Jin, Qiao and Tian, Shubo and Hanna, Karim H. and Shavor, Cindy Soto and Arguello, Lisbeth Garcia and Murray, Patrick and Lu, Zhiyong},
  year = {2024},
  doi = {10.2196/56655},
  url = {https://doi.org/10.2196/56655},
  journal = {Journal of Medical Internet Research},
  volume = {26},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 22; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access},
}

@article{ghodratnama_adapting_2024,
  title = {Adapting {LLMs},
  author = {Ghodratnama, Samira and Zakershahrak, Mehrdad},
  year = {2024},
  doi = {10.1007/978-981-97-0989-2_2},
  url = {https://doi.org/10.1007/978-981-97-0989-2_2},
  journal = {Lecture Notes in Computer Science},
  volume = {14518 LNCS},
  pages = {17 -- 26},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{wolfel_knowledge-based_2024,
  title = {Knowledge-{Based},
  author = {Wölfel, Matthias and Barani Shirzad, Mehrnoush and Reich, Andreas and Anderer, Katharina},
  year = {2024},
  doi = {10.3390/bdcc8010002},
  url = {https://doi.org/10.3390/bdcc8010002},
  journal = {Big Data and Cognitive Computing},
  volume = {8},
  number = {1},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 25; All Open Access; Gold Open Access},
}

@article{zhou_preliminary_2024-1,
  title = {Preliminary {Research},
  author = {Zhou, Xiangguang and Jing, X. W. and Wu, W. K. and Gao, G. Z.},
  year = {2024},
  doi = {10.2118/220833-MS},
  url = {https://doi.org/10.2118/220833-ms},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{jobanputra_teamsaarlst_2024,
  title = {{TeamSaarLST},
  author = {Jobanputra, Mayank and Demberg, Vera},
  year = {2024},
  doi = {10.18653/v1/2024.inlg-genchal.10},
  url = {https://doi.org/10.18653/v1/2024.inlg-genchal.10},
  pages = {92 -- 99},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1; All Open Access; Gold Open Access},
}

@article{umrao_detecting_2024,
  title = {Detecting {Hallucinations},
  author = {Umrao, Sanjana and Parashar, Aditya and Chaubey, Ved Prakash and Shamneesh, Sharma and Ashfaq, Farzeen},
  year = {2024},
  doi = {10.1109/ICETAS62372.2024.11120287},
  url = {https://doi.org/10.1109/icetas62372.2024.11120287},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{xu_framework_2024,
  title = {Framework and construction methodology of underground engineering domain knowledge large language model: {UndergrGPT},
  author = {Xu, Zhenhao and Wang, Zhaoyang and Ren, Pengjie and Zhang, Xiao and Li, Tianhao},
  year = {2024},
  doi = {10.55092/sc20240012},
  url = {https://doi.org/10.55092/sc20240012},
  journal = {Smart Construction},
  volume = {1},
  number = {2},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{feridooni_development_2024,
  title = {Development of a vascular surgery-specific artificial intelligence chat interface using retrieval-augmented generation: {VASC},
  author = {Feridooni, Tiam and Javidan, Arshia Pedram and Mahmood, Daniyal Nasir and Gomes, Zoya and Dueck, Andrew D. and Wheatcroft, Mark David and Szalay, David Anthony},
  year = {2024},
  doi = {10.1016/j.jvsvi.2024.100137},
  url = {https://doi.org/10.1016/j.jvsvi.2024.100137},
  journal = {JVS-Vascular Insights},
  volume = {2},
  note = {Type: Review},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… of RAG include handling extensive documents to provide grounding knowledge and in turn decreasing neural hallucinations by narrowing the LLM's … (B) Our RAG LLM model enhances …},
  annote = {Cited by: 5; All Open Access; Gold Open Access},
}

@article{zhang_system_2024,
  title = {The {System},
  author = {Zhang, Zhixian and Tang, Lina and Bai, Yin and Yu, Xiaoting and Yang, Kairui and Jiang, Peng},
  year = {2024},
  doi = {10.1109/ICISE-IE64355.2024.11025394},
  url = {https://doi.org/10.1109/icise-ie64355.2024.11025394},
  pages = {40 -- 44},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{fahmi_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Fahmi, Rizaldi and Cheon, Soojin and Park, Yesol and Kwon, Joonho},
  year = {2024},
  doi = {10.1109/AIxDKE63520.2024.00032},
  url = {https://doi.org/10.1109/aixdke63520.2024.00032},
  pages = {127 -- 128},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{soularidis_llm-assisted_2024,
  title = {{LLM},
  author = {Soularidis, Andreas and Kotis, Konstantinos I. and Lamolle, Myriam and Mejdoul, Zakaria and Lortal, Gaëlle and Vouros, George A.},
  year = {2024},
  doi = {10.1109/AIxDKE63520.2024.00008},
  url = {https://doi.org/10.1109/aixdke63520.2024.00008},
  pages = {7 -- 12},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{luo_application_2024,
  title = {Application and evaluation of {RAG},
  author = {Luo, Yinhui and Xu, Wenhao and Zeng, Changchang and Fu, Qiang and Xiang, Ziyu},
  year = {2024},
  doi = {10.1109/CNTEIE66268.2024.00032},
  url = {https://doi.org/10.1109/cnteie66268.2024.00032},
  pages = {138 -- 142},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{sinha_fact-centric_2024,
  title = {Fact-{Centric},
  author = {Sinha, Reuben and Shiramatsu, Shun},
  year = {2024},
  doi = {10.1109/WI-IAT62293.2024.00067},
  url = {https://doi.org/10.1109/wi-iat62293.2024.00067},
  pages = {421 -- 425},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{jayawardene_tablinkllm_2024,
  title = {{TabLinkLLM},
  author = {Jayawardene, Iroshani and Avogadro, Roberto and Soylu, Ahmet and Roman, Dumitru},
  year = {2024},
  doi = {10.1109/WI-IAT62293.2024.00035},
  url = {https://doi.org/10.1109/wi-iat62293.2024.00035},
  pages = {206 -- 214},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{kulkarni_bankbot_2024,
  title = {{BankBot},
  author = {Kulkarni, Radhika V. and Paldewar, Ameya and Paliwal, Sanket and Palwe, Amol and Panchal, Soham S.},
  year = {2024},
  doi = {10.1109/ICPIDS65698.2024.00030},
  url = {https://doi.org/10.1109/icpids65698.2024.00030},
  pages = {138 -- 143},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{kumari_innovative_2024,
  title = {Innovative {Corporate},
  author = {Kumari, Simran and Chakraborty, Udit Kr and Nath, Basab and Kumar, Avinash and Srivastava, Pratham and Vivek, Namsani},
  year = {2024},
  doi = {10.1109/OCIT65031.2024.00058},
  url = {https://doi.org/10.1109/ocit65031.2024.00058},
  journal = {2024 OITS …},
  pages = {291 -- 296},
  note = {Type: Conference paper},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… combines LLMs with Retrieval Augmented Generation (RAG). We … responses while reducing hallucinations. Keywords—… where RAG serves as the retriever component whereas the LLM …},
  annote = {Cited by: 0},
}

@inproceedings{sinha_healthcare_2024,
  title = {Healthcare {Diagnostic},
  author = {Sinha, Kushagra and Singh, Vaibhav and Vishnoi, Ankit and Madan, Parul and Shukla, Yadvendra},
  year = {2024},
  doi = {10.1109/EmergIN63207.2024.10961136},
  url = {https://doi.org/10.1109/emergin63207.2024.10961136},
  pages = {333 -- 338},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{kamath_large_2024,
  title = {Large {Language},
  author = {Kamath, Uday and Keenan, Kevin and Somers, Garrett and Sorenson, Sarah},
  year = {2024},
  doi = {10.1007/978-3-031-65647-7},
  url = {https://doi.org/10.1007/978-3-031-65647-7},
  pages = {1 -- 472},
  note = {Type: Book},
  keywords = {source: Scopus},
  annote = {Cited by: 19},
}

@inproceedings{nurhayati_performance_2024,
  title = {Performance {Evaluation},
  author = {Nurhayati, Nurhayati and Abdurrohman, Royyan and Hulliyah, Khodijah and Khairani, Dewi},
  year = {2024},
  doi = {10.1109/ICIC64337.2024.10957519},
  url = {https://doi.org/10.1109/icic64337.2024.10957519},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{gupta_databricks_2024,
  title = {Databricks {Data},
  author = {Gupta, Nikhil and Yip, Jason},
  year = {2024},
  doi = {10.1007/979-8-8688-0444-1},
  url = {https://doi.org/10.1007/979-8-8688-0444-1},
  pages = {1 -- 473},
  note = {Type: Book},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{barron_domain-specific_2024,
  title = {Domain-{Specific},
  author = {Barron, Ryan C. and Grantcharov, Vesselin and Wanna, Selma and Eren, Maksim Ekin and Bhattarai, Manish and Solovyev, Nicholas and Tompkins, George H. and Nicholas, Charles K. and Rasmussen, Kim Ø. and Matuszek, Cynthia and Alexandrov, Boian S.},
  year = {2024},
  doi = {10.1109/ICMLA61862.2024.00258},
  url = {https://doi.org/10.1109/icmla61862.2024.00258},
  pages = {1669 -- 1676},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{lui_gptutor_2024,
  title = {{GPTutor},
  author = {Lui, Richard Wing Cheung and Bai, Haoran and Zhang, Aiden Wen Yi and Chu, Elvin Tsun Him},
  year = {2024},
  doi = {10.1109/AEECA62331.2024.00124},
  url = {https://doi.org/10.1109/aeeca62331.2024.00124},
  pages = {702 -- 707},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{shavaki_knowledge_2024,
  title = {Knowledge {Graph},
  author = {Shavaki, Mahdi Amiri and Omrani, Pouria and Toosi, Ramin and Akhaee, Mohammad Ali},
  year = {2024},
  doi = {10.1109/IKT65497.2024.10892619},
  url = {https://doi.org/10.1109/ikt65497.2024.10892619},
  pages = {78 -- 84},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{earley_what_2023,
  title = {What executives need to know about knowledge management, large language models and generative {AI},
  author = {Earley, Seth},
  year = {2023},
  doi = {10.69554/yqbv7690},
  url = {https://doi.org/10.69554/yqbv7690},
  journal = {Applied Marketing Analytics},
  volume = {9},
  number = {3},
  pages = {215 -- 229},
  note = {Type: Article},
  keywords = {source: Scopus, source: Google Scholar},
  abstract = {… in this paper is retrieval-augmented generation (RAG), which … The use of RAG virtually eliminated hallucinations, secured … entering the large language model (LLM)/ChatGPT market is …},
  annote = {Cited by: 10},
}

@inproceedings{shao_enhancing_2023,
  title = {Enhancing {Retrieval},
  author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  year = {2023},
  doi = {10.18653/v1/2023.findings-emnlp.620},
  url = {https://doi.org/10.18653/v1/2023.findings-emnlp.620},
  pages = {9248 -- 9274},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 71; All Open Access; Gold Open Access},
}

@article{anand_sciphyrag_2023,
  title = {{SciPhyRAG},
  author = {Anand, Avinash and Goel, Arnav and Hira, Medha and Buldeo, Snehal and Kumar, Jatin and Verma, Astha and Gupta, Rushali and Shah, Rajiv Ratn},
  year = {2023},
  doi = {10.1007/978-3-031-49601-1_4},
  url = {https://doi.org/10.1007/978-3-031-49601-1_4},
  journal = {Lecture Notes in Computer Science},
  volume = {14418 LNCS},
  pages = {50 -- 63},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 7},
}

@article{datta_great_2023,
  title = {{GREAT},
  author = {Datta, V. Dinesh and Ganesh, Sakthi and Haas, Roland E. and Talukder, Asoke K.},
  year = {2023},
  doi = {10.1007/978-3-031-49601-1_2},
  url = {https://doi.org/10.1007/978-3-031-49601-1_2},
  journal = {Lecture Notes in Computer Science},
  volume = {14418 LNCS},
  pages = {16 -- 33},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{jiang_active_2023,
  title = {Active {Retrieval},
  author = {Jiang, Zhengbao and Xu, Frank F. and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane A. and Yang, Yiming and Callan, Jamie P. and Neubig, Graham},
  year = {2023},
  doi = {10.18653/v1/2023.emnlp-main.495},
  url = {https://doi.org/10.18653/v1/2023.emnlp-main.495},
  pages = {7969 -- 7992},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 178; All Open Access; Gold Open Access},
}

@inproceedings{agrawal_scmrag_2025,
  title = {{SCMRAG},
  author = {Agrawal, Rishabh and Asrani, Murtaza and Youssef, Hadi and Narayan, Apurva},
  year = {2025},
  booktitle = {Proceedings of the 24th {International},
  pages = {50--58},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  note = {event-place: Detroit, MI, USA},
  keywords = {agents, knowledge graph, LLM, RAG, vectorstore},
  abstract = {Existing Retrieval-Augmented Generation (RAG) systems primarily depend on static knowledge vectorstores which combine semantic similarity algorithms with reranking. This often leads to outdated information and retrieval errors. In this paper, we propose SCMRAG, a Self-Corrective Multihop Retrieval Augmented Generation system for LLM agents. We introduce an LLM-assisted dynamic knowledge graph creation step to enhance information retrieval and mitigate hallucinations. Unlike traditional RAG systems, SCMRAG includes a self-corrective agent driven mechanism that autonomously identifies and retrieves missing information from external web sources. Furthermore, SCMRAG's internal reasoning agent determines whether the knowledge graph provides sufficient information or if a corrective step is needed. It further improves retrieval accuracy and efficiency. We benchmark the effectiveness of SCMRAG on five datasets - MultiHop-RAG, ARC AI2, PopQA, PubHealth, and WikiBio; showing significant improvements in retrieval precision and hallucination reduction across diverse tasks. Our results highlight SCMRAG's potential to redefine how LLM agents interact with knowledge bases, offering a more adaptable and reliable solution for a wide range of applications.},
  address = {Richland, SC},
  series = {{AAMAS},
  isbn = {979-8-4007-1426-9},
}

@inproceedings{dong_simulating_2025,
  title = {Simulating and {Evaluating},
  author = {Dong, Wen and Mohd-Zaid, Fairul},
  year = {2025},
  booktitle = {Proceedings of the 24th {International},
  pages = {639--648},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  note = {event-place: Detroit, MI, USA},
  keywords = {agent-based simulation, collaborative filtering, generative modeling, large language models (llms), online community behavior, social network analysis},
  abstract = {We introduce a multi-agent simulation framework for modeling large-scale online social dynamics by combining retrieval-augmented large language models, generative embedding methods, and collaborative filtering. Our approach learns diverse agent embeddings to capture varying user behaviors and employs a multi-layer perceptron for user-content ranking. We compare three strategies-(1) a generative modeling approach that integrates agent embeddings and collaborative filtering, (2) an LLM-based method grounded in historical context, and (3) a reflection-based clustering technique-and evaluate them on metrics such as comment volume, tree depth, user engagement patterns, and topic distribution. Results show that generative embeddings coupled with collaborative filtering better approximate complex phenomena like localized influencers, specialized subcommunities, and emergent echo chambers. Moreover, our framework supports policy-driven experimentation by incorporating social regularizers (cohesion, polarization, and bias) to simulate scenarios ranging from tightly knit communities to more balanced, cross-cutting interactions. By integrating large-scale data with adaptable LLM-driven agents, this work provides a versatile, data-centric foundation for simulating and analyzing online social ecosystems at scale.},
  address = {Richland, SC},
  series = {{AAMAS},
  isbn = {979-8-4007-1426-9},
}

@inproceedings{shyalika_smartpilotagent-based_2025,
  title = {{SmartPilot},
  author = {Shyalika, Chathurangi and Prasad, Renjith and Al Ghazo, Alaa and Eswaramoorthi, Darssan L. and Shree Muthuselvam, Sara and Sheth, Amit},
  year = {2025},
  booktitle = {Proceedings of the 24th {International},
  pages = {3053--3055},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  note = {event-place: Detroit, MI, USA},
  keywords = {agent-based architecture, copilot, multi-modality, smart manufacturing},
  abstract = {In the dynamic landscape of Industry 4.0, achieving efficiency, precision, and adaptability is essential for optimizing manufacturing operations. SmartPilot is a neurosymbolic and agent-based CoPilot designed to enhance real-time decision-making capabilities in manufacturing. The system addresses three key challenges: anomaly prediction, production forecasting, and domain-specific question answering through an agent-based framework. SmartPilot leverages multimodal data and a compact architecture optimized for edge devices. This paper highlights its innovative combination of agent-based design and neurosymbolic reasoning to enable contextual decision-making in complex environments. The demonstration video, datasets, and supplementary materials are available at https://github.com/ChathurangiShyalika/SmartPilot.},
  address = {Richland, SC},
  series = {{AAMAS},
  isbn = {979-8-4007-1426-9},
}

@incollection{ots_securing_2025,
  title = {Securing the {Entire},
  author = {Ots, Karl},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10955782},
  booktitle = {Securing {Microsoft},
  pages = {125--296},
  publisher = {Wiley},
  keywords = {Databases, Grounding, Organizations, Retrieval augmented generation, Security, Threat modeling, Training, Training data, Tuning, Virtual machines, source: IEEE},
  abstract = {Summary {\textless},
  isbn = {978-1-394-29111-3},
}

@article{agrawal_scmrag_2025-1,
  title = {Scmrag: {Self},
  author = {Agrawal, R. and Asrani, M. and Youssef, H. and {...},
  year = {2025},
  url = {https://www.ifaamas.org/Proceedings/aamas2025/pdfs/p50.pdf},
  journal = {Proceedings of the 24th …},
  note = {Publisher: ifaamas.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… For each factual chunk, we prompt a large language model (LLM) to generate a ‘claim’. A ‘claim’ is defined as a statement or assertion that expresses a belief, opinion, or fact, devoid of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liang_saferag_2025,
  title = {Saferag: {Benchmarking},
  author = {Liang, X. and Niu, S. and Li, Z. and Zhang, S. and Wang, H. and Xiong, F. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.18636},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG … attack scenarios that RAG may encounter. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{thakur_support_2025,
  title = {Support {Evaluation},
  author = {Thakur, N. and Pradeep, R. and Upadhyay, S. and Campos, D. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.15205},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In the TREC 2024 RAG Track, we allowed participants to provide citations for up to 20 passages per answer sentence. To judge each sentence and its cited passage, our protocol …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ammann_securing_2025,
  title = {Securing rag: {A},
  author = {Ammann, L. and Ott, S. and Landolt, C. R. and Lehmann, M. P.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.08728},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Furthermore, the growing system complexity and the fast development of the LLM/RAG … M6: Evaluation improve the factual accuracy and reliability of RAG systems by identifying and (self…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sun_symbioticrag_2025,
  title = {Symbioticrag: {Enhancing},
  author = {Sun, Q. and Bi, T. and Li, S. and Holden, E. J. and Duuring, P. and Niu, K. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.02418},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… reimagines RetrievalAugmented Generation (RAG) systems by … two critical challenges in current RAG systems: the inherently … Retrieval-Augmented Generation (RAG) LLM hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{reza_small_2025,
  title = {Small {Models},
  author = {Reza, Z. and Mazur, A. and Dugdale, M. T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2506.05925},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG and an LLM-based verifier layer, improves both factual … susceptible to jailbreaking and hallucination, where prompt … an additional small (3B) LLM acting as a dedicated response …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ngangmeni_swamped_2025,
  title = {Swamped with {Too},
  author = {Ngangmeni, J. and Rawat, D. B.},
  year = {2025},
  url = {https://www.mdpi.com/2673-2688/6/3/47},
  journal = {AI},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… using two sets of Large Language Model (LLM)-generated … Additionally, the use of randomization gives more confidence … desirable for testing RAG performance than LLM-generated …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_scrag_2025,
  title = {{scRAG},
  author = {Yu, Z. and Zheng, C. and Chen, C. and Hua, X. S. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.findings-acl.53/},
  journal = {Findings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… In this paper, we introduce scRAG, a novel framework that incorporates advanced LLM-based RAG techniques into cross-tissue single-cell annotation. scRAG utilizes LLMs to retrieve …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_secure_2025,
  title = {Secure multi-llm agentic ai and agentification for edge general intelligence by zero-trust: {A},
  author = {Liu, Y. and Zhang, R. and Luo, H. and Lin, Y. and Sun, G. and Niyato, D. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2508.19870},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… [93] presented ControlNet, an AI firewall specifically designed for Retrieval-Augmented Generation (RAG)-based LLM systems. Leveraging neuron activation shift phenomena, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_scholarcopilot_2025-1,
  title = {Scholarcopilot: {Training},
  author = {Wang, Y. and Ma, X. and Nie, P. and Zeng, H. and Lyu, Z. and Zhang, Y. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.00824},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval Augmented Generation In the era of LLM, the Retrieval Augmented Generation (RAG… , improving the generation’s correctness and factuality for downstream tasks, such as …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{ok_synthetic_2025,
  title = {Synthetic {Paths},
  author = {Ok, Changwon and Lee, Eunkyeong and Oh, Dongsuk},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218487989&partnerID=40&md5=fc22c4c157b3a0ac8410dfe7bbb10ad5},
  booktitle = {Proceedings - {International},
  pages = {5168 -- 5180},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{mahalingam_sketch_2025,
  title = {{SKETCH},
  author = {Mahalingam, Aakash and Gande, Vinesh Kumar and Chadha, Aman and Jain, Vinija and Chaudhary, Divya},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217655055&partnerID=40&md5=015ddadf83a272871c9975c5472e8ae5},
  booktitle = {Proceedings - {International},
  volume = {2025-January},
  pages = {27 -- 42},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{li_simple_2025,
  title = {{SIMPLE},
  author = {Li, Mufei and Miao, Siqi and Li, Pan},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010268304&partnerID=40&md5=0eef3b7f8b11e7342d7af4dc989bf464},
  pages = {49797 -- 49825},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{joren_sufficient_2025,
  title = {{SUFFICIENT},
  author = {Joren, Hailey and Zhang, Jianyi and Ferng, Chunsung and Juan, Dacheng and Taly, Ankur and Rashtchian, Cyrus},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010207544&partnerID=40&md5=fc42ed87f6ede2af09ed8772ddb2f52c},
  pages = {48839 -- 48863},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{chen_seeing_2025,
  title = {Seeing {Beyond},
  author = {Chen, Boqi and Khare, Anuj and Kumar, Gaurav and Akula, Arjun R. and Narayana, Pradyumna},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000121798&partnerID=40&md5=f2804e18d424fc3ca20e20f4ea03bc6b},
  booktitle = {Proceedings - {International},
  pages = {410 -- 421},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{martinez_enhancing_2025,
  title = {Enhancing {GPT},
  author = {Martínez, Joseph and Llinas, Brian and Botello, Jhon G. and Padilla, Jose J. and Frydenlund, Erika},
  year = {2025},
  booktitle = {Proceedings of the {Winter},
  pages = {666--677},
  publisher = {IEEE Press},
  keywords = {source: ACM},
  abstract = {Recognizing the limited research on Large Language Models (LLMs) capabilities with low-resource languages, this study evaluates and increases the proficiency of the LLM GPT-3.5 in generating interface and procedural code elements for NetLogo, a multi-agent programming language and modeling environment. To achieve this, we employed "few-shot" prompting and Retrieval-Augmented Generation (RAG) methodologies using two manually created datasets, NetLogoEvalCode and NetLogoEvalInterface. The results demonstrate that GPT-3.5 can generate NetLogo elements and code procedures more effectively when provided with additional examples to learn from, highlighting the potential of LLMs in aiding the development of agent-based models (ABMs). On the other hand, the RAG model obtained a poor performance. We listed possible reasons for this result, which were aligned with RAG's common challenges identified by the state-of-the-art. We propose future research directions for leveraging LLMs for simulation development and instructional purposes in the context of ABMs.},
  address = {Orlando, Florida, USA},
  isbn = {979-8-3315-3420-2},
  series = {{WSC},
}

@inproceedings{qin_eva_2025,
  title = {Eva: {An},
  author = {Qin, Zhiwei (Tony) and Zhou, Jianming},
  year = {2025},
  booktitle = {Proceedings of the 24th {International},
  pages = {3035--3037},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  note = {event-place: Detroit, MI, USA},
  keywords = {llm, multi-agent, multilingual, restaurant operations, voice ai},
  abstract = {Eva is a voice AI agentic system automating restaurant phone operations with individual agents for tasks like order placement and a global agent for multi-restaurant settings. It uses a hierarchical multi-agent architecture with various agent technologies, demonstrating LLM applications for improved efficiency and service in the restaurant industry.},
  address = {Richland, SC},
  series = {{AAMAS},
  isbn = {979-8-4007-1426-9},
}

@article{amirshahi_evaluating_2025,
  title = {Evaluating the {Robustness},
  author = {Amirshahi, S. and Bigdeli, A. and Clarke, C. L. A. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2509.03787},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval augmented generation (RAG) systems provide a method for factually grounding the responses of a Large Language Model (LLM… , RAG systems can reduce hallucinations and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{abolghasemi_evaluation_2025,
  title = {Evaluation of attribution bias in generator-aware retrieval-augmented large language models},
  author = {Abolghasemi, A. and Azzopardi, L. and Hashemi, S. H. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.findings-acl.1087/},
  journal = {Findings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… in RAG pipelines, namely attribution sensitivity and bias with respect to authorship information. We explicitly inform an LLM … number of relevant citations and total citations for the three …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_enhancing_2025,
  title = {Enhancing {Critical},
  author = {Zhu, X. and Chang, S. and Kuik, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.16883},
  journal = {arXiv preprint arXiv:2504.16883},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM… influence reasoning and trust in RAG-based educational settings, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hindi_enhancing_2025-1,
  title = {Enhancing the precision and interpretability of retrieval-augmented generation (rag) in legal technology: {A},
  author = {Hindi, M. and Mohammed, L. and Maaz, O. and Alwarafy, A.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10921633/},
  journal = {IEEE Access},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… stages of the RAG process. However, robust RAG can enhance LLM generation with faithfulness and few hallucinations in responses. In this paper, we discuss the application of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_enhancing_2025-1,
  title = {Enhancing retrieval-augmented generation: a study of best practices},
  author = {Li, S. and Stenzel, L. and Eickhoff, C. and Bahrainian, S. A.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.07391},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM Size: As the generative LLM in our RAG system, we compare the MistralAI 7B instruction … (5) In terms of factuality (Table 3), we observe similar patterns: Contrastive InContext …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gumaan_expertrag_2025,
  title = {{ExpertRAG},
  author = {Gumaan, E.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.08744},
  journal = {arXiv preprint arXiv:2504.08744},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Below, we detail the key components and cite their connections to existing methods. … RAG was shown to improve factuality in generation [22]; we’ll see if ExpertRAG can not only find …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wei_enhanced_2025,
  title = {Enhanced recommendation systems with retrieval-augmented large language model},
  author = {Wei, C. and Duan, K. and Zhuo, S. and Wang, H. and Huang, S. and {...},
  year = {2025},
  url = {https://www.jair.org/index.php/jair/article/view/17809},
  journal = {Journal of Artificial …},
  note = {Publisher: jair.org},
  keywords = {source: Google Scholar},
  abstract = {… that integrates RAG with LLMs to supplement missing data. By leveraging RAG techniques, … • Confidence Threshold θ: The confidence threshold θ determines the filtering quality of the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pelletier_evidence-based_2025-1,
  title = {Evidence-based knowledge synthesis and hypothesis validation: {Navigating},
  author = {Pelletier, A. R. and Ramirez, J. and Sankar, B. S. and Adam, I. and {...},
  year = {2025},
  url = {https://app.jove.com/t/67525/evidence-based-knowledge-synthesis-hypothesis-validation-navigating},
  journal = {Journal of Visualized …},
  note = {Publisher: app.jove.com},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) is a system designed to minimize LLM hallucinations, grounding LLM … LLM (eg, ChatGPT) with PubMed, allowing for the identification of relevant …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{freiberger_explainable_2025,
  title = {Explainable {AI},
  author = {Freiberger, V. and Fleig, A. and Buchmann, E.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.12931},
  journal = {arXiv preprint arXiv:2504.12931},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… retrieval-augmented generation (RAG). We identify a need for adaptive explanation strategies tailored to different user profiles for LLM-… investigate LLM explanations and hallucinations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kamra_evaluating_2025-1,
  title = {Evaluating reinforcement learning based models for test time enhancement in rag},
  author = {Kamra, V. and Gupta, L. and Arora, D. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11086322/},
  journal = {2025 Second International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… context for LLMbased generation. The result is a retrieval-augmented system that not only improves factual accuracy and retrieval performance but also significantly reduces the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{guo_empowering_2025,
  title = {Empowering graphrag with knowledge filtering and integration},
  author = {Guo, K. and Shomer, H. and Zeng, S. and Han, H. and Wang, Y. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.13804},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… retrieval-augmented generation (GraphRAG) enhances LLM … between LLM with GraphRAG and LLM w/o GraphRAG (ie, LLM-… When do we trust the answers given by GraphRAG and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_explainable_2025,
  title = {Explainable {Depression},
  author = {Zhang, L. and Gao, Z. and Zhou, D. and He, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.01315},
  journal = {arXiv preprint arXiv:2503.01315},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… -hoc LLM generation but suffer from hallucination. To address these limitations, we propose RED, a Retrievalaugmented generation … Additionally, to enhance LLM performance in social …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{indebetou_enhancing_2025,
  title = {Enhancing {Troubleshooting},
  author = {Indebetou, E. and Larsson, E.},
  year = {2025},
  url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2:1967184},
  publisher = {diva-portal.org},
  keywords = {source: Google Scholar},
  abstract = {… These findings directly inform and validate the approach taken in this thesis: leveraging a RAG pipeline to enhance accuracy, reduce hallucinations, and improve the reliability of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_evaluating_2025,
  title = {Evaluating {ChatGPT},
  author = {Yu, Y. and Kim, S. and Lee, W. and Koo, B.},
  year = {2025},
  url = {https://academic.oup.com/jcde/article-abstract/12/4/94/8090188},
  journal = {Journal of Computational Design …},
  note = {Publisher: academic.oup.com},
  keywords = {source: Google Scholar},
  abstract = {… could be improved and thus deploy ChatGPT with higher confidence. … how RAG was used with the goal of improving ChatGPT's response exclusively to this subcategory. As ChatGPT-4 …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{galitsky_enhancing_2025,
  title = {Enhancing {RAG},
  author = {Galitsky, B. and Ilvovsky, D. and Morkovkin, A.},
  year = {2025},
  url = {https://dialogue-conf.org/wp-content/uploads/2025/04/GalitskyBIlvovskyDMorkovkinA.110.pdf},
  journal = {Proceedings of the …},
  note = {Publisher: dialogue-conf.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval Augmented Generation (RAG) architectures to address a lack of specific information and hallucination issues of Large Language Models (LLM)… on top of LLM and maintains a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_efficient_2025,
  title = {Efficient {Dynamic},
  author = {Li, W. and Liu, K. and Zhang, X. and Lei, X. and Ma, W. and Liu, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.03165},
  journal = {arXiv preprint arXiv:2504.03165},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factual conflicts and duplicate content for the LLMs to handle. As an advanced approach, Graphbased RAG … step that leverages a large language model (LLM) to generate concise yet …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_each_2025,
  title = {Each to their own: {Exploring},
  author = {Chen, S. and Zhao, Z. and Chen, J.},
  year = {2025},
  url = {https://arxiv.org/abs/2507.17442},
  journal = {arXiv preprint arXiv:2507.17442},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG methods: MixtureEmbedding RAG and Confident RAG. … RAG performs similarly to vanilla RAG, the Confident RAG … for the LLM, and (2) Confident RAG, which employs vanilla RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yao_elevating_2025,
  title = {Elevating legal {LLM},
  author = {Yao, R. and Wu, Y. and Wang, C. and Xiong, J. and Wang, F. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.07912},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… to hallucination, providing answers that appear correct but are unreliable. RetrievalAugmented Generation (RAG) … Compared with the LLaMA-3-8B w/o RAG, our proposed LSIM model …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kadam_enhancing_2025,
  title = {Enhancing {Comprehension},
  author = {Kadam, A. J. and Kute, P. R. and Kale, C. A. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10915367/},
  journal = {… on Intelligent and …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… When synthesizing the proposed LLMs, TTS, and RAG, the system can provide clear … related to LLM hallucinations and factuality errors that means, provide a more credible and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ryan_enronqa_2025,
  title = {Enronqa: {Towards},
  author = {Ryan, M. J. and Xu, D. and Nivera, C. and Campos, D.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.00263},
  journal = {arXiv preprint arXiv:2505.00263},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM’… In our case study we explore factual memorization as an alternative …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{soule_enhancing_2025,
  title = {Enhancing {LLM},
  author = {Soule, R.},
  year = {2025},
  url = {https://digitalcommons.odu.edu/gradresearch_achievementday/2025/engineering/14/},
  publisher = {digitalcommons.odu.edu},
  note = {Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… in the LLM. This approach minimizes "hallucinations," where LLMs generate plausible but incorrect information by grounding responses in retrieved facts. Additionally, RAG enables AI …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{chen_exploring_2025,
  title = {Exploring the {Role},
  author = {Chen, Yingjian and Li, Feiyang and Song, Xingyu and Li, Tianxiao and Xu, Zixin and Chen, Xiujie and Sukeda, Issey and Li, Irene R.},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010688471&partnerID=40&md5=c4d8bce0f477087e3b5092182476c098},
  booktitle = {{CEUR},
  volume = {3985},
  pages = {19 -- 28},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{crista_chat4elderly_2025,
  title = {{Chat4Elderly},
  author = {Crista, Vítor and Martinho, Diogo and Marreiros, Goreti},
  year = {2025},
  booktitle = {Proceedings of the 24th {International},
  pages = {3041--3043},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  note = {event-place: Detroit, MI, USA},
  keywords = {conversation system, generative ai, multi-agent systems, wearable technology},
  abstract = {This demo presents Chat4Elderly, a Multi-Agent System (MAS) designed to support the well-being of elderly users through personalized and proactive interactions. The system combines Large Language Models (LLMs) with smartwatch sensor data to provide real-time, context-aware responses that address both mental and physical health needs. By analyzing conversational patterns and physical activity levels, it adapts to user preferences, offering personalized assistance and engagement. Over time, the system improves its interactions using stored knowledge, increasing personalization and supporting long-term well-being. This approach helps reduce loneliness and enhance quality of life (QoL), creating a more supportive and engaging experience for elderly users.},
  address = {Richland, SC},
  series = {{AAMAS},
  isbn = {979-8-4007-1426-9},
}

@article{woo_custom_2025,
  title = {Custom large language models improve accuracy: comparing retrieval augmented generation and artificial intelligence agents to noncustom models for evidence …},
  author = {Woo, J. J. and Yang, A. J. and Olsen, R. J. and Hasan, S. S. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0749806324008831},
  journal = {… : The Journal of …},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… RAG improved accuracy by an average of 39.7\%, with the highest accuracy rate of 94\% in … RAG-augmented LLM improved ChatGPT4 accuracy rate to 95\%. Thus, Agentic and RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{brehme_can_2025,
  title = {Can {LLMs},
  author = {Brehme, L. and Ströhle, T. and Breu, R.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.20119},
  journal = {arXiv preprint arXiv:2504.20119},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dong_chatiot_2025,
  title = {Chatiot: {Large},
  author = {Dong, Y. and Aung, Y. L. and Chattopadhyay, S. and Zhou, J.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.09896},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… [13] proposed utilizing ChatGPT to enhance IoT trust semantics, aligning with W3C Web of Things (WoT) recommendations16. This work extends the TrUStAPIS framework [52]. Beyond …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jeon_chatcnc_2025,
  title = {{ChatCNC},
  author = {Jeon, J. and Sim, Y. and Lee, H. and Han, C. and Yun, D. and Kim, E. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0278612525000263},
  journal = {Journal of Manufacturing …},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… Leveraging LLM-based multi-agent collaboration and Retrieval-Augmented Generation (RAG… As ChatCNC allows rapid adaptation of LLM Application Programming Interfaces (APIs) via …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{deng_cram_2025-1,
  title = {Cram: {Credibility},
  author = {Deng, B. and Wang, W. and Zhu, F. and Wang, Q. and Feng, F.},
  year = {2025},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/34547},
  journal = {Proceedings of the AAAI …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… In the following, we analyze the effect of varying the number of low-credibility documents fed into the LLM. We conduct experiments using Llama3-8B on the NQ dataset. Specifically, we …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ding_citations_2025,
  title = {Citations and trust in llm generated responses},
  author = {Ding, Y. and Facciani, M. and Joyce, E. and Poudel, A. and {...},
  year = {2025},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/34550},
  journal = {Proceedings of the …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… The widespread adoption of LLMs has led to the development of Retrieval Augmented Generation (RAG) systems. These systems generate explanations by incorporating external …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_credible_2025,
  title = {Credible plan-driven rag method for multi-hop question answering},
  author = {Zhang, N. and Zhang, C. and Tan, Z. and Yang, X. and Deng, W. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.16787},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… For example, RAP [39] establishes a dual-role framework in which a large language model (LLM) functions as a world model and a reasoning agent. During the reasoning process, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{maheshwari_citefix_2025,
  title = {{CiteFix},
  author = {Maheshwari, H. and Tenneti, S. and Nakkiran, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.15629},
  journal = {arXiv preprint arXiv:2504.15629},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lin_concept-based_2025,
  title = {Concept-{Based},
  author = {Lin, C. Y. and Jang, J. S. R.},
  year = {2025},
  url = {https://aclanthology.org/2025.finnlp-1.8/},
  journal = {Proceedings of the Joint Workshop of the 9th …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… Unlike traditional methods focused on reducing LLM hallucinations or modifying data structures, this approach evaluates inherent knowledge uncertainty from an LLM perspective. By …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{landolsi_caprag_2025,
  title = {{CAPRAG},
  author = {Landolsi, H. and Letaief, K. and Taghouti, N. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.13993},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM to be more deterministic and not allowing it to be creative.In our use case, the LLM sticks to the provided context and reduces the hallucination as much as possible, see Table 3. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_cold-start_2025,
  title = {Cold-{Start},
  author = {Yang, W. and Zhang, W. and Liu, Y. and Han, Y. and Wang, Y. and Lee, J. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.20773},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… with sparse metadata and limited or hallucinated knowledge. We introduce ColdRAG, a retrieval-augmented generation framework that equips an LLM with a dynamically built, domain…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ateia_can_2025,
  title = {Can language models critique themselves? {Investigating},
  author = {Ateia, S. and Kruschwitz, U.},
  year = {2025},
  url = {https://arxiv.org/abs/2508.05366},
  journal = {arXiv preprint arXiv:2508.05366},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… offers insights into LLM self-correction and informs future work on comparing the effectiveness of LLM-… However, challenges such as LLM hallucinations and the need to align with expert …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{khadilkar_causal-counterfactual_2025,
  title = {Causal-{Counterfactual},
  author = {Khadilkar, H. and Gupta, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2509.14435},
  journal = {arXiv preprint arXiv:2509.14435},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This graph is populated by a powerful LLM that analyzes a … of fast vector search and LLM-based verification to ensure semantic … A synthesis LLM then analyzes both the factual evidence …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{parekh_cc-rag_2025,
  title = {{CC},
  author = {Parekh, J. R. and Jiang, P. and Han, J.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.08364},
  journal = {arXiv preprint arXiv:2506.08364},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… 2024) improves LLM factuality by retrieving … -RAG outperforms standard RAG and zero-shot LLM baselines both in terms of quantitative metrics and qualitative assessments using LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yao_controlnet_2025,
  title = {Controlnet: {A},
  author = {Yao, H. and Shi, H. and Chen, Y. and Jiang, Y. and Wang, C. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.09593},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… By incorporating external knowledge retrieval, RAG mitigates the hallucination problem … A typical RAG-based LLM system operates by processing user queries through a structured …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{song_chain--thought_2025,
  title = {Chain-of-{Thought},
  author = {Song, H. and Liu, Y. and Zhang, R. and Guo, J. and Fan, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.16367},
  journal = {arXiv preprint arXiv:2505.16367},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… bases, RAG systems can enhance the factual accuracy and reliability of LLM outputs. … To investigate the relationship between the size of the underlying large language model in RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{papagiannopoulos_comparison_2025,
  title = {Comparison of explainability methods for hallucination analysis in {LLMs},
  author = {Papagiannopoulos, I. and {...},
  year = {2025},
  url = {https://open-research-europe.ec.europa.eu/articles/5-191},
  journal = {Open …},
  note = {Publisher: open-research-europe.ec.europa.eu
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… (LLM) hallucinations, which produce fluent yet factually incorrect or illogical outputs. This paper investigates hallucinations as … tools including Retrieval-Augmented Generation (RAG), …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{muhamed_ccrs_2025,
  title = {{CCRS},
  author = {Muhamed, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.20128},
  journal = {arXiv preprint arXiv:2506.20128},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factual accuracy and upto-date information. However, evaluating the multifaceted quality of RAG outputs — spanning aspects such as contextual coherence, query relevance, factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shen_citelab_2025,
  title = {{CiteLab},
  author = {Shen, J. and Zhou, T. and Chen, Y. and Liu, K. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.acl-demo.47/},
  journal = {Proceedings of the 63rd …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… Prompt self-RAG As for Llama3-8B and GPT-4o, there is no trained version for self-RAG, we use prompt to make the LLM retrieve documents and generate, then use an NLI model to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zeng_cite_2025,
  title = {Cite before you speak: {Enhancing},
  author = {Zeng, J. and Liu, H. and Dai, Z. and Tang, X. and Luo, C. and Varshney, S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.04830},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , suggesting that citation generation paradigm substantially … , which appends source citations to LLM outputs while preserving … To diagnose the intrinsic deficiency of LLM used in RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sharifymoghaddam_chatbot_2025,
  title = {Chatbot {Arena},
  author = {Sharifymoghaddam, S. and Upadhyay, S. and Thakur, N. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.20006},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the quality of RAG answers. Nuggets decompose long-form LLM-generated answers into … documents, which are then used by LLMs to generate long-form answers with citations [22…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tan_chatgpt_2025,
  title = {{ChatGPT},
  author = {Tan, J. R. and Lim, D. Y. Z. and Le, Q. and Karande, G. Y. and Chan, L. P. and {...},
  year = {2025},
  url = {https://www.nature.com/articles/s41598-025-88925-1},
  journal = {Scientific Reports},
  note = {Publisher: nature.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… cLLM, and a standard LLM without RAG. We also aimed to compare LLM performance against … , which provides better transparency and confidence in the LLM output. Furthermore, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{baek_crafting_2025,
  title = {Crafting the path: {Robust},
  author = {Baek, I. and Lee, J. and Yang, J. and Lee, H.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10870252/},
  journal = {IEEE Access},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… generates queries with fewer factual inaccuracies. Furthermore, we … in the retrieval-augmented generation scenarios. … of each LLM, we divide the dataset into problems where each LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_commercial_2025,
  title = {Commercial llm agents are already vulnerable to simple yet dangerous attacks},
  author = {Li, A. and Zhou, Y. and Raghuram, V. C. and Goldstein, T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.08586},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… agents may make decisions based on falsified information provided by previously trusted … vulnerabilities in LLM agents, especially in memory and retrieval-augmented generation (RAG)…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kumari_can_2025,
  title = {Can {LLMs},
  author = {Kumari, M. and Chauhan, R. and Garg, P.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0920548925000261},
  journal = {Computer Standards \&Interfaces},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… and integrated a Retrieval-Augmented Generation (RAG) pipeline to enhance performance. The results revealed that fine-tuned LLaMA-2 models, particularly those incorporating RAG, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{wilkerson_case_2025,
  title = {Case {Hallucinations},
  author = {Wilkerson, Kaitlynne and Leake, David B.},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011087821&partnerID=40&md5=923f213fc1c02773e19136dd1bd75652},
  booktitle = {{CEUR},
  volume = {3993},
  pages = {43 -- 54},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{hedi_personalized_2025,
  title = {Personalized {Language},
  author = {Hedi, Tebourbi and Nouzri, Sana and Mualla, Yazan and Najjar, Amro},
  year = {2025},
  booktitle = {Proceedings of the 24th {International},
  pages = {3032--3034},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  note = {event-place: Detroit, MI, USA},
  keywords = {language learning, llms, mas, personalized learning, rag},
  abstract = {The integration of Artificial Intelligence (AI) into education is transforming language learning. Current chatbot-based tools primarily focus on vocabulary acquisition and conversation, overlooking the holistic needs of effective language learning, such as grammar, reading, and listening skills. These limitations are further compounded by the challenges of low-resource languages like Luxembourgish. This demonstration https://www.youtube.com/watch?v=5bxVHsuK-Hs presents a Multi-Agent System (MAS) powered by Large Language Models (LLMs), integrated with Retrieval-Augmented Generation (RAG) to address these challenges. Our system personalizes learning by employing specialized agents for specialized tasks, ensuring a comprehensive and adaptive experience. To mitigate inaccuracies, human-on-the-loop (here teacher) validation enhances content quality and aligns with pedagogical standards inspired by the National Institute of Languages of Luxembourg (INL). Attendees will experience interactive demonstrations showcasing how the system delivers tailored educational experiences through innovative agent workflows and user-centric design.},
  address = {Richland, SC},
  series = {{AAMAS},
  isbn = {979-8-4007-1426-9},
}

@inproceedings{gebelli_personalised_2025,
  title = {Personalised {Explainable},
  author = {Gebellí, Ferran and Hriscu, Lavinia and Ros, Raquel and Lemaignan, Séverin and Sanfeliu, Alberto and Garrell, Anaís},
  year = {2025},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1304--1308},
  publisher = {IEEE Press},
  keywords = {explainability, llm, personalisation, xhri},
  abstract = {In the field of Human-Robot Interaction (HRI), a key challenge lies in enabling humans to comprehend the decisions and behaviours of robots. One promising approach involves leveraging Theory of Mind (ToM) frameworks, wherein a robot estimates the mental model that a user holds about its functioning and compares this with the representation of its internal mental model. This comparison allows the robot to identify potential mismatches and generate communicative actions to bridge such gaps. Effective communication requires the robot to maintain unique mental models for each user and personalise explanations based on past interactions. To address this, we propose an architecture grounded in Large Language Models (LLMs) that operationalises this theoretical framework. We demonstrate the feasibility of this approach through qualitative examples, showcasing responses provided by a robot patrolling a geriatric hospital.},
  address = {Melbourne, Australia},
  series = {{HRI},
}

@article{zou_poisonedrag_2025,
  title = {\{{PoisonedRAG},
  author = {Zou, W. and Geng, R. and Wang, B. and Jia, J.},
  year = {2025},
  url = {https://www.usenix.org/conference/usenixsecurity25/presentation/zou-poisonedrag},
  journal = {34th USENIX Security Symposium …},
  note = {Publisher: usenix.org},
  keywords = {source: Google Scholar},
  abstract = {… database of a RAG system to induce an LLM to generate an … RAG enables an LLM to utilize external knowledge in a plug-and-play manner. Moreover, RAG can reduce hallucinations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cohn_personalizing_2025,
  title = {Personalizing {Student},
  author = {Cohn, C. and Rayala, S. and Snyder, C. and Fonteles, J. and Jain, S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.17238},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations can undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM … log-contextualized RAG (LC-RAG), which enhances RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{finlayson_post-training_2025,
  title = {Post-training an {LLM},
  author = {Finlayson, M. and Kulikov, I. and Bikel, D. M. and Oguz, B. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.10596},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM RAG performance by fine-tuning on retrievalaugmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations… for training RAG-enabled …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hu_position_2025,
  title = {Position: {Towards},
  author = {Hu, J. and Dong, Y. and Ao, S. and Li, Z. and Wang, B. and Singh, L. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.01714},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… and Large Language Modelpowered Multi-Agent Systems (LLM… and Retrieval-Augmented Generation have expanded LLM … on confidence levels and uses textual prompts to convey …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hu_patchwork_2025,
  title = {Patchwork: {A},
  author = {Hu, B. and Pabon, L. and Agarwal, S. and Akella, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.07833},
  journal = {arXiv preprint arXiv:2505.07833},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… As illustrated, a RAG system supplements an LLM with a knowledge base—often an … both the factual accuracy and generation quality of LLM outputs. Moreover, RAG systems offer a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sun_pankb_2025,
  title = {{PanKB},
  author = {Sun, B. and Pashkova, L. and Pieters, P. A. and Harke, A. S. and {...},
  year = {2025},
  url = {https://academic.oup.com/nar/article-abstract/53/D1/D806/7906839},
  journal = {Nucleic Acids …},
  note = {Publisher: academic.oup.com},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations (40–42). Modern databases can benefit from the integration of a RAG-LLM … -access pangenomics articles and integrates a RAG-enhanced LLM interface (AI Assistant). …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{basit_pennylang_2025,
  title = {Pennylang: {Pioneering},
  author = {Basit, A. and Innan, N. and Asif, M. H. and Shao, M. and Kashif, M. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.02497},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… curated to train/fine-tune LLM-based quantum code assistance. Our key … LLM training efficiency; and (3) a thorough evaluation, based on a Retrieval-Augmented Generation (RAG) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_perspectives_2025,
  title = {Perspectives: {LLM},
  author = {Wu, S. and Shi, C. and Leung, A. and Otake, Y. and Konishi, C. and Zhou, M. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S3050483X25000358},
  journal = {Geodata and AI},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {This paper explores the transformative potential of Large Language Model (LLM)-based agentic artificial intelligence (AI) in addressing longstanding challenges in geotechnical …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_pseudo-knowledge_2025,
  title = {Pseudo-{Knowledge},
  author = {Yang, Y. and Wu, H. and Wang, T. and Yang, J. and Ma, H. and Luo, G.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.00309},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , RAG enhances factual … LLM-VDB (LLM with Vector Database RAG). In this setup, we enhance the language model’s capabilities by integrating a retrievalaugmented generation (RAG) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{noauthor_proceedings_2025,
  title = {Proceedings of the 2025 {AAAI},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016685791&partnerID=40&md5=e8f537ee19d42f3b15b8d16a3addaaca},
  volume = {5},
  number = {1},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_proceedings_2025-1,
  title = {Proceedings - 19th {IEEE},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016119442&partnerID=40&md5=bd352e483669b71ad5f8068f01ef1017},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_proceedings_2025-2,
  title = {Proceedings - 2025 {IEEE},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009592210&partnerID=40&md5=5dfbbbe5607ba331b9dd25ad2da5a7f4},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{wolfe_implications_2025,
  title = {The {Implications},
  author = {Wolfe, Robert and Mitra, Tanushree},
  year = {2025},
  booktitle = {Proceedings of the 2024 {AAAI},
  pages = {1595--1607},
  publisher = {AAAI Press},
  keywords = {source: ACM},
  abstract = {Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.},
  address = {San Jose, California, USA},
  series = {{AIES},
}

@inproceedings{jokinen_towards_2025,
  title = {Towards {Domain},
  author = {Jokinen, Kristiina and Wilcock, Graham},
  year = {2025},
  booktitle = {Proceedings of the 2025 {ACM},
  pages = {1373--1377},
  publisher = {IEEE Press},
  keywords = {conversational grounding, human-robot dialogues, knowledge graphs, sustainability},
  abstract = {Knowledge graphs have been used to improve robot dialogues by providing more sophisticated world knowledge. We now propose a new role for knowledge graphs in GenAI-based HRI that aims to reduce dialogue errors by better conversational grounding. This approach uses both domain knowledge graphs and dialogue history graphs, constructing shared knowledge via entity linking. We present first steps towards these aims, and also address sustainability by supporting the use of smaller models.},
  address = {Melbourne, Australia},
  series = {{HRI},
}

@article{das_two-layer_2025-1,
  title = {Two-layer retrieval-augmented generation framework for low-resource medical question answering using reddit data: proof-of-concept study},
  author = {Das, S. and Ge, Y. and Guo, Y. and Rajwal, S. and Hairston, J. M. and {...},
  year = {2025},
  url = {https://www.jmir.org/2025/1/e66220/},
  journal = {Journal of Medical …},
  note = {Publisher: jmir.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… the performance of a quantized large language model (Nous-… of relevance, length, hallucination, coverage, and coherence … issue with LLM-generated text for MQA is “hallucination”: …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{leschanowsky_transparent_2025,
  title = {Transparent nlp: {Using},
  author = {Leschanowsky, A. and Kolagar, Z. and Çano, E. and Habernal, I. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.06652},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, because RAG systems are built on … LLM hallucination when providing such generated answers to data processing questions. This is a well-known problem of RAG and LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{martinon_towards_2025,
  title = {Towards a rigorous evaluation of {RAG},
  author = {Martinon, G. and Brionne, AL de and Bohard, J. and Lojou, A. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2507.21753},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This study evaluates a RAG system used in due diligence for an investment fund. We … and LLM-Judge annotations to identify system failures, like hallucinations, off-topic, failed citations, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dhole_retrieve_2025,
  title = {To retrieve or not to retrieve? uncertainty detection for dynamic retrieval augmented generation},
  author = {Dhole, K. D.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.09292},
  journal = {arXiv preprint arXiv:2501.09292},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Unlike traditional methods that rely on rigid heuristics or external classifiers, uncertainty detection leverages the inherent variability in LLM-generated responses to estimate confidence …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ni_tp-rag_2025,
  title = {{TP},
  author = {Ni, H. and Liu, F. and Ma, X. and Su, L. and Wang, S. and Yin, D. and Xiong, H. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.08694},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… may indicate hallucinations by the LLM agents. • Repetition Rate (RR): We measure the frequency of POI repetition in plans to assess basic commonsense awareness of the agents. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tas_turk-lettucedetect_2025,
  title = {Turk-{LettuceDetect},
  author = {Taş, S. and Huseyni, M. E. and Ezerceli, Ö and Bayraktar, R. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2509.17671},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… When analyzing LLM hallucination behavior, we observe that GPT-4.1 and Mistral models achieve high recall (up to 0.9938), indicating a strong tendency to generate content flagged as …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lesperance_taxonomic_2025,
  title = {Taxonomic {Reasoning},
  author = {Lesperance, N. and Ratnasingham, S. and Taylor, G. W.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.10886},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the RAG model relative to the Naïve LLM shows reasoning over the additional retrieved context provides a small boost to LLM confidence … The RAG models did not see the same sharp …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{krupp_talk2xopen-source_2025,
  title = {{Talk2X},
  author = {Krupp, L. and Geißler, D. and Hevesi, P. and Hirsch, M. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.03343},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Integrated into websites, LLM-powered chatbots offer … an adapted retrieval-augmented generation approach (RAG) … sources we enabled Talk2X to cite its sources of information, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{vungarala_tpu-gen_2025-1,
  title = {Tpu-gen: {Llm},
  author = {Vungarala, D. and Elbtity, M. E. and Pandit, K. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11106005/},
  journal = {… Conference on LLM …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations leveraging RAG and fine-tuning, to align best for the LLMs to streamline the approximate TPU design generation process considering budgetary constraints (eg, power, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tang_lottery_2025,
  title = {The {Lottery},
  author = {Tang, Z. and Liu, X. and Wang, Q. and Dong, P. and He, B. and Chu, X. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.17535},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… in LLMs related to retrievalaugmented generation, multi-step … enhance LLM performance. Then, we propose a lottery … within LLM parameters if RAG can accurately retrieve factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_timerag_2025-1,
  title = {Timerag: {Boosting},
  author = {Yang, S. and Wang, D. and Zheng, H. and Jin, R.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10889933/},
  journal = {ICASSP 2025-2025 IEEE …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Our work demonstrates the potential of RAG in amplifying LLM performance in time series forecasting, which offers a promising approach for future research in knowledgeenhanced …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhou_efficiency_2025,
  title = {The {Efficiency},
  author = {Zhou, H. and Gu, H. and Liu, X. and Zhou, K. and Liang, M. and Xiao, Y. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02173},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… that combines Retrieval-Augmented Generation (RAG) with … real-time predictive confidence assessments across multiple … efficient, real-time LLM deployment in commercial systems. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sood_paradigm_2025,
  title = {The {Paradigm},
  author = {Sood, A. K. and Zeadally, S. and Hong, E. K.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0045790625002502},
  journal = {Computers and Electrical Engineering},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… We present a taxonomy of hallucinations in LLMs for cybersecurity, including mapping LLM responses to classification … Finally, we discuss mitigation strategies to combat hallucinations. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{adam_traceable_2025,
  title = {Traceable {LLM},
  author = {Adam, D. and Kliegr, T.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457325000706},
  journal = {Information Processing \&Management},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… , our approach is to avoid using internal LLM factual knowledge altogether. Instead, verified … To assess the possible application of this retrieval augmented generation (RAG) workflow …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sheng_talk2traffic_2025-1,
  title = {Talk2traffic: {Interactive},
  author = {Sheng, Z. and Huang, Z. and Qu, Y. and Leng, Y. and {...},
  year = {2025},
  url = {https://openaccess.thecvf.com/content/CVPR2025W/WDFM-AD/html/Sheng_Talk2Traffic_Interactive_and_Editable_Traffic_Scenario_Generation_for_Autonomous_Driving_CVPRW_2025_paper.html},
  journal = {Proceedings of the …},
  note = {Publisher: openaccess.thecvf.com},
  keywords = {source: Google Scholar},
  abstract = {… a retrieval-augmented generation (RAG) approach that grounds MLLMs’ outputs in verified code snippets, effectively reducing hallucinations … that reduces hallucinations and ensures …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kumar_tfhe-coder_2025,
  title = {Tfhe-coder: {Evaluating},
  author = {Kumar, M. and Xue, J. and Zheng, M. and Lou, Q.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.12217},
  journal = {arXiv preprint arXiv:2503.12217},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… generation (RAG) is introduced to provide the LLM with … LLM has access to accurate API definitions and usage examples, reducing errors such as incorrect function calls or hallucinated …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{ma_role_2025,
  title = {The {Role},
  author = {Ma, Chuangtao},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015692597&partnerID=40&md5=db9b224fe16411ddb110caff9b605e5d},
  booktitle = {{CEUR},
  volume = {4018},
  pages = {4 -- 11},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{huang_trust_2025,
  title = {{TO},
  author = {Huang, Yukun and Chen, Sanxing and Cai, Hongyi and Dhingra, Bhuwan},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010216227&partnerID=40&md5=ded391c226ff0803bcb2f23a25b5a9f4},
  pages = {50497 -- 50526},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{sahitaj_towards_2025,
  title = {Towards {Automated},
  author = {Sahitaj, Premtim and Maab, Iffat and Yamagishi, Junichi and Kolanowski, Jawan and Möller, Sebastian and Schmitt, Vera},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009905987&partnerID=40&md5=804a0d103ea8f2fc53e00860ea6f6ef9},
  booktitle = {{CEUR},
  volume = {3986},
  pages = {11 -- 23},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{shastri_automating_2025,
  title = {Automating {Transparency},
  author = {Shastri, Ishana and Jain, Shomik and Engelhardt, Barbara and Wilson, Ashia},
  year = {2025},
  booktitle = {Proceedings of the 2024 {AAAI},
  pages = {1357--1367},
  publisher = {AAAI Press},
  keywords = {source: ACM},
  abstract = {Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.},
  address = {San Jose, California, USA},
  series = {{AIES},
}

@article{oche_systematic_2025,
  title = {A systematic review of key retrieval-augmented generation (rag) systems: {Progress},
  author = {Oche, A. J. and Folashade, A. G. and Ghosal, T. and Biswas, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2507.18910},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG architecture is illustrated in Figure 1 below. A key distinction between RAG and pure large language model (LLM… Overall, RAG became a cornerstone for credible LLM deployments …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_application_2025,
  title = {Application of retrieval-augmented generation for interactive industrial knowledge management via a large language model},
  author = {Chen, L. C. and Pardeshi, M. S. and Liao, Y. X. and Pai, K. C.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0920548925000248},
  journal = {Computer Standards \&Interfaces},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… model with a retrieval-augmented generation (RAG)-based LLM as a sustainable solution … RAG-based generative pretrained transformer (GPT) models for customized solutions. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_survey_2025-3,
  title = {A survey of graph retrieval-augmented generation for customized large language models},
  author = {Zhang, Q. and Chen, S. and Bei, Y. and Yuan, Z. and Zhou, H. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.13958},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… new hallucinations and even experiencing severe catastrophic forgetting [11]. Retrieval-Augmented generation (RAG… within RAG focuses on bolstering the quality and efficiency of LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bingol_accuracy_2025,
  title = {Accuracy of {Current},
  author = {Bingöl, F. G. and Ağagündüz, D. and Bingol, M. C.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1051227625000135},
  journal = {Journal of Renal Nutrition},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… Customization of LLMs in specific areas such as nutrition or the development of a nutrition-specific RAG framework by improving LLM structures with current guidelines and articles may …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{voznyuk_advacheck_2025,
  title = {Advacheck at semeval-2025 task 3: {Combining},
  author = {Voznyuk, A. and Gritsai, G. and Grabovoy, A.},
  year = {2025},
  url = {https://aclanthology.org/2025.semeval-1.160/},
  journal = {Proceedings of the 19th …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… from LLM judges into initial answer. Through this method, we aim to detect and fix factual hallucinations from the text and thus contribute to ongoing efforts in hallucination detection and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mei_survey_2025,
  title = {A survey of multimodal retrieval-augmented generation},
  author = {Mei, L. and Mo, S. and Yang, Z. and Chen, C.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.08748},
  journal = {arXiv preprint arXiv:2504.08748},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This approach enhances response accuracy and timeliness, particularly in domain-specific contexts, while reducing the risk of hallucinations common in LLM outputs. In multi-turn …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{abootorabi_ask_2025,
  title = {Ask in any modality: {A},
  author = {Abootorabi, M. M. and Zobeiri, A. and Dehghani, M. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.08826},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RetrievalAugmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual … enhance ASR accuracy through LLM in-context learning. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{misrahi_adapting_2025,
  title = {Adapting {Large},
  author = {Misrahi, A. and Chirkova, N. and Louis, M. and Nikoulina, V.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.02411},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factuality, but multi-domain applications face challenges like lack of diverse benchmarks … for RAG. We show that commonly used tuning of an LLM for RAG on standard question …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xie_rag-based_2025,
  title = {A rag-based multi-agent llm system for natural hazard resilience and adaptation},
  author = {Xie, Y. and Jiang, B. and Mallick, T. and Bergerson, J. D. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.17200},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM … datasets, and scientific literature through an RAG framework, the system ensures both the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cossio_comprehensive_2025,
  title = {A comprehensive taxonomy of hallucinations in large language models},
  author = {Cossio, M.},
  year = {2025},
  url = {https://arxiv.org/abs/2508.01781},
  journal = {arXiv preprint arXiv:2508.01781},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal … For instance, Retrieval-Augmented Generation (RAG) is frequently cited as an effective …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kurland_augmenting_2025,
  title = {Augmenting large language models with automated, bibliometrics-powered literature search for knowledge distillation: a pilot study for common spinal pathologies},
  author = {Kurland, D. B. and Alber, D. A. and Palla, A. and Souza, DN de and {...},
  year = {2025},
  url = {https://journals.lww.com/neurosurgery/fulltext/2025/08000/augmenting_large_language_models_with_automated,.12.aspx},
  journal = {…},
  note = {Publisher: journals.lww.com},
  keywords = {source: Google Scholar},
  abstract = {… RAG retrieval augmented generation … that every statement in the LLM-generated summary is cited. By backing up LLM-generated summaries with citations, our approach provides …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_automated_2025,
  title = {Automated literature research and review-generation method based on large language models},
  author = {Wu, S. and Ma, X. and Luo, D. and Li, L. and Shi, X. and Chang, X. and {...},
  year = {2025},
  url = {https://academic.oup.com/nsr/article-abstract/12/6/nwaf169/8120226},
  journal = {National Science …},
  note = {Publisher: academic.oup.com},
  keywords = {source: Google Scholar},
  abstract = {… ; LitLLM [20] combining RAG with LLM reranking to generate high-quality … hallucination mitigation, we employed a confusion matrix to classify outcomes according to whether the LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_addressing_2025,
  title = {Addressing the sustainable {AI},
  author = {Wu, H. and Wang, X. and Fan, Z.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.08262},
  journal = {arXiv preprint arXiv:2501.08262},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… While traditional LLM applications depend solely on the … -enabled LLM agents or Retrieval-Augmented Generation (RAG) [22… advanced applications by mitigating LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{singh_advanced_2025,
  title = {Advanced {Real},
  author = {Singh, G. and Singh, P. and Singh, M.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.15290},
  journal = {arXiv preprint arXiv:2501.15290},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This section discusses the primary advantages of the RAG based LLM model over the … Additionally, the challenge of LLM hallucination, where the model may generate unnecessary or …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_survey_2025,
  title = {A survey of personalization: {From},
  author = {Li, X. and Jia, P. and Xu, D. and Wen, Y. and Zhang, Y. and Zhang, W. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.10147},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG … as outdated responses and hallucinations, which severely …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tang_adapting_2025,
  title = {Adapting to non-stationary environments: {Multi},
  author = {Tang, X. and Li, J. and Du, N. and Xie, S.},
  year = {2025},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/33380},
  journal = {Proceedings of the AAAI Conference on …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs that encapsulate extensive factual … method across different Large Language Model generators to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_application_2025,
  title = {Application of large language models in medicine},
  author = {Liu, F. and Zhou, H. and Gu, B. and Zou, X. and Huang, J. and Wu, J. and {...},
  year = {2025},
  url = {https://www.nature.com/articles/s44222-025-00279-5},
  journal = {Nature Reviews …},
  note = {Publisher: nature.com},
  keywords = {source: Google Scholar},
  abstract = {… to minimize hallucinations, … ChatGPT on clinical scenario evaluations, particularly in terms of completeness and safety. Another example is QA-RAG 61 , which utilizes RAG with LLM for …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_ai-driven_2025,
  title = {{AI},
  author = {Zhang, S. and Huang, M. and Liu, S. and Meng, F. and Xie, Y. and Ren, X. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11029010/},
  journal = {IEEE …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… online information, employing retrieval-augmented generation (RAG) to mitigate the lack of specialized knowledge and reduce hallucinations in large language model outputs. Chain-of-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_agrigpt_2025,
  title = {Agrigpt: {A},
  author = {Yang, B. and Zhang, Y. and Feng, L. and Chen, Y. and Zhang, J. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2508.08632},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factual grounding, we employ Tri-RAG, a three-channel RetrievalAugmented Generation … , we propose a three channel RetrievalAugmented Generation (Tri-RAG) framework that incor…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xu_align-grag_2025,
  title = {Align-{GRAG},
  author = {Xu, D. and Jia, P. and Li, X. and Zhang, Y. and Wang, M. and Liu, Q. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.16237},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of LLM remain, leading to challenges like hallucinations (… Retrieval-augmented generation (RAG) systems have been … the LLM’s outputs in verifiable, external knowledge, RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ilapaka_comprehensive_2025,
  title = {A {Comprehensive},
  author = {Ilapaka, A. and Ghosh, R.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11017017/},
  journal = {2025 7th International Congress on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… The chatbot uses Retrieval-Augmented Generation (RAG) to improve the relevance of its responses and make interactions more personalized. It also leverages LangChain’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ding_automatic_2025,
  title = {An automatic patent literature retrieval system based on {LLM},
  author = {Ding, Y. and Wu, Y. and Ding, Z.},
  year = {2025},
  url = {https://arxiv.org/abs/2508.14064},
  journal = {arXiv preprint arXiv:2508.14064},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… in this study, RetrievalAugmented Generation (RAG) serves as a core framework, acting as a bridge between semantic retrieval and contextual generation. RAG combines the strengths …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bazgir_agentichypothesis_2025,
  title = {Agentichypothesis: {A},
  author = {Bazgir, A. and Zhang, Y.},
  year = {2025},
  url = {https://openreview.net/forum?id=UeeyfR4CUg},
  journal = {Towards Agentic AI for Science: Hypothesis …},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… for LLM-based hypothesis generation, including Retrieval Augmented Generation (RAG), … LLMs employ citation graph integration to ground hypotheses in existing literature, thereby …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_smart_2025,
  title = {A smart multimodal healthcare copilot with powerful llm reasoning},
  author = {Zhao, X. and Liu, S. and Yang, S. Y. and Miao, C.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.02470},
  journal = {arXiv preprint arXiv:2506.02470},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of the backbone LLM. Retrieval-Augmented Generation To provide backbone LLM with case-… information and mitigate hallucinations in generated outputs, we apply the RAG method, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{devine_aloftrag_2025,
  title = {{ALoFTRAG},
  author = {Devine, P.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.11929},
  journal = {arXiv preprint arXiv:2501.11929},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG … increase the accuracy of LLM-based RAG systems while using …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_review_2025,
  title = {A review of prominent paradigms for llm-based agents: {Tool},
  author = {Li, X.},
  year = {2025},
  url = {https://aclanthology.org/2025.coling-main.652/},
  journal = {Proceedings of the 31st International Conference on …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM-profiled roles as the foundation for the development of algorithmic frameworks across different paradigms. Notably, Wang et al. (2024a) also discuss LLM … to their citations on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{shirdel_aprescot_2025,
  title = {{AprèsCoT},
  author = {Shirdel, M. and Rorseth, J. and Godfrey, P. and Golab, L. and Srivastava, D. and {...},
  year = {2025},
  url = {https://www.openproceedings.org/2025/conf/edbt/paper-337.pdf},
  publisher = {openproceedings.org},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… up the nearest documents to the output generated by the LLM as a form of citation. However, our … RAG mode, where the facts identified by the subgraph retriever are given to the LLM as …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{anaroua_ai-driven_2025,
  title = {{AI},
  author = {Anaroua, F. I. and Li, Q. and Tang, Y. and Liu, H. P.},
  year = {2025},
  url = {https://arxiv.org/abs/2509.20369},
  journal = {arXiv preprint arXiv:2509.20369},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The paper concludes with implementation lessons and a roadmap (RAG integration, hallucination mitigation, and LTI 1.3/OpenID Connect) to guide multi-course evaluations and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yue_survey_2025,
  title = {A survey of large language model agents for question answering},
  author = {Yue, M.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.19213},
  journal = {arXiv preprint arXiv:2503.19213},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… in retrieval-augmented generation systems is determining when to trust the LLM’s internal knowledge versus relying on external documents. The critical LLM in SelfRAG addresses this …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{odubola_ai_2025,
  title = {{AI},
  author = {Odubola, O. and Adeyemi, T. S. and Olajuwon, O. O. and {...},
  year = {2025},
  url = {https://www.researchgate.net/profile/Oluwatimilehin-Odubola/publication/389525988_AI_in_Social_Good_LLM_powered_Interventions_in_Crisis_Management_and_Disaster_Response/links/67ddb99e35f7044c924f6afe/AI-in-Social-Good-LLM-powered-Interventions-in-Crisis-Management-and-Disaster-Response.pdf},
  journal = {Journal of Artificial …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… retrieval-augmented generation (RAG) for flood disaster reporting has also been explored, with studies demonstrating that LLM… databases, improving factual accuracy and reducing the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wan_aviationllm_2025,
  title = {{AviationLLM},
  author = {Wan, J. and Shen, F. and Li, F. and Sun, Y. and Li, Y. and Zhang, S.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.14336},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations … RAG to develop an aviation training-oriented LLM. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kermani_systematic_2025,
  title = {A {Systematic},
  author = {Kermani, A. and Perez-Rosas, V. and Metsis, V.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.24307},
  journal = {arXiv preprint arXiv:2503.24307},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG can operate with much fewer training examples than is usually required to fine-tune an LLM model on a specific task, we consider RAG … We implement a RAG model that …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_adaptive_2025,
  title = {Adaptive {Contextual},
  author = {Liu, G. and Liu, Y. and Wang, J. and Du, H. and Niyato, D. and Kang, J. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.09383},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Biased or skewed training data may cause the LLM to generate hallucinations, ie, … This paper has introduced an ACC framework for mobileedge LLM services, enhancing RAG through …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{luo_agentauditor_2025,
  title = {Agentauditor: {Human},
  author = {Luo, H. and Dai, S. and Ni, C. and Li, X. and Zhang, G. and Wang, K. and Liu, T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2506.00641},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM … of LLM outputs and reducing model hallucinations, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{chen_self-improving_2025,
  title = {A {Self},
  author = {Chen, Xiaorui and Hao, Haiyan},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008009216&partnerID=40&md5=3e0ed587d66038792fd2e010a3e32e3b},
  booktitle = {Proceedings of the {International},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{ocker_grounded_2025,
  title = {A {Grounded},
  author = {Ocker, Felix and Deigmoeller, Joerg and Smirnov, Pavel and Eggert, Julian P.},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017117149&partnerID=40&md5=60254072155f8735400959884a8a9a08},
  booktitle = {{CEUR},
  volume = {4020},
  pages = {1 -- 8},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{agrawal_algoace_2025,
  title = {{AlgoAce},
  author = {Agrawal, Anav and Vie, Jill Jênn},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017115267&partnerID=40&md5=1ff3a4ee54b42f54386351f1546d7ff4},
  booktitle = {{CEUR},
  volume = {4019},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{leemann_auto-gda_2025,
  title = {{AUTO},
  author = {Leemann, Tobias and Petridis, Periklis and Vietri, Giuseppe and Manousakas, DIonysis and Roth, Aaron L. and Aydöre, Sergül},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010261957&partnerID=40&md5=00d53ba28c1e27e4ef4a44edcafa83b5},
  pages = {90986 -- 91017},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{chen_attention_2025,
  title = {{ATTENTION},
  author = {Chen, Shijie and Gutierrez, Bernal Jiménez and Su, Yu},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010249161&partnerID=40&md5=23bfc973cfb6d14351645c8fc8ddd763},
  pages = {100148 -- 100170},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{firsanova_aggile_2025,
  title = {{AGGILE},
  author = {Firsanova, Victoria and Khlusova, Yana},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498345&partnerID=40&md5=298247b0d9a24b29d9dfabd7f1c7b737},
  booktitle = {{CEUR},
  volume = {3977},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{arnold_pilot_2025,
  title = {A {Pilot},
  author = {Arnold, Christian M. and Robertson, Paul and Robertson, Zoe and Laddaga, Robert M. and Katz, Boris and Barbu, Andrei},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005138825&partnerID=40&md5=92452f1477d21048c791ff0169212739},
  booktitle = {{CEUR},
  volume = {3957},
  pages = {391 -- 401},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhang_automatic_2025,
  title = {An {Automatic},
  author = {Zhang, Chi and Datla, Vivek V. and Shrivastava, Aditya and Samuel, Alfy and Huang, Zhiqi and Kumar, Anoop and Liu, Daben},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000157880&partnerID=40&md5=f75d433471cccbae6cd13b10a7f72836},
  booktitle = {Proceedings - {International},
  pages = {603 -- 611},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{chen_decentralized_2024,
  title = {Decentralized natural policy gradient with variance reduction for collaborative multi-agent reinforcement learning},
  author = {Chen, Jinchi and Feng, Jie and Gao, Weiguo and Wei, Ke},
  year = {2024},
  journal = {J. Mach. Learn. Res.},
  volume = {25},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {decentralized optimization, multi-agent reinforcement learning, natural policy gradient, variance reduction, source: ACM},
  abstract = {This paper studies a policy optimization problem arising from collaborative multi-agent reinforcement learning in a decentralized setting where agents communicate with their neighbors over an undirected graph to maximize the sum of their cumulative rewards. A novel decentralized natural policy gradient method, dubbed Momentum-based Decentralized Natural Policy Gradient (MDNPG), is proposed, which incorporates natural gradient, momentum-based variance reduction, and gradient tracking into the decentralized stochastic gradient ascent framework. The O(n-1ε-3) sample complexity for MDNPG to converge to an ε-stationary point has been established under standard assumptions, where n is the number of agents. It indicates that MDNPG can achieve the optimal convergence rate for decentralized policy gradient methods and possesses a linear speedup in contrast to centralized optimization methods. Moreover, superior empirical performance of MDNPG over other state-of-the-art algorithms has been demonstrated by extensive numerical experiments.},
  issn = {1532-4435},
  month = {jan},
}

@article{prabhune_deploying_2024,
  title = {Deploying large language models with retrieval augmented generation},
  author = {Prabhune, S. and Berndt, D. J.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.11895},
  journal = {arXiv preprint arXiv:2411.11895},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (LLM) are sometimes hampered by tendencies to hallucinate or create non-factual responses, … have increasingly focused on methods to ground generated outputs in factual data. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ke_development_2024,
  title = {Development and testing of retrieval augmented generation in large language models–a case study report},
  author = {Ke, Y. H. and Jin, L. and Elangovan, K. and Abdullah, H. R. and Liu, N. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.01733},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM-RAG pipeline, tailored specifically for preoperative medicine. The primary objective is to evaluate the accuracy of the LLM-RAG … Both models have similar hallucination rates of 1.2\%…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chang_detecting_2024,
  title = {Detecting hallucination and coverage errors in retrieval augmented generation for controversial topics},
  author = {Chang, T. A. and Tomanek, K. and Hoffmann, J. and Thain, N. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2403.08904},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In this paper, we investigate how LLMs can be used with retrieval augmented generation for … in the tuned LLM responses. In retrieval augmented generation, factual information is re…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_dragin_2024,
  title = {{DRAGIN},
  author = {Su, W. and Tang, Y. and Ai, Q. and Wu, Z. and Liu, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2403.10081},
  journal = {arXiv preprint arXiv:2403.10081},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG enhances LLMs by retrieving and incorporating relevant … of RAG typically rely on single-round retrieval, using the LLM’s … retrieval dynamically when the LLM’s confidence (ie, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{khan_developing_2024,
  title = {Developing retrieval augmented generation ({RAG},
  author = {Khan, A. A. and Hasan, M. T. and Kemell, K. K. and Rasku, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.15944},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG models enhance the factual grounding of their outputs from the up-to-date knowledge. A generic workflow of Retrieval Augmented Generation (RAG… knowledge, the RAG process is …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ni_diras_2024,
  title = {Diras: {Efficient},
  author = {Ni, J. and Schimanski, T. and Lin, M. and Sachan, M. and Ash, E. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.14162},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… ]; “Ask” means the result is calibrated by the generated confidence score in [Confidence] field; and “Tok” means we take the token-level probability of “Yes/No” after “[Guess]:” as the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{arslan_driving_2024,
  title = {Driving sustainable energy transitions with a multi-source {RAG},
  author = {Arslan, M. and Mahdjoubi, L. and Munawar, S.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S0378778824009435},
  journal = {Energy and Buildings},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… this gap, this research introduces an Energy Chatbot, a sustainable IS that utilizes Large Language Models (LLMs) integrated with multi-source Retrieval Augmented Generation (RAG). …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiao_duetrag_2024,
  title = {Duetrag: {Collaborative},
  author = {Jiao, D. and Cai, L. and Huang, J. and Zhang, W. and Tang, S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2405.13002},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG … learns to independently weigh the credibility of Mi and Me’…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{barron_domain-specific_2024-1,
  title = {Domain-specific retrieval-augmented generation using vector stores, knowledge graphs, and tensor factorization},
  author = {Barron, R. C. and Grantcharov, V. and Wanna, S. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10903241/},
  journal = {2024 International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… a highly domainspecific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{peng_data_2024,
  title = {Data extraction attacks in retrieval-augmented generation via backdoors},
  author = {Peng, Y. and Wang, J. and Yu, H. and Houmansadr, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.01705},
  journal = {arXiv preprint arXiv:2411.01705},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Wikichat: Stopping the hallucination of large language model chatbots by few-shot … a scenario where the RAG system owner directly uses a publicly available LLM that has already been …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{eghbali_-hallucinator_2024,
  title = {De-{Hallucinator},
  author = {Eghbali, A. and Pradel, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2401.01701},
  journal = {arXiv preprint arXiv:2401.01701},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-augmented generation (RAG) [28] proposes to retrieve relevant context based on … we refer to as the RAG prompt type. The idea of augmenting an LLM with well-grounded facts …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{park_development_2024,
  title = {Development of dental consultation chatbot using retrieval augmented llm},
  author = {Park, J.},
  year = {2024},
  journal = {The Journal of the Institute of Internet …},
  note = {Publisher: The Institute of Internet …
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ong_development_2024,
  title = {Development and testing of a novel large language model-based clinical decision support systems for medication safety in 12 clinical specialties},
  author = {Ong, J. C. L. and Jin, L. and Elangovan, K. and Lim, G. Y. S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.01741},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The accuracy of DRP detection with RAG-LLM improved in several categories but at the … This study established that a RAG-LLM based CDSS significantly boosts the accuracy of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cai_driving_2024,
  title = {Driving with regulation: {Interpretable},
  author = {Cai, T. and Liu, Y. and Zhou, Z. and Ma, H. and Zhao, S. Z. and Wu, Z. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.04759},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (TRR) Agent based on Retrieval-Augmented Generation (RAG) to … powered by a Large Language Model (LLM) to interpret … effectiveness in enhancing LLM accuracy and factual correct…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_dmqr-rag_2024,
  title = {Dmqr-rag: {Diverse},
  author = {Li, Z. and Wang, J. and Jiang, Z. and Mao, H. and Chen, Z. and Du, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.13154},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In contrast, “What is the citation count for the Transformer paper?” is a general-purpose … In this section, we will first explore various LLM-based rewriting strategies from an informational …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{werheid_designing_2024,
  title = {Designing an llm-based copilot for manufacturing equipment selection},
  author = {Werheid, J. and Melnychuk, O. and Zhou, H. and Huber, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.13774},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, no studies have been identified that propose LLM-driven methods or tools … To address this gap, we propose a factual-driven copilot based on RAG-LLMs designed to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{huang_datagen_2024,
  title = {Datagen: {Unified},
  author = {Huang, Y. and Wu, S. and Gao, C. and Chen, D. and Zhang, Q. and {...},
  year = {2024},
  url = {https://openreview.net/forum?id=F5R0lG74Tu},
  journal = {The Thirteenth …},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… a Retrieval-Augmented Generation (RAG)-based validation method to check the factuality of … Motivated by this finding, we require the LLM to generate Python code to solve the given …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{burgan_developing_2024,
  title = {Developing a {Retrieval},
  author = {Burgan, C. and Kowalski, J. and Liao, W.},
  year = {2024},
  url = {https://www.pwvas.org/index.php/pwvas/article/view/1068},
  journal = {… of the West Virginia Academy of Science},
  note = {Publisher: pwvas.org},
  keywords = {source: Google Scholar},
  abstract = {… Testing each LLM helped assess answer types and accuracy. … a local LLM based on Google's Gemini model. Ollama framework aids in automatic LLM selection based on user prompts. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{chen_dynamic_2024,
  title = {Dynamic supplementation of federated search results for reducing hallucinations in llms},
  author = {Chen, J. and Huang, X. and Li, Y.},
  year = {2024},
  url = {https://files.osf.io/v1/resources/x5vge/providers/osfstorage/66615e3565e1de503e893ab6?format=pdf\&action=download\&direct\&version=2},
  publisher = {files.osf.io},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… The use of retrieval-augmented generation (RAG) … in hallucinations by grounding responses in verifiable facts [34… inference for reducing large language model hallucinations. In: …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sharma_decoding_2024,
  title = {Decoding {BACnet},
  author = {Sharma, R. and Okada, H. and Oba, T. and Subramanian, K. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.15428},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… ’s outputs in factual information retrieved from trusted sources [Nightfall]. … RAG (Method 1) In summarization generation without RAG, we pass the processed packet content to the LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bouvard_derby_2024,
  title = {Derby {LLM},
  author = {Bouvard, C. and Ciancone, M. and Gourru, A. and {...},
  year = {2024},
  url = {https://hal.science/hal-04638460/},
  journal = {10 ème Conférence …},
  note = {Publisher: hal.science},
  keywords = {source: Google Scholar},
  abstract = {… le processus expérimental mis en place pour comparer la RAG et le fine-tuning d’un LLM. … La fidélité peut facilement être liée à la détection d’hallucinations. La plupart des outils …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{rashid_monotonic_2020,
  title = {Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author = {Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  year = {2020},
  journal = {J. Mach. Learn. Res.},
  volume = {21},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {multi-agent coordination, multi-agent learning, reinforcement learning, source: ACM},
  abstract = {In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.},
  issn = {1532-4435},
  month = {jan},
}

@article{zhou_malib_2023,
  title = {{MALib},
  author = {Zhou, Ming and Wan, Ziyu and Wang, Hanjing and Wen, Muning and Wu, Runzhe and Wen, Ying and Yang, Yaodong and Yu, Yong and Wang, Jun and Zhang, Weinan},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {source: ACM},
  abstract = {Population-based multi-agent reinforcement learning (PB-MARL) encompasses a range of methods that merge dynamic population selection with multi-agent reinforcement learning algorithms (MARL). While PB-MARL has demonstrated notable achievements in complex multi-agent tasks, its sequential execution is plagued by low computational efficiency due to the diversity in computing patterns and policy combinations. We propose a solution involving a stateless central task dispatcher and stateful workers to handle PB-MARL's subroutines, thereby capitalizing on parallelism across various components for efficient problem-solving. In line with this approach, we introduce MALib, a parallel framework that incorporates a task control model, independent data servers, and an abstraction of MARL training paradigms. The framework has undergone extensive testing and is available under the MIT license (https://github.com/sjtu-marl/malib).},
  month = {jan},
  issn = {1532-4435},
}

@article{narang_multiplayer_2023,
  title = {Multiplayer performative prediction: learning in decision-dependent games},
  author = {Narang, Adhyyan and Faulkner, Evan and Drusvyatskiy, Dmitriy and Fazel, Maryam and Ratliff, Lillian J.},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {source: ACM},
  abstract = {Learning problems commonly exhibit an interesting feedback mechanism wherein the population data reacts to competing decision makers' actions. This paper formulates a new game theoretic framework for this phenomenon, called multi-player performative prediction. We focus on two distinct solution concepts, namely (i) performatively stable equilibria and (ii) Nash equilibria of the game. The latter equilibria are arguably more informative, but are generally computationally difficult to find since they are solutions of nonmonotone games. We show that under mild assumptions, the performatively stable equilibria can be found efficiently by a variety of algorithms, including repeated retraining and the repeated (stochastic) gradient method. We then establish transparent sufficient conditions for strong monotonicity of the game and use them to develop algorithms for finding Nash equilibria. We investigate derivative free methods and adaptive gradient algorithms wherein each player alternates between learning a parametric description of their distribution and gradient steps on the empirical risk. Synthetic and semi-synthetic numerical experiments illustrate the results.},
  month = {jan},
  issn = {1532-4435},
}

@article{saxena_minimizing_2023,
  title = {Minimizing factual inconsistency and hallucination in large language models},
  author = {Saxena, S. and Prasad, S. and Prakash, M. V. and Shankar, A. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2311.13878},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucination into input, context, and factconflicting. In particular, fact-conflicting hallucination … Existing approaches such as Retrieval-Augmented Generation (RAG) [10] augment LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lan_warpdrive_2022,
  title = {{WarpDrive},
  author = {Lan, Tian and Srinivasa, Sunil and Wang, Huan and Zheng, Stephan},
  year = {2022},
  journal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {deep reinforcement learning, GPU acceleration, multi-agent systems, source: ACM},
  abstract = {WarpDrive is a flexible, lightweight, and easy-to-use open-source framework for end-to-end deep multi-agent reinforcement learning (MARL) on a Graphics Processing Unit (GPU), available at https://github.com/salesforce/warp-drive. It addresses key system bottlenecks when applying MARL to complex environments with high-dimensional state, observation, or action spaces. For example, WarpDrive eliminates data copying between the CPU and GPU and runs thousands of simulations and agents in parallel. It also enables distributed training on multiple GPUs and scales to millions of agents. In all, WarpDrive enables orders-of-magnitude faster MARL compared to common CPU-GPU implementations. For example, WarpDrive yields 2.9 million environment steps/second with 2000 environments and 1000 agents (at least 100× faster than a CPU version) in a 2d-Tag simulation. It is user-friendly: e.g., it provides a lightweight, extendable Python interface and flexible environment wrappers. It is also compatible with PyTorch. In all, WarpDrive offers a platform to significantly accelerate reinforcement learning research and development.},
  issn = {1532-4435},
  month = {jan},
}

@article{li_f2a2_2023,
  title = {{F2A2},
  author = {Li, Wenhao and Jin, Bo and Wang, Xiangfeng and Yan, Junchi and Zha, Hongyuan},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {actor-critic, cooperative MARL, decentralized, primal-dual method, source: ACM},
  abstract = {Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications due to non-interactivity between agents, the curse of dimensionality, and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. This paper proposes a exible fully decentralized actor-critic MARL framework, which can combine most of the actor-critic methods and handle large-scale general cooperative multi-agent settings. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluation are jointly optimized, which can stabilize multi-agent policy learning. Furthermore, the proposed framework can achieve scalability and stability for the large-scale environment. This framework also reduces information transmission by the parameter sharing mechanism and novel modeling-other-agents methods based on theory-of-mind and online supervised learning. Sufficient experiments in cooperative Multi-agent Particle Environment and StarCraft II show that the proposed decentralized MARL instantiation algorithms perform competitively against conventional centralized and decentralized methods.},
  issn = {1532-4435},
  month = {jan},
}

@article{colverd_floodbrain_2023,
  title = {Floodbrain: {Flood},
  author = {Colverd, G. and Darm, P. and Silverberg, L. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2311.02597},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Despite our efforts to curb hallucination risks via a human-in-the-loop inspect methodology, … This tool is designed for collaborative report writing between human and LLM, to be used …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{izacard_atlas_2023,
  title = {Atlas: few-shot learning with retrieval augmented language models},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {information retrieval, language models, retrieval augmented language models, source: ACM},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval-augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval-augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and Natural Questions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameter model by 3\% despite having 50x fewer parameters.},
  issn = {1532-4435},
  month = {jan},
}

@article{rahman_general_2023,
  title = {A general learning framework for open ad hoc teamwork {Using},
  author = {Rahman, Arrasy and Carlucho, Ignacio and Höpner, Niklas and Albrecht, Stefano V.},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {source: ACM},
  abstract = {Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully observable case to compute an agent's optimal policy under partial observability in open ad hoc teamwork. Empirical results demonstrate that our solution can learn efficient policies in open ad hoc teamwork in fully and partially observable cases. Further analysis demonstrates that our methods' success is a result of effectively learning the effects of teammates' actions while also inferring the inherent state of the environment under partial observability.},
  month = {jan},
  issn = {1532-4435},
}

@article{saad-falcon_ares_2023,
  title = {Ares: {An},
  author = {Saad-Falcon, J. and Khattab, O. and Potts, C. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2311.09476},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… confidence interval for each component of the RAG, we find the midpoint of each confidence interval and use the midpoints to rank the RAG … т for RAG ranking with the ARES LLM judge …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jeong_study_2023,
  title = {A study on the implementation of generative ai services using an enterprise data-based llm application architecture},
  author = {Jeong, C.},
  year = {2023},
  url = {https://arxiv.org/abs/2309.01105},
  journal = {arXiv preprint arXiv:2309.01105},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , RAG informs the LLM of pertinent queries and associated reference materials in advance, mitigating hallucination … pertinent to LLM, as well as demarcates the realm of RAG that is the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{smith_strategic_2023,
  title = {Strategic knowledge transfer},
  author = {Smith, Max Olan and Anthony, Thomas and Wellman, Michael P.},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {empirical game theoretic analysis, multiagent learning, reinforcement learning, transfer learning, source: ACM},
  abstract = {In the course of playing or solving a game, it is common to face a series of changing other-agent strategies. These strategies often share elements: the set of possible policies to play has overlap, and the policies are sampled at the beginning of play by possibly differing distributions. As it faces the series of strategies, therefore, an agent has the opportunity to transfer its learned play against the previously encountered other-agent policies. We tackle two problems: (1) how can learned responses transfer across changing opponent strategies, and (2) how can this transfer be used to reduced the cumulative cost of learning in game solving. The first problem we characterize as the strategic knowledge transfer problem. For value-based response policies, we demonstrate that Q-Mixing approximately solves this problem by appropriately averaging the component Q-values. Solutions to the first problem can be applied to reduce the computational cost of learning-based game solving algorithms. We offer two algorithms that operate within the Policy-Space Response Oracles (PSRO) framework. Mixed-Oracles reduces the per-policy construction cost by transferring responses from previously encountered opponents. Mixed-Opponents performs strategic knowledge transfer by combining the previously encountered opponents into a single novel policy. Experimental evaluation of these methods on general-sum grid-world games provide evidence about their advantages and limitations in comparison to standard PSRO.},
  issn = {1532-4435},
  month = {jan},
}

@article{wang_survey_2023,
  title = {Survey on factuality in large language models: {Knowledge},
  author = {Wang, C. and Liu, X. and Yue, Y. and Tang, X. and Zhang, T. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2310.07521},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality… This framework primarily assesses the RAG system’s ability …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{asai_self-rag_2023-1,
  title = {Self-rag: {Self},
  author = {Asai, A. and Wu, Z. and Wang, Y. and Sil, A. and {...},
  year = {2023},
  url = {https://openreview.net/forum?id=jbNjgmE0OP},
  journal = {NeurIPS 2023 workshop on …},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… This work aims to improve the factuality of LLM outputs, the lack of which continues to cause numerous real-world problems (eg, spread of misinformation and provision of incorrect and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ornik_learning_2021,
  title = {Learning and planning for time-varying {MDPs},
  author = {Ornik, Melkior and Topcu, Ufuk},
  year = {2021},
  journal = {J. Mach. Learn. Res.},
  volume = {22},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {changing environment, Markov decision processes, maximum likelihood estimation, online learning, uncertainty quantification, source: ACM},
  abstract = {This paper proposes a formal approach to online learning and planning for agents operating in a priori unknown, time-varying environments. The proposed method computes the maximally likely model of the environment, given the observations about the environment made by an agent earlier in the system run and assuming knowledge of a bound on the maximal rate of change of system dynamics. Such an approach generalizes the estimation method commonly used in learning algorithms for unknown Markov decision processes with time-invariant transition probabilities, but is also able to quickly and correctly identify the system dynamics following a change. Based on the proposed method, we generalize the exploration bonuses used in learning for time-invariant Markov decision processes by introducing a notion of uncertainty in a learned time-varying model, and develop a control policy for time-varying Markov decision processes based on the exploitation and exploration trade-off. We demonstrate the proposed methods on four numerical examples: a patrolling task with a change in system dynamics, a two-state MDP with periodically changing outcomes of actions, a wind ow estimation task, and a multi-armed bandit problem with periodically changing probabilities of different rewards.},
  issn = {1532-4435},
  month = {jan},
}

@article{chavan_global-local_2025,
  title = {Global-{Local},
  author = {Chavan, Apala Lahiri and Sivasubramaniam, Revathy},
  year = {2025},
  journal = {J. User Exper.},
  volume = {19},
  number = {3},
  pages = {115--122},
  note = {Place: Bloomingdale, IL
Publisher: Usability Professionals' Association},
  keywords = {source: ACM},
  abstract = {We live in a connected world in which people and places are closer than ever, thanks to advances in technology and transportation. This interconnectedness has opened huge global markets for products and services which, for designers, presents both exciting opportunities and tough challenges. Designing for people from different cultures is difficult because it involves catering to a need—the core characteristic of the product—but in a way that aligns with their cultural expectations. It is a careful balance because excessive localization, or too little of it, can result in failure. Notable examples are American companies such as Mattel®, Home Depot®, and Best Buy™, which struggled in China due to differences in how people shop there. Even as big a player as Google™ has faced tough competition in places like South Korea.},
  issn = {1931-3357},
  month = {feb},
}

@article{barry_graphrag_2025,
  title = {Graphrag: leveraging graph-based efficiency to minimize hallucinations in llm-driven rag for finance data},
  author = {Barry, M. and Caillaut, G. and Halftermeyer, P. and Qader, R. and {...},
  year = {2025},
  url = {https://hal.science/hal-04907346/},
  journal = {… Knowledge Graph \& …},
  note = {Publisher: hal.science},
  keywords = {source: Google Scholar},
  abstract = {… DeepEval relies on a strong LLM to automatically score RAG … In order to evaluate the propensity to hallucinate, we report in … a good proxy for hallucination because we expect the LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_graph-based_2025,
  title = {Graph-based {Approaches},
  author = {Zhu, Z. and Huang, T. and Wang, K. and Ye, J. and Chen, X. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.10499},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM capabilities but also provide actionable strategies for leveraging graph expertise to advance RAG … in many RAG systems by serving as structured repositories of factual knowledge. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{da_ge-chat_2025,
  title = {{GE},
  author = {Da, L. and Shah, P. M. and Liou, K. R. and Zhang, J. and Wei, H.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.10143},
  journal = {arXiv preprint arXiv:2505.10143},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… trust issues among users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph enhanced retrieval-augmented generation … We compare with the direct LLM source …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{filice_generating_2025,
  title = {Generating {Diverse},
  author = {Filice, S. and Horowitz, G. and Carmel, D. and Karnin, Z. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.12789},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge-bases. arXiv preprint arXiv:2403.10446 (2024). [18] …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{filice_generating_2025-1,
  title = {Generating {Q},
  author = {Filice, S. and Horowitz, G. and Carmel, D. and Karnin, Z. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.acl-industry.33/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM (Claude Sonnet 3.5). Appendix D provides further details about the metrics, as well as the LLM … Enhancing llm factual accuracy with rag to counter hallucinations: A case study on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{patel_graph-enhanced_2025,
  title = {Graph-{Enhanced},
  author = {Patel, P.},
  year = {2025},
  url = {https://arxiv.org/abs/2509.14267},
  journal = {arXiv preprint arXiv:2509.14267},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… a novel retrieval-augmented generation (RAG) … the factual grounding. We examine recent advances in knowledge-augmented RAG and chatbots based on large language models (LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mavromatis_gnn-rag_2025,
  title = {Gnn-rag: {Graph},
  author = {Mavromatis, C. and Karypis, G.},
  year = {2025},
  url = {https://aclanthology.org/2025.findings-acl.856/},
  journal = {Findings of the Association for …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… The input given to the LLM contains the KG factual information along with the question and a prompt. For instance, the input becomes “Knowledge: Jamaica → language\_spoken → …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{muller_grouse_2025,
  title = {{GroUSE},
  author = {Muller, Sacha and Loison, António and Omrani, Bilel and Viaud, Gautier},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218500040&partnerID=40&md5=6a46fd103e200b5feb9cc139a0cc2f17},
  booktitle = {Proceedings - {International},
  pages = {4510 -- 4534},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{cerqueira_grounded_2025,
  title = {Grounded {Ethical},
  author = {Cerqueira, Jose Antonio Siqueira De and Khan, Ayman Asad and Rousi, Rebekah A. and Xi, Nannan and Hamari, Juho J. and Kemell, Kai Kristian and Abrahamsson, Pekka},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218445480&partnerID=40&md5=177ec8602df7141e5ba2d379301de978},
  booktitle = {{CEUR},
  volume = {3921},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{cutler_stochastic_2024,
  title = {Stochastic {Approximation},
  author = {Cutler, Joshua and Díaz, Mateo and Drusvyatskiy, Dmitriy},
  year = {2024},
  journal = {J. Mach. Learn. Res.},
  volume = {25},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {asymptotic normality, decision-dependent distributions, local asymptotic minimax optimality, performative prediction, stochastic approximation, source: ACM},
  abstract = {We analyze a stochastic approximation algorithm for decision-dependent problems, wherein the data distribution used by the algorithm evolves along the iterate sequence. The primary examples of such problems appear in performative prediction and its multiplayer extensions. We show that under mild assumptions, the deviation between the average iterate of the algorithm and the solution is asymptotically normal, with a covariance that clearly decouples the effects of the gradient noise and the distributional shift. Moreover, building on the work of Hájek and Le Cam, we show that the asymptotic performance of the algorithm with averaging is locally minimax optimal.},
  issn = {1532-4435},
  month = {jan},
}

@inproceedings{barnett_seven_2024-1,
  title = {Seven {Failure},
  author = {Barnett, Scott and Kurniawan, Stefanus and Thudumu, Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
  year = {2024},
  booktitle = {2024 {IEEE},
  pages = {194--199},
  keywords = {Case Study, Chatbots, Education, Information retrieval, RAG, Retrieval Augmented Generation, Robustness, SE4AI, Semantic search, Software, Task analysis, source: IEEE},
  abstract = {Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS• Software and its engineering → Empirical software validation.},
  month = {apr},
}

@article{martin_semantic_2024,
  title = {Semantic verification in large language model-based retrieval augmented generation},
  author = {Martin, A. and Witschel, H. F. and Mandl, M. and {...},
  year = {2024},
  url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/31199},
  journal = {Proceedings of the AAAI …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… verification in Large Language Model-based Retrieval Augmented Generation (LLM-RAG) … Large Language Models (LLMs) in maintaining factual integrity, this research proposes an …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nguyen_sfr-rag_2024,
  title = {Sfr-rag: {Towards},
  author = {Nguyen, X. P. and Pandit, S. and Purushwalkam, S. and Xu, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.09916},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… SFRRAG, a small LLM that is instruction-tuned with an emphasis on context-grounded generation and hallucination … on the generator LLM component of the RAG framework. Traditional …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{joren_sufficient_2024,
  title = {Sufficient context: {A},
  author = {Joren, H. and Zhang, J. and Ferng, C. S. and Juan, D. C. and Taly, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.06037},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective … Another line of study aims to reduce LLM hallucinations in RAG settings. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bora_systematic_2024-1,
  title = {Systematic analysis of retrieval-augmented generation-based llms for medical chatbot applications},
  author = {Bora, A. and Cuayáhuitl, H.},
  year = {2024},
  url = {https://www.mdpi.com/2504-4990/6/4/116},
  journal = {Machine Learning and Knowledge Extraction},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… of RAG systems to mitigate hallucinations [13], a prevalent issue with LLMs. Hallucination … When an LLM receives a query, the RAG system employs the same embedding model used …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_simple_2024,
  title = {Simple is effective: {The},
  author = {Li, M. and Miao, S. and Li, P.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.20724},
  journal = {arXiv preprint arXiv:2410.20724},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To evaluate the capability of LLM reasoners in knowledge-grounded hallucination-free question answering, we introduce WebQSP-sub and CWQ-sub, where we remove samples …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_simulated_2024,
  title = {Simulated patient systems are intelligent when powered by large language model-based {AI},
  author = {Yu, H. and Zhou, J. and Li, L. and Chen, S. and Gallifant, J. and {...},
  year = {2024},
  url = {https://www.researchgate.net/profile/Huizi-Yu/publication/384447372_AIPatient_Simulating_Patients_with_EHRs_and_LLM_Powered_Agentic_Workflow/links/66fd50b1869f1104c6c3dba0/AIPatient-Simulating-Patients-with-EHRs-and-LLM-Powered-Agentic-Workflow.pdf},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Reasoning RAG leverages six LLM powered agents spanning tasks including retrieval, KG … Our Reasoning RAG agentic framework improves accuracy and minimizes hallucination risk…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tang_symphony_2024,
  title = {Symphony: {Towards},
  author = {Tang, N. and Yang, C. and Zhang, Z. and Luo, Y. and Fan, J. and {...},
  year = {2024},
  url = {http://sites.computer.org/debull/A24dec/p135.pdf},
  journal = {IEEE Data Eng …},
  note = {Publisher: sites.computer.org},
  keywords = {source: Google Scholar},
  abstract = {… trust in LLM outputs. By 2023, analysts estimated that chatbots hallucinate as much as 27\% of the time1, and factual … natural language question Q requiring factual or objective answers, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ong_surgeryllm_2024,
  title = {{SurgeryLLM},
  author = {Ong, C. S. and Obey, N. T. and Zheng, Y. and Cohan, A. and {...},
  year = {2024},
  url = {https://www.nature.com/articles/s41746-024-01391-3},
  journal = {npj Digital …},
  note = {Publisher: nature.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… RAG improves LLM output by incorporating information from an approved, trusted, curated … We sought to assess the feasibility of and potential benefits of incorporating RAG in a LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kortukov_studying_2024,
  title = {Studying large language model behaviors under context-memory conflicts with real documents},
  author = {Kortukov, E. and Rubinstein, A. and Nguyen, E. and Oh, S. J.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.16032},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… -world RAG application and allows us to study LLM knowledge-… , or irrelevant documents into the LLM prompt. However, … goal is to achieve factual correctness using RAG. Asserting the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{saleh_sg-rag_2024,
  title = {{SG},
  author = {Saleh, A. O. M. and Tür, G. and Saygin, Y.},
  year = {2024},
  url = {https://aclanthology.org/2024.icnlsp-1.45.pdf},
  journal = {Proceedings of the 7th International …},
  note = {Publisher: aclanthology.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Large Language Models (LLM) such as GPT3 and Llama tend to hallucinate, especially for … LLM for our evaluation. To analyze that issue further, we evaluated SG-RAG, and RAG on the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{arslan_sustainable_2024,
  title = {Sustainable digitalization of business with multi-agent rag and llm},
  author = {Arslan, M. and Munawar, S. and Cruz, C.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050924023627},
  journal = {Procedia Computer Science},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… and enhance factuality and reasoning in LLMs. MetaGPT [33] introduces a specialized LLM … AutoGen2 [11], an open-source framework, enables developers to create LLM applications …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{iannelli_sla_2024,
  title = {{SLA},
  author = {Iannelli, M. and Kuchipudi, S. and Dvorak, V.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.06832},
  journal = {arXiv preprint arXiv:2412.06832},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucination, ie the generation of false assertions [10]. Retrieval Augmented Generation (RAG) … , enabling the LLM to function as a reasoning layer rather than a static repository of facts. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{garigliotti_sdg_2024,
  title = {{SDG},
  author = {Garigliotti, D.},
  year = {2024},
  url = {https://aclanthology.org/2024.climatenlp-1.19/},
  journal = {Proceedings of the 1st Workshop on Natural …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… by an LLM to refer to each passage that the LLM considers to … allowing that the LLM may hallucinate typical reference … as expected that ChatGPT is the best performing LLM in several …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_stall_2024,
  title = {Stall+: {Boosting},
  author = {Liu, J. and Chen, Y. and Liu, M. and Peng, X. and Lou, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.10018},
  journal = {arXiv preprint arXiv:2406.10018},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG with individual or multiple integration strategies can further substantially improve LLM-based … To mitigate the hallucination of LLM-based generation, there is an increasing body of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fossi_swiftdossier_2024,
  title = {Swiftdossier: {Tailored},
  author = {Fossi, G. and Boulaimen, Y. and Outemzabet, L. and Jeanray, N. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.15817},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of an advanced RAG system can help the LLM to … citations to allow us to track back which sources are used for each section and slide. For example, when the LLM is using the RAG, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{arslan_sustainable_2024-1,
  title = {Sustainable {Energy},
  author = {Arslan, M. and Munawar, S. and Sibilla, M.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10836639/},
  journal = {… International Conference on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… This section starts by describing the process of creating a dataset aimed at enhancing RAG-LLM-based IS, developed for the Energy sector to serve as an Energy QA Assistant. The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kortukov_studying_2024-1,
  title = {Studying large language model behaviors under realistic knowledge conflicts},
  author = {Kortukov, E. and Rubinstein, A. and Nguyen, E. and Oh, S. J.},
  year = {2024},
  url = {https://openreview.net/forum?id=N0VeagzHq1},
  journal = {CoRR},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… : Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding. In RAG, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_seeing_2024,
  title = {Seeing is believing: {Black},
  author = {Li, Y. and Liu, G. and Yang, Y. and Wang, C.},
  year = {2024},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240619234L/abstract},
  journal = {arXiv e-prints},
  note = {Publisher: ui.adsabs.harvard.edu},
  keywords = {source: Google Scholar},
  abstract = {… to mitigate common LLM issues such as hallucinations and outdated … RAG systems, making them susceptible to attacks like jailbreaks and prompt injections, the security of the RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{noauthor_sigir-ap_2024,
  title = {{SIGIR},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215525372&partnerID=40&md5=2e14aa68b7e9aa3ebcc889b137a9122e},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_sicsa-reallm_2024,
  title = {{SICSA},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210021824&partnerID=40&md5=a37311d16502cece39ea494853bb34f1},
  booktitle = {{CEUR},
  volume = {3822},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{collado-montanez_separating_2024,
  title = {Separating {Linguistic},
  author = {Collado-Montañez, Jaime},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207942482&partnerID=40&md5=1cbd34555c4d57f3f6b225758090062d},
  booktitle = {{CEUR},
  volume = {3797},
  pages = {173 -- 177},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{asai_self-rag_2024,
  title = {{SELF},
  author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200583576&partnerID=40&md5=11f85d13cc96fb96d39fbb16ba5f717d},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 126},
}

@article{chalumeau_qdax_2024,
  title = {{QDax},
  author = {Chalumeau, Felix and Lim, Bryan and Boige, Raphaël and Allard, Maxime and Grillotti, Luca and Flageat, Manon and Macé, Valentin and Richard, Guillaume and Flajolet, Arthur and Pierrot, Thomas and Cully, Antoine},
  year = {2024},
  journal = {J. Mach. Learn. Res.},
  volume = {25},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {source: ACM},
  abstract = {qdax is an open-source library with a streamlined and modular API for Quality-Diversity (QD) optimisation algorithms in jax. The library serves as a versatile tool for optimisation purposes, ranging from black-box optimisation to continuous control. QDAX offers implementations of popular QD, Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by various examples. All the implementations can be just-in-time compiled with JAX, facilitating efficient execution across multiple accelerators, including GPUs and TPUs. These implementations effectively demonstrate the framework's flexibility and user-friendliness, easing experimentation for research purposes. Furthermore, the library is thoroughly documented and has 93\% test coverage.},
  month = {jan},
  issn = {1532-4435},
}

@article{raina_question-based_2024,
  title = {Question-based retrieval using atomic units for enterprise rag},
  author = {Raina, V. and Gales, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2405.12363},
  journal = {arXiv preprint arXiv:2405.12363},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM using the RAG pipeline. … RAG as the information content of the context is based on extracts from novels. As the stories are fictional and not factual, the parametric memory of an LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{beauchemin_quebec_2024,
  title = {Quebec {Automobile},
  author = {Beauchemin, David and Gagnon, Zachary and Khoury, Richard},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216927418&partnerID=40&md5=555e93fb7f633adc5844318ab0f77446},
  pages = {48 -- 60},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{hang_under-bagging_2022,
  title = {Under-bagging nearest neighbors for imbalanced classification},
  author = {Hang, Hanyuan and Cai, Yuchao and Yang, Hanfang and Lin, Zhouchen},
  year = {2022},
  journal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {arithmetic mean measure, bagging, ensemble learning, imbalanced classification, k-nearest neighbors, learning theory, optimal convergence rates, under-sampling, source: ACM},
  abstract = {In this paper, we propose an ensemble learning algorithm called under-bagging k-nearest neighbors (under-bagging k-NN) for imbalanced classification problems. On the theoretical side, by developing a new learning theory analysis, we show that with properly chosen parameters, i.e., the number of nearest neighbors k, the expected sub-sample size s, and the bagging rounds B, optimal convergence rates for under-bagging k-NN can be achieved under mild assumptions w.r.t. the arithmetic mean (AM) of recalls. Moreover, we show that with a relatively small B, the expected sub-sample size s can be much smaller than the number of training data n at each bagging round, and the number of nearest neighbors k can be reduced simultaneously, especially when the data are highly imbalanced, which leads to substantially lower time complexity and roughly the same space complexity. On the practical side, we conduct numerical experiments to verify the theoretical results on the benefits of the under-bagging technique by the promising AM performance and efficiency of our proposed algorithm.},
  issn = {1532-4435},
  month = {jan},
}

@article{patterson_generalized_2022,
  title = {A generalized projected bellman error for off-policy value estimation in reinforcement learning},
  author = {Patterson, Andrew and White, Adam and White, Martha},
  year = {2022},
  journal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {off-policy learning, reinforcement learning, temporal difference learning, source: ACM},
  abstract = {Many reinforcement learning algorithms rely on value estimation, however, the most widely used algorithms–namely temporal difference algorithms–can diverge under both off-policy sampling and nonlinear function approximation. Many algorithms have been developed for off-policy value estimation based on the linear mean squared projected Bellman error (PBE) and are sound under linear function approximation. Extending these methods to the nonlinear case has been largely unsuccessful. Recently, several methods have been introduced that approximate a different objective–the mean-squared Bellman error (BE)– which naturally facilitate nonlinear approximation. In this work, we build on these insights and introduce a new generalized PBE that extends the linear PBE to the nonlinear setting. We show how this generalized objective unifies previous work and obtain new bounds for the value error of the solutions of the generalized objective. We derive an easy-to-use, but sound, algorithm to minimize the generalized objective, and show that it is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation.},
  issn = {1532-4435},
  month = {jan},
}

@article{zhang_global_2022,
  title = {Global optimality and finite sample analysis of softmax off-policy actor critic under state distribution mismatch},
  author = {Zhang, Shangtong and Des Combes, Remi Tachet and Laroche, Romain},
  year = {2022},
  journal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {actor-critic, density ratio, distribution Mismatch, off-policy learning, policy gradient, source: ACM},
  abstract = {In this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy. Our work goes beyond existing works on the optimality of policy gradient methods in that existing works use the exact policy gradient for updating the policy parameters while we use an approximate and stochastic update step. Our update step is not a gradient update because we do not use a density ratio to correct the state distribution, which aligns well with what practitioners do. Our update is approximate because we use a learned critic instead of the true value function. Our update is stochastic because at each step the update is done for only the current state action pair. Moreover, we remove several restrictive assumptions from existing works in our analysis. Central to our work is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous Markov chains, based on its uniform contraction properties.},
  issn = {1532-4435},
  month = {jan},
}

@article{zhang_truncated_2022,
  title = {Truncated emphatic temporal difference methods for prediction and control},
  author = {Zhang, Shangtong and Whiteson, Shimon},
  year = {2022},
  journal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {approximate value iteration, emphatic methods, finite sample analysis, off-policy learning, reinforcement learning, source: ACM},
  abstract = {Emphatic Temporal Diérence (TD) methods are a class of off-policy Reinforcement Learning (RL) methods involving the use of followon traces. Despite the theoretical success of emphatic TD methods in addressing the notorious deadly triad of off-policy RL, there are still two open problems. First, followon traces typically suffer from large variance, making them hard to use in practice. Second, though Yu (2015) confirms the asymptotic convergence of some emphatic TD methods for prediction problems, there is still no finite sample analysis for any emphatic TD method for prediction, much less control. In this paper, we address those two open problems simultaneously via using truncated followon traces in emphatic TD methods. Unlike the original followon traces, which depend on all previous history, truncated followon traces depend on only finite history, reducing variance and enabling the finite sample analysis of our proposed emphatic TD methods for both prediction and control.},
  issn = {1532-4435},
  month = {jan},
}

@article{zhang_dynamic_2021,
  title = {Dynamic tensor recommender systems},
  author = {Zhang, Yanqing and Bi, Xuan and Tang, Niansheng and Qu, Annie},
  year = {2021},
  journal = {J. Mach. Learn. Res.},
  volume = {22},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {contextual information, dynamic recommender systems, polynomial spline approximation, prediction interval, product sales forecasting, source: ACM},
  abstract = {Recommender systems have been extensively used by the entertainment industry, business marketing and the biomedical industry. In addition to its capacity of providing preference-based recommendations as an unsupervised learning methodology, it has been also proven useful in sales forecasting, product introduction and other production related businesses. Since some consumers and companies need a recommendation or prediction for future budget, labor and supply chain coordination, dynamic recommender systems for precise forecasting have become extremely necessary. In this article, we propose a new recommendation method, namely the dynamic tensor recommender system (DTRS), which aims particularly at forecasting future recommendation. The proposed method utilizes a tensor-valued function of time to integrate time and contextual information, and creates a time-varying coefficient model for temporal tensor factorization through a polynomial spline approximation. Major advantages of the proposed method include competitive future recommendation predictions and effective prediction interval estimations. In theory, we establish the convergence rate of the proposed tensor factorization and asymptotic normality of the spline coefficient estimator. The proposed method is applied to simulations, IRI marketing data and Last.fm data. Numerical studies demonstrate that the proposed method outperforms existing methods in terms of future time forecasting.},
  issn = {1532-4435},
  month = {jan},
}

@article{henderson_towards_2020,
  title = {Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  year = {2020},
  journal = {J. Mach. Learn. Res.},
  volume = {21},
  number = {1},
  note = {Publisher: JMLR.org},
  keywords = {climate change, deep learning, energy efficiency, green computing, reinforcement learning, source: ACM},
  abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.},
  issn = {1532-4435},
  month = {jan},
}

@article{namer_what_2025,
  title = {What 96 {Designers},
  author = {Namer, Lexi and Joines, Sharon},
  year = {2025},
  journal = {J. User Exper.},
  volume = {20},
  number = {3},
  pages = {125--142},
  note = {Place: Bloomingdale, IL
Publisher: Usability Professionals' Association},
  keywords = {digital product design, ethical awareness, harm reduction, harm-aware design, survey, user experience design, source: ACM},
  abstract = {Potential harm within digital product design has long been underexplored, despite the growing influence that consumer-facing digital products exert on individuals' daily lives. This paper presents the methodology and findings from an online survey conducted with 96 US-based UX- and product-designers working on customer-facing digital products. The survey focused on the attitudes, behaviors, challenges, and needs that designers encounter while considering harm in their daily work. Our findings resulted in several recommendations for future research to develop practice-based design solutions that enable designers to more effectively identify, discuss, and mitigate potential harm stemming from their work.},
  month = {may},
  issn = {1931-3357},
}

@article{zeng_worse_2025,
  title = {Worse than zero-shot? a fact-checking dataset for evaluating the robustness of rag against misleading retrievals},
  author = {Zeng, L. and Gupta, R. and Motwani, D. and Yang, D. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.16101},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-augmented generation (RAG) systems have shown significant promise in mitigating LLM hallucination and enhancing trustworthiness. By combining the generative …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zou_weak--strong_2025,
  title = {Weak-to-{Strong},
  author = {Zou, D. and Chen, Y. and Li, M. and Miao, S. and Liu, C. and Han, B. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2506.22518},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) … Recent efforts in text-based RAG have explored using LLM feedback to directly optimize the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cohen_wixqa_2025,
  title = {Wixqa: {A},
  author = {Cohen, D. and Burg, L. and Pykhnivskyi, S. and Gur, H. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.08643},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… context and avoided introducing factual errors or hallucinations. For our experiments, we utilized GPT-4o as the LLM judge for this metric. • Context Recall: An LLM-based judge metric …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_what_2025-1,
  title = {What are {Models},
  author = {Wang, P. and Liu, Y. and Lu, Y. and Hong, J. and Wu, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.13490},
  journal = {arXiv preprint arXiv:2502.13490},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of RAG on LLM inference. For each question, the answer was converted into a retrieval-augmented knowledge base (RAG) to assist the LLM. … : with and without RAG. Features such as …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{zhang_wikigenbench_2025,
  title = {{WIKIGENBENCH},
  author = {Zhang, Jiebin and Yu, Eugene J. and Chen, Qinyu and Xiong, Chenhao and Zhu, Dawei and Qian, Han and Song, Mingbo and Xiong, Weimin and Li, Xiaoguang and Liu, Qun and Li, Sujian},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218502400&partnerID=40&md5=4ab1f3c4ddd441a0d6652f76a00cfc69},
  booktitle = {Proceedings - {International},
  pages = {5191 -- 5210},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_workshop_2025,
  title = {Workshop on {Generative},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217667532&partnerID=40&md5=37ba42220fa19d6f67561cd2dd5e4328},
  booktitle = {Proceedings - {International},
  volume = {2025-January},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_workshop_2025-1,
  title = {Workshop on {Advanced},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009784071&partnerID=40&md5=5c4e3510f1d89763919f44fbcd1a4f9b},
  journal = {Lecture Notes in Computer Science},
  volume = {15835 LNAI},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@misc{noauthor_ieee_nodate,
  title = {{IEEE},
  url = {https://ieeexplore.proxyucr.elogim.com/search/searchresult.jsp?action=search&newsearch=true&matchBoolean=true&queryText=(%22All%20Metadata%22:%22retrieval-augmented%20generation%22%20OR%20%22All%20Metadata%22:%22RAG%22)%20AND%20(%22All%20Metadata%22:%22large%20language%20model%22%20OR%20%22All%20Metadata%22:%22LLM%22%20OR%20%22All%20Metadata%22:%22LLMs%22%20OR%20%22All%20Metadata%22:%22generative%20AI%22%20OR%20%22All%20Metadata%22:%22ChatGPT%22)%20AND%20(%22All%20Metadata%22:%22trust%22%20OR%20%22All%20Metadata%22:%22confidence%22%20OR%20%22All%20Metadata%22:%22credibility%22%20OR%20%22All%20Metadata%22:%22user%20perception%22%20OR%20%22All%20Metadata%22:%22perceived%20reliability%22%20OR%20%22All%20Metadata%22:%22overtrust%22%20OR%20%22All%20Metadata%22:%22calibration%22%20OR%20%22All%20Metadata%22:%22hallucination%22%20OR%20%22All%20Metadata%22:%22hallucinations%22%20OR%20%22All%20Metadata%22:%22factuality%22%20OR%20%22All%20Metadata%22:%22factual%20accuracy%22%20OR%20%22All%20Metadata%22:%22faithfulness%22%20OR%20%22All%20Metadata%22:%22citation%22%20OR%20%22All%20Metadata%22:%22citations%22%20OR%20%22All%20Metadata%22:%22reference%22%20OR%20%22All%20Metadata%22:%22source%20attribution%22%20OR%20%22All%20Metadata%22:%22provenance%22%20OR%20%22All%20Metadata%22:%22grounding%22%20OR%20%22All%20Metadata%22:%22explainable%20artificial%20intelligence%22%20OR%20%22All%20Metadata%22:%22explainable%20AI%22%20OR%20%22All%20Metadata%22:%22XAI%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22interpretable%20AI%22%20OR%20%22All%20Metadata%22:%22interpretable%20artificial%20intelligence%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22AI%20explainability%22%20OR%20%22All%20Metadata%22:%22AI%20interpretability%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22transparent%20AI%22%20OR%20%22All%20Metadata%22:%22AI%20transparency%22%20%20%20%20%20%20OR%20%22All%20Metadata%22:%22model%20interpretability%22%20OR%20%22All%20Metadata%22:%22post-hoc%20explanation%22%20OR%20%22All%20Metadata%22:%22feature%20attribution%22)&ranges=2020_2025_Year},
  keywords = {source: IEEE},
  file = {IEEE Xplore Search Results:C\:\\Users\\Marco\\Zotero\\storage\\VQJXQ254\\searchresult.html:text/html},
  urldate = {2025-10-26},
}

@book{farris_notitle_2025,
  author = {Farris, Drew and Raff, Edward and Biderman, Stella},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/11079740},
  publisher = {Manning},
  note = {Publication Title: How Large Language Models Work},
  keywords = {AI book, artificial intelligence, ChatGPT, Gemini, GPT, large language models, LLM application, machine learning, natural language processing, prompt engineering, retrieval augmented generation, RLHF, supervised fine-tuning, tokenization, transformers, source: IEEE},
  abstract = {Learn how large language models like GPT and Gemini work under the hood in plain English. How Large Language Models Work translates years of expert research on Large Language Models into a readable, focused introduction to working with these amazing systems. It explains clearly how LLMs function, introduces the optimization techniques to fine-tune them, and shows how to create pipelines and processes to ensure your AI applications are efficient and error-free. In How Large Language Models Work you will learn how to: Test and evaluate LLMs Use human feedback, supervised fine-tuning, and Retrieval Augmented Generation (RAG) Reducing the risk of bad outputs, high-stakes errors, and automation bias Human-computer interaction systems Combine LLMs with traditional ML How Large Language Models Work is authored by top machine learning researchers at Booz Allen Hamilton, including researcher Stella Biderman, Director of AI/ML Research Drew Farris, and Director of Emerging AI Edward Raff. They lay out how LLM and GPT technology works in plain language that’s accessible and engaging for all.},
  isbn = {978-1-63343-708-1},
}

@book{kimothi_notitle_2025,
  author = {Kimothi, Abhinav},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/11079738},
  publisher = {Manning},
  note = {Publication Title: A Simple Guide to Retrieval Augmented Generation},
  keywords = {AI hallucination reduction, AI pipelines, generative AI, knowledge base, LangChain, large language models, LLMs, OpenAI, prompt engineering, Python AI, RAG evaluation, RAG guide, RAG tools, retrieval augmented generation, Transformers, vector databases, source: IEEE},
  abstract = {Everything you need to know about Retrieval Augmented Generation in one human-friendly guide. Retrieval Augmented Generation—or RAG—enhances an LLM’s available data by adding context from an external knowledge base, so it can answer accurately about proprietary content, recent information, and even live conversations. RAG is powerful, and with A Simple Guide to Retrieval Augmented Generation, it’s also easy to understand and implement! In A Simple Guide to Retrieval Augmented Generation you’ll learn: The components of a RAG system How to create a RAG knowledge base The indexing and generation pipeline Evaluating a RAG system Advanced RAG strategies RAG tools, technologies, and frameworks A Simple Guide to Retrieval Augmented Generation gives an easy, yet comprehensive, introduction to RAG for AI beginners. You’ll go from basic RAG that uses indexing and generation pipelines, to modular RAG and multimodal data from images, spreadsheets, and more.},
  isbn = {978-1-63343-585-8},
}

@book{raieli_notitle_2025,
  author = {Raieli, Salvatore and Iuculano, Gabriele},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/11099035},
  publisher = {Packt Publishing},
  note = {Publication Title: Building AI Agents with LLMs, RAG, and Knowledge Graphs: A practical guide to autonomous and modern AI agents},
  keywords = {source: IEEE},
  abstract = {Master LLM fundamentals to advanced techniques like RAG, reinforcement learning, and knowledge graphs to build, deploy, and scale intelligent AI agents that reason, retrieve, and act autonomouslyKey FeaturesImplement RAG and knowledge graphs for advanced problem-solvingLeverage innovative approaches like LangChain to create real-world intelligent systemsIntegrate large language models, graph databases, and tool use for next-gen AI solutionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThis AI agents book addresses the challenge of building AI that not only generates text but also grounds its responses in real data and takes action. Authored by AI specialists with deep expertise in drug discovery and systems optimization, this guide empowers you to leverage retrieval-augmented generation (RAG), knowledge graphs, and agent-based architectures to engineer truly intelligent behavior. By combining large language models (LLMs) with up-to-date information retrieval and structured knowledge, you'll create AI agents capable of deeper reasoning and more reliable problem-solving. Inside, you'll find a practical roadmap from concept to implementation. You’ll discover how to connect language models with external data via RAG pipelines for increasing factual accuracy and incorporate knowledge graphs for context-rich reasoning. The chapters will help you build and orchestrate autonomous agents that combine planning, tool use, and knowledge retrieval to achieve complex goals. Concrete Python examples built on popular libraries, along with real-world case studies, reinforce each concept and show you how these techniques come together. By the end of this book, you’ll be well-equipped to build intelligent AI agents that reason, retrieve, and interact dynamically, empowering you to deploy powerful AI solutions across industries.What you will learnLearn how LLMs work, their structure, uses, and limits, and design RAG pipelines to link them to external dataBuild and query knowledge graphs for structured context and factual groundingDevelop AI agents that plan, reason, and use tools to complete tasksIntegrate LLMs with external APIs and databases to incorporate live dataApply techniques to minimize hallucinations and ensure accurate outputsOrchestrate multiple agents to solve complex, multi-step problemsOptimize prompts, memory, and context handling for long-running tasksDeploy and monitor AI agents in production environmentsWho this book is forIf you are a data scientist or researcher who wants to learn how to create and deploy an AI agent to solve limitless tasks, this book is for you. To get the most out of this book, you should have basic knowledge of Python and Gen AI. This book is also excellent for experienced data scientists who want to explore state-of-the-art developments in LLM and LLM-based applications.},
  isbn = {978-1-83508-038-2},
}

@book{anthapu_notitle_2025,
  author = {Anthapu, Ravindranatha and Agarwal, Siddhant and Webber, Dr. Jim and Risch, Dr. Julian},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/11099031},
  publisher = {Packt Publishing},
  note = {Publication Title: Building Neo4j-Powered Applications with LLMs: Create LLM-driven search and recommendations applications with Haystack, LangChain4j, and Spring AI},
  keywords = {source: IEEE},
  abstract = {A comprehensive guide to building cutting-edge generative AI applications using Neo4j's knowledge graphs and vector search capabilitiesKey FeaturesDesign vector search and recommendation systems with LLMs using Neo4j GenAI, Haystack, Spring AI, and LangChain4jApply best practices for graph exploration, modeling, reasoning, and performance optimizationBuild and consume Neo4j knowledge graphs and deploy your GenAI apps to Google CloudPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEmbark on an expert-led journey into building LLM-powered applications using Retrieval-Augmented Generation (RAG) and Neo4j knowledge graphs. Written by Ravindranatha Anthapu, Principal Consultant at Neo4j, and Siddhant Agrawal, a Google Developer Expert in GenAI, this comprehensive guide is your starting point for exploring alternatives to LangChain, covering frameworks such as Haystack, Spring AI, and LangChain4j. As LLMs (large language models) reshape how businesses interact with customers, this book helps you develop intelligent applications using RAG architecture and knowledge graphs, with a strong focus on overcoming one of AI’s most persistent challenges—mitigating hallucinations. You'll learn how to model and construct Neo4j knowledge graphs with Cypher to enhance the accuracy and relevance of LLM responses. Through real-world use cases like vector-powered search and personalized recommendations, the authors help you build hands-on experience with Neo4j GenAI integrations across Haystack and Spring AI. With access to a companion GitHub repository, you’ll work through code-heavy examples to confidently build and deploy GenAI apps on Google Cloud. By the end of this book, you’ll have the skills to ground LLMs with RAG and Neo4j, optimize graph performance, and strategically select the right cloud platform for your GenAI applications.What you will learnDesign, populate, and integrate a Neo4j knowledge graph with RAGModel data for knowledge graphsIntegrate AI-powered search to enhance knowledge explorationMaintain and monitor your AI search application with HaystackUse LangChain4j and Spring AI for recommendations and personalizationSeamlessly deploy your applications to Google Cloud PlatformWho this book is forThis LLM book is for database developers and data scientists who want to leverage knowledge graphs with Neo4j and its vector search capabilities to build intelligent search and recommendation systems. Working knowledge of Python and Java is essential to follow along. Familiarity with Neo4j, the Cypher query language, and fundamental concepts of databases will come in handy.},
  isbn = {978-1-83620-622-4},
}

@book{de_notitle_2025,
  author = {De, Banibrata},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/11107332},
  publisher = {Packt Publishing},
  note = {Publication Title: Hands-On MLOps on Azure: Automate, secure, and scale ML workflows with the Azure ML CLI, GitHub, and LLMOps},
  keywords = {source: IEEE},
  abstract = {A practical guide to building, deploying, automating, monitoring, and scaling ML and LLM solutions in productionKey FeaturesBuild reproducible ML pipelines with Azure ML CLI and GitHub ActionsAutomate ML workflows end to end, including deployment and monitoringApply LLMOps principles to deploy and manage generative AI responsibly across cloudsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionEffective machine learning (ML) now demands not just building models but deploying and managing them at scale. Written by a seasoned senior software engineer with high-level expertise in both MLOps and LLMOps, Hands-On MLOps on Azure equips ML practitioners, DevOps engineers, and cloud professionals with the skills to automate, monitor, and scale ML systems across environments. The book begins with MLOps fundamentals and their roots in DevOps, exploring training workflows, model versioning, and reproducibility using pipelines. You'll implement CI/CD with GitHub Actions and the Azure ML CLI, automate deployments, and manage governance and alerting for enterprise use. The author draws on their production ML experience to provide you with actionable guidance and real-world examples. A dedicated section on LLMOps covers operationalizing large language models (LLMs) such as GPT-4 using RAG patterns, evaluation techniques, and responsible AI practices. You'll also work with case studies across Azure, AWS, and GCP that offer practical context for multi-cloud operations. Whether you're building pipelines, packaging models, or deploying LLMs, this guide delivers end-to-end strategy to build robust, scalable systems. By the end of this book, you'll be ready to design, deploy, and maintain enterprise-grade ML solutions with confidence. What you will learnUnderstand the DevOps to MLOps transitionBuild reproducible, reusable pipelines using the Azure ML CLISet up CI/CD for training and deployment workflowsMonitor ML applications and detect model/data driftCapture and secure governance and lineage dataOperationalize LLMs using RAG and prompt flowsApply MLOps across Azure, AWS, and GCP use casesWho this book is forThis book is for DevOps and Cloud engineers and SREs interested in or responsible for managing the lifecycle of machine learning models. Professionals who are already familiar with their ML workloads and want to improve their practices, or those who are new to MLOps and want to learn how to effectively manage machine learning models in this environment, will find this book beneficial. The book is also useful for technical decision-makers and project managers looking to understand the process and benefits of MLOps.},
  isbn = {978-1-83620-032-1},
}

@inproceedings{djoudi_notitle_2025,
  author = {Djoudi, Anya Amel Nait and null, null and null, null and Badache, Ismail and Chifu, Adrian Gabriel and Bellot, Patrice},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019062765&partnerID=40&md5=9eb12756c985549ca3f719687c2de8d7},
  booktitle = {{CEUR},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{borazio_notitle_2025,
  author = {Borazio, Federico and null, null and Croce, Danilo and Basili, Roberto},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019054960&partnerID=40&md5=2a7a116863c4a3bafcf4d9b2c85568d6},
  booktitle = {{CEUR},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{kim_notitle_2025,
  author = {Kim, To Eun and Coelho, João and Onilude, Gbemileke and null, null},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019052751&partnerID=40&md5=0793589bbc7cf9cb419a1bbe4226846f},
  booktitle = {{CEUR},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@book{bourne_notitle_2024,
  author = {Bourne, Keith and Es, Shahul},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10769240},
  publisher = {Packt Publishing},
  note = {Publication Title: Unlocking Data with Generative AI and RAG: Enhance generative AI systems by integrating internal data with large language models using RAG},
  keywords = {source: IEEE},
  abstract = {Leverage cutting-edge generative AI techniques such as RAG to realize the potential of your data and drive innovation as well as gain strategic advantageKey FeaturesOptimize data retrieval and generation using vector databasesBoost decision-making and automate workflows with AI agentsOvercome common challenges in implementing real-world RAG systemsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionGenerative AI is helping organizations tap into their data in new ways, with retrieval-augmented generation (RAG) combining the strengths of large language models (LLMs) with internal data for more intelligent and relevant AI applications. The author harnesses his decade of ML experience in this book to equip you with the strategic insights and technical expertise needed when using RAG to drive transformative outcomes. The book explores RAG’s role in enhancing organizational operations by blending theoretical foundations with practical techniques. You’ll work with detailed coding examples using tools such as LangChain and Chroma’s vector database to gain hands-on experience in integrating RAG into AI systems. The chapters contain real-world case studies and sample applications that highlight RAG’s diverse use cases, from search engines to chatbots. You’ll learn proven methods for managing vector databases, optimizing data retrieval, effective prompt engineering, and quantitatively evaluating performance. The book also takes you through advanced integrations of RAG with cutting-edge AI agents and emerging non-LLM technologies. By the end of this book, you’ll be able to successfully deploy RAG in business settings, address common challenges, and push the boundaries of what’s possible with this revolutionary AI technique.What you will learnUnderstand RAG principles and their significance in generative AIIntegrate LLMs with internal data for enhanced operationsMaster vectorization, vector databases, and vector search techniquesDevelop skills in prompt engineering specific to RAG and design for precise AI responsesFamiliarize yourself with AI agents' roles in facilitating sophisticated RAG applicationsOvercome scalability, data quality, and integration issuesDiscover strategies for optimizing data retrieval and AI interpretabilityWho this book is forThis book is for AI researchers, data scientists, software developers, and business analysts looking to leverage RAG and generative AI to enhance data retrieval, improve AI accuracy, and drive innovation. It is particularly suited for anyone with a foundational understanding of AI who seeks practical, hands-on learning. The book offers real-world coding examples and strategies for implementing RAG effectively, making it accessible to both technical and non-technical audiences. A basic understanding of Python and Jupyter Notebooks is required.},
  isbn = {978-1-83588-791-2},
}

@book{gheorghiu_notitle_2024,
  author = {Gheorghiu, Andrei},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10540158},
  publisher = {Packt Publishing},
  note = {Publication Title: Building Data-Driven Applications with LlamaIndex: A practical guide to retrieval-augmented generation (RAG) to enhance LLM applications},
  keywords = {source: IEEE},
  abstract = {Solve real-world problems easily with artificial intelligence (AI) using the LlamaIndex data framework to enhance your LLM-based Python applications Key FeaturesExamine text chunking effects on RAG workflows and understand security in RAG app developmentDiscover chatbots and agents and learn how to build complex conversation enginesBuild as you learn by applying the knowledge you gain to a hands-on projectBook DescriptionDiscover the immense potential of Generative AI and Large Language Models (LLMs) with this comprehensive guide. Learn to overcome LLM limitations, such as contextual memory constraints, prompt size issues, real-time data gaps, and occasional ‘hallucinations’. Follow practical examples to personalize and launch your LlamaIndex projects, mastering skills in ingesting, indexing, querying, and connecting dynamic knowledge bases. From fundamental LLM concepts to LlamaIndex deployment and customization, this book provides a holistic grasp of LlamaIndex's capabilities and applications. By the end, you'll be able to resolve LLM challenges and build interactive AI-driven applications using best practices in prompt engineering and troubleshooting Generative AI projects.What you will learnUnderstand the LlamaIndex ecosystem and common use casesMaster techniques to ingest and parse data from various sources into LlamaIndexDiscover how to create optimized indexes tailored to your use casesUnderstand how to query LlamaIndex effectively and interpret responsesBuild an end-to-end interactive web application with LlamaIndex, Python, and StreamlitCustomize a LlamaIndex configuration based on your project needsPredict costs and deal with potential privacy issuesDeploy LlamaIndex applications that others can useWho this book is forThis book is for Python developers with basic knowledge of natural language processing (NLP) and LLMs looking to build interactive LLM applications. Experienced developers and conversational AI developers will also benefit from the advanced techniques covered in the book to fully unleash the capabilities of the framework.},
  isbn = {978-1-80512-440-5},
}

@book{bahree_notitle_2024,
  author = {Bahree, Amit},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10745288},
  publisher = {Manning},
  note = {Publication Title: Generative AI in Action},
  keywords = {architectural patterns, Bard, ChatGPT, Copilot, enterprise, ethics, hallucinations, integration, jailbreaks, LLMs, model fine tuning, multi-modality, prompt engineering, RAG, safety, source: IEEE},
  abstract = {Generative AI can transform your business by streamlining the process of creating text, images, and code. This book will show you how to get in on the action! Generative AI in Action is the comprehensive and concrete guide to generative AI you’ve been searching for. It introduces both AI’s fundamental principles and its practical applications in an enterprise context—from generating text and images for product catalogs and marketing campaigns, to technical reporting, and even writing software. Inside, author Amit Bahree shares his experience leading Generative AI projects at Microsoft for nearly a decade, starting well before the current GPT revolution. Inside Generative AI in Action you will find: A practical overview of of generative AI applications Architectural patterns, integration guidance, and best practices for generative AI The latest techniques like RAG, prompt engineering, and multi-modality The challenges and risks of generative AI like hallucinations and jailbreaks How to integrate generative AI into your business and IT strategy Generative AI in Action is full of real-world use cases for generative AI, showing you where and how to start integrating this powerful technology into your products and workflows. You’ll benefit from tried-and-tested implementation advice, as well as application architectures to deploy GenAI in production at enterprise scale.},
  isbn = {978-1-63343-694-7},
}

@book{bustos_notitle_2024,
  author = {Bustos, Juan Pablo and Soria, Luis Lopez and Arsanjani, Dr. Ali},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10769230},
  publisher = {Packt Publishing},
  note = {Publication Title: Generative AI Application Integration Patterns: Integrate large language models into your applications},
  keywords = {source: IEEE},
  abstract = {Unleash the transformative potential of GenAI with this comprehensive guide that serves as an indispensable roadmap for integrating large language models into real-world applications. Gain invaluable insights into identifying compelling use cases, leveraging state-of-the-art models effectively, deploying these models into your applications at scale, and navigating ethical considerations.Key FeaturesGet familiar with the most important tools and concepts used in real scenarios to design GenAI appsInteract with GenAI models to tailor model behavior to minimize hallucinationsGet acquainted with a variety of strategies and an easy to follow 4 step frameworks for integrating GenAI into applicationsBook DescriptionExplore the transformative potential of GenAI in the application development lifecycle. Through concrete examples, you will go through the process of ideation and integration, understanding the tradeoffs and the decision points when integrating GenAI. With recent advances in models like Google Gemini, Anthropic Claude, DALL-E and GPT-4o, this timely resource will help you harness these technologies through proven design patterns. We then delve into the practical applications of GenAI, identifying common use cases and applying design patterns to address real-world challenges. From summarization and metadata extraction to intent classification and question answering, each chapter offers practical examples and blueprints for leveraging GenAI across diverse domains and tasks. You will learn how to fine-tune models for specific applications, progressing from basic prompting to sophisticated strategies such as retrieval augmented generation (RAG) and chain of thought. Additionally, we provide end-to-end guidance on operationalizing models, including data prep, training, deployment, and monitoring. We also focus on responsible and ethical development techniques for transparency, auditing, and governance as crucial design patterns.What you will learnConcepts of GenAI: pre-training, fine-tuning, prompt engineering, and RAGFramework for integrating AI: entry points, prompt pre-processing, inference, post-processing, and presentationPatterns for batch and real-time integrationCode samples for metadata extraction, summarization, intent classification, question-answering with RAG, and moreEthical use: bias mitigation, data privacy, and monitoringDeployment and hosting options for GenAI modelsWho this book is forThis book is not an introduction to AI/ML or Python. It offers practical guides for designing, building, and deploying GenAI applications in production. While all readers are welcome, those who benefit most include: Developer engineers with foundational tech knowledge Software architects seeking best practices and design patterns Professionals using ML for data science, research, etc., who want a deeper understanding of Generative AI Technical product managers with a software development background This concise focus ensures practical, actionable insights for experienced professionals},
  isbn = {978-1-83588-761-5},
}

@book{palmer_notitle_2024,
  author = {Palmer, Rachelle and Perlmutter, Ben and Gangadhar, Ashwin and Larew, Nicholas and Narváez, Sigfrido and Rueckstiess, Thomas and Weller, Henry and Alake, Richmond and Ranjan, Shubham},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10769331},
  publisher = {Packt Publishing},
  note = {Publication Title: Building AI Intensive Python Applications: Create intelligent apps with LLMs and vector databases},
  keywords = {source: IEEE},
  abstract = {Master retrieval-augmented generation architecture and fine-tune your AI stack, along with discovering real-world use cases and best practices to create powerful AI appsKey FeaturesGet to grips with the fundamentals of LLMs, vector databases, and Python frameworksImplement effective retrieval-augmented generation strategies with MongoDB AtlasOptimize AI models for performance and accuracy with model compression and deployment optimizationPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThe era of generative AI is upon us, and this book serves as a roadmap to harness its full potential. With its help, you’ll learn the core components of the AI stack: large language models (LLMs), vector databases, and Python frameworks, and see how these technologies work together to create intelligent applications. The chapters will help you discover best practices for data preparation, model selection, and fine-tuning, and teach you advanced techniques such as retrieval-augmented generation (RAG) to overcome common challenges, such as hallucinations and data leakage. You’ll get a solid understanding of vector databases, implement effective vector search strategies, refine models for accuracy, and optimize performance to achieve impactful results. You’ll also identify and address AI failures to ensure your applications deliver reliable and valuable results. By evaluating and improving the output of LLMs, you’ll be able to enhance their performance and relevance. By the end of this book, you’ll be well-equipped to build sophisticated AI applications that deliver real-world value.What you will learnUnderstand the architecture and components of the generative AI stackExplore the role of vector databases in enhancing AI applicationsMaster Python frameworks for AI developmentImplement Vector Search in AI applicationsFind out how to effectively evaluate LLM outputOvercome common failures and challenges in AI developmentWho this book is forThis book is for software engineers and developers looking to build intelligent applications using generative AI. While the book is suitable for beginners, a basic understanding of Python programming is required to make the most of it.},
  isbn = {978-1-83620-724-5},
}

@book{antic_notitle_2024,
  author = {Antić, Zhenya and Chakravarty, Saurabh},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10769274},
  publisher = {Packt Publishing},
  note = {Publication Title: Python Natural Language Processing Cookbook: Over 60 recipes for building powerful NLP solutions using Python and LLM libraries},
  keywords = {source: IEEE},
  abstract = {Updated to include three new chapters on transformers, natural language understanding (NLU) with explainable AI, and dabbling with popular LLMs from Hugging Face and OpenAIKey FeaturesLeverage ready-to-use recipes with the latest LLMs, including Mistral, Llama, and OpenAI modelsUse LLM-powered agents for custom tasks and real-world interactionsGain practical, in-depth knowledge of transformers and their role in implementing various NLP tasks with open-source and advanced LLMsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionHarness the power of Natural Language Processing to overcome real-world text analysis challenges with this recipe-based roadmap written by two seasoned NLP experts with vast experience transforming various industries with their NLP prowess. You’ll be able to make the most of the latest NLP advancements, including large language models (LLMs), and leverage their capabilities through Hugging Face transformers. Through a series of hands-on recipes, you’ll master essential techniques such as extracting entities and visualizing text data. The authors will expertly guide you through building pipelines for sentiment analysis, topic modeling, and question-answering using popular libraries like spaCy, Gensim, and NLTK. You’ll also learn to implement RAG pipelines to draw out precise answers from a text corpus using LLMs. This second edition expands your skillset with new chapters on cutting-edge LLMs like GPT-4, Natural Language Understanding (NLU), and Explainable AI (XAI)—fostering trust and in your NLP models. By the end of this book, you'll be equipped with the skills to apply advanced text processing techniques, use pre-trained transformer models, build custom NLP pipelines to extract valuable insights from text data to drive informed decision-making.What you will learnUnderstand fundamental NLP concepts along with their applications using examples in PythonClassify text quickly and accurately with rule-based and supervised methodsTrain NER models and perform sentiment analysis to identify entities and emotions in textExplore topic modeling and text visualization to reveal themes and relationships within textLeverage Hugging Face and OpenAI LLMs to perform advanced NLP tasksUse question-answering techniques to handle both open and closed domainsApply XAI techniques to better understand your model predictionsWho this book is forThis updated edition of the Python Natural Language Processing Cookbook is for data scientists, machine learning engineers, and developers with a background in Python. Whether you’re looking to learn NLP techniques, extract valuable insights from textual data, or create foundational applications, this book will equip you with basic to intermediate skills. No prior NLP knowledge is necessary to get started. All you need is familiarity with basic programming principles. For seasoned developers, the updated sections offer the latest on transformers, explainable AI, and Generative AI with LLMs.},
  isbn = {978-1-80324-144-9},
}

@book{meyer_notitle_2024,
  author = {Meyer, Lucas A.},
  year = {2024},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10769215},
  publisher = {Packt Publishing},
  note = {Publication Title: Building AI Applications with Microsoft Semantic Kernel: Easily integrate generative AI capabilities and copilot experiences into your applications},
  keywords = {source: IEEE},
  abstract = {Unlock the power of GenAI by effortlessly linking your C\# and Python apps with cutting-edge models, orchestrating diverse AI services with finesse, and crafting bespoke applications through immersive, real-world examplesKey FeaturesLink your C\# and Python applications with the latest AI models from OpenAICombine and orchestrate different AI services such as text and image generatorsCreate your own AI apps with real-world use case examples that show you how to use basic generative AI, create images, process documents, use a vector databasePurchase of the print or Kindle book includes a free PDF eBookBook DescriptionIn the fast-paced world of AI, developers are constantly seeking efficient ways to integrate AI capabilities into their apps. Microsoft Semantic Kernel simplifies this process by using the GenAI features from Microsoft and OpenAI. Written by Lucas A. Meyer, a Principal Research Scientist in Microsoft’s AI for Good Lab, this book helps you get hands on with Semantic Kernel. It begins by introducing you to different generative AI services such as GPT-3.5 and GPT-4, demonstrating their integration with Semantic Kernel. You’ll then learn to craft prompt templates for reuse across various AI services and variables. Next, you’ll learn how to add functionality to Semantic Kernel by creating your own plugins. The second part of the book shows you how to combine multiple plugins to execute complex actions, and how to let Semantic Kernel use its own AI to solve complex problems by calling plugins, including the ones made by you. The book concludes by teaching you how to use vector databases to expand the memory of your AI services and how to help AI remember the context of earlier requests. You’ll also be guided through several real-world examples of applications, such as RAG and custom GPT agents. By the end of this book, you'll have gained the knowledge you need to start using Semantic Kernel to add AI capabilities to your applications.What you will learnWrite reusable AI prompts and connect to different AI providersCreate new plugins that extend the capabilities of AI servicesUnderstand how to combine multiple plugins to execute complex actionsOrchestrate multiple AI services to accomplish a taskLeverage the powerful planner to automatically create appropriate AI callsUse vector databases as additional memory for your AI tasksDeploy your application to ChatGPT, making it available to hundreds of millions of usersWho this book is forThis book is for beginner-level to experienced .NET or Python software developers who want to quickly incorporate the latest AI technologies into their applications, without having to learn the details of every new AI service. Product managers with some development experience will find this book helpful while creating proof-of-concept applications. This book requires working knowledge of programming basics.},
  isbn = {978-1-83546-959-0},
}

@inproceedings{cummings_rag_2025,
  title = {{RAG},
  author = {Cummings, Aaron and Zhang, Xinyue and Olaniran, Mercy and Akintomide, Modupe},
  year = {2025},
  booktitle = {2025 {IEEE},
  pages = {139--143},
  note = {ISSN: 2832-2975},
  keywords = {Computational modeling, Context modeling, Dementia, FineTuning, Healthcare, Large Language Model, Large language models, Pipelines, Question Answering, Question answering (information retrieval), Retrieval augmented generation, Retrieval Augmented Generation, Scalability, Terminology, Training, source: IEEE},
  abstract = {In closed-domain Question Answering (QA), Large Language Models (LLMs) often fail to deliver responses specialized enough for niche subdomains. Broadly trained models may not capture the nuanced terminology and contextual precision required in these fields, which frequently lack domain-specific conversational data and face computational constraints. To address this, we propose a methodology leveraging a Retrieval-Augmented Generation (RAG) framework that integrates data extraction with fine-tuning using domain-specific question-answer pairs. Our approach employs Question-Answer Generation (QAG) to create tailored training datasets, enabling fine-tuned models to incorporate specialized jargon and context while remaining computationally accessible to domain experts. To exemplify this methodology, we demonstrate its application within the medical domain through a case study centered on the creation of a dementia care chat assistant. A significant benefit of this approach lies in its ease of replication across various domains and scalability for integration into diverse user groups, making it a versatile solution for enhancing chat assistants.CCS Concepts• Computing methodologies → Natural language generation; • Human-centered computing → Natural language interfaces.},
  month = {jun},
}

@incollection{bergeret_retrievalx2010augmented_2025,
  title = {Retrieval\&\#x2010;{Augmented},
  author = {Bergeret, Olivier and Abbasi, Asif and Farvault, Joel},
  year = {2025},
  url = {https://ieeexplore.proxyucr.elogim.com/document/10982315},
  booktitle = {{GenAI},
  pages = {263--294},
  publisher = {Wiley},
  keywords = {Accuracy, Adaptation models, Artificial intelligence, Data models, Information retrieval, Knowledge based systems, Large language models, Prompt engineering, Retrieval augmented generation, Soft sensors, source: IEEE},
  abstract = {{\textless},
  isbn = {978-1-394-28130-5},
}

@article{mortaheb_rag-check_2025-1,
  title = {Rag-check: {Evaluating},
  author = {Mortaheb, M. and Khojastepour, M. A. A. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.03995},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In particular, multi-modal RAG systems process each … provided to the LLM in the final stage of the RAG system along … that provide hallucination scores specifically for multi-modal RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_retrieval-augmented_2025-1,
  title = {Retrieval-augmented generation with conflicting evidence},
  author = {Wang, H. and Prasad, A. and Stengel-Eskin, E. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.13079},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… using the parametric knowledge of the LLM (ie, the No RAG baseline which does not see … , making it harder for the LLM to distinguish factual documents from inaccurate ones. These …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sardana_real-time_2025,
  title = {Real-{Time},
  author = {Sardana, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.21157},
  journal = {arXiv preprint arXiv:2503.21157},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… between 0 and 1 indicating how confident we can be that the … while hallucination sometimes refers to specific types of LLM … (ie what ultimately matters to users of a real RAG system). …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gan_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Gan, A. and Yu, H. and Zhang, K. and Liu, Q. and Yan, W. and Huang, Z. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.14891},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG … method to measure the hallucination in RAG, which indicates the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_rag-kg-il_2025,
  title = {Rag-kg-il: {A},
  author = {Yu, H. Q. and McQuade, F.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.13514},
  journal = {arXiv preprint arXiv:2503.13514},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… than RAG-only but generates significantly more hallucinations when measured against our provided ground truth. To summarise, RAG-KG-IL … In contrast, RAG-only shows the highest …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xiao_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Xiao, L. and Dai, W. and Chen, S. and Qin, B. and Shi, C. and Jing, H. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.05475},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… credible evidence, our work builds an effective RAG framework to address the hallucination … Settings We choose GLM4-9B-chat(THUDM 2024) LLM as the base LLM for all baseline and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zheng_retrieval_2025,
  title = {Retrieval augmented generation and understanding in vision: {A},
  author = {Zheng, X. and Weng, Z. and Lyu, Y. and Jiang, L. and Xue, H. and Ren, B. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.18016},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… [8] investigate the integration of knowledge graphs with LLM-based RAG systems. Zhou et al. [9] focus on … We divide it into RAG-related datasets and hallucination detection datasets. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{spielberger_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Spielberger, G. and Artinger, F. M. and Reb, J. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.20963},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of LLM agents (in our case the ReAct agent) for Agentic RAG … (2024b) addressed the issue of hallucinations in LLM-based … reduced the number of hallucinated topics while producing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{berman_retrieval_2025-1,
  title = {Retrieval augmented therapy suggestion for molecular tumor boards: algorithmic development and validation study},
  author = {Berman, E. and Malek, H. Sundberg and Bitzer, M. and Malek, N. and {...},
  year = {2025},
  url = {https://www.jmir.org/2025/1/e64364/},
  journal = {Journal of Medical …},
  note = {Publisher: jmir.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… As shown in Table 1, we do not instruct the LLM to cite a specified number of sources. As a … In this study, we developed a RAG approach to LLM text generation to develop treatment …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_radio_2025-1,
  title = {Radio: {Real},
  author = {Zhu, J. and Guo, H. and Shi, W. and Chen, Z. and Meo, P. De},
  year = {2025},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/34809},
  journal = {Proceedings of the AAAI …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… enhanced real-time hallucination detection approach for triggering retrieval within dynamic RAG frameworks which takes into … The woRAG means the LLM does not apply any RAG. SR-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ren_retrieval-augmented_2025,
  title = {Retrieval-{Augmented},
  author = {Ren, T. and Zhang, Z. and Jia, B. and Zhang, S.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417425009285},
  journal = {Expert Systems with Applications},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… This paper proposed a novel Retrieval-Augmented Generation (RAG)-aided model to assist in identifying the causes of aviation safety incidents. The model consists of a retrieval …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{akbar_retrieval_2025,
  title = {Retrieval {Augmented},
  author = {Akbar, K. A. and Uddin, M. N. and Khan, L. and Hockstad, T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.18426},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM hallucination in legal contexts is critical due to: 1) the … Specifically, RAG mitigates LLM hallucinations by supplying … To this end, we develop a RAG-based LLM capable of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_ratt_2025-1,
  title = {Ratt: {A},
  author = {Zhang, J. and Wang, X. and Ren, W. and Jiang, L. and Wang, D. and {...},
  year = {2025},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/34876},
  journal = {Proceedings of the AAAI …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM thought structures. With optimized tree structure and RAG, our method is capable of reducing factual … pθ with parameters θ, we aim to enhance the performance of LLM on task A …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hu_removal_2025,
  title = {Removal of {Hallucination},
  author = {Hu, W. and Zhang, W. and Jiang, Y. and Zhang, C. J. and Wei, X. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.18581},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The capabilities of MAD align well with the second-order hallucination problem in RAG, and this … We provide detailed comparisons of average LLM and retrieval calls on StrategyQA, as …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pradhan_ragevalx_2025,
  title = {{RAGEvalX},
  author = {Pradhan, R.},
  year = {2025},
  url = {https://ijctece.com/index.php/IJCTEC/article/view/170},
  journal = {International Journal of Computer Technology and …},
  note = {Publisher: ijctece.com},
  keywords = {source: Google Scholar},
  abstract = {… ABSTRACT: Retrieval-Augmented Generation (RAG) has emerged as a cornerstone for building context-aware and factual Large Language Model (LLM) applications. However, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mohsin_retrieval_2025-1,
  title = {Retrieval augmented generation with multi-modal llm framework for wireless environments},
  author = {Mohsin, M. A. and Bilal, A. and Bhattacharya, S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.07670},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… captured in real time to train the RAG-based LLM for wireless environment perception. Due to … RAG-based LLM models do not hallucinate much in comparison to fine-tuned models due …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_r3-rag_2025,
  title = {R3-{RAG},
  author = {Li, Y. and Luo, Q. and Li, X. and Li, B. and Cheng, Q. and Wang, B. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.23794},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG … R3-RAG, which uses Reinforcement learning to make the LLM learn how …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{grislain_rag_2025-1,
  title = {Rag with differential privacy},
  author = {Grislain, N.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11050672/},
  journal = {2025 IEEE Conference on Artificial Intelligence …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… (RAG) has emerged as the dominant technique to provide Large Language Models (LLM) with fresh and relevant context, mitigating the risk of hallucinations and improving the overall …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gan_rag-mcp_2025,
  title = {Rag-mcp: {Mitigating},
  author = {Gan, T. and Sun, Q.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.03275},
  journal = {arXiv preprint arXiv:2505.03275},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In particular, RAG-MCP yields substantially higher accuracy in choosing the appropriate tool and reduces errors such as hallucinated or mis-parameterized function calls. These results …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mostafa_rag-enabled_2025,
  title = {Rag-enabled intent reasoning for application-network interaction},
  author = {Mostafa, S. and Abdel-Aziz, M. K. and Elbamby, M. S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.09339},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… translation, and avoid hallucination in the translated intents. … -RAG framework compared to LLM and vanilla-RAG benchmarks… it to an LLM (no-RAG) model, where a prompt and LLM are …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tavasoli_responsible_2025,
  title = {Responsible innovation: {A},
  author = {Tavasoli, A. and Sharbaf, M. and Madani, S. M.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.02165},
  journal = {arXiv preprint arXiv:2504.02165},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… deploy RAG for up-to-date references while using parameter-efficient tuning to embed … could opt for a proprietary model supplemented by RAG for continuous factual grounding. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pang_reconstruction_2025,
  title = {Reconstruction of landslide events using {LLM},
  author = {Pang, H. and Lo, M. K. and Leung, Y. F. and Wu, S.},
  year = {2025},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5340542},
  journal = {Available at SSRN 5340542},
  note = {Publisher: papers.ssrn.com},
  keywords = {source: Google Scholar},
  abstract = {… In 100 this study, RAG is applied to extract … agentic LLM that processes witness accounts and 154 outputs a report summarizing technical information related to a landslide, with citations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{wang_richrag_2025,
  title = {{RichRAG},
  author = {Wang, Shuting and Yu, Xin and Wang, Mang and Chen, Weipeng and Zhu, Yutao and Dou, Zhicheng},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218504376&partnerID=40&md5=b9da46758fa9e42f0738a175e81c6b7f},
  booktitle = {Proceedings - {International},
  pages = {11317 -- 11333},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{tel_rag_2025,
  title = {{RAG},
  author = {Tel, Tolga},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011097591&partnerID=40&md5=66aa434fab68d9b661e5968829a77de3},
  booktitle = {{CEUR},
  volume = {3993},
  pages = {111 -- 115},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{sun_redeep_2025,
  title = {{REDEEP},
  author = {Sun, Zhongxiang and Zang, Xiaoxue and Zheng, Kai and Xu, Jun and Zhang, Xiao and Yu, Weijie and Song, Yang and Li, Han},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010266950&partnerID=40&md5=cfa5652cea20f146a799c650cf61b723},
  pages = {102578 -- 102607},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{jiang_reasoning-enhanced_2025,
  title = {{REASONING},
  author = {Jiang, Pengcheng and Xiao, Cao and Jiang, Minhao and Bhatia, Parminder and Kass-Hout, Taha and Sun, Jimeng and Han, Jiawei},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010212722&partnerID=40&md5=78ab38187148116e3f71a2cdb4633838},
  pages = {14546 -- 14591},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{li_rag-ddr_2025,
  title = {{RAG},
  author = {Li, Xinze and Mei, Sen and Liu, Zhenghao and Yan, Yukun and Wang, Shuo and Yu, Shi and Zeng, Zheni and Chen, Hao and Yu, Ge and Liu, Zhiyuan and Sun, Maosong and Xiong, Chenyan},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010192409&partnerID=40&md5=062d8b94f694546fa9222ecf653b2932},
  pages = {68459 -- 68480},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{malandri_re-fin_2025,
  title = {{RE},
  author = {Malandri, Lorenzo and Mercorio, Fabio and Mezzanzanica, Mario and Pallucchini, Filippo},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000193593&partnerID=40&md5=3b2a76a600b0c14951c93a70f06f01e8},
  booktitle = {Proceedings - {International},
  pages = {751 -- 759},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{li_enhancing_2024-1,
  title = {Enhancing llm factual accuracy with rag to counter hallucinations: {A},
  author = {Li, J. and Yuan, Y. and Zhang, Z.},
  year = {2024},
  url = {https://arxiv.org/abs/2403.10446},
  journal = {arXiv preprint arXiv:2403.10446},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hal… This research highlights the potential of RAG systems in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lee_enhancing_2024-1,
  title = {Enhancing large language model reliability: minimizing hallucinations with dual retrieval-augmented generation based on the latest diabetes guidelines},
  author = {Lee, J. and Cha, H. and Hwangbo, Y. and Cheon, W.},
  year = {2024},
  url = {https://www.mdpi.com/2075-4426/14/12/1131},
  journal = {Journal of Personalized Medicine},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… novel retrieval system to enhance LLM reliability in diabetes … a dual retrieval-augmented generation (RAG) system … an effective dual RAG system that enhances LLM reliability in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_evaluating_2024,
  title = {Evaluating quality of answers for retrieval-augmented generation: {A},
  author = {Wang, Y. and Hernandez, A. G. and Kyslyi, R. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.18064},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of answer quality evaluation in Retrieval-Augmented Generation (RAG) applications using vRAG… grading hallucinations, we fix the temperature parameter for each LLM call at T = 0.0. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{da_evidencechat_2024,
  title = {Evidencechat: {A},
  author = {Da, L. and Shah, P. M. and Singh, A. and Wei, H.},
  year = {2024},
  url = {https://www.researchgate.net/profile/Parth-Shah-142/publication/385173895_EvidenceChat_A_RAG_Enhanced_LLM_Framework_for_Trustworthy_and_Evidential_Response_Generation/links/671983dadf4b534d4eeddf46/EvidenceChat-A-RAG-Enhanced-LLM-Framework-for-Trustworthy-and-Evidential-Response-Generation.pdf},
  journal = {… A RAG Enhanced LLM …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… upon the retrieval augmented generation agent (RAG agent), … the process of constructing the RAG agent, and then explain … Towards mitigating LLM hallucination via self reflection. In …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_enhanced_2024,
  title = {Enhanced large language models-based legal query responses through retrieval augmented generation},
  author = {Li, R.},
  year = {2024},
  url = {https://books.google.com/books?hl=en\&lr=\&id=RpUjEQAAQBAJ\&oi=fnd\&pg=PA237\&dq=%22retrieval+augmented+generation%22%7C%22rag%22+%22large+language+model%22%7C%22llm%22%7C%22chatgpt%22+trust%7Cconfidence%7Ccredibility%7Challucination%7Cfactuality%7Ccitation\&ots=z7ads6N7Yw\&sig=ZddnJKk6QmuOAhvZUZpk_a_ZO4w},
  journal = {Proceedings of the 2024 International Conference on …},
  note = {Publisher: books.google.com},
  keywords = {source: Google Scholar},
  abstract = {… LLM solving law problems using RAG is proposed. RAG is used to overcome hallucination … evaluation, which indicates that the LLM’s hallucination problem is basically solved, and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gummadi_enhancing_2024-1,
  title = {Enhancing communication and data transmission security in rag using large language models},
  author = {Gummadi, V. and Udayaraju, P. and Sarabu, V. R. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10763024/},
  journal = {2024 4th …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG with LLM produces more accurate results and essential user information. This can gain more trust … This paper proposes a large language model (LLM) to secure data in RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{abolghasemi_evaluation_2024,
  title = {Evaluation of {Attribution},
  author = {Abolghasemi, A. and Azzopardi, L. and Hashemi, S. Hadi and {...},
  year = {2024},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv241012380A/abstract},
  journal = {arXiv e …},
  note = {Publisher: ui.adsabs.harvard.edu},
  keywords = {source: Google Scholar},
  abstract = {… in RAG pipelines, namely attribution sensitivity and bias with respect to authorship information. We explicitly inform an LLM … documents can influence LLMs' trust, and how they attribute …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pelletier_explainable_2024,
  title = {Explainable biomedical hypothesis generation via retrieval augmented generation enabled large language models},
  author = {Pelletier, A. R. and Ramirez, J. and Adam, I. and Sankar, S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.12888},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) is a system designed to minimize LLM hallucinations… reliable and trustworthy sources, RAG grounds LLM responses in evidence, enhancing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{singhal_evidence-backed_2024,
  title = {Evidence-backed fact checking using {RAG},
  author = {Singhal, R. and Patwa, P. and Patwa, P. and Chadha, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.12060},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Answer Generation: After generating the question, we provide a single document to an LLM … ments in retrieval-augmented generation (RAG) pipelines which alleviate hallucination and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_enhancing_2024,
  title = {Enhancing large language model performance to answer questions and extract information more accurately},
  author = {Zhang, L. and Jijo, K. and Setty, S. and Chung, E. and Javid, F. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.01722},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of the LLM to be a financial chatbot that does not hallucinate … to the LLM the same way it would be in a traditional RAG … finance LLM, we need to develop a strong RAG model too, or …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{meduri_efficient_2024,
  title = {Efficient {RAG},
  author = {Meduri, K. and Nadella, G. S. and Gonaygunta, H. and {...},
  year = {2024},
  url = {https://www.researchgate.net/profile/Karthik-Meduri/publication/380265505_Efficient_RAG_Framework_for_Large-Scale_Knowledge_Bases/links/66330b9a08aa54017ad48c42/Efficient-RAG-Framework-for-Large-Scale-Knowledge-Bases.pdf},
  journal = {Efficient RAG …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… to maximize LLM efficiency and resource usage, whereas RAG combines external … the generating process, RAG generates replies that are rich in background and factual. The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{huang_embedding-informed_2024,
  title = {Embedding-{Informed},
  author = {Huang, C. and Xia, Y. and Wang, R. and Xie, K. and Yu, T. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.03514},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Our approach reveals higher confidence in the LLM’s intrinsic knowledge than PARAG, resulting in fewer retrievals and more precise answers, demonstrating our method’s superior …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{darji_enhancing_2024-1,
  title = {Enhancing financial risk analysis using rag-based large language models},
  author = {Darji, A. and Kheni, F. and Chodvadia, D. and Goel, P. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10841711/},
  journal = {2024 3rd …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Zhang, “Enhancing LLM factual accuracy with RAG to counter hallucinations: A case study on domain-specific queries in private knowledge-bases,” arXiv preprint arXiv:2403.10446, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{alinejad_evaluating_2024,
  title = {Evaluating the retrieval component in llm-based question answering systems},
  author = {Alinejad, A. and Kumar, K. and Vahdat, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.06458},
  journal = {arXiv preprint arXiv:2406.06458},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… erating inaccurate responses or hallucinations. Although the … retrievers in Retrieval-Augmented Generation (RAG)-based … well as potential errors and hallucinations in their responses. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kim_enhancing_2024,
  title = {Enhancing {Scientific},
  author = {Kim, S. and Mazumder, R.},
  year = {2024},
  url = {https://arxiv.org/abs/2409.15076},
  journal = {arXiv preprint arXiv:2409.15076},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We describe the development of the BCO assistant tool that leverages RAG … as LLM hallucination and long…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{vidivelli_efficiency-driven_2024,
  title = {Efficiency-{Driven},
  author = {Vidivelli, S. and Ramachandran, M. and {...},
  year = {2024},
  url = {https://file.sciopen.com/sciopen_public/1838514775850065922.pdf},
  journal = {Computers, Materials \& …},
  note = {Publisher: file.sciopen.com
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… RAG permits it to get to outer information for uncommon questions outside its pre-… and Factuality: Fine-tuning assists the LLM with learning area explicit wording and accurate data. RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gutu_exploring_2024,
  title = {Exploring data analysis methods in generative models: from {Fine},
  author = {Guțu, B. M. and Popescu, N.},
  year = {2024},
  url = {https://www.mdpi.com/2073-431X/13/12/327},
  journal = {Computers},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… and timeliness in LLM outputs. By incorporating RAG, the problem of hallucination in LLMs is … the advantages of RAG by integrating real-time data retrieval with LLM capabilities, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{church_emerging_2024-1,
  title = {Emerging trends: a gentle introduction to {RAG},
  author = {Church, K. W. and Sun, J. and Yue, R. and Vickers, P. and {...},
  year = {2024},
  url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-a-gentle-introduction-to-rag/4FF461F4066A0C16135F2D2849E3356A},
  journal = {Natural Language …},
  note = {Publisher: cambridge.org},
  keywords = {source: Google Scholar},
  abstract = {… Without RAG, an LLM trained on 2021 data would likely hallucinate when asked about 2023. RAG fills in gaps in the knowledge base by uploading a pdf file, sample\_files/World\_Series/…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{khaled_evaluating_2024,
  title = {Evaluating large language models for {Arabic},
  author = {Khaled, S. and Mohamed, E. H. and Medhat, W.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050924030114},
  journal = {Procedia Computer Science},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… experimenting generative generative LLMs and RAG models on text classification and … LLM model and the RAG methodology for handling the generative generative LLM hallucinations…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{rackauckas_evaluating_2024,
  title = {Evaluating rag-fusion with ragelo: an automated elo-based framework},
  author = {Rackauckas, Z. and Câmara, A. and Zavrel, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.14783},
  journal = {arXiv preprint arXiv:2406.14783},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… ,” we leverage an LLM-as-a-judge process, where a strong LLM is used to evaluate the … the judging LLM, enabling higher reliability and trust when evaluating different RAG pipelines. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{saha_enhancing_2024,
  title = {Enhancing international graduate student experience through ai-driven support systems: {A},
  author = {Saha, B. and Saha, U.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10651944/},
  journal = {… Conference on Data Science and Its …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… 3 shows that the custom RAG-LLM provides specific, … useful, whereas the regular LLM provides general information lacking … generate fabricated responses or hallucinate, the use of a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_eyegpt_2024-1,
  title = {Eyegpt: {Ophthalmic},
  author = {Chen, X. and Zhao, Z. and Zhang, W. and Xu, P. and Gao, L. and Xu, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2403.00840},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM designed specifically for ophthalmology, using three optimization strategies including role-playing, finetuning, and retrieval-augmented generation. In … In this study, hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xue_enhanced_2024,
  title = {Enhanced multimodal rag-llm for accurate visual question answering},
  author = {Xue, J. and Deng, Q. and Yu, F. and Wang, Y. and Wang, J. and Li, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.20927},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… an enhanced multimodal RAGLLM framework for accurate … Bing, “Can ChatGPT-like generative models guarantee factual … , “Overcoming llm challenges using rag-driven precision in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{kieu_empowering_2024,
  title = {Empowering {Automotive},
  author = {KIEU, K. and BERGSTRAND, O.},
  year = {2024},
  url = {https://gupea.ub.gu.se/handle/2077/83663},
  publisher = {gupea.ub.gu.se},
  keywords = {source: Google Scholar},
  abstract = {… This study calls attention to the complex nature of hallucinations and errors that can occur even with accurate context. To overcome some of the challenges found in an …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hsu_evaluating_2024,
  title = {Evaluating {ChatNetZero},
  author = {Hsu, A. and Laney, M. and Zhang, J. and Manya, D. and {...},
  year = {2024},
  url = {https://openreview.net/forum?id=MmTaM7lmvu},
  journal = {… meets Climate Change …},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… a large-language model (LLM) chatbot developed through Retrieval-Augmented Generation (RAG… for our assessment of the factual accuracy of five LLM outputs, including ChatNetZero. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nazar_enwar_2024,
  title = {Enwar: {A},
  author = {Nazar, A. M. and Celik, A. and Selim, M. Y. and Abdallah, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.18104},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… assessment of an LLM’s capabilities across various metrics such as answer relevancy, factual correctness, and hallucinations avoidance. However, RAG-based systems require a more …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{roychowdhury_evaluation_2024,
  title = {Evaluation of rag metrics for question answering in the telecom domain},
  author = {Roychowdhury, S. and Soman, S. and Ranjani, H. G. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.12873},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We establish, for RQ3, that Factual Correctness metric improves with instruction fine tuning … purposes in RAG pipeline. We demonstrate that domain adaptation of RAG LLM improves the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{al-zuraiqi_evaluating_2024,
  title = {Evaluating {Alignment},
  author = {Al-Zuraiqi, A. and Greer, D.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10903215/},
  journal = {2024 International Conference on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… These methods aim to eliminate LLM hallucination, where the model … Retrieval-Augmented Generation (RAG) and the training dataset as benchmark systems for the fine-tuned LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gonzalez_exploring_2024,
  title = {Exploring augmentation and cognitive strategies for {AI},
  author = {Gonzalez, R. A. and DiPaola, S.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.10890},
  journal = {arXiv preprint arXiv:2404.10890},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of whether LLM hallucinations can be directly equated to human hallucinations remains … Augmented RAG (Autonoesis): GPT-3.5 with RAG using the LLM-generated autobiography (…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_emerge_2024-1,
  title = {Emerge: {Integrating},
  author = {Zhu, Y. and Ren, C. and Wang, Z. and Zheng, X. and Xie, S. and {...},
  year = {2024},
  url = {https://www.researchgate.net/profile/Zixiang-Wang-13/publication/381126633_EMERGE_Integrating_RAG_for_Improved_Multimodal_EHR_Predictive_Modeling/links/6668192da54c5f0b945da986/EMERGE-Integrating-RAG-for-Improved-Multimodal-EHR-Predictive-Modeling.pdf},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Entities Refinement: Considering the hallucination issue associated with LLM, we design a … in the original text; secondly, we leverage LLM to filter entities not in disease type; and finally, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lee_evaluating_2024,
  title = {Evaluating consistencies in llm responses through a semantic clustering of question answering},
  author = {Lee, Y. and Kim, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.15440},
  journal = {arXiv preprint arXiv:2410.15440},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… as context like the RAG pattern or use Zero-shot-CoT to improve performance of LLM itself. We apply … We do not consider model confidence in this study, but plan to do so in future work. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiang_efficient_2024,
  title = {Efficient knowledge infusion via {KG},
  author = {Jiang, Z. and Zhong, L. and Sun, M. and Xu, J. and Sun, R. and Cai, H. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.03746},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… -hallucination responses. Firstly, we train a knowledge extraction model based on an LLM using … As for basic 2-shot RAG experiment on ChatGPT-3.5, although there is an improvement …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{noauthor_emnlp_2024,
  title = {{EMNLP},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216812904&partnerID=40&md5=49be7e7d710a377b84f298b6f3133b41},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{penzkofer_evaluating_2024,
  title = {Evaluating and {Fine},
  author = {Penzkofer, Vinzent and Baumann, Timo},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216766595&partnerID=40&md5=cc66e9b01a75d3eb9369b4416d744db3},
  pages = {57 -- 64},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{oro_evaluating_2024,
  title = {Evaluating {Retrieval},
  author = {Oro, Ermelinda and Granata, Francesco Maria and Lanza, Antonio and Bachir, Amir and de Grandis, Luca and Ruffolo, Massimo},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205590448&partnerID=40&md5=db794c2d8058e9c827a5d03c019d5441},
  booktitle = {{CEUR},
  volume = {3762},
  pages = {129 -- 134},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kang_c-rag_2024,
  title = {C-rag: {Certified},
  author = {Kang, M. and Gürel, N. M. and Yu, N. and Song, D. and Li, B.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.03181},
  journal = {arXiv preprint arXiv:2402.03181},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… C-RAG, a novel framework to certify generation risks for RAG … for RAG models and certify an upper confidence bound of … of RAG is lower than that of the corresponding vanilla LLM in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{yan_corrective_2024,
  title = {Corrective retrieval augmented generation},
  author = {Yan, S. Q. and Gu, J. C. and Zhu, Y. and Ling, Z. H.},
  year = {2024},
  url = {https://openreview.net/forum?id=JnWJbrnaUE},
  publisher = {openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… (LLMs) inevitably exhibit hallucinations since the accuracy … LLM ChatGPT on the document retrieval results was shown in Table 4. The prompts of ChatGPT, ChatGPT-CoT, and ChatGPT…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jang_calibrated_2024,
  title = {Calibrated decision-making through llm-assisted retrieval},
  author = {Jang, C. and Lee, H. and Lee, S. and Lee, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.08891},
  journal = {arXiv preprint arXiv:2411.08891},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… response z and the LLM’s confidence c. Our goal is to align the model confidence with accuracy of … an unseen LLM as the RAG model for decision-making task. We use Mistral-7B for the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{addison_c-fedrag_2024,
  title = {C-fedrag: {A},
  author = {Addison, P. and Nguyen, M. T. H. and Medan, T. and Shah, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.13163},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG has emerged as a popular method that offers significant … for grounding LLMs in factual information (Shuster et al.… and impact of RAG-based systems in reducing LLM hallucinations, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_crag-comprehensive_2024,
  title = {Crag-comprehensive rag benchmark},
  author = {Yang, X. and Sun, K. and Xin, H. and Sun, Y. and Bhalla, N. and {...},
  year = {2024},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/1435d2d0fca85a84d83ddcb754f58c29-Abstract-Datasets_and_Benchmarks_Track.html},
  journal = {Advances in …},
  note = {Publisher: proceedings.neurips.cc},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM… the Comprehensive RAG Benchmark (CRAG), a factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_controlling_2024-1,
  title = {Controlling risk of retrieval-augmented generation: a counterfactual prompting framework},
  author = {Chen, L. and Zhang, R. and Guo, J. and Fan, Y. and Cheng, X.},
  year = {2024},
  url = {https://arxiv.org/abs/2409.16146},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… latent factors affecting RAG’s confidence in its predictions: … To guide RAG models in assessing their own confidence based … We find that risk control works better with ChatGPT than with …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xu_crp-rag_2024,
  title = {Crp-rag: {A},
  author = {Xu, K. and Zhang, K. and Li, J. and Huang, W. and Wang, Y.},
  year = {2024},
  url = {https://www.mdpi.com/2079-9292/14/1/47},
  journal = {Electronics},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… CRP-RAG outperforms the best LLM and RAG baselines by 2.46 in open-… 4.2 in factual verification. Experiments also show the superior factual consistency and robustness of CRP-RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{singh_chunkrag_2024,
  title = {Chunkrag: {Novel},
  author = {Singh, I. S. and Aggarwal, R. and Allahverdiyev, I. and Taha, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.19572},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We introduced ChunkRAG, an LLM-driven chunk filtering method that enhances retrieval-augmented generation precision and factuality through dynamic greedy chunk aggregation. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{baumann_combining_2024,
  title = {Combining retrieval-augmented generation and few-shot learning for model synthesis of uncommon dsls},
  author = {Baumann, N. and Diaz, J. S. and Michael, J. and Netz, L. and Nqiri, H. and {...},
  year = {2024},
  url = {https://dl.gi.de/items/2e7293bb-7e2b-4682-9c45-af6022ad9fa5},
  journal = {… 2024 Satellite Events},
  note = {Publisher: dl.gi.de},
  keywords = {source: Google Scholar},
  abstract = {… LLM’s context. Furthermore, we employ a technique known as ’retrieval-augmented generation’ (RAG) to assist the LLM … It has been used as way to mitigate LLM hallucinations [To24], …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_ctrla_2024,
  title = {{CtrlA},
  author = {Liu, H. and Zhang, H. and Guo, Z. and Wang, J. and Dong, K. and Li, X. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2405.18727},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM toward honesty and monitor its confidence, we extract features aligned with the directions of honesty and confidence within LLM’s … —we can shift the LLM’s representation space to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{roychowdhury_confusedpilot_2024,
  title = {Confusedpilot: {Confused},
  author = {RoyChowdhury, A. and Luo, M. and Sahu, P. and Banerjee, S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.04870},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of security vulnerabilities of RAG systems that confuse Copilot … in RAG, corrupting the responses generated by the LLM. … how malicious actors can exploit trust and shared access to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nikishina_creating_2024,
  title = {Creating a {Taxonomy},
  author = {Nikishina, I. and Sevgili, Ö and Li, M. M. and Biemann, C. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.02854},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… for RAG applications, illustrating how RAG can be systematically implemented to improve LLM … Those papers already comprise over 2000 citations, resulting in more than twenty-eight …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{rouzrokh_conflare_2024,
  title = {Conflare: conformal large language model retrieval},
  author = {Rouzrokh, P. and Faghani, S. and Gamble, C. U. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.04287},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This mitigates hallucinations and allows updating the knowledge without retraining the LLM. … Despite the popularity of RAG, it does not guarantee that an LLM will generate a valid …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_citation-enhanced_2024-1,
  title = {Citation-enhanced generation for {LLM},
  author = {Li, W. and Li, J. and Ma, W. and Liu, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.16063},
  journal = {arXiv preprint arXiv:2402.16063},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, a vital challenge of LLMbased chatbots is that they may produce hallucinated … been made to alleviate hallucination, such as retrieval augmented generation and reinforcement …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ayyamperumal_current_2024,
  title = {Current state of {LLM},
  author = {Ayyamperumal, S. G. and Ge, L.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.12934},
  journal = {arXiv preprint arXiv:2406.12934},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… at the internal word embedding cite[] representation of an LLM without directly looking at the … RAG architectures address this vulnerability by grounding LLM responses in provided …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dhole_conqret_2024,
  title = {{ConQRet},
  author = {Dhole, K. D. and Shu, K. and Agichtein, E.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.05206},
  journal = {arXiv preprint arXiv:2412.05206},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the LLM Judges are prompted to generate the individual RAG … LLM judges are reliable for identifying hallucinated content in their argument. To simulate varying levels of hallucinations, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{tsai_comparative_2024,
  title = {Comparative analysis of automatic literature review using {Mistral},
  author = {Tsai, H. C. and Huang, Y. F. and Kuo, C. W.},
  year = {2024},
  url = {https://assets-eu.researchsquare.com/files/rs-4022248/v1_covered_31b35741-397a-47a8-aa52-d71a4150153f.pdf},
  publisher = {assets-eu.researchsquare.com},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… of applying the Mistral LLM, augmented with Retrieval-Augmented Generation, to the … LLM-powered literature review itself is not immune to the generic issue of LLM hallucination in LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tamanna_chatgpt_2024,
  title = {{ChatGPT},
  author = {Tamanna, S. B. and Uddin, G. and Wang, S. and Xia, L. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.07360},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG-based ChatGPT (ie, ChatGPT … hallucinated by producing incorrect or irrelevant answers. We manually examined each hallucination case and identified two limitations in ChatGPT …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sacoransky_chatgpt_2024,
  title = {{ChatGPT},
  author = {Sacoransky, E. and Kwan, B. Y. M. and Soboleski, D.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S0363018824001130},
  journal = {Current Problems in Diagnostic …},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… ChatGPT and assistive AI have significant potential to transform radiology reporting, … few-shot prompting, ChatGPT, and Retrieval Augmented Generation (RAG) into diagnostic workflows…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shen_citekit_2024,
  title = {Citekit: {A},
  author = {Shen, J. and Zhou, T. and Chen, Y. and Liu, K.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.04662},
  journal = {arXiv preprint arXiv:2408.04662},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Prompt self-RAG As for Llama3-8B and GPT-4o, there is no trained version for self-RAG, we use prompt to make the LLM retrieve documents and generate, then use an NLI model to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiao_can_2024,
  title = {Can we trust embodied agents? exploring backdoor attacks against embodied {LLM},
  author = {Jiao, R. and Xie, S. and Yue, J. and Sato, T. and Wang, L. and Wang, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2405.20774},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… for RAG-based LLM decisionmaking systems (BALD-RAG). The … from a database to augment the LLM’s input context. Recent … attack mechanism for RAG-based LLM systems as follows. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_ctrla_2024-1,
  title = {Ctrla: {Adaptive},
  author = {Liu, H. and Zhang, H. and Guo, Z. and Dong, K. and Li, X. and Lee, Y. Q. and {...},
  year = {2024},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240518727L/abstract},
  journal = {arXiv e …},
  note = {Publisher: ui.adsabs.harvard.edu},
  keywords = {source: Google Scholar},
  abstract = {… to regulate the LLM's behavior by manipulating its representations for increased honesty, and a confidence probe to monitor the internal states of LLM and assess confidence levels, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tan_chinese_2024,
  title = {Chinese safetyqa: {A},
  author = {Tan, Y. and Zheng, B. and Zheng, B. and Cao, K. and Jing, H. and Wei, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.15265},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To address these challenges and better evaluate the factuality ability of LLMs to … factuality abilities of existing LLMs and analyze how these capabilities relate to LLM abilities, eg, RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{trofimova_coderefine_2024,
  title = {{CodeRefine},
  author = {Trofimova, E. and Sataev, E. and Jowhari, A. S.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.13366},
  journal = {arXiv preprint arXiv:2408.13366},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval Augmented Generation (RAG) concept to improve the quality and accuracy of generated text. In RAG, … helps control LLM hallucinations, as we aim to avoid the LLM introducing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{he_chinese_2024,
  title = {Chinese simpleqa: {A},
  author = {He, Y. and Li, S. and Liu, J. and Tan, Y. and Wang, W. and Huang, H. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.07140},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… After that, to ensure the quality of Chinese SimpleQA, we use LLM to remove samples, … , which guides the LLM to evaluate the factual correctness of answers based on the RAG system. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{omar_chatgpt_2024,
  title = {{ChatGPT},
  author = {Omar, M. and Ullanat, V. and Loda, M. and Marchionni, L. and {...},
  year = {2024},
  url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(24)00114-6/fulltext},
  journal = {The Lancet Digital …},
  note = {Publisher: thelancet.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… the performance of GPT4DFCI-RAG against the generic ChatGPT-4 model in responding to … ChatGPT-4 also showed a high rate of hallucinations, featuring papers or applications that …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhou_cogmg_2024,
  title = {Cogmg: {Collaborative},
  author = {Zhou, T. and Chen, Y. and Liu, K. and Zhao, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.17231},
  journal = {arXiv preprint arXiv:2406.17231},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM … preference alignment are familiar with RAG, we utilize prompt engineering …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yin_chathpc_2024,
  title = {chathpc: {Empowering},
  author = {Yin, J. and Hines, J. and Herron, E. and Ghosal, T. and Liu, H. and {...},
  year = {2024},
  url = {https://www.osti.gov/biblio/2538074},
  journal = {Journal of …},
  note = {Publisher: osti.gov},
  keywords = {source: Google Scholar},
  abstract = {… technical complexities and gaps, LLM trustworthiness, safety, … Retrieval augmented generation (RAG) has emerged as a … hybrid deployment integrates RAG with LLM generation, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gregory_chatgpt_2024,
  title = {{ChatGPT},
  author = {Gregory, G. and Vito, L.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S1544612324013783},
  journal = {Finance Research Letters},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… known as Retrieval Augmented Generation (RAG)… Retrieval Augmented Generation (RAG) techniques generate more “specific, diverse, and factual language than a state-of-the-art LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chang_conversational_2024,
  title = {Conversational product recommendation using {LLM},
  author = {Chang, T. J. and Lin, L. H. M. and Tsai, R. T. H.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10602608/},
  journal = {… Internet of Things and Big Data …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval augmented generation (RAG) is expected to serve … key for LLM to avoid hallucinations, provide credible information, … In this study, we used LLM as a user and sales …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{armant_can_2024,
  title = {Can {Knowledge},
  author = {Armant, Vincent and Mouakher, Amira and Vargas-Rojas, Felipe and Symeonidou, Danai and Guérin, Joris and Mougenot, Isabelle and Desconnets, Jean Christophe},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211180646&partnerID=40&md5=5406aeb192f487bae497d1a983229118},
  booktitle = {{CEUR},
  volume = {3833},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{anantha_context_2024,
  title = {Context {Tuning},
  author = {Anantha, Raviteja and Bethi, Tharun and Vodianik, Danil and Chappidi, Srinivas},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188662450&partnerID=40&md5=3f5361f13607a04776ebc34f7519e6b1},
  pages = {15 -- 22},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{kurniawan_cykg-rag_2024,
  title = {{CyKG},
  author = {Kurniawan, Kabul and Kiesling, Elmar and Ekelhart, Andreas},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002730892&partnerID=40&md5=aecc4dc3ce324cdf9c9473ba10603bfc},
  booktitle = {{CEUR},
  volume = {3950},
  pages = {51 -- 64},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{wu_clasheval_2024,
  title = {{ClashEval},
  author = {Wu, Kevin E. and Wu, Eric and Zou, James Y.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000548768&partnerID=40&md5=778686bf20bc3637d8d55c9f121472f0},
  booktitle = {Advances in {Neural},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 6},
}

@article{choi_combining_2024,
  title = {Combining {Multiple},
  author = {Choi, Jason Ingyu and Collins, Marcus D. and Agichtein, Eugene and Rokhlenko, Oleg and Malmasi, Shervin},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000213823&partnerID=40&md5=ab4fc178f9ac8d71e9d28e1f5e3a7a0f},
  pages = {40 -- 50},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{niu_ragtruth_2023,
  title = {Ragtruth: {A},
  author = {Niu, C. and Wu, Y. and Zhu, J. and Xu, S. and Shum, K. and Zhong, R. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2401.00396},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… in identifying hallucinations by fine-tuning LLM with … wordlevel hallucination evaluation dataset specifically for the RAG … method of fine-tuning LLM for hallucination detection. It is shown …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gao_retrieval-augmented_2023,
  title = {Retrieval-augmented generation for large language models: {A},
  author = {Gao, Y. and Xiong, Y. and Gao, X. and Jia, K. and Pan, J. and Bi, Y. and {...},
  year = {2023},
  url = {https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: simg.baai.ac.cn
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… , such as hallucinations, slow … RAG The Naive RAG research paradigm represents the earliest methodology gained prominence shortly after the widespread adoption of ChatGPT…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{bergvall_reducing_2023,
  title = {Reducing {LLM},
  author = {Bergvall, P.},
  year = {2023},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5223641},
  publisher = {papers.ssrn.com},
  keywords = {source: Google Scholar},
  abstract = {… in hallucinations, though we also explore the trade-offs between strict factual grounding and … This work highlights the potential of RAG pipelines to improve the reliability of LLM-powered …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{levonian_retrieval-augmented_2023,
  title = {Retrieval-augmented generation to improve math question-answering: {Trade},
  author = {Levonian, Z. and Li, C. and Zhu, W. and Gade, A. and Henkel, O. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2310.03184},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… implementation We adopted a commercially-realistic chatbot context as the underlying LLM, … We identified 51 factual and conceptual questions that have sufficient context to be …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{manathunga_retrieval_2023,
  title = {Retrieval augmented generation and representative vector summarization for large unstructured textual data in medical education},
  author = {Manathunga, S. S. and Illangasekara, Y. A.},
  year = {2023},
  url = {https://arxiv.org/abs/2308.00479},
  journal = {arXiv preprint arXiv:2308.00479},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) … pharmacology from LLM without a non-parametric knowledgebase and a RAG model with …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ryu_retrieval-based_2023,
  title = {Retrieval-based evaluation for {LLMs},
  author = {Ryu, C. and Lee, S. and Pang, S. and Choi, C. and Choi, H. and {...},
  year = {2023},
  url = {https://aclanthology.org/2023.nllp-1.13/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{_rag_2023,
  title = {{RAG},
  author = {{조찬영},
  year = {2023},
  url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11652073},
  journal = {Proceedings of KIIT Conference},
  note = {Publisher: dbpia.co.kr},
  keywords = {source: Google Scholar},
  abstract = {… LLM is widely used to support chatbots. However, LLM is susceptible to hallucinations that … framework, a type of RAG, to solve the hallucination problem. The proposed architecture uses …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{su_implementing_2024,
  title = {Implementing retrieval-augmented generation (rag) for large language models to build confidence in traditional chinese medicine},
  author = {Su, X. and Gu, Y.},
  year = {2024},
  url = {https://files.osf.io/v1/resources/ns2v3_v1/providers/osfstorage/66678b7a65e1de5555893ab1?action=download\&direct\&version=1},
  publisher = {files.osf.io},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… [14] MI Rafat, “Ai-powered legal virtual assistant: Utilizing rag-optimized llm for housing dispute resolution in finland.” 2024. [15] K. Krishna, “Towards robust long-form text generation …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{chidipothu_improving_2024,
  title = {Improving large language model (llm) performance with retrieval augmented generation (rag): {Development},
  author = {Chidipothu, N. and Samuel, J. and Esguerra, J. and Anderson, R. and {...},
  year = {2024},
  url = {https://scholarship.libraries.rutgers.edu/esploro/outputs/preprint/Improving-large-language-model-LLM-performance/991032165917804646?institution=01RUT_INST},
  publisher = {scholarship.libraries.rutgers.edu},
  keywords = {source: Google Scholar},
  abstract = {… significantly greater than 1 will result in hallucinations. In our design, we choose a low temperature so that the LLM prioritizes factuality during the response process rather than …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{miao_integrating_2024-1,
  title = {Integrating retrieval-augmented generation with large language models in nephrology: advancing practical applications},
  author = {Miao, J. and Thongprayoon, C. and Suppadungsuk, S. and {...},
  year = {2024},
  url = {https://www.mdpi.com/1648-9144/60/3/445},
  journal = {Medicina},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… factual data, the RAG approach effectively reduces the occurrence of inaccuracy or hallucinations… To illustrate the process of creating a customized ChatGPT model with a RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_defense_2024,
  title = {In defense of rag in the era of long-context language models},
  author = {Yu, T. and Xu, A. and Akkiraju, R.},
  year = {2024},
  url = {https://arxiv.org/abs/2409.01666},
  journal = {arXiv preprint arXiv:2409.01666},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… making RAG less attractive. Recent studies show that long-context LLMs significantly … RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{setty_improving_2024,
  title = {Improving retrieval for rag based question answering models on financial documents},
  author = {Setty, S. and Thakkar, H. and Lee, A. and Chung, E. and Vidra, N.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.07221},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… documents into the large language model. In order to prevent hallucinations, some sort of in-… Fake RAG uses the same LLM model but is given the correct context to answer the original …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{perak_incorporating_2024,
  title = {Incorporating dialect understanding into llm using rag and prompt engineering techniques for causal commonsense reasoning},
  author = {Perak, B. and Beliga, S. and Meštrović, A.},
  year = {2024},
  url = {https://aclanthology.org/2024.vardial-1.19/},
  journal = {… of the Eleventh Workshop on NLP …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… various prompts engineering and the Retrieval-Augmented Generation (RAG) technique. Initially, … Next, we enhance prompts using the RAG technique specifically for the Chakavian and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{alessio_improving_2024,
  title = {Improving rag systems via sentence clustering and reordering},
  author = {Alessio, M. and Faggioli, G. and Ferro, N. and Nardini, F. M. and {...},
  year = {2024},
  url = {https://www.research.unipd.it/bitstream/11577/3538814/2/paper4.pdf},
  journal = {CEUR WORKSHOP …},
  note = {Publisher: research.unipd.it
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… well-known Large Language Model (LLM) hallucination problem, … with LLM positional dependencies and the difficulties of RAG … of RAG responses and to the use of an “LLMas-a-judge”. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pradeep_initial_2024,
  title = {Initial nugget evaluation results for the trec 2024 rag track with the autonuggetizer framework},
  author = {Pradeep, R. and Thakur, N. and Upadhyay, S. and Campos, D. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.09607},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (RAG) Track. There is, obviously, tremendous excitement and interest in RAG, but we feel that the evaluation of RAG … Thus, we do not consider possible LLM hallucinations at all. We will …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fang_ingest-and-ground_2024,
  title = {Ingest-{And},
  author = {Fang, C. and Larson, D. and Zhu, S. and Zeng, S. and Summer, W. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.02825},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… efficiency with LLM and RAG. To reduce hallucination, we continually pre-train the base LLM model with a privacy-specific knowledge base and then augment it with a semantic RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_identifying_2024,
  title = {Identifying performance-sensitive configurations in software systems through code analysis with llm agents},
  author = {Wang, Z. and Kim, D. J. and Chen, T. H.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.12806},
  journal = {arXiv preprint arXiv:2406.12806},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… and that copies bear this notice and the full citation on the first page. Copyrights for components of … on large language models (LLM) agents and retrieval-augmented generation (RAG). …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{rachha_incorporating_2024,
  title = {Incorporating {LLM},
  author = {Rachha, A. K.},
  year = {2024},
  url = {https://vtechworks.lib.vt.edu/items/3d08a8cd-effe-4e41-9830-0204637e53da},
  publisher = {vtechworks.lib.vt.edu},
  keywords = {source: Google Scholar},
  abstract = {… This section leverages the feature of RAG to enhance LLM capabilities to provide accurate and contextually … The reviewers highlighted the reliability of the LLM to counter hallucinations. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{han_improving_2024,
  title = {Improving {Assessment},
  author = {Han, Zifei and Lin, Jionghao and Gurung, Ashish and Thomas, Danielle R. and Chen, Eason and Borchers, Conrad and Gupta, Shivang and Koedinger, Kenneth R.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203842530&partnerID=40&md5=153e0790aa4f52ed2c9aa79d5f39eb99},
  booktitle = {Proceedings of {Machine},
  volume = {257},
  pages = {66 -- 76},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ozaki_understanding_2024,
  title = {Understanding the impact of confidence in retrieval augmented generation: {A},
  author = {Ozaki, S. and Kato, Y. and Feng, S. and Tomita, M. and Hayashi, K. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.20309},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We evaluate if RAG boosts LLM confidence using entropy, best probability, accuracy, and Adaptive Calibration Error. In our multiple-choice QA task, each question has one correct …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_unims-rag_2024,
  title = {Unims-rag: {A},
  author = {Wang, H. and Huang, W. and Deng, Y. and Wang, R. and Wang, Z. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2401.13256},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… as hallucinations [36], factuality [37] … -RAG w/ GPT4o is much better than UniMS-RAG w/ ChatGPT due to more accurate similarity labels provided by GPT4o, we believe that UniMS-RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pasquarelli__2024,
  title = {… {Utility},
  author = {Pasquarelli, L. and Koutcheme, C. and Hellas, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.13326},
  journal = {arXiv preprint arXiv:2410.13326},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… With RAG, search results can be traced back to the original source, providing the user with … the source of the LLM answers – RAG has also been found to reduce hallucination [18]1. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nemeth_using_2024,
  title = {Using a {RAG},
  author = {Németh, R. and Tátrai, A. and Szabó, M. and Tamási, Á},
  year = {2024},
  url = {https://www.ksh.hu/statszemle_archive/en/2024/2024_02/2024_02_003.pdf},
  journal = {Hungarian Statistical Review},
  note = {Publisher: ksh.hu
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… They were asked whether they used ChatGPT or had used a similar tool during their undergraduate studies, and we also asked them about their trust in these tools. Notably, user and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{ogbo-gebhardt_using_2024,
  title = {Using a {Large},
  author = {Ogbo-Gebhardt, E. and Ogbo, O.},
  year = {2024},
  url = {https://www.econstor.eu/handle/10419/302517},
  publisher = {econstor.eu},
  keywords = {source: Google Scholar},
  abstract = {… LLM-powered assistant] was most useful?” and “On a scale of 1 to 5, where a higher number indicates more trust, to what extent do you trust … located the LLM inference servers and RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_usable_2024,
  title = {Usable {XAI},
  author = {Wu, X. and Zhao, H. and Zhu, Y. and Shi, Y. and Yang, F. and Hu, L. and Liu, T. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2403.08946},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Given a query prompt and its ChatGPT response, we aim to build a classifier to detect if the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{antico_unimib_2024,
  title = {Unimib {Assistant},
  author = {Antico, C. and Giordano, S. and Koyuturk, C. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.19554},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… between the LLM and the data. Finally, practical testing of how the RAG-LLM system … ChatGPT remains prone to hallucinations and provides unclickable links from time to time if …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_unigen_2024,
  title = {Unigen: {A},
  author = {Wu, S. and Huang, Y. and Gao, C. and Chen, D. and Zhang, Q. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.18966},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Generation (RAG)-based validation method to check the factuality of generated statements to … Motivated by this finding, we require the LLM to generate Python code to solve the given …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{xian_understanding_2024,
  title = {Understanding {Data},
  author = {Xian, X. and Wang, T. and You, L. and Qi, Y.},
  year = {2024},
  url = {https://openreview.net/forum?id=2aL6gcFX7q},
  publisher = {openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) effectively alleviates these problems by … the factual accuracy of LLM-generated content. However, recent studies reveal that RAG systems are …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cheng_unified_2024,
  title = {Unified active retrieval for retrieval augmented generation},
  author = {Cheng, Q. and Li, X. and Li, S. and Zhu, Q. and Yin, Z. and Shao, Y. and Li, L. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.12534},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… with only LLM, RAG involves an additional retrieval process and the longer LLM input, … entries from Self-RAG’s non-retrieval-required data, and factual knowledge questions from TAQA …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{huang_understanding_2024,
  title = {Understanding the planning of {LLM},
  author = {Huang, X. and Liu, W. and Chen, X. and Wang, X. and Wang, H. and {...},
  year = {2024},
  url = {https://www.researchgate.net/profile/Xu-Huang-37/publication/380756642_Understanding_the_planning_of_LLM_agents_A_survey/links/664d5a9dbc86444c72f64b6f/Understanding-the-planning-of-LLM-agents-A-survey.pdf},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… trajectories may lead to LLM experiencing hallucinations, deviating from the … LLM-Agents, there are currently two major approaches to enhance planning abilities through memory: RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{siragusa_unipa-gpt_2024,
  title = {Unipa-{GPT},
  author = {Siragusa, Irene and Pirrone, Roberto},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214426005&partnerID=40&md5=5b30020b93ef53258163ddf847601ecb},
  booktitle = {{CEUR},
  volume = {3878},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{fore_unlearning_2024,
  title = {Unlearning {Climate},
  author = {Fore, Michael and Singh, Simranjit and Lee, Chaehong and Pandey, Amritanshu and Anastasopoulos, Antonios and Stamoulis, Dimitrios},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204448847&partnerID=40&md5=c5ecf9d1c0656b00a191da6157277b6d},
  pages = {178 -- 192},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{noauthor_uncertainlp_2024,
  title = {{UncertaiNLP},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188661032&partnerID=40&md5=9d3acf2024519c49a637c850efa75aaa},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{chow_unified_2024,
  title = {Unified {Generative},
  author = {Chow, Wei and Li, Juncheng and Yu, Qifan and Pan, Kaihang and Fei, Hao and Ge, Zhiqi and Yang, Shuai and Siliang, Tang and Zhang, Hanwang and Sun, Qianru},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000489048&partnerID=40&md5=da04cde9542bddb2b204d14d2b66c4d8},
  booktitle = {Advances in {Neural},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{zhou_trustworthiness_2024,
  title = {Trustworthiness in retrieval-augmented generation systems: {A},
  author = {Zhou, Y. and Liu, Y. and Li, X. and Jin, J. and Qian, H. and Liu, Z. and Li, C. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.10102},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the RAG context: As shown in Figure 1, we define trustworthiness across six dimensions: (1) … scenarios, we focus on assessing the correctness of the intermediate steps in the LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zimmerman_two-tiered_2024-1,
  title = {Two-tiered encoder-based hallucination detection for retrieval-augmented generation in the wild},
  author = {Zimmerman, I. and Tredup, J. and Selfridge, E. and {...},
  year = {2024},
  url = {https://aclanthology.org/2024.emnlp-industry.2/},
  journal = {Proceedings of the 2024 …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… -tuned LLM … RAG, the model was given the input prompt including the user’s question, retrieved KBs, and a sentence from the LLM Response (the statement being classified for factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pham_towards_2024,
  title = {Towards reliable medical question answering: {Techniques},
  author = {Pham, D. K. and Vo, B. Q.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.13808},
  journal = {arXiv preprint arXiv:2408.13808},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Key methods covered in the paper include Retrieval-Augmented Generation (RAG)-based … LLM limitations in healthcare applications and provide insights into addressing hallucinations. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ma_think--graph_2024,
  title = {Think-on-graph 2.0: {Deep},
  author = {Ma, S. and Xu, C. and Jiang, X. and Li, M. and Qu, H. and Yang, C. and Mao, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.10805},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {Retrieval-augmented generation (RAG) has improved large language models (LLMs) by using knowledge retrieval to overcome knowledge deficiencies. However, current RAG … RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{william_text_2024,
  title = {Text {Embedding},
  author = {William, I. O. and Altamimi, M.},
  year = {2024},
  url = {https://www.researchgate.net/profile/Mubarak-Altamimi/publication/381105654_Text_Embedding_Implementation_Using_Retrieval_Augmented_Generation_RAG_Model_Combined_With_Large_Language_Model/links/665c78e30b0d2845747c0fe8/Text-Embedding-Implementation-Using-Retrieval-Augmented-Generation-RAG-Model-Combined-With-Large-Language-Model.pdf},
  journal = {International Journal of Advanced Natural …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… evaluation for tasks requiring factual accuracy, to assess the performance of your RAG model. We follow software engineering concepts and maintain excellent coding standards …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fatehkia_t-rag_2024,
  title = {T-{RAG},
  author = {Fatehkia, M. and Lucas, J. K. and Chawla, S.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.07483},
  journal = {arXiv preprint arXiv:2402.07483},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG… As we observed earlier, finetuned models are still prone to hallucinations especially …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_towards_2024,
  title = {Towards more robust retrieval-augmented generation: {Evaluating},
  author = {Su, J. and Zhou, J. P. and Zhang, Z. and Nakov, P. and Cardie, C.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.16708},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) systems have emerged as a promising solution to mitigate LLM hallucinations … use the Non-RAG setting as a proxy to gauge an LLM’s internal …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_towards_2024,
  title = {Towards understanding retrieval accuracy and prompt quality in rag systems},
  author = {Zhao, S. and Huang, Y. and Song, J. and Wang, Z. and Wan, C. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.19463},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We also observed that higher LLM confidence (perplexity) aligned with higher retrieval … In this paper, our study of LLMdriven RAG system design offers the first exploratory un…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ridder_hallurag_2024,
  title = {The {HalluRAG},
  author = {Ridder, F. and Schilling, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.17056},
  journal = {arXiv preprint arXiv:2412.17056},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We aim to train an MLP to identify sentence-level hallucinations by analyzing specific internal states in RAG applications. In particular, we focus on closed-domain hallucinations, which …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_tcaf_2024,
  title = {{TCAF},
  author = {Zhao, J. and Liu, X.},
  year = {2024},
  url = {https://openreview.net/forum?id=nvjfWv7uGY},
  journal = {… Cup Workshop for Retrieval Augmented Generation},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… the LLM to cite the ID of the supporting document in its response, this design significantly reduces the incidence of hallucination, … LLM to match that of a 70B LLM in the RAG system. The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiang_tc-rag_2024,
  title = {Tc-rag: turing-complete rag's case study on medical llm systems},
  author = {Jiang, X. and Fang, Y. and Qiu, R. and Zhang, H. and Xu, Y. and Chen, H. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.09199},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG not only mitigates hallucination issues during LLM inference but also provides up-to-date, task-specific knowledge, significantly boosting both interpretability and performance on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{fazlija_toward_2024,
  title = {Toward optimising a retrieval augmented generation pipeline using large language model},
  author = {Fazlija, G.},
  year = {2024},
  url = {https://wwwmatthes.in.tum.de/file/1vzgjh1an34u0/Sebis-Public-Website/-/Master-s-Thesis-Gentrit-Fazlija/Master%20Thesis_Gentrit%20Fazlija_signed.pdf},
  publisher = {wwwmatthes.in.tum.de},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… This approach not only "shows" the LLM the anticipated quality and relevance of responses but also aids in mitigating issues of hallucination and inconsistency. By demonstrating …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_geometry_2024,
  title = {The {Geometry},
  author = {Yang, E. and Amar, J. and Lee, J. H. and Kumar, B. and Jia, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.18044},
  journal = {arXiv preprint arXiv:2407.18044},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Unlike relying solely on the LLM’s internal knowledge, RAG … knowledge base to inform the LLM’s answer generation … QB-RAG are more frequently grounded on our trusted source of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{habib_taxtajweez_2024,
  title = {Taxtajweez: {A},
  author = {Habib, M. A. and Amin, S. and Oqba, M. and Jaipal, S. and {...},
  year = {2024},
  url = {https://journals.flvc.org/FLAIRS/article/view/135648},
  journal = {The International …},
  note = {Publisher: journals.flvc.org},
  keywords = {source: Google Scholar},
  abstract = {… Generation (RAG) system powered by the OpenAI GPT-3.5-turbo LLM, designed specifically … , TaxTajweez leverages the RAG pipeline to mitigate model hallucinations, enhancing the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{finardi_chronicles_2024,
  title = {The chronicles of rag: {The},
  author = {Finardi, P. and Avila, L. and Castaldoni, R. and Gengo, P. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2401.07883},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… possible to obtain the maximum score that an evaluated LLM could reach for a RAG system. … Demonstrating confidence in transfer learning, we found that a few examples were sufficient…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{suresh_towards_2024-1,
  title = {Towards a rag-based summarization agent for the electron-ion collider},
  author = {Suresh, K. and Kackar, N. and Schleck, L. and Fanelli, C.},
  year = {2024},
  url = {https://arxiv.org/abs/2403.15729},
  journal = {arXiv preprint arXiv:2403.15729},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Since its proposal, RAG methods have continuously evolved to achieve superior performance in grounding LLM to truth and reducing hallucinations. Different RAG methods that involve …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_targeting_2024,
  title = {Targeting the core: {A},
  author = {Li, X. and Li, Z. and Kosuga, Y. and Yoshida, Y. and Bian, V.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.04415},
  journal = {arXiv preprint arXiv:2412.04415},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , hallucinations, privacy breaches, and a lack of transparency. This paper investigates a critical vulnerability: adversarial attacks targeting the LLM … the fragility of existing LLM defenses. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liang_thames_2024,
  title = {Thames: {An},
  author = {Liang, M. and Arun, A. and Wu, Z. and Munoz, C. and Lutch, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.11353},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Unlike these frameworks, our type definitions were designed not for retrievalaugmented generation (RAG) systems but for general LLM hallucination evaluation. We also introduced …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{abeysinghe_challenges_2024,
  title = {The challenges of evaluating llm applications: {An},
  author = {Abeysinghe, B. and Circi, R.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.03339},
  journal = {arXiv preprint arXiv:2406.03339},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Other components performance such as the semantic search used for retrieval in RAG is not … But this is not the case always, in most instances LLM evaluators tend to be overly confident …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{leto_toward_2024,
  title = {Toward optimal search and retrieval for rag},
  author = {Leto, A. and Aguerrebere, C. and Bhati, I. and Willke, T. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.07396},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… with separately trained retriever and LLM components, as training … citation metrics vary with more retrieved documents, adding new data to a small literature on attributed QA with RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nikbakht_tspec-llm_2024,
  title = {Tspec-llm: {An},
  author = {Nikbakht, R. and Benzaghta, M. and Geraci, G.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.01768},
  journal = {arXiv preprint arXiv:2406.01768},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the RAG framework, detail how to employ TSpecLLM for RAG, … Confidence levels: We analyze the confidence levels of … the questionnaire when applying RAG on the TSpec-LLM dataset. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kashmira_tobugraph_2024,
  title = {{TOBUGraph},
  author = {Kashmira, S. and Dantanarayana, J. L. and Brodsky, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.05447},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (I4) Hallucinations during memory retrieval failures: Baseline RAG models hallucinate when … Figure 5b shows RAGv2 hallucinating because RAG relies on unstructured data, losing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{anderson_design_2024,
  title = {The design of an llm-powered unstructured analytics system},
  author = {Anderson, E. and Fritz, J. and Lee, A. and Li, B. and Lindblad, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.00847},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of simple LLM-based operations, we can make the query plan as a whole more reliable than RAG-… While the RAG approach somewhat mitigates hallucination, LLM context windows are …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gong_potential_2024,
  title = {The potential clinical utility of the customized large language model in gastroenterology: {A},
  author = {Gong, E. J. and Bang, C. S. and Lee, J. J. and Park, J. and Kim, E. and Kim, S. and {...},
  year = {2024},
  url = {https://www.mdpi.com/2306-5354/12/1/1},
  journal = {Bioengineering},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Conventional GPT-4o is an advanced LLM capable of retrieval-augmented generation (RAG… and reproducibility of LLM outputs is another key regulatory requirement to foster trust and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_thinking_2024,
  title = {Thinking with knowledge graphs: {Enhancing},
  author = {Wu, X. and Tsioutsiouliklis, K.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.10654},
  journal = {arXiv preprint arXiv:2412.10654},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… These summaries are subsequently used by the LLM via RAG to help answer questions … relationships helps improve LLM multi-hop reasoning ability and reduce hallucination. The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{kim_thorr_2024,
  title = {{THoRR},
  author = {Kim, Kihun and Kim, Mintae and Lee, Hokyung and Park, Seongik and Han, Youngsub and Jeon, Byoung-ki},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207524029&partnerID=40&md5=ae6dfa0a2f47b4fa1992d7b91dae75f2},
  booktitle = {{CEUR},
  volume = {3784},
  pages = {50 -- 55},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhang_trustworthy_2024,
  title = {Trustworthy {Alignment},
  author = {Zhang, Zongmeng and Shi, Yufeng and Zhu, Jinhua and Zhou, Wengang and Qi, Xiang and Zhang, Peng and Li, Houqiang},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203826205&partnerID=40&md5=4df19b55392ac0a1c4fd77a5f8c9ac89},
  booktitle = {Proceedings of {Machine},
  volume = {235},
  pages = {59827 -- 59850},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ramoneda_role_2024,
  title = {The {Role},
  author = {Ramoneda, Pedro and Parada-Cabaleiro, Emilia and Weck, Benno and Serra, Xavier},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000189844&partnerID=40&md5=bbfe780a4b835cbbd5a8e56bf696571b},
  pages = {81 -- 86},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{wu_medical_2024,
  title = {Medical graph rag: {Towards},
  author = {Wu, J. and Zhu, J. and Qi, Y. and Chen, J. and Xu, M. and Menolascina, F. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.04187},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG data to credible medical papers and foundational medical dictionaries. This process generates triples [RAG … It enhances LLM reasoning and ensures responses are traceable to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{song_measuring_2024,
  title = {Measuring and enhancing trustworthiness of {LLMs},
  author = {Song, M. and Sim, S. H. and Bhardwaj, R. and Chieu, H. L. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.11242},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We define hallucination as an erroneous LLM response, categorized into five types: (1) … We aim to measure two aspects of an LLM in RAG: 1) the Correctness of the generated …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{asbai_mitigating_2024,
  title = {Mitigating {Hallucination},
  author = {Asbai, A.},
  year = {2024},
  url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2:1887431},
  publisher = {diva-portal.org},
  keywords = {source: Google Scholar},
  abstract = {… This study evaluated the RAG hallucination mitigation strategy on ChatGPT and Gemini to … More specifically, the study evaluated the RAG techniques’ ability to mitigate hallucination in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{agrawal_mindful-rag_2024-1,
  title = {Mindful-rag: {A},
  author = {Agrawal, G. and Kumarage, T. and Alghamdi, Z. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10852457/},
  journal = {2024 2nd International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… of ChatGPT without RAG on both datasets. The results, presented in Figure 1, show that Mindful-RAG, … Zhang, “Enhancing llm factual accuracy with rag to counter hallucinations: A case …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tang_multihop-rag_2024,
  title = {Multihop-rag: {Benchmarking},
  author = {Tang, Y. and Yang, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2401.15391},
  journal = {arXiv preprint arXiv:2401.15391},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations … However, we find that existing RAG systems are inadequate in answering multi-hop …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_multi-source_2024,
  title = {Multi-source knowledge pruning for retrieval-augmented generation: {A},
  author = {Yu, S. and Cheng, M. and Liu, Q. and Wang, D. and Yang, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.13694},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… on either the LLM’s internal knowledge or external knowledge. Another combines the LLM’s … the internal knowledge of LLM before retrieval tends to generate hallucinations due to the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{qi_model_2024-1,
  title = {Model internals-based answer attribution for trustworthy retrieval-augmented generation},
  author = {Qi, J. and Sarti, G. and Fernández, R. and Bisazza, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.13663},
  journal = {arXiv preprint arXiv:2406.13663},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… 15For completeness, we also report MIRAGE results without self-citation prompting in … 16ALCE is an evaluation framework for RAG, evaluating LLM responses in terms of citation quality…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{xiong_merging_2024,
  title = {Merging mixture of experts and retrieval augmented generation for enhanced information retrieval and reasoning},
  author = {Xiong, X. and Zheng, M.},
  year = {2024},
  url = {https://assets-eu.researchsquare.com/files/rs-3978298/v1_covered_c16e21c8-7b04-448d-bc48-d44b0f430b4e.pdf},
  publisher = {assets-eu.researchsquare.com},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… through research demonstrating RAG’s capacity to improve factual accuracy and relevance … to systematically integrate MoE and RAG into the Mistral Large Language Model (LLM), and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chang_main-rag_2024,
  title = {Main-rag: {Multi},
  author = {Chang, C. Y. and Jiang, Z. and Rakesh, V. and Pan, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2501.00332},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This indicates that the LLM (here is Mistral-7B) is more confident … , suggesting that the LLM is less confident and may misjudge … We acknowledge that LLM inference under RAG workflow …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_m-rag_2024,
  title = {M-{RAG},
  author = {Wang, Z. and Teo, S. X. and Ouyang, J. and Xu, Y. and Shi, W.},
  year = {2024},
  url = {https://arxiv.org/abs/2405.16420},
  journal = {arXiv preprint arXiv:2405.16420},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… M-RAG, designed to facilitate RAG across multiple partitions of a database. M-RAG addresses … However, its quality faces significant challenges such as low precision, hallucination, and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{choi_malade_2024,
  title = {{MALADE},
  author = {Choi, J. and Palumbo, N. and Chalasani, P. and Engelhard, M. M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.01869},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… a confidence score that indicates how confident an LLM is about its label assignment. These scores permit a rigorous quantitative evaluation against the well-established Observational …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kriman_measuring_2024,
  title = {Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation},
  author = {Kriman, N. E.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.15171},
  journal = {arXiv preprint arXiv:2408.15171},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In this section, we describe our approach for evaluating the factuality of summary texts with respect to their source texts. Our methodology leverages a large language model (LLM) to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{amatriain_measuring_2024,
  title = {Measuring and mitigating hallucinations in large language models: amultifaceted approach},
  author = {Amatriain, X.},
  year = {2024},
  url = {https://amatria.in/blog/images/Mitigating_Hallucinations.pdf},
  publisher = {amatria.in},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… A hallucination in an LLM is defined as ”the generation of content that is nonsensical … , RAG can be effectively utilized to ground LLM responses, reducing the likelihood of hallucinations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kimura_mapping_2024,
  title = {Mapping drug terms via integration of a retrieval-augmented generation algorithm with a large language model},
  author = {Kimura, E. and Kawakami, Y. and Inoue, S. and {...},
  year = {2024},
  url = {https://synapse.koreamed.org/articles/1516088909},
  journal = {Healthcare Informatics …},
  note = {Publisher: synapse.koreamed.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… RAG that distinguished the final candidates from the baseline. We assessed the efficacy of the LLM with RAG … To prevent LLM hallucinations, all candidate strings were verified against …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{penkov_mitigating_2024,
  title = {Mitigating hallucinations in large language models via semantic enrichment of prompts: {Insights},
  author = {Penkov, S.},
  year = {2024},
  url = {https://aclanthology.org/2024.clib-1.30/},
  journal = {Proceedings of the Sixth International Conference on …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… By embedding deeper semantic understanding directly into LLM prompts, our approach extends the ethos behind RAG, enhancing its capability to improve LLM reliability. This proactive …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bei_manufacturing_2024-1,
  title = {Manufacturing domain qa with integrated term enhanced rag},
  author = {Bei, Y. and Fang, Z. and Mao, S. and Yu, S. and Jiang, Y. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10649905/},
  journal = {… Joint Conference on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucination issues in domain-specific LLMs, the most prevalent method currently is RetrievalAugmented Generation (RAG… is then input into a Large Language Model (LLM) to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{thakur_mirage-bench_2024,
  title = {{MIRAGE},
  author = {Thakur, N. and Kazi, S. and Luo, G. and Lin, J. and Ahmad, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.13716},
  journal = {arXiv preprint arXiv:2410.13716},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… ized arena-based multilingual RAG benchmark for 18 diverse … RAG extensively coupling both heuristic features and LLM as … We use boostrapping to obtain confidence bounds for better …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{besta_multi-head_2024,
  title = {Multi-head rag: {Solving},
  author = {Besta, M. and Kubicek, A. and Gerstenberger, R. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.05085},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations (by grounding the LLM reply in reliable … We ensure the relevance of our RAG datasets in real use cases … all of which actively use RAG in their own LLM infrastructures. Our …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gao_modular_2024,
  title = {Modular rag: {Transforming},
  author = {Gao, Y. and Xiong, Y. and Wang, M. and Wang, H.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.21059},
  journal = {arXiv preprint arXiv:2407.21059},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… challenges, such as hallucination and … RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lu_multimodal_2024,
  title = {Multimodal large language model driven scenario testing for autonomous vehicles},
  author = {Lu, Q. and Wang, X. and Jiang, Y. and Zhao, G. and Ma, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.06450},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… but also encourages rational thought with minimal hallucination. For further evidence of the … testing an LLM-driven AV within them. Thirdly, we showcase the effectiveness of RAG module…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{noauthor_mai-xai24_2024,
  title = {{MAI},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210016656&partnerID=40&md5=657ed9ed152a33fdd58cc97ac5c1fafd},
  booktitle = {{CEUR},
  volume = {3803},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{nguyen_my_2024,
  title = {My {Climate},
  author = {Nguyen, Vincent and Karimi, Sarvnaz and Hallgren, Willow and Harkin, Ashley and Prakash, Mahesh},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204439657&partnerID=40&md5=77ba849127c1ef47344eef4bc5e39a70},
  pages = {27 -- 45},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 5},
}

@article{wu_multirag_2025-1,
  title = {Multirag: a knowledge-guided framework for mitigating hallucination in multi-source retrieval augmented generation},
  author = {Wu, W. and Wang, H. and Li, B. and Huang, P. and Zhao, X. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11113128/},
  journal = {2025 IEEE 41st …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… a promising solution to address hallucination issues in Large … hallucination in multi-source retrieval-augmented generation … context of the LLM to generate a credible retrieval answer. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sok_metarag_2025,
  title = {{MetaRAG},
  author = {Sok, C. and Luz, D. and Haddam, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2509.09360},
  journal = {arXiv preprint arXiv:2509.09360},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We obtain F using an LLM-based extractor with a fixed prompt that enforces one proposition per line, prohibits paraphrasing or inference beyond A, and co-reference resolution. The full …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_medical_2025,
  title = {Medical graph rag: {Evidence},
  author = {Wu, J. and Zhu, J. and Qi, Y. and Chen, J. and Xu, M. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.acl-long.1381/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG data to credible medical papers and foundational medical dictionaries. This process generates triples [RAG … It enhances LLM reasoning and ensures responses are traceable to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cerqueira_mapping_2025,
  title = {Mapping trustworthiness in large language models: {A},
  author = {Cerqueira, JS de and Kemell, K. K. and Rousi, R. and Xi, N. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.04785},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shi_mkrag_2025,
  title = {Mkrag: {Medical},
  author = {Shi, Y. and Xu, S. and Yang, T. and Liu, Z. and Liu, T. and Li, X. and {...},
  year = {2025},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC12099378/},
  journal = {AMIA Annual …},
  note = {Publisher: pmc.ncbi.nlm.nih.gov
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… This work underscores the potential of RAG to enhance LLM performance, offering a … LLM performance on the CounterFact dataset 6 , which consists of general domain factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zheng_miriad_2025,
  title = {{MIRIAD},
  author = {Zheng, Q. and Abdullah, S. and Rawal, S. and Zakka, C. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2506.06091},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM-based annotation to a small sample of 15,000 QA pairs, instructing GPT-4 to assess each example along two axes: factual … retrieval-augmented generation (RAG), we compare RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xu_mantra_2025,
  title = {Mantra: {Enhancing},
  author = {Xu, Y. and Lin, F. and Yang, J. and Tsantalis, N.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.14340},
  journal = {arXiv preprint arXiv:2503.14340},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Moreover, in comparison to IntelliJ’s LLM-powered refactoring … emphasize the growing potential of LLM-based systems in … copies bear this notice and the full citation on the first page. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{massenon__2025,
  title = {” {My},
  author = {Massenon, R. and Gambo, I. and Khan, J. A. and Agbonkhese, C. and {...},
  year = {2025},
  url = {https://www.nature.com/articles/s41598-025-15416-8},
  journal = {Scientific Reports},
  note = {Publisher: nature.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… User-Reported LLM Hallucination Detection algorithm were … of user reports indicative of LLM hallucinations, which was … When a user reports a factual error in a RAG-powered app, it …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lavrinovics_multihal_2025,
  title = {{MultiHal},
  author = {Lavrinovics, E. and Biswas, R. and Hose, K. and Bjerva, J.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.14101},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the factuality of LLM outputs. The main advantage of RAG is that it does not require retraining the generator LLM, a … However, RAG is still limited by the LLM context window size [56], its …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{darwish_mitigating_2025,
  title = {Mitigating {LLM},
  author = {Darwish, A. M. and Rashed, E. A. and Khoriba, G.},
  year = {2025},
  url = {https://www.mdpi.com/2078-2489/16/7/517},
  journal = {Information},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… For instance, models like MrRank and RAG-Prompter refine retrieval quality, while approaches such as WikiChat and Retrieval-Augmented Generation demonstrate the empirical …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{he_multi-faceted_2025,
  title = {Multi-{Faceted},
  author = {He, P. and Xing, Y. and Xu, H. and Xiang, Z. and Tang, J.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.14182},
  journal = {arXiv preprint arXiv:2502.14182},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… applications such as Retrieval-augmented generation (RAG) and LLM agents which retrieve … We propose to explore how trust-centric data poisoning can be leveraged to defend against …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_multimodal_2025,
  title = {Multimodal large language model for wheat breeding: a new exploration of smart breeding},
  author = {Yang, G. and Li, Y. and He, Y. and Zhou, Z. and Ye, L. and Fang, H. and Luo, Y. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0924271625001339},
  journal = {ISPRS Journal of …},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… fine-tuning (SFT), retrieval-augmented generation (RAG), and … the combination of SFT, RAG, and RLHF technologies can … generated answer, and reduce hallucinations and biases. The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{restrepo_multi-ophthalingua_2025,
  title = {Multi-{OphthaLingua},
  author = {Restrepo, D. and Wu, C. and Tang, Z. and Shuai, Z. and {...},
  year = {2025},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/35053},
  journal = {Proceedings of the …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… ities in LLM performance for ophthalmology, we present … We curated a diverse set of RAG corpora from three distinct sources… Full details of our RAG sources can be found in Table 4. We …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fatouros_marketsenseai_2025,
  title = {Marketsenseai 2.0: {Enhancing},
  author = {Fatouros, G. and Metaxas, K. and Soldatos, J. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.00415},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… combining Retrieval-Augmented Generation and LLM agents… This work marks a significant advancement in applying LLM … lengthy documents, mitigating hallucination risks, and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cao_multi-agent_2025,
  title = {Multi-{Agent},
  author = {Cao, H. and Driouich, I. and Singh, R. and Thomas, E.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.02867},
  journal = {arXiv preprint arXiv:2504.02867},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM based applications such as Retrieval Augmented Generation (RAG) systems, several … answer and the factual similarity based on arguments measured by LLM judge to arrive at the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_meet2mitigate_2025,
  title = {{Meet2Mitigate},
  author = {Chen, G. and Alsharef, A. and Ovid, A. and Albert, A. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1474034624007195},
  journal = {Advanced Engineering …},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… (M2M) framework, which integrates cutting-edge technologies, including speaker diarization, automatic speech recognition (ASR), LLMs, and retrieval-augmented generation (RAG) to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{tashkovska_memoire_2025,
  title = {Memoire: {Harnessing},
  author = {Tashkovska, Matea and Neshaei, Seyed Parsa and Mejia-Domenzain, Paola and Käser, Tanja},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011254844&partnerID=40&md5=1621532bd889318945b70dbe495f984f},
  booktitle = {{CEUR},
  volume = {3994},
  pages = {65 -- 73},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{song_measuring_2025,
  title = {{MEASURING},
  author = {Song, Maojia and Sim, Shang Hong and Bhardwaj, Rishabh and Chieu, Hai Leong and Majumder, Navonil and Poria, Soujanya},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010244761&partnerID=40&md5=27e1ab2ddaef3c4965582fdbefc9f422},
  pages = {47215 -- 47255},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{hwang_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Hwang, J. and Park, J. and Park, H. and Kim, D. and Park, S. and Ok, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.22954},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG enhances LLM performance by retrieving information from external databases, reducing hallucinations, and improving access to up-to-date knowledge (Lewis et al.…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{es_ragas_2024,
  title = {Ragas: {Automated},
  author = {Es, S. and James, J. and Anke, L. E. and {...},
  year = {2024},
  url = {https://aclanthology.org/2024.eacl-demo.16/},
  journal = {Proceedings of the 18th …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… factual answers are more stable: when an answer is factual, we … this is less likely to be the case for hallucinated answers. … To estimate faithfulness, we first use an LLM to extract a set of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_retrieval_2024,
  title = {Retrieval augmented generation (rag) and beyond: {A},
  author = {Zhao, S. and Yang, Y. and Wang, Z. and He, Z. and Qiu, L. K. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.14924},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… tailoring LLM applications to meet specific industry needs. Through techniques like RAG and fine tuning, data augmented LLM … • Reduction in Model Hallucination: Data augmented LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_retrieval-augmented_2024,
  title = {Retrieval-augmented generation for ai-generated content: {A},
  author = {Zhao, P. and Zhang, H. and Yu, Q. and Wang, Z. and Geng, Y. and Fu, F. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.19473},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… -based RAG is also applicable to scenarios that use LLM … iteratively by employing a static LLM for document ranking and … -to-end RAG system, enhancing the retrieval quality and factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hu_rag_2024,
  title = {Rag and rau: {A},
  author = {Hu, Y. and Lu, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.19543},
  journal = {arXiv preprint arXiv:2404.19543},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (NLP), yet they encounter challenges such as hallucination and the need for domain-specific … In Sections 6 and 7, we presented some evaluation criteria for large language model tasks …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bechard_reducing_2024,
  title = {Reducing hallucination in structured outputs via {Retrieval},
  author = {Béchard, P. and Ayala, O. M.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.08189},
  journal = {arXiv preprint arXiv:2404.08189},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… • We provide an application of RAG in workflow generation, a structured … RAG reduces hallucination and improves results. • We demonstrate that RAG allows deploying a smaller LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sree_retrieval-augmented_2024,
  title = {Retrieval-augmented generation based large language model chatbot for improving diagnosis for physical and mental health},
  author = {Sree, Y. B. and Sathvik, A. and Akshit, D. S. H. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10815693/},
  journal = {2024 6th International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… be ingested in order for a Large Language Model (LLM) to be trained … By fine tuning an LLM we can engineer a specialized … This database - comprises more than 35 million citations for …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{he_retrieving_2024-1,
  title = {Retrieving, rethinking and revising: {The},
  author = {He, B. and Chen, N. and He, X. and Yan, L. and Wei, Z. and Luo, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.05801},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… introduce a novel mechanism for enhancing the factuality and consistency in RAG. … RAG Augmentation: To enhance the diversity and robustness of the training data, we utilize ChatGPT …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_rac_2024,
  title = {Rac: {Efficient},
  author = {Li, C. and Flanigan, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.15667},
  journal = {arXiv preprint arXiv:2410.15667},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We report numbers with retrieval augmented generation (RAG) and without RAG. * indicates we reproduced a previous approach using the same retrieved documents and LLM as our …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{efeoglu_retrieval-augmented_2024,
  title = {Retrieval-augmented generation-based relation extraction},
  author = {Efeoglu, S. and Paschke, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.13397},
  journal = {arXiv preprint arXiv:2404.13397},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucination issues on these datasets, our RAG-… RAG-based LLM prompting approach. We also claim that RAG4RE has outperformed the performance of the simple query (Vanilla LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiang_rag-thief_2024,
  title = {Rag-thief: {Scalable},
  author = {Jiang, C. and Pan, X. and Hong, G. and Bao, C. and Yang, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.14110},
  journal = {arXiv preprint arXiv:2411.14110},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG mitigates the issue of hallucinations in LLMs by incorporating realtime, domain-specific … In this experiment, we fix the LLM component of the RAG application as GLM-4-Plus, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{feldman_ragged_2024-1,
  title = {Ragged edges: {The},
  author = {Feldman, P. and Foulds, J. R. and Pan, S.},
  year = {2024},
  url = {https://arxiv.org/abs/2403.01193},
  journal = {arXiv preprint arXiv:2403.01193},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{muludi_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Muludi, K. and Fitria, K. M. and Triloka, J.},
  year = {2024},
  url = {https://search.ebscohost.com/login.aspx?direct=true\&profile=ehost\&scope=site\&authtype=crawler\&jrnl=2158107X\&AN=176379076\&h=mmXDdbx%2FAKKXE5HxVbZop7XacHkdE6WLAF%2BzfxKas%2BXNrm8iN7hcr%2FcAbBl17BHfwst667MJ%2FM%2FG8lWuepC86w%3D%3D\&crl=c},
  journal = {International Journal of …},
  note = {Publisher: search.ebscohost.com},
  keywords = {source: Google Scholar},
  abstract = {… for QA systems without modifications tend to generate hallucinatory answers, lacking … the large language model within the ChatGPT systems, gpt3.5-turbo within the framework of RAG. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{friel_ragbench_2024,
  title = {Ragbench: {Explainable},
  author = {Friel, R. and Belyi, M. and Sanyal, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.11005},
  journal = {arXiv preprint arXiv:2407.11005},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods strug… outperforms LLM judges in hallucination/attribution detection but also excels on the new RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chan_rq-rag_2024,
  title = {Rq-rag: {Learning},
  author = {Chan, C. M. and Xu, C. and Yuan, R. and Luo, H. and Xue, W. and Guo, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.00610},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… prone to generating inaccurate or hallucinatory responses. This … these challenges, Retrieval-Augmented Generation (RAG) … while we train the LLM to refine queries by itself in this …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{arasteh_radiorag_2024,
  title = {{RadioRAG},
  author = {Arasteh, S. T. and Lotfinia, M. and Bressem, K. and {...},
  year = {2024},
  url = {https://www.researchgate.net/profile/Soroosh-Tayebi-Arasteh/publication/382459583_RadioRAG_Factual_large_language_models_for_enhanced_diagnostics_in_radiology_using_online_retrieval_augmented_generation/links/6782ac0f32c79152e3cd3b68/RadioRAG-Factual-large-language-models-for-enhanced-diagnostics-in-radiology-using-online-retrieval-augmented-generation.pdf},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Secondly, LLMs can access up-to-date information through RAG, while conventional LLM … the occurrence of hallucinations and 2) RadioRAG improves the accuracy of LLM responses to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sun_redeep_2024,
  title = {Redeep: {Detecting},
  author = {Sun, Z. and Zang, X. and Zheng, K. and Song, Y. and Xu, J. and Zhang, X. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.11414},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , a novel method that detects hallucinations by decoupling LLM’s utilization of … RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ding_retrieve_2024,
  title = {Retrieve only when it needs: {Adaptive},
  author = {Ding, H. and Pang, L. and Wei, Z. and Shen, H. and Cheng, X.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.10612},
  journal = {arXiv preprint arXiv:2402.10612},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Our objective is to enhance the factuality of LLM responses by integrating parametric and … ChatGPT language model. We re-implement all baselines, except Self-RAG, using ChatGPT to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yuan_rag-driver_2024,
  title = {Rag-driver: {Generalisable},
  author = {Yuan, J. and Sun, S. and Omeiza, D. and Zhao, B. and Newman, P. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.10828},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Abstract—We need to trust robots that use often opaque AI methods. They need to explain … , we introduce RAG-Driver, a novel retrieval-augmented multi-modal large language model …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{toukmaji_retrieval-augmented_2024-1,
  title = {Retrieval-augmented generation and llm agents for biomimicry design solutions},
  author = {Toukmaji, C. and Tee, A.},
  year = {2024},
  url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/31210},
  journal = {Proceedings of the AAAI Symposium Series},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… to mitigate hallucination issues in LLMs is LLM agents. … We evaluate the quality of LLM-generated biomimetic design … As a result, we opt to use the prompting LLM with RAG setting …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{khaliq_ragar_2024,
  title = {Ragar, your falsehood radar: {Rag},
  author = {Khaliq, M. A. and Chang, P. and Ma, M. and Pflugfelder, B. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.12065},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucination in text generation, current fact-checking pipelines often implement a RAG approach, wherein an LLM … pairs generated by our LLM-based and RAG-augmented reasoning …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chirkova_retrieval-augmented_2024,
  title = {Retrieval-augmented generation in multilingual settings},
  author = {Chirkova, N. and Rau, D. and Déjean, H. and Formal, T. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.01463},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-augmented generation (RAG) has recently emerged as a promising … LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_retrieval_2024,
  title = {Retrieval {Augmented},
  author = {Yu, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.14838},
  journal = {arXiv preprint arXiv:2407.14838},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , we aim to foster greater trust in blockchain technologies. This trust is crucial for the widespread … In order to construct an expansive and robust dataset for our RAG-LLM pipeline, we …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bhat_retrieval_2024,
  title = {Retrieval augmented generation (rag) based restaurant chatbot with ai testability},
  author = {Bhat, V. and Cheerla, S. D. and Mathew, J. R. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10730393/},
  journal = {2024 IEEE 10th …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… of ChatGPT, the latest research focuses on utilizing Retrieval Augmented Generation (RAG) … Therefore, in this paper, we propose a multi-modal integration of the RAG model and LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{veturi_rag_2024,
  title = {Rag based question-answering for contextual response prediction system},
  author = {Veturi, S. and Vaichal, S. and Jagadheesh, R. L. and Tripto, N. I. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.03708},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Overall, RAG LLM improves accuracy by reducing hallucinations … Evaluating RAG LLM and BERT-based models on … We thoroughly evaluate the quality of RAG LLM and BERT …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sohn_rationale-guided_2024,
  title = {Rationale-guided retrieval augmented generation for medical question answering},
  author = {Sohn, J. and Park, Y. and Yoon, C. and Park, S. and Hwang, H. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.00300},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the LLM prompt increases the confidence of the LLM’s rationale (or reduces its perplexity). tion), a novel framework that improves the reliability of RAG in … for the base LLM, selectively …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hasegawa_rag_2024-1,
  title = {Rag certainty: {Quantifying},
  author = {Hasegawa, K. and Hidano, S. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10903445/},
  journal = {… Conference on Machine …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… • We propose a metric called RAG certainty to quantify the certainty of LLM outputs in a RAG … Since we use the RAG certainty to identify whether the output is hallucinated, prioritizing the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cheng_remoterag_2024,
  title = {{RemoteRAG},
  author = {Cheng, Y. and Zhang, L. and Wang, J. and Yuan, M. and Yao, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.12775},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… into the context of prompts to improve the output of LLM (Izacard and Grave… LLM to provide answers with credible literature makes RAG an important technique in the application of LLM, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_realm_2024,
  title = {Realm: {Rag},
  author = {Zhu, Y. and Ren, C. and Xie, S. and Liu, S. and Ji, H. and Wang, Z. and Sun, T. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.07016},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… REALM, a Retrieval-Augmented Generation (RAG) driven … Firstly, we apply Large Language Model (LLM) to encode … Entities Refinement: To mitigate hallucination issues of LLM, we …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jeong_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Jeong, M. and Kim, T. and Kim, S. and Kim, H.},
  year = {2024},
  journal = {International …},
  note = {Publisher: Korea Institute of Construction …
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sharma_retrieval_2024,
  title = {Retrieval augmented generation for domain-specific question answering},
  author = {Sharma, S. and Yoon, D. S. and Dernoncourt, F. and Sultania, D. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.14760},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… -aware finetuning of a Large Language model and use a query … Our overall approach reduces hallucinations during genera… the source credibility (Helpx {\textgreater},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ji_rag-rlrc-laysum_2024,
  title = {Rag-rlrc-laysum at biolaysumm: {Integrating},
  author = {Ji, Y. and Li, Z. and Meng, R. and Sivarajkumar, S. and Wang, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2405.13179},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (2023), which also serves as our baseline LLM. We aim to create … The RAG-RLRC-LaySum framework effectively simplifies complex biomedical texts, enhancing readability and factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{henkel_retrieval-augmented_2024,
  title = {Retrieval-augmented generation to improve math question-answering: {Trade},
  author = {Henkel, O. and Levonian, Z. and Li, C. and {...},
  year = {2024},
  url = {https://educationaldatamining.org/edm2024/proceedings/2024.EDM-short-papers.28/},
  journal = {Proceedings of the …},
  note = {Publisher: educationaldatamining.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… We identified 51 factual and conceptual questions that have sufficient context to be … RAG implementation - We adopted a chatbot context as the underlying LLM, generating all …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{freitas_retail-gpt_2024,
  title = {Retail-gpt: leveraging retrieval augmented generation (rag) for building e-commerce chat assistants},
  author = {Freitas, BAT de and Lotufo, R. A.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.08925},
  journal = {arXiv preprint arXiv:2408.08925},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… significant hallucination … RAG-based approach for building conversational chatbots for retail e-commerce. The system is built around a DIET classifier and a large language model (LLM) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{stefano_rag_2024,
  title = {Rag and roll: {An},
  author = {Stefano, G. De and Schönherr, L. and Pellegrino, G.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.05025},
  journal = {arXiv preprint arXiv:2408.05025},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Traditional LLM applications generate responses based on factual … is LLM hallucinations, which are responses to users’ questions that are inconsistent or incoherent with the factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zelin_rare_2024,
  title = {Rare disease diagnosis using knowledge guided retrieval augmentation for {ChatGPT},
  author = {Zelin, C. and Chung, W. K. and Jeanne, M. and Zhang, G. and {...},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046424001205},
  journal = {Journal of Biomedical …},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… ChatGPT’s suitability for rare disease diagnostic support with the enhancement provided by Retrieval Augmented Generation (RAG… hallucinations and improve the accuracy of ChatGPT, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@misc{pahwa_reliable_2024,
  title = {Reliable {Medical},
  author = {Pahwa, K.},
  year = {2024},
  note = {Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ko_retrieval_2024,
  title = {Retrieval {Augmented},
  author = {Ko, K. and Nyein, T. Y. and Oo, K. K. and Oo, T. Z. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10754919/},
  journal = {2024 5th International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… By combining the power of RAG and the LLM, this document query system offers a robust … LIMITATION Hallucination is a key challenge in LLM. LLM hallucination occurs when the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{joshi_robust_2024-1,
  title = {Robust multi model rag pipeline for documents containing text, table \&images},
  author = {Joshi, P. and Gupta, A. and Kumar, P. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10574972/},
  journal = {2024 3rd International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Multimodal RAG over two different other multimodal LLM ie, … the proposed solution fits best with LLM in different cases. … also hallucinate because the context which is provided to LLM for …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{han_rag-qa_2024,
  title = {{RAG},
  author = {Han, R. and Zhang, Y. and Qi, P. and Xu, Y. and Wang, J. and Liu, L. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.13998},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… aligns with RAG-QA ARENA, but the 95\% confidence interval (… We rank these pairs with the same LLM evaluator as in the … We propose a reliable LLM-based evaluation framework, RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shlyk_real_2024,
  title = {{REAL},
  author = {Shlyk, D. and Groza, T. and Montanelli, S. and Cavalleri, E. and {...},
  year = {2024},
  url = {https://air.unimi.it/handle/2434/1122499},
  journal = {Proceedings of the 23rd …},
  note = {Publisher: air.unimi.it},
  keywords = {source: Google Scholar},
  abstract = {… RAG in the field of biomedical concept recognition (CR). In REAL, we employ RAG to assist the LLM … Our approach relies on the LLM to produce factual definitions for extracted mentions…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bernardi_report_2024-1,
  title = {Report generation from x-ray imaging by retrieval-augmented generation and improved image-text matching},
  author = {Bernardi, M. L. and Cimitile, M.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10650332/},
  journal = {2024 International Joint Conference …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… the pretraining of the LLM network using RAG our approach is … A promising solution to improve LLM accuracy and credibility … represented by the Retrieval-Augmented Generation (RAG) […},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pan_raglog_2024-1,
  title = {Raglog: {Log},
  author = {Pan, J. and Liang, W. S. and Yidi, Y.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10607047/},
  journal = {2024 IEEE World Forum on Public …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… responses with no other textual hallucination noted. … RAG construct with a locally deployed LLM. This is to address any privacy and confidentiality concerns with the use of online LLM. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_raft_2024,
  title = {Raft: {Adapting},
  author = {Zhang, T. and Patil, S. G. and Jain, N. and Shen, S. and Zaharia, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2403.10131},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… is trained as a general-purpose LLM is largely dependent on the … Here, we know apriori the domain in which the LLM will be … and in-addition, clearly citing sources enhances the model’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kulkarni_reinforcement_2024-1,
  title = {Reinforcement learning for optimizing rag for domain chatbots},
  author = {Kulkarni, M. and Tangarajan, P. and Kim, K. and Trivedi, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2401.06800},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… and an LLM, we optimize the number of LLM tokens using … to the RAG, which interacts with the RAG pipeline through … a prompt that minimizes hallucinations with ChatGPT. Apart from …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{reichman_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Reichman, B. and Heck, L.},
  year = {2024},
  url = {https://southnlp.github.io/southnlp2024/papers/southnlp2024-poster-46.pdf},
  journal = {arXiv preprint arXiv:2402.11035},
  note = {Publisher: southnlp.github.io
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… the retrieval augmented generation (RAG) paradigm for improving large language models (LLM… Trust in these systems to give accurate information is crucial to their ability to help people …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sundar_revolutionizing_2024-1,
  title = {Revolutionizing assessment: {Ai},
  author = {Sundar, K. and Manohar, E. and Vijay, K. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10760285/},
  journal = {2024 2nd International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… We discovered ChatGPT's ability to advance medical … AI systems confront challenges like hallucinations, legal dangers, and … Next, we turn to the question of evaluation of RAG and other …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_rageval_2024,
  title = {Rageval: {Scenario},
  author = {Zhu, K. and Luo, Y. and Xu, D. and Yan, Y. and Liu, Z. and Yu, S. and Wang, R. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.01262},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To validate the consistency between LLM evaluations and human assessments, we compare the LLM-reported metrics—completeness, hallucination, and irrelevance—with those …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tan_revprag_2024,
  title = {{RevPRAG},
  author = {Tan, X. and Luan, H. and Luo, M. and Sun, X. and Chen, P. and Dai, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.18948},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We conducted experiments to test if our approach can distinguish hallucinations and RAG poisoning. Fig. 8 shows the t-SNE representation of mean activations for poisoned response …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{christmann_rag-based_2024,
  title = {Rag-based question answering over heterogeneous data and text},
  author = {Christmann, P. and Weikum, G.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.07420},
  journal = {arXiv preprint arXiv:2412.07420},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The RAG paradigm came up as a principled way of enhancing LLM factuality incl. provenance and mitigating the risk of hallucination [12, 21]. It is highly related to the earlier retriever-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ouyang_revisiting_2024,
  title = {Revisiting the solution of meta kdd cup 2024: {Crag},
  author = {Ouyang, J. and Luo, Y. and Cheng, M. and Wang, D. and Yu, S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.15337},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the RAG Baseline, our solutions demonstrate superior results with reduced hallucination rates … that this study will make a modest contribution to the broader RAG and LLM communities. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_retcare_2024,
  title = {Retcare: {Towards},
  author = {Wang, Z. and Zhu, Y. and Gao, J. and Zheng, X. and Zeng, Y. and {...},
  year = {2024},
  url = {https://openreview.net/forum?id=jqo1vk63qE},
  journal = {… Intelligence and Data …},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… that they place significant trust in authoritative medical literature. … leveraging the Retrieval-Augmented Generation (RAG) … medical large language model’s pretraining phase and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lyu_retrieve-plan-generation_2024,
  title = {Retrieve-plan-generation: {An},
  author = {Lyu, Y. and Niu, Z. and Xie, Z. and Zhang, C. and Xu, T. and Wang, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.14979},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), … And we evaluate the Self-RAG model, which enhances the standard RAG by introducing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sun_r-bot_2024,
  title = {R-bot: {An},
  author = {Sun, Z. and Zhou, X. and Li, G. and Yu, X. and Feng, J. and Zhang, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.01661},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To address the hallucination problem of LLMs [25], we perform an offline stage to extract query … of LLMs, including LLM prompting and retrieval augmented generation (RAG), which are …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{desrochers_reducing_2024,
  title = {Reducing hallucinations in large language models through contextual position encoding},
  author = {Desrochers, S. and Wilson, J. and Beauchesne, M.},
  year = {2024},
  url = {https://files.osf.io/v1/resources/exjqb_v1/providers/osfstorage/665921bd65e1de48d5893f4d?action=download\&direct\&version=2},
  publisher = {files.osf.io},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… to lower hallucination rates [16], [17]. Retrieval-augmented generation (RAG) techniques … Mistral Large, a state-of-the-art large language model, was chosen for this study due to its …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bui_rambo_2024,
  title = {Rambo: {Enhancing},
  author = {Bui, T. D. and Luu-Van, D. T. and Nguyen, T. P. and Nguyen, T. T. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.15204},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, the employed code LLM may hallucinate code, … To mitigate code LLM hallucination, after analyzing, RAMBO … Code LLM’s Size: We studied the effect of code LLM sizes on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_rag4itops_2024,
  title = {{RAG4ITOps},
  author = {Zhang, T. and Jiang, Z. and Bai, S. and Zhang, T. and Lin, L. and Liu, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.15805},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM to utilize relevant and latest background knowledge, and it enables the LLM to generate factual … In addition to using RAG to enhance the LLM’s understanding of documents, we …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nazar_revolutionizing_2024,
  title = {Revolutionizing undergraduate learning: {CourseGPT},
  author = {Nazar, A. M. and Selim, M. Y. and Gaffar, A. and Ahmed, S.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.18310},
  journal = {arXiv preprint arXiv:2407.18310},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This work evaluates RAG-based-LLM effectiveness as intelligent university course … RAGs overcome outdated knowledge and hallucination by combining the LLM’s intrinsic knowledge …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{_rag_2024,
  title = {{RAG},
  author = {{정효정},
  year = {2024},
  url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11891076},
  journal = {대한전자공학회 학술 …},
  note = {Publisher: dbpia.co.kr},
  keywords = {source: Google Scholar},
  abstract = {… hallucination problems. Therefore, we introduce the framework that can evaluate the performance of RAG methods using the RAG … , answer, and context, LLM answers to queries. Then, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{lea_reducing_2024,
  title = {Reducing {AI},
  author = {Lea, D. M. and Cooley, R. S. and Cutshaw, M. A. and Priest, Z. M.},
  year = {2024},
  url = {https://www.osti.gov/biblio/2474834},
  publisher = {osti.gov},
  keywords = {source: Google Scholar},
  abstract = {… ) attempts to diminish hallucination by providing context to the LLM from data stores (… The LLM uses this context to formulate its response. RAG systems can still suffer from hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{buhnila_retrieve_2024,
  title = {Retrieve, {Generate},
  author = {Buhnila, Ioana and Sinha, Aman and Constant, Matthieu},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204905785&partnerID=40&md5=01d079dcca16b43a1d1944fcd2058b7d},
  pages = {189 -- 203},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{wendelken_roux-lette_2024,
  title = {Roux-lette at “{Discharge},
  author = {Wendelken, Suzanne M. and Antony, A. and Korutla, Rajashekar and Pachipala, B. and Mahajan, Danish and Shanahan, J. G. and Saba, Walid S.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204422297&partnerID=40&md5=7d2a93cbf33b6b7877f0f82e5d0463ad},
  pages = {719 -- 723},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{rehulka_rag_2024,
  title = {{RAG},
  author = {Řehulka, Erik and Šuppa, Marek},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201614999&partnerID=40&md5=832380a21dc27483fce0ccfb200ec746},
  booktitle = {{CEUR},
  volume = {3740},
  pages = {3021 -- 3031},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{garigliotti_retrieval-augmented_2024,
  title = {Retrieval-{Augmented},
  author = {Garigliotti, Darío},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002709216&partnerID=40&md5=83aec9594ccc26341748d3c5cf8cd724},
  booktitle = {{CEUR},
  volume = {3950},
  pages = {23 -- 30},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{shi_reference_2024,
  title = {Reference {Trustable},
  author = {Shi, Luohe and Yao, Yao and Li, Zuchao and Zhang, Lefei and Zhao, Hai},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000474591&partnerID=40&md5=d634d05b7658407471e531e2f3f498fa},
  booktitle = {Advances in {Neural},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{oreski_retrieval_2024,
  title = {Retrieval {Augmented},
  author = {Oreški, Dijana and Vlahek, Dino},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000393580&partnerID=40&md5=cc250260e7cc30b3db83002109d7a406},
  booktitle = {{CEUR},
  volume = {3938},
  pages = {12 -- 23},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{han_automating_2024-1,
  title = {Automating systematic literature reviews with retrieval-augmented generation: a comprehensive overview},
  author = {Han, B. and Susnjak, T. and Mathrani, A.},
  year = {2024},
  url = {https://www.mdpi.com/2076-3417/14/19/9103},
  journal = {Applied Sciences},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… RAG enhances LLM performance by grounding responses in dynamically updated and … citations based on context; RAG models tested on this benchmark outperformed ChatGPT-4. In …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{gai_achieving_2024,
  title = {Achieving higher factual accuracy in llama llm with weighted distribution of retrieval-augmented generation},
  author = {Gai, Z. and Tong, L. and Ge, Q.},
  year = {2024},
  url = {https://files.osf.io/v1/resources/ctw8v_v1/providers/osfstorage/664c901d2f167d17c20e7d42?action=download\&direct\&version=1},
  publisher = {files.osf.io},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) with the Llama Large language model significantly enhances factual … the effectiveness of the weighted RAG mechanism in prioritizing high-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shi_ask-eda_2024-1,
  title = {Ask-eda: {A},
  author = {Shi, L. and Kazda, M. and Sears, B. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10691824/},
  journal = {2024 IEEE LLM Aided …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… exhibits noticeable enhancements over both sparse-only and dense-only RAG. One of our … models to enhance RAG even further. We will also explore improving the LLM model further …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_astute_2024,
  title = {Astute rag: {Overcoming},
  author = {Wang, F. and Wan, X. and Sun, R. and Chen, J. and Arık, SÖ},
  year = {2024},
  url = {https://arxiv.org/abs/2410.07176},
  journal = {arXiv preprint arXiv:2410.07176},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Second, we propose Astute RAG, which explicitly addresses conflicts between LLM-internal … accurate, relevant, and hallucinationfree. Moreover, we allow the LLM to perform adaptive …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gupta_comprehensive_2024,
  title = {A comprehensive survey of retrieval-augmented generation (rag): {Evolution},
  author = {Gupta, S. and Ranjan, R. and Singh, S. N.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.12837},
  journal = {arXiv preprint arXiv:2410.12837},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG systems utilize self-attention within the LLM to manage … the generation of coherent, factual outputs from incomplete or … a RAG system, BART has been shown to improve the factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{priola_addressing_2024,
  title = {Addressing hallucinations with rag and nmiss in italian healthcare llm chatbots},
  author = {Priola, M. P.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.04235},
  journal = {arXiv preprint arXiv:2412.04235},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… I develop a RAG-based architecture to integrate external knowledge into the generation process, ensuring that models are grounded in factual data. The methodology leverages a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{alan_rag-based_2024,
  title = {A rag-based question answering system proposal for understanding islam: {Mufassirqas},
  author = {Alan, A. Y. and Karaarslan, E. and Aydin, Ö},
  year = {2024},
  url = {https://arxiv.org/abs/2401.15378},
  journal = {arXiv preprint arXiv:2401.15378},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM chatbots use NLP techniques to establish connections … false information, known as hallucination. Also, the chatbots' … -based Retrieval Augmented Generation (RAG) approach to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{malik_assessing_2024,
  title = {Assessing {ChatGPT4},
  author = {Malik, S. and Kharel, H. and Dahiya, D. S. and Ali, H. and {...},
  year = {2024},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11372545/},
  journal = {Annals of …},
  note = {Publisher: pmc.ncbi.nlm.nih.gov
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… ChatGPT-4’s performance was also compared to that of ChatGPT-3.5 and ChatGPT4-RAG. … In addition, none of the gastroenterologists surveyed would trust ChatGPT to autonomously …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{patel_comparative_2024,
  title = {A comparative analysis of large language models with retrieval-augmented generation based question answering system},
  author = {Patel, H. N. and Surti, A. and Goel, P. and Patel, B.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10714814/},
  journal = {… on I-SMAC (IoT in Social …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Zhang, “Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge-bases,” arXiv preprint arXiv:2403.10446, 2024. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tonmoy_comprehensive_2024,
  title = {A comprehensive survey of hallucination mitigation techniques in large language models},
  author = {Tonmoy, S. and Zaman, S. M. and Jain, V. and Rani, A. and {...},
  year = {2024},
  url = {https://www.amanchadha.com/research/2401.01313.pdf},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: amanchadha.com
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… In essence, the contributions of this paper to the realm of LLM hallucination are threefold: … RAG effectively mitigates the issue of hallucination in LLMs by generating responses that are …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bahaj_asthmabot_2024,
  title = {{AsthmaBot},
  author = {Bahaj, A. and Ghogho, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2409.15815},
  journal = {arXiv preprint arXiv:2409.15815},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… from factual sources to augment the limited knowledge of the LLM and increase the likelihood of having a factual … We used the Google Gemini LLM to infer the results of different queries. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yuan_hybrid_2024,
  title = {A hybrid {RAG},
  author = {Yuan, Y. and Liu, C. and Yuan, J. and Sun, G. and Li, S. and Zhang, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.05141},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations by integrating external knowledge bases. In this paper, we introduce a hybrid RAG … attribute predictors to reduce hallucinations, conducted LLM Knowledge Extractor and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{ammann_analysis_2024,
  title = {Analysis of {Risks},
  author = {Ammann, L. and Ott, S.},
  year = {2024},
  publisher = {OST Ostschweizer Fachhochschule},
  note = {Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{arslan_survey_2024,
  title = {A {Survey},
  author = {Arslan, M. and Ghanem, H. and Munawar, S. and Cruz, C.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050924021860},
  journal = {Procedia computer science},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… generation, significantly reducing the risk of hallucinations and improving the overall quality of … how RAG integration with the LLM handles queries that fall outside the scope of the LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chandrasekhar_amgpt_2024,
  title = {{AMGPT},
  author = {Chandrasekhar, A. and Chan, J. and Ogoke, F. and {...},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S2772369024000409},
  journal = {Additive Manufacturing …},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… We introduce “AMGPT”, a specialized LLM text generator … Hugging Face in a Retrieval-Augmented Generation (RAG) setup, … embeddings from the RAG setup accelerate response times …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mandikal_ancient_2024,
  title = {Ancient {Wisdom},
  author = {Mandikal, P.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.11903},
  journal = {arXiv preprint arXiv:2408.11903},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… is often hindered by factual inaccuracies and hallucinations, … potential of retrieval-augmented generation (RAG) models for … benchmark a RAG model against a standard, non-RAG LLM, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{benfenati_retrieval-augmented_2024,
  title = {A retrieval-augmented generation application for question-answering in nutrigenetics domain},
  author = {Benfenati, D. and Filippis, GM De and Rinaldi, A. M. and {...},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050924025092},
  journal = {Procedia Computer …},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… Large Language Model, circumventing the exhaustive model fine-tuning process. As a result, our RAG … Towards mitigating llm hallucination via self reflection, in: The 2023 Conference …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{saha_advancing_2024,
  title = {Advancing retrieval-augmented generation with inverted question matching for enhanced qa performance},
  author = {Saha, B. and Saha, U. and Malik, M. Z.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10781379/},
  journal = {IEEE Access},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… between our LLM-powered modified RAG system and a traditional RAG system, employing … A major challenge with RAG is its tendency to hallucinate with large text volumes. Our study …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{simon_methodology_2024,
  title = {A methodology for evaluating rag systems: {A},
  author = {Simon, S. and Mailach, A. and Dorn, J. and Siegmund, N.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.08801},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… a RAG system and a vanilla LLM as they represent the factual and timely data on which an LLM can … What if we had not compared the RAG system to the vanilla LLM baseline? The …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{he_simple_2024,
  title = {A simple yet effective retrieval-augmented generation framework for the meta {KDD},
  author = {He, L. and Li, R. and Shen, S. and Lu, J. and Zhu, L. and Su, Y. and {...},
  year = {2024},
  url = {https://openreview.net/forum?id=s9QAadOn6H},
  journal = {… Augmented Generation},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… and the full citation on the … LLM to generate reasoning trajectories and actions for specific tasks in an interleaved manner. For this task, we created a web retrieval tool that allows LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yadav_aeroquery_2024-1,
  title = {Aeroquery rag and llm for aerospace query in designs, development, standards, certifications},
  author = {Yadav, S.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10677028/},
  journal = {2024 IEEE International Conference on Electronics …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations, delays, and inefficiencies.To address this issue, our approach leverages the concepts of Retrieval-Augmented Generation (RAG) and … capabilities through RAG, enabling …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{raja_rag-based_2024,
  title = {A rag-based medical assistant especially for infectious diseases},
  author = {Raja, M. and Yuvaraajan, E.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10544639/},
  journal = {2024 International Conference on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… retrieval augmented generation (RAG) blends external information sources with massive language models. Without requiring the LLM to be retrained, RAG ingests real-time factual … LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{leemann_auto-gda_2024,
  title = {Auto-{GDA},
  author = {Leemann, T. and Petridis, P. and Vietri, G. and Manousakas, D. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.03461},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… While retrieval-augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jeong_study_2024,
  title = {A study on the implementation method of an agent-based advanced rag system using graph},
  author = {Jeong, C.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.19994},
  journal = {arXiv preprint arXiv:2407.19994},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , and being less susceptible to hallucinations, this study aims … database and then using an LLM to verify the relevance of … standard RAG pipeline to generate a response using the LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{rafat_ai-powered_2024,
  title = {{AI},
  author = {Rafat, M. I.},
  year = {2024},
  url = {https://www.theseus.fi/handle/10024/859826},
  publisher = {theseus.fi},
  keywords = {source: Google Scholar},
  abstract = {… This question investigates the specific impact of RAG optimization on the factual accuracy of LLMgenerated responses within the domain of housing disputes in Finland, comparing the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_llm_2024,
  title = {An llm agent for automatic geospatial data analysis},
  author = {Chen, Y. and Wang, W. and Lobry, S. and Kurtz, C.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.18792},
  journal = {arXiv preprint arXiv:2410.18792},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, we lack a geospatial RAG database to prompt … API hallucinations account for up to 15\% of all hallucinations … In this work, we opt to implement RAG only when the library …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{marcantonio_artificial_2024,
  title = {Artificial {Intelligence},
  author = {Marcantonio, G. Di},
  year = {2024},
  url = {https://u-pad.unimc.it/handle/11393/343610},
  journal = {… 2024-Navigare la complessità-Infrastrutture e …},
  note = {Publisher: u-pad.unimc.it},
  keywords = {source: Google Scholar},
  abstract = {… research projects (such as INTERPARES TRUST AI) that seek to … by the use of LLM and RAG in digital environments designed … Users need to trust that the information they access about …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sung_new_2024-1,
  title = {A new pipeline for generating instruction dataset via {RAG},
  author = {Sung, C. W. and Lee, Y. K. and Tsai, Y. T.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10633534/},
  journal = {2024 IEEE 48th Annual Computers …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… of LLMs and the Retrieval-Augmented Generation (RAG) related … The resulting fine-tuned LLM demonstrates showcases the … domain intricacies while maintaining factual accuracy. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{rezaei_at-rag_2024,
  title = {At-rag: {An},
  author = {Rezaei, M. R. and Hafezi, M. and Satpathy, A. and Hodge, L. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.12886},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The Hallucination Grader: Verifies the factual accuracy of the answer by cross-… LLM to identify the date and reason for the visit. Our method retrieved the correct details, while naive RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_alleviating_2024-1,
  title = {Alleviating action hallucination for llm-based embodied agents via inner and outer alignment},
  author = {Li, K. and Zheng, Q. and Zhan, Y. and Zhang, C. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10826957/},
  journal = {2024 7th …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… strategy based on retrievalaugmented generation proposed in this … Secondly, the RAG system generates actions by retrieving … reducing the risk of hallucinatory content [19]. Finally, the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yamin_applications_2024,
  title = {Applications of llms for generating cyber security exercise scenarios},
  author = {Yamin, M. M. and Hashmi, E. and Ullah, M. and Katt, B.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10695083/},
  journal = {IEEE Access},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM hallucination to create a sophisticated and adaptive cyber threat scenario. Our approach transforms this hallucination … LLM The bounded rationality of this LLM, enhanced by RAG, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shethiya_architecting_2024,
  title = {Architecting {Intelligent},
  author = {Shethiya, A. S.},
  year = {2024},
  url = {http://academianexusjournal.com/index.php/anj/article/view/25},
  journal = {Academia Nexus Journal},
  note = {Publisher: academianexusjournal.com},
  keywords = {source: Google Scholar},
  abstract = {… patterns that support LLM integration, such as retrieval-augmented generation (RAG), … —such as showing source documents or confidence scores—can further reinforce trust[12]. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_review_2024,
  title = {A review of prominent paradigms for llm-based agents: {Tool},
  author = {Li, X.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.05804},
  journal = {arXiv preprint arXiv:2406.05804},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM-profiled roles as the foundation for the development of algorithmic frameworks across different paradigms. Notably, Wang et al. (2024a) also discuss LLM … to their citations on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_agentpoison_2024,
  title = {Agentpoison: {Red},
  author = {Chen, Z. and Xiang, Z. and Xiao, C. and Song, D. and {...},
  year = {2024},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/eb113910e9c3f6242541c1652e30dfd6-Abstract-Conference.html},
  journal = {Advances in Neural …},
  note = {Publisher: proceedings.neurips.cc},
  keywords = {source: Google Scholar},
  abstract = {… We consider LLM agents with a RAG mechanism based on corpus retrieval. For a user query q, we retrieve knowledge or past experiences from a memory database D, containing a set …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{aksoy_architecting_2024,
  title = {Architecting and {Evaluating},
  author = {Aksoy, N. and Güven, Z. A. and Ünalır, M. O.},
  year = {2024},
  url = {https://www.researchgate.net/profile/Yeliz-Karaca/publication/382079038_Demystifying_Fractional_Order_Chaotic_Respiratory_Disease_System_with_XAI/links/668c9a85c1cf0d77ffc38d60/Demystifying-Fractional-Order-Chaotic-Respiratory-Disease-System-with-XAI.pdf#page=214},
  journal = {ICAMƩ'24},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… , and it was observed that LLM did not create hallucinations when a correct" prompt" was … LLM was observed to answer questions not included in the dataset and produce hallucinations…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_arl2_2024-1,
  title = {Arl2: {Aligning},
  author = {Zhang, L. and Yu, Y. and Wang, K. and Zhang, C.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.13542},
  journal = {arXiv preprint arXiv:2402.13542},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG has shown promising results in improving LLM re- … more effectively to LLM because it’s trained on LLM-labeled … in middle, LLM can still obtain some external factual information …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dong_survey_2024-1,
  title = {A survey of llm-based agents: {Theories},
  author = {Dong, X. and Zhang, X. and Bu, W. and Zhang, D. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10748304/},
  journal = {2024 3rd International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… With the scheme, RAG has demonstrated impressive strength for knowledge retrieval and … , LLM-based agents ought to invoke the planning reflection to handle hallucination and “…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gu_survey_2024,
  title = {A survey on llm-as-a-judge},
  author = {Gu, J. and Jiang, X. and Shi, Z. and Tan, H. and Zhai, X. and Xu, C. and Li, W. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.15594},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge … Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{meyer_comparison_2024,
  title = {A {Comparison},
  author = {Meyer, S. and Singh, S. and Tam, B. and Ton, C. and Ren, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.03562},
  journal = {arXiv preprint arXiv:2408.03562},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… [9] RAG is a text generation method for outsourcing relevant information, from a knowledge … , factual and quality information, to supply an LLM with contextual clues for producing factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{neumann_llm-driven_2024,
  title = {An llm-driven chatbot in higher education for databases and information systems},
  author = {Neumann, A. T. and Yin, Y. and Sowe, S. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10706931/},
  journal = {IEEE Transactions on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… “hallucinations,” where it produces incorrect content [47], [48], [49], [50]. The presented solution in this article adopts an retrieval-augmented generation (RAG… utilizing an RAG approach […},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{redelaar_attributed_2024,
  title = {Attributed {Question},
  author = {Redelaar, Felicia and van Drie, Romy A.N. and Verberne, Suzan and de Boer, Maaike H.T.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216932007&partnerID=40&md5=756bba2df695d85f3d127ed6de52fe6b},
  pages = {154 -- 165},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{ullrich_aic_2024,
  title = {{AIC},
  author = {Ullrich, Herbert and Mlynář, Tomáš and Drchal, Jan},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210659993&partnerID=40&md5=d26171b7bbf5ca15fd3af902c1c84b83},
  pages = {137 -- 150},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{yokoyama_aggregating_2024,
  title = {Aggregating {Impressions},
  author = {Yokoyama, Hibiki and Tsuchida, Rikuto and Buma, Kosei and Miyakawa, Sho and Utsuro, Takehito and Yoshioka, Masaharu},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204884253&partnerID=40&md5=6da87cea93f74b845eb50be2047f06b0},
  pages = {59 -- 72},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{li_alphafin_2024,
  title = {{AlphaFin},
  author = {Li, Xiang and Li, Zhenyu and Shi, Chen and Xu, Yong and Du, Qing and Tan, Mingkui and Huang, Jun},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195934691&partnerID=40&md5=d7f5acab0a70f5aff9a75286b4203402},
  pages = {773 -- 783},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 6},
}

@inproceedings{gonzalez_torres_automated_2024,
  title = {Automated {Question},
  author = {González Torres, Juan José and Bîndilă, Mihai Bogdan and Hofstee, Sebastiaan B.H.C. and Szondy, Daniel and Nguyen, Quang Hung and Wang, Shenghui and Englebienne, Gwenn},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195187720&partnerID=40&md5=a0f86b391fb3d4f8998babf57c56d152},
  pages = {204 -- 214},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{toxopeus_accelerate_2024,
  title = {Accelerate your subsurface workflow: chatting with your data using custom plugins},
  author = {Toxopeus, Gerrit J. and Aarsheim, G. and Kallestad, Jakob Vigerust and Mortensen, J. H.H. and Lervik, T. and Haughom, E. and Abeland, J. O.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003181055&partnerID=40&md5=432b7ddde57e19b97c1498339ed03e16},
  volume = {2},
  pages = {1171 -- 1175},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{kirchenbauer_hallucination_2024,
  title = {Hallucination reduction in large language models with retrieval-augmented generation using wikipedia knowledge},
  author = {Kirchenbauer, J. and Barns, C.},
  year = {2024},
  url = {https://files.osf.io/v1/resources/pv7r5/providers/osfstorage/6657166cd835c421594ce333?format=pdf\&action=download\&direct\&version=1},
  journal = {ArXiv Preprint. https://doi. org/10.31219/osf. io …},
  note = {Publisher: files.osf.io
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-augmented generation (RAG) has been widely recognized for its potential to enhance the factual accuracy of LLM … Implementations of RAG combined a retrieval module with …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_honest_2024,
  title = {Honest {AI},
  author = {Chen, X. and Wang, L. and Wu, W. and Tang, Q. and Liu, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.09699},
  journal = {arXiv preprint arXiv:2410.09699},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… out that RAG alone is not enough to alleviate hallucination in … Our results show that the hybrid approach using both RAG and … Great thanks to Ermo Wei, who shared invaluable LLM fine-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhu_halueval-wild_2024,
  title = {Halueval-wild: {Evaluating},
  author = {Zhu, Z. and Yang, Y. and Sun, Z.},
  year = {2024},
  url = {https://arxiv.org/abs/2403.04307},
  journal = {arXiv preprint arXiv:2403.04307},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG during reference answer generation (discussed in Section 2.3). To further validate the effectiveness of RAG… a pioneering benchmark for evaluating LLM hallucinations in real-world …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_how_2024-1,
  title = {How much can rag help the reasoning of llm?},
  author = {Liu, J. and Lin, J. and Liu, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.02338},
  journal = {arXiv preprint arXiv:2410.02338},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… new knowledge and reducing hallucinations. However, the deep understanding of RAG remains limited, how does RAG help the reasoning process and can RAG help improve the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{edwards_hybrid_2024,
  title = {Hybrid context retrieval augmented generation pipeline: {LLM},
  author = {Edwards, C.},
  year = {2024},
  url = {https://arxiv.org/abs/2405.15436},
  journal = {arXiv preprint arXiv:2405.15436},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… into an LLM as a prompt to generate a relevant response. The challenges of the Naive RAG … , and generation issues, including hallucinations where generated responses are not based …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_hybrid_2024,
  title = {Hybrid {RAG},
  author = {Su, C. and Wen, J. and Kang, J. and Wang, Y. and Su, Y. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10812735/},
  journal = {IEEE Internet of …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… To this end, this article proposes a hybrid Retrieval-Augmented Generation (RAG)-… of MLLMs by using hybrid RAG that filters different unimodal RAG results using multimodal metrics …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_hallucination-aware_2024,
  title = {Hallucination-aware {Optimization},
  author = {Liu, Y. and Liu, G. and Zhang, R. and Niyato, D. and Xiong, Z. and Kim, D. I. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.06007},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… mitigation strategies, including both model-based approaches, such as prompt engineering and Retrieval Augmented Generation (RAG), and system-based methods, such as Mixture-of…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_how_2024,
  title = {How well do {LLMs},
  author = {Wu, K. and Wu, E. and Cassasola, A. and Zhang, A. and Wei, K. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.02008},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with retrieval augmented generation (RAG… Given the rapid pace of LLM development and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{goyal_hacking_2024,
  title = {Hacking, the lazy way: {LLM},
  author = {Goyal, D. and Subramanian, S. and Peela, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.09493},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In our research, we introduce a new concept called “LLM Augmented Pentesting” … implementation of Retrieval-Augmented Generation (RAG) minimizes hallucinations and en…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{kalra_hypa-rag_2024,
  title = {{HyPA},
  author = {Kalra, Rishi and Wu, Zekun and Gulley, Ayesha and Hilliard, Airlie and Guan, Xin and Koshiyama, Adriano Soares and Treleaven, Philip C.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216564032&partnerID=40&md5=3da5d6954cf104b3d9a661f21f7d5ccd},
  pages = {237 -- 256},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{basaragin_how_2024,
  title = {How do you know that? {Teaching},
  author = {Bašaragin, Bojana and Ljajić, Adela and Medvecki, Darija and Cassano, Lorenzo and Košprdić, Miloš and Milošević, Nikola},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204109013&partnerID=40&md5=d920b3fcb8ffcc97abd036d73568b0e7},
  pages = {536 -- 547},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@article{chen_benchmarking_2024-1,
  title = {Benchmarking large language models in retrieval-augmented generation},
  author = {Chen, J. and Lin, H. and Han, X. and Sun, L.},
  year = {2024},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/29728},
  journal = {Proceedings of the AAAI Conference on …},
  note = {Publisher: ojs.aaai.org},
  keywords = {source: Google Scholar},
  abstract = {… use ChatGPT to conduct additional evaluation of the responses. Specifically, we prompt ChatGPT … can reflect information that is not present in the document or identify any factual errors. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xue_badrag_2024,
  title = {Badrag: {Identifying},
  author = {Xue, J. and Zheng, M. and Hu, Y. and Liu, F. and Chen, X. and Lou, Q.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.00083},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM’s output by injecting real, biased articles into the RAG corpus. This method causes the … the LLM’s alignment because the selected passages are factual and likely included in the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_black-box_2024,
  title = {Black-box opinion manipulation attacks to retrieval-augmented generation of large language models},
  author = {Chen, Z. and Liu, J. and Liu, H. and Cheng, Q. and Zhang, F. and Lu, W. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.13757},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… As RAG is designed to overcome the hallucination problem in LLMs and enhance their … of the RAG. Specially, the manipulator can only call the interface of the LLM in RAG instead of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{bouchard_building_2024,
  title = {Building {LLMs},
  author = {Bouchard, L. F. and Peters, L.},
  year = {2024},
  url = {https://books.google.com/books?hl=en\&lr=\&id=olJTEQAAQBAJ\&oi=fnd\&pg=PT10\&dq=%22retrieval+augmented+generation%22%7C%22rag%22+%22large+language+model%22%7C%22llm%22%7C%22chatgpt%22+trust%7Cconfidence%7Ccredibility%7Challucination%7Cfactuality%7Ccitation\&ots=iZN1YUXqn6\&sig=TEn3lsT2U5bz7gb1CUGfOro1R7k},
  publisher = {books.google.com},
  note = {Type: BOOK},
  keywords = {source: Google Scholar},
  abstract = {… Reducing hallucinations by limiting the LLM to answer based on existing chosen data. 2. Helping with explainability, error checking, and copyright issues by clearly referencing its …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{knollmeyer_benchmarking_2024-1,
  title = {Benchmarking of retrieval augmented generation: {A},
  author = {Knollmeyer, S. and Caymazer, O. and Koval, L. and {...},
  year = {2024},
  url = {https://www.scitepress.org/Papers/2024/130657/130657.pdf},
  journal = {Proceedings of the …},
  note = {Publisher: scitepress.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… of Large Language Models (LLM), traditional benchmarks have … performance of Retrieval Augmented Generation (RAG) systems. … citation quality focuses on assessing whether an LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{clop_backdoored_2024,
  title = {Backdoored retrievers for prompt injection attacks on retrieval augmented generation of large language models},
  author = {Clop, C. and Teglia, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.14479},
  journal = {arXiv preprint arXiv:2410.14479},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM (3), which generates an answer based on the retrieved content (4). Since the LLM’s answer is grounded in recent and factual … understanding of LLM vulnerabilities to RAG prompt …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shandilya_boosting_2024,
  title = {Boosting the {Capabilities},
  author = {Shandilya, B. and Palmer, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.00387},
  journal = {arXiv preprint arXiv:2410.00387},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… augmented generation (RAG) framework backed by a large language model (LLM) to correct … Instead of evaluating the retriever, we make the LLM itself generate confidence scores for …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_biorag_2024,
  title = {Biorag: {A},
  author = {Wang, C. and Long, Q. and Xiao, M. and Cai, X. and Wu, C. and Meng, Z. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.01107},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) … fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sanna_building_2024,
  title = {Building certified medical chatbots: {Overcoming},
  author = {Sanna, L. and Bellan, P. and Magnolini, S. and Segala, M. and {...},
  year = {2024},
  url = {https://aclanthology.org/2024.cl4health-1.15/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… Finally, we propose a modular RAG model to implement a Large Language Model in a … Moreover, the model is quite sparse, with an average confidence on correct predictions of 0.27. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kang_bim_2024,
  title = {{BIM},
  author = {Kang, T. W. and Park, S. H.},
  year = {2024},
  journal = {Journal of KIBIM},
  note = {Publisher: Korean Institute of Building …
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{andersson_bridging_2024,
  title = {Bridging the {Skills},
  author = {Andersson, M. and Enqvist, T.},
  year = {2024},
  url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2:1883256},
  publisher = {diva-portal.org},
  keywords = {source: Google Scholar},
  abstract = {… ) is an architecture that strengthens the capabilities of the LLM. It … In this thesis, the notion of using an LLM and the RAG … the general trust towards and evaluation of AI and RAG in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{riedler_beyond_2024,
  title = {Beyond text: {Optimizing},
  author = {Riedler, M. and Langer, S.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.21943},
  journal = {arXiv preprint arXiv:2410.21943},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM-as-a-Judge approach. Our results reveal that multimodal RAG can outperform single-modality RAG … they share common LLM limitations, including inaccuracies, hallucinations, and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_bordirlines_2024,
  title = {{BORDIRLINES},
  author = {Li, Bryan and Haider, Samar and Luo, Fiona and Agashe, Adwait and Callison-Burch, Chris},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216921129&partnerID=40&md5=4bb68c54c1d258051aa7249007c63a9a},
  pages = {1 -- 13},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{zhou_boosting_2024,
  title = {Boosting the {Potential},
  author = {Zhou, Yujia and Liu, Zheng and Dou, Zhicheng},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000514057&partnerID=40&md5=1878887f114d614aa68d2ce32b0b294a},
  booktitle = {Advances in {Neural},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@article{zhang_hallucination_2025-1,
  title = {Hallucination mitigation for retrieval-augmented large language models: a review},
  author = {Zhang, W. and Zhang, J.},
  year = {2025},
  url = {https://www.mdpi.com/2227-7390/13/5/856},
  journal = {Mathematics},
  note = {Publisher: mdpi.com},
  keywords = {source: Google Scholar},
  abstract = {… Investigating the root causes of hallucinations in the RAG paradigm helps to understand the mechanisms of its components at different stages and how their limitations affect LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{rahman_hallucination_2025,
  title = {Hallucination to truth: {A},
  author = {Rahman, S. S. and Islam, M. A. and Alam, M. M. and Zeba, M. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2508.03860},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This review systematically analyzes how LLM-generated content is evaluated for factual … [30] introduce Fact-CheckThen-RAG which improves LLM factual accuracy by evaluating each …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{anjum_halo_2025-1,
  title = {Halo: {Hallucination},
  author = {Anjum, S. and Zhang, H. and Zhou, W. and Paek, E. J. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11121104/},
  journal = {2025 IEEE/ACM …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… In LangChain, multiquery generation [8] utilizes an LLM to … integrates the LLM through an API, in our case, ChatGPT-3.5, … Retrieval augmented generation (RAG) is a novel framework …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{godinez_hysemrag_2025,
  title = {{HySemRAG},
  author = {Godinez, A.},
  year = {2025},
  url = {https://arxiv.org/abs/2508.05666},
  journal = {arXiv preprint arXiv:2508.05666},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… citation data in the context, the system prevents the Large Language Model from generating its own citations, … limitations of standard RAG architectures, namely noisy retrieval and LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mala_hybrid_2025,
  title = {Hybrid {Retrieval},
  author = {Mala, C. S. and Gezici, G. and Giannotti, F.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.05324},
  journal = {arXiv preprint arXiv:2504.05324},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We have also evaluated our hybrid RAG pipeline by comparing it with the baseline LLM(Llama-3-instruct-8B) model, the results are illustrated in Appendix B. The results emphasize the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_hoprag_2025,
  title = {Hoprag: {Multi},
  author = {Liu, H. and Wang, Z. and Chen, X. and Li, Z. and Xiong, F. and Yu, Q. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.12442},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Specifically, we adopt LLM to generate two groups of pseudoqueries for each passage pi: (1… in the vertices and thus avoids LLM hallucination during summarization, information loss …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jeon_hybrid_2025,
  title = {Hybrid large language model approach for prompt and sensitive defect management: {A},
  author = {Jeon, K. and Lee, G.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1474034624007274},
  journal = {Advanced Engineering Informatics},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… closed-source LLM for generating a … retrieval-augmented generation (GraphRAG)-based QA systems, which have been extensively studied recently. Our results show that the hybrid LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ping_hdlcore_2025-1,
  title = {Hdlcore: {A},
  author = {Ping, H. and Li, S. and Zhang, P. and Cheng, A. and Duan, S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.16528},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… • Efficient Heterogeneous RAG: We establish a comprehensive heterogeneous database from carefully selected open-source HDL datasets. Our proposed RAG system extracts multiple …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pattnayak_hybrid_2025,
  title = {Hybrid ai for responsive multi-turn online conversations with novel dynamic routing and feedback adaptation},
  author = {Pattnayak, P. and Agarwal, A. and Meghwani, H. and Patel, H. L. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2506.02097},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… 2024) further optimizes RAG for precision and adaptability in complex applications. By re… inference, RAG systems mitigate common LLM challenges such as hallucinations and outdated …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_healthq_2025,
  title = {Healthq: {Unveiling},
  author = {Wang, Z. and Li, H. and Huang, D. and Kim, H. S. and Shin, C. W. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S2352648325000315},
  journal = {Smart Health},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… for evaluating the questioning capabilities of LLM healthcare chains. By implementing advanced LLM chains, including Retrieval-Augmented Generation (RAG), Chain of Thought (CoT)…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_how_2025,
  title = {How llms react to industrial spatio-temporal data? assessing hallucination with a novel traffic incident benchmark dataset},
  author = {Li, Q. and Tan, M. and Zhao, X. and Zhang, D. and Zhang, D. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.naacl-industry.4/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… Moreover, the predominance of English in LLM development … Generation (RAG) to further examine LLM hallucinations in … the types of hallucinations that RAG can mitigate and how it …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_human-llm_2025,
  title = {Human-llm collaboration in generative design for customization},
  author = {Wang, X. and Jiang, Z. and Xiong, Y. and Liu, A.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0278612525000731},
  journal = {Journal of Manufacturing Systems},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… Against the background, this paper explores the potential of LLM in redefining GDfC. Based … three human-LLM collaboration schemes to demonstrate the potential roles of LLM in GDfC. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{paudel_hallucinot_2025,
  title = {Hallucinot: {Hallucination},
  author = {Paudel, B. and Lyzhov, A. and Joshi, P. and Anand, P.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.07069},
  journal = {arXiv preprint arXiv:2504.07069},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… for detecting hallucinations in large language model (LLM) … taxonomy of LLM responses specific to hallucination in enterprise … to LLM response verification builds on established RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tyndall_impact_2025,
  title = {Impact of retrieval augmented generation and large language model complexity on undergraduate exams created and taken by {AI},
  author = {Tyndall, E. and Gayheart, C. and Some, A. and Genz, J. and Wagner, T. and {...},
  year = {2025},
  url = {https://www.cambridge.org/core/journals/data-and-policy/article/impact-of-retrieval-augmented-generation-and-large-language-model-complexity-on-undergraduate-exams-created-and-taken-by-ai-agents/498B8CB29AA37695AC5EACBE1B2089B4},
  journal = {Data \&Policy},
  note = {Publisher: cambridge.org},
  keywords = {source: Google Scholar},
  abstract = {… allows it to leverage RAG and the textbook … ChatGPT-4 Turbo model, which did not have access to the college textbook. While this study did not directly test for hallucinations, citations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_improving_2025-3,
  title = {Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical …},
  author = {Liu, S. and McCoy, A. B. and Wright, A.},
  year = {2025},
  journal = {Journal of the American Medical …},
  note = {Publisher: Oxford Academic
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kumar_improving_2025,
  title = {Improving the reliability of {LLMs},
  author = {Kumar, A. and Kim, H. and Nathani, J. S. and Roy, N.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.09031},
  journal = {arXiv preprint arXiv:2505.09031},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… retrieval-augmented generation (RAG), as well as applying self-consistency and selfverification strategies, can reduce hallucinations and improve factual … to tackle LLM Hallucinations. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hou_improving_2025-1,
  title = {Improving dietary supplement information retrieval: {Development},
  author = {Hou, Y. and Bishop, J. R. and Liu, H. and Zhang, R.},
  year = {2025},
  url = {https://www.jmir.org/2025/1/e67677/},
  journal = {Journal of medical Internet research},
  note = {Publisher: jmir.org
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… An RAG system operates in two phases: first, it retrieves relevant data from a … LLM to generate responses [20,21]. By integrating iDISK with an RAG system, we mitigate the hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_injury_2025,
  title = {Injury degree appraisal of large language model based on retrieval-augmented generation and deep learning},
  author = {Zhang, F. and Luo, Y. and Gao, Z. and Han, A.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0160252725000032},
  journal = {International Journal of Law and Psychiatry},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… a novel approach that combines Retrieval-Augmented Generation (RAG) with graph-based … By integrating this model with a graph-based knowledge base, our RAG strategy significantly …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lakatos_investigating_2025-1,
  title = {Investigating the performance of retrieval-augmented generation and domain-specific fine-tuning for the development of {AI},
  author = {Lakatos, R. and Pollner, P. and Hajdu, A. and Joo, T.},
  year = {2025},
  url = {https://www.mdpi.com/2504-4990/7/1/15},
  journal = {Machine Learning and Knowledge …},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… the performance of RAG and DFT on several LLM architectures, … RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_implementation_2025,
  title = {Implementation of {Retrieval},
  author = {Yang, C. B. and Kim, Y. S.},
  year = {2025},
  journal = {Smart Media Journal},
  note = {Publisher: THE KOREAN INSTITUTE OF …
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wen_interactivesurvey_2025,
  title = {Interactivesurvey: {An},
  author = {Wen, Z. and Cao, J. and Wang, Z. and Guo, B. and Yang, R. and Liu, S.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.08762},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Citation Generation To facilitate researchers’ reading experience, we also generate citations in … Categorization also includes the time for LLM-based RAG, we anticipate that with more …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{xing_investigating_2025,
  title = {Investigating knowledge graphs as structured external memory to enhance large language models' generation for mathematical concept answering},
  author = {Xing, W. and Li, C. and Li, H. and Zhu, W. and Lyu, B. and Yan, Z.},
  year = {2025},
  url = {https://osf.io/mx83s_v2/},
  publisher = {osf.io},
  keywords = {source: Google Scholar},
  abstract = {… However, the issue of LLM hallucination has been well-documented in the research … RAG is using a knowledge graph (KG). In this paper, we design a method to construct a KG with LLM…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{noauthor_ictir_2025,
  title = {{ICTIR},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013792692&partnerID=40&md5=20090fa2896f14818c33f80e360afd50},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_icsob-c_2025,
  title = {{ICSOB},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218677724&partnerID=40&md5=66cf2a5328aa6f0fb536d3be4ef2207f},
  booktitle = {{CEUR},
  volume = {3921},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_international_2025,
  title = {International {Conference},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010609553&partnerID=40&md5=f9451467becb3b6779703978ff98906a},
  journal = {Lecture Notes in Networks and Systems},
  volume = {1355 LNNS},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_icocseti_2025,
  title = {{ICoCSETI},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009967373&partnerID=40&md5=0e47d45688e8511c09c9cb9b3f3d160c},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{clay_information_2025,
  title = {Information for {Conversation},
  author = {Clay, Alex and Jimeńez-Ruiz, Ernesto},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003708783&partnerID=40&md5=3228c55802e79874af0d38ce187b5f58},
  booktitle = {{CEUR},
  volume = {3953},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{sharma_og-rag_2024,
  title = {Og-rag: {Ontology},
  author = {Sharma, K. and Kumar, P. and Li, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.15235},
  journal = {arXiv preprint arXiv:2412.15235},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Through extensive experiments on two agriculture datasets and a news dataset, we demonstrate that OG-RAG significantly improves the factual accuracy of LLM-generated responses, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{islam_open-rag_2024-2,
  title = {Open-rag: {Enhanced},
  author = {Islam, S. B. and Rahman, M. A. and Hossain, K. S. M. and Hoque, E. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.01782},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… tokens and measure the confidence of outputs conditioned on … that our OPENRAG significantly improves the overall factual ac… We define OPEN-RAG LLM as a model MG that, given an …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_optimizing_2024,
  title = {Optimizing {LLM},
  author = {Zhao, Y. and Singh, P. and Bhathena, H. and Ramos, B. and {...},
  year = {2024},
  url = {https://aclanthology.org/2024.naacl-industry.23/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… Instruction Following Ability Practical RAG deployments typically require the LLM returns answer in a specific language style and may require citations in certain structured format that …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{finsas_optimizing_2024,
  title = {Optimizing rag systems for technical support with llm-based relevance feedback and multi-agent patterns},
  author = {Finsås, M. and Maksim, J.},
  year = {2024},
  url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/3160478},
  publisher = {ntnuopen.ntnu.no},
  keywords = {source: Google Scholar},
  abstract = {… 3.5 we examine studies on mitigating hallucination in LLMs and multi-agent LLM design patterns, … current methods for evaluating LLM-based systems, including RAG-specific evaluation …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kumar_overcoming_2024,
  title = {Overcoming llm challenges using rag-driven precision in coffee leaf disease remediation},
  author = {Kumar, S. S. and Khan, AKMA and Banday, I. A. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10543859/},
  journal = {… in Computer Science …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Serving as a dynamic bridge, RAG minimizes the risk of hallucination, enhancing GenAI application … RAG’s role in overcoming LLM limitations is central to our research, promising a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{koo_optimizing_2024,
  title = {Optimizing query generation for enhanced document retrieval in rag},
  author = {Koo, H. and Kim, M. and Hwang, S. J.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.12325},
  journal = {arXiv preprint arXiv:2407.12325},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucination of LLM continues to undermine user belief. Among the strategies to mitigate, the Retrieval-Augmented Generation (RAG… query, we utilize a Large Language Model (LLM) to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{feng_optimizing_2024,
  title = {Optimizing microservice deployment in edge computing with large language models: {Integrating},
  author = {Feng, K. and Luo, L. and Xia, Y. and Luo, B. and He, X. and Li, K. and Zha, Z. and Xu, B. and {...},
  year = {2024},
  url = {https://www.mdpi.com/2073-8994/16/11/1470},
  journal = {Symmetry},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Additionally, we prompted the LLM to generate code without the use of the RAG database … for the LLM to generate appropriate responses, significantly enhancing the factual accuracy …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_ontology-aware_2024,
  title = {Ontology-aware rag for improved question-answering in cybersecurity education},
  author = {Zhao, C. and Agrawal, G. and Kumarage, T. and Tan, Z. and Deng, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.14191},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG approach helps reduce hallucinations and address domain knowledge issues to some extent, the reliability of LLM-… cybersecurity content using RAG and validate LLM outputs with …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jin_orthodoc_2024,
  title = {Orthodoc: {Multimodal},
  author = {Jin, Y. and Zhang, Y.},
  year = {2024},
  url = {https://arxiv.org/abs/2409.09052},
  journal = {arXiv preprint arXiv:2409.09052},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… their conditions and enhancing doctor-patient trust. Computed Tomography (CT) is … Retrieval-Augmented Generation(RAG) module capable of effectively mitigating model hallucinations…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kaintura_orassistant_2024,
  title = {{ORAssistant},
  author = {Kaintura, A. and Luar, S. S. and Almeida, I. I.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.03845},
  journal = {arXiv preprint arXiv:2410.03845},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… as the base LLM model to build and test ORAssistant. Early evaluation results of the RAG-based … ’s output by ensuring that knowledge is retrieved from trusted data sources, generating …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kluibenschadl_fiction_2024,
  title = {{FROM},
  author = {Kluibenschädl, S. and Schlögl, S. and Kruschel, S.},
  year = {2024},
  url = {https://library.iated.org/view/KLUIBENSCHADL2024FRO},
  journal = {ICERI2024 Proceedings},
  note = {Publisher: library.iated.org},
  keywords = {source: Google Scholar},
  abstract = {… Our work has outlined how a RAG-augmented educational LLM can be built using various … We have furthermore shown how the RAG-augmentation helped improve factual accuracy, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{krishna_fact_2024,
  title = {Fact, fetch, and reason: {A},
  author = {Krishna, S. and Krishna, K. and Mohananey, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.12941},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinated questions and answers from the obtained set. We then evaluated the same LLM … while also mitigating the issues of hallucination present in LLM-generated content. Human …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zeng_federated_2024-2,
  title = {Federated recommendation via hybrid retrieval augmented generation},
  author = {Zeng, H. and Yue, Z. and Jiang, Q. and Wang, D.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10825302/},
  journal = {2024 IEEE International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Our proposed hybrid retrieval mechanism and LLM-… LLM, overcoming data sparsity and heterogeneity in FR. Finally, the RAG nature of GPT-FedRec also prevents LLM hallucination, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sui_fidelis_2024,
  title = {Fidelis: {Faithful},
  author = {Sui, Y. and He, Y. and Liu, N. and He, X. and Wang, K. and Hooi, B.},
  year = {2024},
  url = {https://arxiv.org/abs/2405.13873},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations and enable verifiable reasoning, we propose FiDeLiS to improve the factuality of LLM … Our retrieval module Path-RAG mitigate this issue by constraining candidate set St …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ju_flooding_2024,
  title = {Flooding spread of manipulated knowledge in llm-based multi-agent communities},
  author = {Ju, T. and Wang, Y. and Ma, X. and Cheng, P. and Zhao, H. and Wang, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.07791},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… our attack method can successfully induce LLM-based agents to … through popular retrieval-augmented generation frameworks, … The LLM may not always verify the factual accuracy …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jain_rag_2024,
  title = {From rag to riches: {Retrieval},
  author = {Jain, P. and Soares, L. B. and Kwiatkowski, T.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.00361},
  journal = {arXiv preprint arXiv:2407.00361},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , the typical approaches chain LLM generations with calls to … evidence corpus using a single LLM and decoding process. … is partially attributed, with LLM hallucinating based on partial …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bao_faithbench_2024,
  title = {Faithbench: {A},
  author = {Bao, F. S. and Li, M. and Qu, R. and Luo, G. and Wan, E. and Tang, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.13210},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-… Filtering samples by LLM To balance annotator effort with our goal of LLM diversity, we …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mohammadjafari_natural_2024,
  title = {From natural language to sql: {Review},
  author = {Mohammadjafari, A. and Maida, A. S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.01066},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of LLM-based text-to-SQL systems, from early rule-based models to advanced LLM approaches that use (RAG… transparent AI procedures to build trust in LLM-based text-to-SQL systems. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yepes_financial_2024,
  title = {Financial report chunking for effective retrieval augmented generation},
  author = {Yepes, A. J. and You, Y. and Milczek, J. and Laverde, S. and Li, R.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.05131},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… problem with LLMs [15,43] when recovering factual information directly from an LLM. In RAG, instead of answering a user query directly using an LLM, the user query is used to retrieve …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{elsharef_facilitating_2024,
  title = {Facilitating threat modeling by leveraging large language models},
  author = {Elsharef, I. and Zeng, Z. and Gu, Z.},
  year = {2024},
  url = {https://www.ndss-symposium.org/wp-content/uploads/aiscc2024-16-paper.pdf},
  journal = {Workshop on AI Systems with …},
  note = {Publisher: ndss-symposium.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Table V shows the example of hallucination from RAG-based LLM … this RAG-enhance LLM solution for threat modeling. Based on the observations in this study, the RAGenhanced LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{rothman__2024,
  title = {… for {Natural},
  author = {Rothman, D.},
  year = {2024},
  url = {https://books.google.com/books?hl=en\&lr=\&id=q9P3EAAAQBAJ\&oi=fnd\&pg=PP1\&dq=%22retrieval+augmented+generation%22%7C%22rag%22+%22large+language+model%22%7C%22llm%22%7C%22chatgpt%22+trust%7Cconfidence%7Ccredibility%7Challucination%7Cfactuality%7Ccitation\&ots=-g2PrskpL0\&sig=pnxifQa6ht_TYS3vx4v8LABKL9U},
  publisher = {books.google.com},
  note = {Type: BOOK},
  keywords = {source: Google Scholar},
  abstract = {… I want to thank the corporations that trusted me from the start to deliver artificial intelligence … Retrieval Augmented Generation (RAG). We will implement an example of automated RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{maes_fixing_2024,
  title = {Fixing {Reference},
  author = {Maes, S. H.},
  year = {2024},
  publisher = {OSF},
  note = {Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{deventer_interests_2024,
  title = {From {Interests},
  author = {Deventer, H. Van and Mills, M. and Evrard, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.19312},
  journal = {arXiv preprint arXiv:2412.19312},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We introduce a two-stage retrieval process for a RAG-based LLM course recommender … with both an explanation and a confidence rating. We show that the LLM’s embedding space …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kommineni_human_2024,
  title = {From human experts to machines: {An},
  author = {Kommineni, V. K. and König-Ries, B. and Samuel, S.},
  year = {2024},
  url = {https://arxiv.org/abs/2403.08345},
  journal = {arXiv preprint arXiv:2403.08345},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, … studies, a prerequisite to trust and validation of results. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lin_flame_2024,
  title = {Flame: {Factuality},
  author = {Lin, S. C. and Gao, L. and Oguz, B. and Xiong, W. and {...},
  year = {2024},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/d16152d53088ad779ffa634e7bf66166-Abstract-Conference.html},
  journal = {Advances in Neural …},
  note = {Publisher: proceedings.neurips.cc},
  keywords = {source: Google Scholar},
  abstract = {… the LLM alignment process more factual, by first identifying factors that lead to hallucination in … This is likely because the supervision from RAG contains information unknown to the LLM; …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{barnett_fine-tuning_2024,
  title = {Fine-tuning or fine-failing? debunking performance myths in large language models},
  author = {Barnett, S. and Brannelly, Z. and Kurniawan, S. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2406.11201},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… integration allows RAG systems to enhance LLM responses by leveraging domain-specific … , demonstrated a significant reduction in hallucination compared to other generalised LLMs …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{edge_local_2024,
  title = {From local to global: {A},
  author = {Edge, D. and Trinh, H. and Cheng, N. and Bradley, J. and Chao, A. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.16130},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Specifically, our approach uses the LLM to infer the potential users would use the RAG … AFaCTA: Assisting the annotation of factual claim detection with reliable LLM annotators. In Ku, L.…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{_fine-tuning_2024,
  title = {Fine-tuning 과 {RAG},
  author = {{손지원},
  year = {2024},
  url = {https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11825516},
  journal = {Proceedings of KIIT …},
  note = {Publisher: dbpia.co.kr},
  keywords = {source: Google Scholar},
  abstract = {… the RAG method … , hallucinations and answer errors occurred frequently, and accuracy and correct response rate were low. On the other hand, the RAG method had fewer hallucinations …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{tekkesinoglu_feature_2024,
  title = {From {Feature},
  author = {Tekkesinoglu, Sule and Kunze, Lars},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210017268&partnerID=40&md5=e6e40614fddb549ddd0b546564e69448},
  booktitle = {{CEUR},
  volume = {3803},
  pages = {114 -- 132},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{lala_paperqa_2023,
  title = {Paperqa: {Retrieval},
  author = {Lála, J. and O'Donoghue, O. and Shtedritski, A. and Cox, S. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2312.07559},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Nevertheless, standard RAG models follow a fixed, linear flow, … breaking RAG into modular pieces, allowing an agent LLM to … We assessed citation hallucinations from GPT-3.5, GPT4, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tamber_benchmarking_2025,
  title = {Benchmarking {LLM},
  author = {Tamber, M. S. and Bao, F. S. and Xu, C. and Luo, G. and Kazi, S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.04847},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This paper presents our efforts to measure LLM hallucinations with a focus on … introduce hallucinations when summarizing documents. We discuss Vectara’s existing LLM hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhang_beefbot_2025,
  title = {Beefbot: {Harnessing},
  author = {Zhang, Z. and Wilson, C. A. and Hay, R. and {...},
  year = {2025},
  url = {https://aclanthology.org/2025.coling-demos.7/},
  journal = {Proceedings of the …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… Model fine-tuning can inject factual knowledge into LLM parameters and provide promising results for completing in-domain tasks. We finetune the Llama-3 which is the latest …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lu_boosting_2025,
  title = {Boosting {GPT},
  author = {Lu, S. and Cosgun, E.},
  year = {2025},
  url = {https://academic.oup.com/bioinformaticsadvances/article-abstract/5/1/vbaf019/8002096},
  journal = {Bioinformatics Advances},
  note = {Publisher: academic.oup.com},
  keywords = {source: Google Scholar},
  abstract = {… In our project, we aimed to improve LLM performance in genomics by adding variant annotation data to LLMs by retrieval-augmented generation (RAG) and fine-tuning techniques. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_biomedrag_2025,
  title = {Biomedrag: {A},
  author = {Li, M. and Kilicoglu, H. and Xu, H. and Zhang, R.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046424001874},
  journal = {Journal of Biomedical Informatics},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-augmented generation (RAG) involves a solution by retrieving knowledge from an established database to enhance the performance of large language models (LLM)… the LLM. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiang_bi_2025,
  title = {Bi'an: {A},
  author = {Jiang, Z. and Sun, M. and Zhang, Z. and Liang, L.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.19209},
  journal = {arXiv preprint arXiv:2502.19209},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… an LLM to assess whether the RAG system’s output aligns with the input text. However, the application of LLM-as-a-Judge in RAG hallucination … , for RAG hallucination detection and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tomkou_bridging_2025-1,
  title = {Bridging industrial expertise and xr with llm-powered conversational agents},
  author = {Tomkou, D. and Fatouros, G. and Andreou, A. and Makridis, G. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2504.05527},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Building upon this existing research, our work integrates RAG-enhanced LLM conversational … from LLM Chat Engine to a user query requesting assistance in troubleshooting citing the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cui_bailicai_2025,
  title = {Bailicai: {A},
  author = {Cui, L. and Liu, Y. and Ouyang, C. and Yu, Y. and Zhang, J. and {...},
  year = {2025},
  url = {https://file.sciopen.com/sciopen_public/1895014766676131841.pdf},
  journal = {Big Data Mining and …},
  note = {Publisher: file.sciopen.com
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… In addition, the integration of RAG and LLM in the medical field is still in its infancy[25], and … from factual accuracy[15, 16]. Early research on RAG primarily focused on developing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{shandilya_boosting_2025,
  title = {Boosting the {Capabilities},
  author = {Shandilya, Bhargav and Palmer, Alexis},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218492374&partnerID=40&md5=69ec59b9d9594d9cadcbaec2332300a2},
  booktitle = {Proceedings - {International},
  pages = {7470 -- 7483},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{li_benchmarking_2025,
  title = {{BENCHMARKING},
  author = {Li, Yangning and Li, Yinghui and Wang, Xinyu and Jiang, Yong and Zhang, Zhen and Zheng, Xinran and Wang, Hui and Zheng, Haitao and Huang, Fei and Zhou, Jingren and Yu, Philip S.},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010268589&partnerID=40&md5=49e099f7db1a4f114dacec9d75085fd6},
  pages = {90296 -- 90318},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{procko_graph_2024,
  title = {Graph retrieval-augmented generation for large language models: {A},
  author = {Procko, T. T. and Ochoa, O.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10771030/},
  journal = {2024 Conference on AI, Science …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… This paper surveys work incorporating KGs with LLM RAG, intending to equip scientists with a … to tasks from several domains and lessens LLM hallucination. Edge et al. present a Graph …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{an_golden-retriever_2024,
  title = {Golden-retriever: high-fidelity agentic retrieval augmented generation for industrial knowledge base},
  author = {An, Z. and Ding, X. and Fu, Y. C. and Chu, C. C. and Li, Y. and Du, W.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.00798},
  journal = {arXiv preprint arXiv:2408.00798},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Despite its advantages, RAG also faces challenges to be … documents, RAG’s LLM backbone may hallucinate and … method with vanilla LLM (without RAG) and the vanilla RAG method. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mavromatis_gnn-rag_2024,
  title = {Gnn-rag: {Graph},
  author = {Mavromatis, C. and Karypis, G.},
  year = {2024},
  url = {https://arxiv.org/abs/2405.20139},
  journal = {arXiv preprint arXiv:2405.20139},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The input given to the LLM contains the KG factual information along with the question and a prompt. For instance, the input becomes “Knowledge: Jamaica → language\_spoken → …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{he_g-retriever_2024,
  title = {G-retriever: {Retrieval},
  author = {He, X. and Tian, Y. and Sun, Y. and Chawla, N. and {...},
  year = {2024},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/efaf1c9726648c8ba363a5c927440529-Abstract-Conference.html},
  journal = {Advances in …},
  note = {Publisher: proceedings.neurips.cc},
  keywords = {source: Google Scholar},
  abstract = {… the LLM’s pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output of the GNN. Our RAG-based design mitigates hallucinations … the LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{peng_graph_2024,
  title = {Graph retrieval-augmented generation: {A},
  author = {Peng, B. and Zhu, Y. and Liu, Y. and Bo, X. and Shi, H. and Hong, C. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2408.08921},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… knowledge base, RAG refines LLM outputs, effectively mitigating issues such as “hallucination”, lack … entities in databases presents challenges for RAG systems. In response, GraphRAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{taiwo_generative_2024,
  title = {Generative {AI},
  author = {Taiwo, R. and Bello, I. T. and Abdulai, S. F. and Yussif, A. M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2402.09939},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… A retrievalaugmented generation (RAG) system was implemented to improve the base LLM further. This mitigated hallucinated text by grounding outputs in relevant dataset …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{gonzalez_generative_2024,
  title = {Generative {AI},
  author = {Gonzalez, Claudio and Onwuegbuche, Faithful Chiagoziem},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002675634&partnerID=40&md5=55fe03d63c85bb5594fc328e2018c801},
  booktitle = {{CEUR},
  volume = {3910},
  pages = {408 -- 420},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{dong_how_2023,
  title = {How to build an {AI},
  author = {Dong, C.},
  year = {2023},
  url = {https://arxiv.org/abs/2311.17696},
  journal = {arXiv preprint arXiv:2311.17696},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The system effectively utilizes LLMs and RAG techniques to create an adaptive knowledge base, delivering accurate and personalized responses to student queries. Citations are …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xie_weknow-rag_2024,
  title = {Weknow-rag: {An},
  author = {Xie, W. and Liang, X. and Liu, Y. and Ni, K. and Cheng, H. and Hu, Z.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.07611},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… level is below the threshold, we conclude that the LLM lacks sufficient confidence to answer the question and will output "I don’t know". Experiments on performance with various …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cherumanal_walert_2024-1,
  title = {Walert: {Putting},
  author = {Cherumanal, S. P. and Tian, L. and Abushaqra, F. M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2401.07216},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of an open-source LLM; (ii) monitoring the problem of hallucinations (ie, the introduction of … of LLM-based conversational systems [18]. Finally, we plan to deploy RAG approaches to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{li_wiping_2024,
  title = {Wiping out the limitations of {Large},
  author = {Li, M. M. and Nikishina, I. and Sevgili, Ö and Semmann, M.},
  year = {2024},
  url = {https://www.alexandria.unisg.ch/entities/publication/e03dcba0-380a-4546-864a-c25f97c92fc9},
  publisher = {alexandria.unisg.ch},
  keywords = {source: Google Scholar},
  abstract = {… for RAG applications, illustrating how RAG can be systematically implemented to improve LLM tasks and … shift roles of human workers?; How does RAGs foster trust in humans? …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xiong_when_2024,
  title = {When graph meets retrieval augmented generation for wireless networks: {A},
  author = {Xiong, Y. and Zhang, R. and Liu, Y. and Niyato, D. and Xiong, Z. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.07189},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… frameworks in networking, including robust updates, mitigation of hallucination, and … In RAG, an LLM is augmented by an external knowledge base that retrieves and supplies relevant …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jovanovic_ward_2024,
  title = {Ward: {Provable},
  author = {Jovanović, N. and Staab, R. and Baader, M. and Vechev, M.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.03537},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The query q is generally combined with Dq, and fed into an LLM to generate a more factual response r = M(q, Dq). Expanding D also enables access to new information without costly …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wan_what_2024-1,
  title = {What evidence do language models find convincing?},
  author = {Wan, A. and Wallace, E. and Klein, D.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.11782},
  journal = {arXiv preprint arXiv:2402.11782},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We use this dataset to perform sensitivity and counterfactual analyses to explore which text … production RAG models work. However, we could have instead just directly asked the LLM, “…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{leekha_war_2024,
  title = {War of {Words},
  author = {Leekha, Rohan Singh and Simek, Olga and Dagli, Charlie K.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200443688&partnerID=40&md5=1dc1aaa21c3e1c7f352b573c2dcd7709},
  booktitle = {Proceedings of the {International},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{hou_wikicontradict_2024,
  title = {{WikiContradict},
  author = {Hou, Yufang and Pascale, Alessandra and Carnerero-Cano, Javier and Tchrakian, Tigran T. and Marinescu, Radu and Daly, Elizabeth M. and Padhi, Inkit and Sattigeri, Prasanna S.},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000533993&partnerID=40&md5=e276b1ce26774dfbb3cb1fc79e294253},
  booktitle = {Advances in {Neural},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{azher_futuregen_2025-1,
  title = {Futuregen: {Llm},
  author = {Azher, I. A. and Mokarrama, M. J. and Guo, Z. and Choudhury, S. R. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.16561},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… an LLM-as-a-judge approach for evaluation. Our results demonstrated that the RAGbased approach with LLM feedback … Hallucination Rate We evaluated hallucinations by treating each …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lee_finetune-rag_2025,
  title = {Finetune-{RAG},
  author = {Lee, Z. P. and Lin, A. and Tan, C.},
  year = {2025},
  url = {https://arxiv.org/abs/2505.10792},
  journal = {arXiv preprint arXiv:2505.10792},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , a simple and effective fine-tuning approach that features the first-of-its-kind RAG … -RAG improves factual accuracy by 21.2\% over the base model. We also propose Bench-RAG, an LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{aquino_rag_2025,
  title = {From {RAG},
  author = {Aquino, GA e and Azevedo, NS de and Okimoto, L. Y. S. and {...},
  year = {2025},
  url = {https://www.preprints.org/frontend/manuscript/12d92f418fc17b4bd3e6b6144acf951c/download_pub},
  publisher = {preprints.org},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… in LLM development, ranging from traditional RAG techniques to … progression from traditional RAG to graph-based RAG, and … hallucinations and factual inconsistencies, diminishing user …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shojaee_federated_2025,
  title = {Federated retrieval augmented generation for multi-product question answering},
  author = {Shojaee, P. and Harsha, S. S. and Luo, D. and Maharaj, A. and Yu, T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.14998},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG-QA approaches either query all domains indiscriminately, increasing computational costs and LLM hallucinations, … significantly boosts multi-product RAG-QA performance in terms …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mukherjee_documents_2025,
  title = {From {Documents},
  author = {Mukherjee, M. and Kim, S. and Chen, X. and Luo, D. and Yu, T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.15237},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The confidence score allows us to filter the KG based on its … for confidence score for constructing the KG for the KG-RAG … our KG-RAG based retriever can be integrated with an LLM to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{su_fast_2025,
  title = {Fast or better? balancing accuracy and cost in retrieval-augmented generation with flexible user control},
  author = {Su, J. and Healey, J. and Nakov, P. and Cardie, C.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.12145},
  journal = {arXiv preprint arXiv:2502.12145},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations … However, existing RAG frameworks often apply …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gutierrez_rag_2025,
  title = {From rag to memory: {Non},
  author = {Gutiérrez, B. J. and Shu, Y. and Qi, W. and Zhou, S. and Su, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.14802},
  journal = {arXiv preprint arXiv:2502.14802},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… continual learning solution for production LLM systems. However, … Several RAG frameworks that engage an LLM to explicitly … To evaluate how well RAG systems retain factual memory …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ferrag_llm_2025,
  title = {From llm reasoning to autonomous ai agents: {A},
  author = {Ferrag, M. A. and Tihanyi, N. and Debbah, M.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.19678},
  journal = {arXiv preprint arXiv:2504.19678},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… and hallucinated responses [7], [8], a limitation that RetrievalAugmented Generation (RAG) … employing reflection, planning, and multi-agent collaboration has given rise to Agentic RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_financial_2025,
  title = {Financial analysis: {Intelligent},
  author = {Wang, J. and Ding, W. and Zhu, X.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.06279},
  journal = {arXiv preprint arXiv:2504.06279},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Our findings validate the effectiveness of integrating RAG technology with LLMs for financial analysis tasks and provide valuable insights for future developments in intelligent financial …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang__2025,
  title = {From" {Hallucination},
  author = {Wang, Q.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.14392},
  journal = {arXiv preprint arXiv:2503.14392},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the Anchor-RAG framework to mitigate hallucinations. Unlike … analyze the root causes of hallucinations in LLMs. Based on … in reducing hallucinations, enhancing LLM performance, and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hyk_queries_2025,
  title = {From queries to criteria: {Understanding},
  author = {Hyk, A. and McCormick, K. and Zhong, M. and Ciucă, I. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2507.15715},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… : an LLM-powered retrieval-augmented generation bot for … we deploy is a retrieval augmented generation (RAG) LLM: a … passed to an LLM to provide a response with citations to the user…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{ongris_frog_2025,
  title = {{FrOG},
  author = {Ongris, Jaycent Gunawan and Tjitrahardja, Eduardus and Darari, Fariz and Ekaputra, Fajar J.},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017116638&partnerID=40&md5=fdae2ea8884ef0e03c866796d59acbc9},
  booktitle = {{CEUR},
  volume = {4020},
  pages = {116 -- 134},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{ming_faitheval_2025,
  title = {{FAITHEVAL},
  author = {Ming, Yifei and Purushwalkam, Senthil and Pandit, Shrey and Ke, Zixuan and Nguyen, Xuan Phi and Xiong, Caiming and Joty, Shafiq Rayhan},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010276740&partnerID=40&md5=3324b70f735fc314ed436005631cb179},
  pages = {85296 -- 85322},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@article{chen_llms_2024,
  title = {Llms are biased evaluators but not biased for retrieval augmented generation},
  author = {Chen, Y. S. and Jin, J. and Kuo, P. T. and Huang, C. W. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.20833},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… selfpreference effect in RAG frameworks. Instead, we observe that factual accuracy significantly … Our work extends the exploration of LLM’s self-preference to the RAG framework and …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_longrag_2024,
  title = {Longrag: {A},
  author = {Zhao, Q. and Wang, R. and Cen, Y. and Zha, D. and Tan, S. and Dong, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.18050},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hu_lrp4rag_2024,
  title = {Lrp4rag: {Detecting},
  author = {Hu, H. and He, C. and Xie, X. and Zhang, Q.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.15533},
  journal = {arXiv preprint arXiv:2408.15533},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, during the RAG process, we find that the LLM’s internal thoughts diverge significantly from the correct answer, leading to failure in answering "Who did John Evelyn support …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pipitone_legalbench-rag_2024,
  title = {Legalbench-rag: {A},
  author = {Pipitone, N. and Alami, G. H.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.10343},
  journal = {arXiv preprint arXiv:2408.10343},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the context window of an LLM, which poses a significant risk … of hallucinated content at the generation step of RAG systems… phase of the RAG pipeline, assessing how well the LLM can …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chiang_llamp_2024,
  title = {{LLaMP},
  author = {Chiang, Y. and Hsieh, E. and Chou, C. H. and Riebesell, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2401.17244},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Our result indicates that without RAG, vanilla LLMs suffer from hallucinations and misclassify the magnetic orderings of materials. LLaMP with GPT-4 as backend can counteract the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tao_llm-r_2024,
  title = {{LLM},
  author = {Tao, L. and Huang, Q. and Wu, X. and Zhang, W. and Wu, Y. and Li, B. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.04476},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… for fine-tuning the LLM. This method … RetrievalAugmented Generation (RAG) technologies are adopted to optimize the generation steps and mitigate the phenomenon of hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{comendant_large_2024,
  title = {Large language model-based sport coaching system using retrieval-augmented generation and user models},
  author = {Comendant, C.},
  year = {2024},
  publisher = {University of Twente},
  note = {Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yu_large-language_2024,
  title = {Large-language models: {The},
  author = {Yu, S. and Ran, N. and Liu, J.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S2949747724000344},
  journal = {Artificial Intelligence Chemistry},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {Large Language Models (LLMs), such as GPT-4, are precipitating a new "industrial revolution" by significantly enhancing productivity across various domains. These models encode an …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_llms_2024,
  title = {Llms know what they need: {Leveraging},
  author = {Wang, K. and Duan, F. and Li, P. and Wang, S. and Cai, X.},
  year = {2024},
  url = {https://arxiv.org/abs/2404.14043},
  journal = {arXiv preprint arXiv:2404.14043},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination … fully utilize the LLM’s parametric knowledge, we prompt the LLM to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jacobs_leveraging_2024-1,
  title = {Leveraging lecture content for improved feedback: {Explorations},
  author = {Jacobs, S. and Jaschke, S.},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10663001/},
  journal = {2024 36th International Conference on …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… to the Large Language Model GPT-4 as external knowledge source together with timestamps as metainformation by using RAG. The purpose of this is to prevent hallucinations and to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{belyi_luna_2024,
  title = {Luna: an evaluation foundation model to catch language model hallucinations with high accuracy and low cost},
  author = {Belyi, M. and Friel, R. and Shao, S. and Sanyal, A.},
  year = {2024},
  url = {https://arxiv.org/abs/2406.00975},
  journal = {arXiv preprint arXiv:2406.00975},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of RAG in production settings, we identify long-context RAG … precision long-context RAG hallucination detection. Through extensive … We find that LLM responses often contain transition …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cederlund_llmrag_2024,
  title = {Llmrag: {An},
  author = {Cederlund, O. and Alawadi, S. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10710181/},
  journal = {2024 9th International …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… The introduction of an LLM agent as a digital co-worker to IT technicians in support … as Retrieval-Augmented Generation (RAG), the LLM is equipped to avoid generating hallucinations […},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fuad_llm-ref_2024,
  title = {Llm-ref: {Enhancing},
  author = {Fuad, K. A. A. and Chen, L.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.00294},
  journal = {arXiv preprint arXiv:2411.00294},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of our tool over existing RAG-based systems. The proposed LLM-Ref demonstrates significant per… Despite their popularity, RAG-based systems fall short in offering citations. While …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sriramanan_llm-check_2024,
  title = {Llm-check: {Investigating},
  author = {Sriramanan, G. and Bharti, S. and Sadasivan, V. S. and {...},
  year = {2024},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/3c1e1fdf305195cd620c118aaa9717ad-Abstract-Conference.html},
  journal = {Advances in …},
  note = {Publisher: proceedings.neurips.cc},
  keywords = {source: Google Scholar},
  abstract = {… detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ravi_lynx_2024,
  title = {Lynx: {An},
  author = {Ravi, S. S. and Mielczarek, B. and Kannappan, A. and Kiela, D. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2407.08488},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… detection LLM that is capable of advanced reasoning on challenging real-world hallucination … , our work focuses on the problem of hallucination detection as it applies to RAG settings. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{benzinho_llm_2024,
  title = {{LLM},
  author = {Benzinho, J. and Ferreira, J. and Batista, J. and Pereira, L. and {...},
  year = {2024},
  url = {https://www.mdpi.com/2076-3417/14/19/8856},
  journal = {applied sciences},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… RAG provides models with sources of information that can be cited and consulted by the user, thus enhancing confidence … agent powered by RAG-based LLM technology, we can …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dejean_let_2024,
  title = {Let your {LLM},
  author = {Déjean, H.},
  year = {2024},
  url = {https://arxiv.org/abs/2412.11536},
  journal = {arXiv preprint arXiv:2412.11536},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , indicating that the LLM is confident in answering questions without resorting to RAG. TriviaQA is … efficient to halt the LLM as early as possible and then assess whether RAG activation is …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{masoudifard_leveraging_2024,
  title = {Leveraging graph-rag and prompt engineering to enhance llm-based automated requirement traceability and compliance checks},
  author = {Masoudifard, A. and Sorond, M. M. and Madadi, M. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.08593},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Moreover, hallucination, where models generate factually incorrect yet plausible content, … Graph-RAG to retrieve the most relevant content from reference texts. Graph-RAG enhances …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{abshari_llm-assisted_2024,
  title = {Llm-assisted physical invariant extraction for cyber-physical systems anomaly detection},
  author = {Abshari, D. and Fu, C. and Sridhar, M.},
  year = {2024},
  url = {https://www.researchgate.net/profile/Danial-Abshari/publication/385920298_LLM-assisted_Physical_Invariant_Extraction_for_Cyber-Physical_Systems_Anomaly_Detection/links/679c8c94207c0c20fa6b0b57/LLM-assisted-Physical-Invariant-Extraction-for-Cyber-Physical-Systems-Anomaly-Detection.pdf},
  journal = {arXiv preprint arXiv:2411.10918},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… To overcome the challenges of the unstructured documents format and LLM hallucination, we propose the enhancement of multi-modal RAG and dedicated chain-of-thought prompt …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{park_literature_2024,
  title = {Literature review of {AI},
  author = {Park, D. M. and Lee, H. J.},
  year = {2024},
  journal = {Informatization Policy},
  note = {Publisher: National Information Society Agency
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{emonet_llm-based_2024,
  title = {Llm-based sparql query generation from natural language over federated knowledge graphs},
  author = {Emonet, V. and Bolleman, J. and Duvaud, S. and Farias, TM de and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.06062},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We introduce a Retrieval-Augmented Generation (RAG) system for translating user … To enhance accuracy and reduce hallucinations in query generation, our system utilises metadata …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liang_learning_2024,
  title = {Learning to trust your feelings: {Leveraging},
  author = {Liang, Y. and Song, Z. and Wang, H. and Zhang, J.},
  year = {2024},
  url = {https://arxiv.org/abs/2401.15449},
  journal = {arXiv preprint arXiv:2401.15449},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of fact-conflicting hallucination, where LLM produces fluent … hallucination mitigation methods, such as retrieval augmentation generation (RAG), address fact-conflict hallucination of LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{nikolakopoulos_large_2024,
  title = {Large language models in modern forensic investigations: {Harnessing},
  author = {Nikolakopoulos, A. and Evangelatos, S. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10654427/},
  journal = {… \& Education (EEITE)},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… of a Retrieval Augmented Generation (RAG) LLM, trained with … Suspect recommendation with confidence level estimation: … confidence level, feeding them to the RAG LLM for the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hashemi_llm-rubric_2024,
  title = {{LLM},
  author = {Hashemi, H. and Eisner, J. and Rosset, C. and Durme, B. Van and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2501.00274},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… But can LLM evaluation be trusted? It solves the time, scaling, and … LLM but by a real human. The assistant in these three systems may be summarized as “no RAG” (DS1), “oracle RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chen_leverage_2024,
  title = {Leverage knowledge graph and large language model for law article recommendation: {A},
  author = {Chen, Y. and Chen, M. and Zhu, Y. and Pei, J. and Chen, S. and Zhou, Y. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2410.04949},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… TFIDF-RAG, LightRAG, Graph-RAG method. This indicates that the proposed approach effectively mitigates the hallucinations in LLMs. We believe this is because other RAG method …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_llm_2024-1,
  title = {Llm inference serving: {Survey},
  author = {Li, B. and Jiang, Y. and Gadepally, V. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10938426/},
  journal = {2024 IEEE High …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM serving include retrieval-augmented generation (RAG) and mixtureof-experts (MoE) inference. RAG … tendency to generate inaccurate or fabricated information (hallucinations) [45]. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jho_leveraging_2024,
  title = {Leveraging generative {AI},
  author = {Jho, H.},
  year = {2024},
  url = {https://www.researchgate.net/profile/Hunkoog-Jho/publication/383561189_Leveraging_Generative_AI_in_Physics_Education_Addressing_Hallucination_Issues_in_Large_Language_Models/links/6819b0a0d1054b0207ea3d26/Leveraging-Generative-AI-in-Physics-Education-Addressing-Hallucination-Issues-in-Large-Language-Models.pdf},
  journal = {New Phys},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… , reasoning, iterative querying, and Retrieval-Augmented Generation (RAG). These methods aim … Comparison of answers about the same question generated by generic LLM and RAG. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{_llm_2024,
  title = {{LLM},
  author = {{강태욱},
  year = {2024},
  url = {https://scholar.kyobobook.co.kr/article/detail/4010069980447},
  journal = {KIBIM Magazine},
  note = {Publisher: scholar.kyobobook.co.kr},
  keywords = {source: Google Scholar},
  abstract = {… processing, and hallucination problems must be solved. This study proposes an LLM-based … This study focuses on the RAG (Retrieval- Augmented Generation) document generation …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{xie_leveraging_2024,
  title = {Leveraging {Grounded},
  author = {Xie, Eric and Xiong, Guangzhi and Yang, Haolin and Coleman, Olivia Fudge and Kennedy, Michael J. and Zhang, Aidong},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219553031&partnerID=40&md5=ace3fc343db7dbd2e2ec0d9158b48ff7},
  booktitle = {Proceedings of {Machine},
  volume = {264},
  pages = {207 -- 220},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 1},
}

@inproceedings{zemicheal_llm_2024,
  title = {{LLM},
  author = {ZeMicheal, Tadesse and Chen, Hsin and Davis, Shawn and Allen, Rachel and Demoret, Michael and Song, Ashley},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218407315&partnerID=40&md5=3b6b5ffd7ae35fa504261cce4bba00a9},
  booktitle = {{CEUR},
  volume = {3920},
  pages = {161 -- 173},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{thakur_nomiracl_2023,
  title = {Nomiracl: {Knowing},
  author = {Thakur, N. and Bonifacio, L. and Zhang, X. and Ogundepo, O. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2312.11361},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM hallucinations against first-stage retrieval errors in RAG. (… lenges in LLM robustness by often hallucinating an answer … on LLM’s generation results, and find several hallucination …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_knowledge_2024,
  title = {Knowledge graph integration and self-verification for comprehensive retrieval-augmented generation},
  author = {Wu, C. and Shen, T. and Yan, R. and Wang, H. and Liu, Z. and {...},
  year = {2024},
  url = {https://openreview.net/forum?id=457wTt0ngj},
  journal = {… Retrieval Augmented …},
  note = {Publisher: openreview.net},
  keywords = {source: Google Scholar},
  abstract = {… contextual understanding and reduce hallucinations on RAG. LLM’s advanced capabilities … advanced capabilities of LLM to enhance the robustness and credibility of our information …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yang_knowledge_2024,
  title = {Knowledge {Graph},
  author = {Yang, C. and Xu, R. and Luo, L. and Pan, S.},
  year = {2024},
  url = {https://www.cs.emory.edu/~jyang71/files/klc.pdf},
  journal = {IEEE Data Engineering Bulletin},
  note = {Publisher: cs.emory.edu
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… of KG and LLM co-learning, especially through a structure-oriented retrieval augmented generation (… facts from KGs during the LLM reasoning process, which are used to mitigate factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sanmartin_kg-rag_2024,
  title = {Kg-rag: {Bridging},
  author = {Sanmartin, D.},
  year = {2024},
  url = {https://arxiv.org/abs/2405.12035},
  journal = {arXiv preprint arXiv:2405.12035},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… employs an LLM configured with a standard RAG prompt as … -RAG approach against vector RAG and no RAG specifically, … to quantify the incidence of hallucinations. These metrics allow …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lima_know_2024,
  title = {Know {Your},
  author = {Lima, RT De and Gupta, S. and Berrospi, C. and Mishra, L. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2411.19710},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… ) have been developed for automated LLMassisted evaluation of RAG systems. While these … probability of an LLM hallucination grows with each query. These hallucinations can lead to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{radhakrishnan_knowing_2024,
  title = {Knowing {When},
  author = {Radhakrishnan, P. and Chen, J. and Xu, B. and Ramaswami, P. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2409.13741},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… For RAG, we measure the fraction of LLM-generated statistical claims that are accurate (ie, not hallucinated). This means we evaluate the generated statistical value against the table …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xiao_krail_2024,
  title = {Krail: {A},
  author = {Xiao, X. and Chen, P. and Qi, B. and Zhao, H. and Liang, J. and Tong, J. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.18627},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM-based two-stage framework: We propose a novel LLM- … of LLM factual accuracy with RAG to mitigate hallucinations, … Inspired by these advancements, we aim to leverage RAG to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{febrian_kemenkeugpt_2024,
  title = {{KemenkeuGPT},
  author = {Febrian, G. F. and Figueredo, G.},
  year = {2024},
  url = {https://arxiv.org/abs/2407.21459},
  journal = {arXiv preprint arXiv:2407.21459},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM … ChatGPT and LLaMA, this approach achieves a 15\% to 48\% performance gain in accuracy and F1 scores [Zhang et al. 2023]. RAG addresses the issue of factual accuracy in LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{noauthor_knowllm_2024,
  title = {{KnowLLM},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204904251&partnerID=40&md5=dfebedd156b2fd9e9827b3c0f7da4388},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_knowledgenlp_2024,
  title = {{KnowledgeNLP},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204892873&partnerID=40&md5=f25eb3d69cd9ba28a97ae467c5286220},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{xu_knowledge_2024,
  title = {Knowledge {Graph},
  author = {Xu, Zihao and Shen, Zhejun and Zhou, Qunzhi and Ristoski, Petar},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002727392&partnerID=40&md5=c8ab1c010fa6698e51f3a777260abcd6},
  booktitle = {{CEUR},
  volume = {3950},
  pages = {31 -- 39},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zhang_knowgpt_2024,
  title = {{KnowGPT},
  author = {Zhang, Qinggang and Dong, Junnan and Chen, Hao and Zha, Daochen and Yu, Zailiang and Huang, Xiao},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000537404&partnerID=40&md5=1fccdd8217ca84a55e4d4f0ec8d42ff8},
  booktitle = {Advances in {Neural},
  volume = {37},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 5},
}

@article{bansal_understanding_2025,
  title = {Understanding and {Mitigating},
  author = {Bansal, R. and Chandra, R. and Lulla, K.},
  year = {2025},
  url = {https://www.researchgate.net/profile/Rishab-Bansal-5/publication/392513063_Understanding_and_Mitigating_Strategies_for_Large_Language_Model_LLMs_Hallucinations_in_HR_Chatbots/links/68bdfa68c76fc271eb321a42/Understanding-and-Mitigating-Strategies-for-Large-Language-Model-LLMs-Hallucinations-in-HR-Chatbots.pdf},
  journal = {International Journal of …},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… [19] Under a RAG system, the LLM is expected to base its response on relevant material obtained from a trustworthy knowledge source (such as a corporate HR policy database or …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{budakoglu_unveiling_2025,
  title = {Unveiling the {Power},
  author = {Budakoglu, G. and Emekci, H.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10887212/},
  journal = {IEEE Access},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… The present work has shown that fine-tuned LLM integration in RAG pipelines is not without its … emphasis needs to be given to semantic and factual integrity in the generated text. Our …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gao_u-niah_2025,
  title = {U-niah: {Unified},
  author = {Gao, Y. and Xiong, Y. and Wu, W. and Huang, Z. and Li, B. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.00353},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We identify typical error patterns including omission due to noise, hallucination under high noise critical condition, and self-doubt behaviors. Our work not only highlights the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ma_unifying_2025,
  title = {Unifying {Large},
  author = {Ma, C. and Chen, Y. and Wu, T. and Khan, A. and Wang, H.},
  year = {2025},
  url = {https://www.openproceedings.org/2025/conf/edbt/paper-T4.pdf},
  journal = {EDBT},
  note = {Publisher: openproceedings.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… The Graph RAG enhances the explainability of LLM responses by tracing relevant subgraphs … Mitigating large language model hallucinations via autonomous knowledge graph-based …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{rajendran_uppgiftsanpassning_2025,
  title = {Uppgiftsanpassning av {LLM},
  author = {Rajendran, A. Asalatha and Saji, B. Annamma},
  year = {2025},
  url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2:1969027},
  publisher = {diva-portal.org},
  keywords = {source: Google Scholar},
  abstract = {… introduces a hybrid approach that combines LoRA and RAG… patterns efficiently, while RAG injects relevant external knowledge … of LoRA and RAG based on the model's confidence. This …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_using_2025,
  title = {Using {Large},
  author = {Wang, Chenkai and Ke, Chengrong and Huang, Ming Siang and Chong, Inn Wen and Yang, Yihsin and Tseng, S. Vincent Shin Mu and Dai, Hong Jie},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212611510&partnerID=40&md5=59439a85628c0fdd94322a4ad548c719},
  journal = {Pacific Symposium on Biocomputing},
  volume = {30},
  pages = {121 -- 137},
  note = {Type: Article},
  keywords = {source: Scopus},
  annote = {Cited by: 3},
}

@inproceedings{wright_using_2025,
  title = {Using {Digital},
  author = {Wright, Brian and Guruvayur, Vishwanath and Napolitano, Luke and Ozar, Doruk and Rivera, Ali and Sai, Ananya and Tafesse, Bereket},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013474172&partnerID=40&md5=7d42e3a5c9b4a3443cb0133e10be7f17},
  booktitle = {{CEUR},
  volume = {4010},
  pages = {59 -- 70},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{xu_using_2025,
  title = {Using {Clinical},
  author = {Xu, Xingru and Dumontier, Michel J. and Sun, Chang},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012093656&partnerID=40&md5=9413007753d2abbc6649416d698bf552},
  booktitle = {{CEUR},
  volume = {4001},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{meng_kerag_r_2025,
  title = {{KERAG},
  author = {Meng, Z. and Yi, Z. and Ounis, I.},
  year = {2025},
  url = {https://arxiv.org/abs/2507.05863},
  journal = {arXiv preprint arXiv:2507.05863},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… to inaccuracies or hallucinations, … RAG by pre-training a graph attention network (GAT) to select the most relevant triple for the target users for the used LLM, thereby enhancing the LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_knowledge_2025,
  title = {Knowledge graph retrieval-augmented generation for llm-based recommendation},
  author = {Wang, S. and Fan, W. and Feng, Y. and Lin, S. and Ma, X. and Wang, S. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02226},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… advancements, LLM-… LLM backbones, particularly issues of hallucinations and the lack of up-todate and domain-specific knowledge. Recently, Retrieval-Augmented Generation (RAG) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ma_knowledge_2025,
  title = {Knowledge graph-based retrieval-augmented generation for schema matching},
  author = {Ma, C. and Chakrabarti, S. and Khan, A. and Molnár, B.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.08686},
  journal = {arXiv preprint arXiv:2501.08686},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To highlight the hallucinations mitigation of our KGRAG4SM, we ask the LLM to return the results for the given schema-matching questions and provide explanations. Case Verification. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jiang_knowledge_2025,
  title = {Knowledge assimilation: {Implementing},
  author = {Jiang, J. and Yan, L. and Liu, H. and Xia, Z. and Wang, H. and Yang, Y. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705125002448},
  journal = {Knowledge-based …},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… a novel Knowledge-guided Agriculture LLM (KALLM) designed … At the sentence level, we introduce a self-reflective RAG … LLMs and the current SFT-RAG pipeline show that our KALLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{hasan_llm_2025,
  title = {{LLM},
  author = {Hasan, S. and Rezai, A.},
  year = {2025},
  url = {https://www.diva-portal.org/smash/record.jsf?pid=diva2:1968861},
  publisher = {diva-portal.org},
  keywords = {source: Google Scholar},
  abstract = {… this study is to compare factual accuracy, retrieval quality and hyperparameter sensitivity of a RAG system built on a relatively smaller LLM, against a state-of-the-art large LLM on open-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{dong_leveraging_2025,
  title = {Leveraging {LLM},
  author = {Dong, G. and Li, X. and Zhang, Y. and Deng, M.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.21384},
  journal = {arXiv preprint arXiv:2506.21384},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , factuality, and relevance of generated text. Recent efforts [5, 7, 23, 24] have leveraged RAG to … better alignment with the information needs of the LLM. Additionally, some studies [2, 43] …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kovacs_lettucedetect_2025,
  title = {Lettucedetect: {A},
  author = {Kovács, Á and Recski, G.},
  year = {2025},
  url = {https://arxiv.org/abs/2502.17125},
  journal = {arXiv preprint arXiv:2502.17125},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… both hallucinations and coverage errors in LLM-generated responses. Fine-tuned LLM … note that we were unable to compare our results with RAG-HAT on this task because they did not …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{sobhan_llm-assisted_2025,
  title = {{LLM},
  author = {Sobhan, S. and Haque, M. A.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.23136},
  journal = {arXiv preprint arXiv:2506.23136},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… On the other hand, the typical RAG pipeline suffered from hallucination because the question was about transformer, and also the given document was about transformer, even though it …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{li_lokis_2025,
  title = {Loki's {Dance},
  author = {Li, C. and Wang, P. and Wang, C. and Zhang, L. and Liu, Z. and Ye, Q. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2507.02870},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… First, we establish the first unified theoretical framework for LLM hallucinations, formally re… In conclusion, RAG constitutes a fundamental approach to augmenting the capabilities of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{erickson_llm_2025,
  title = {{LLM},
  author = {Erickson, J. S. and Santos, H. and Pinheiro, V. and Mccusker, J. P. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826824000398},
  journal = {Journal of Web …},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… hallucinations and hence provides more insight into where in the LLM’s response a hallucination … , verifying, and analyzing LLM responses using RAG systems and Knowledge Graphs (…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{smajic_large_2025-1,
  title = {Large language models for structured and semi-structured data, recommender systems and knowledge base engineering: a survey of recent techniques and …},
  author = {Smajić, A. and Karlović, R. and Dasko, M. Bobanović and Lorencin, I.},
  year = {2025},
  url = {https://www.mdpi.com/2079-9292/14/15/3153},
  journal = {Electronics},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Although this review primarily focuses on the technical and application aspects of LLM-… like RAG and KG enhancement significantly improve factual accuracy and reduce hallucinations. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bezerra_llmquoter_2025,
  title = {Llmquoter: enhancing rag capabilities through efficient quote extraction from large contexts},
  author = {Bezerra, Y. F. and Weigang, L.},
  year = {2025},
  url = {https://arxiv.org/abs/2501.05554},
  journal = {arXiv preprint arXiv:2501.05554},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The applications of LLM distillation are diverse, ranging from mitigating hallucinations to … To achieve this, we employ a distillation process in which a large LLM generates high-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wahidur_legal_2025-1,
  title = {Legal query rag},
  author = {Wahidur, R. S. M. and Kim, S. and Choi, H. and Bhatti, D. S. and Lee, H. N.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10887211/},
  journal = {IEEE Access},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG aids in reducing hallucinations and facilitates continuous knowledge updates and … embedding LLM and the generative LLM. In contrast, the RAG Layer integrates advanced RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{galat_llm_2025,
  title = {{LLM},
  author = {Galat, D. and Molla-Aliod, D.},
  year = {2025},
  url = {https://arxiv.org/abs/2509.08596},
  journal = {arXiv preprint arXiv:2509.08596},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… through RAG consistently improves answer quality and factual accuracy compared to LLM … terms LLM or Large Language Model in their title, and 3 additional papers used the terms …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{belyi_luna_2025,
  title = {Luna: {A},
  author = {Belyi, M. and Friel, R. and Shao, S. and Sanyal, A.},
  year = {2025},
  url = {https://aclanthology.org/2025.coling-industry.34/},
  journal = {Proceedings of the 31st …},
  note = {Publisher: aclanthology.org},
  keywords = {source: Google Scholar},
  abstract = {… of RAG in production settings, we identify long-context RAG … high precision long-context RAG hallucination detection. Through … 2023) LLM judges leverage LLM’s inherent reasoning …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{ning_less_2025,
  title = {Less {LLM},
  author = {Ning, J. and Kong, Y. and Long, Y. and Callan, J.},
  year = {2025},
  url = {https://arxiv.org/abs/2510.02657},
  journal = {arXiv preprint arXiv:2510.02657},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {Retrieval-Augmented Generation (RAG) couples document … that corpus scaling consistently strengthens RAG and can often … stronger RAG, often comparable to enlarging the LLM itself. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zhao_lmar_2025,
  title = {Lmar: {Language},
  author = {Zhao, Y. and Ding, Y. and Zhang, Z. and Yao, D. and Xu, Y.},
  year = {2025},
  url = {https://arxiv.org/abs/2508.05672},
  journal = {arXiv preprint arXiv:2508.05672},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… sources, RAG substantially mitigates hallucination and … LLM-augmented text clustering mechanism that distills the contextual understanding and discrimination capabilities of the LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{iranmanesh_llm-assisted_2025,
  title = {{LLM},
  author = {Iranmanesh, S. and Saadany, H. and Vakaj, E.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.16813},
  journal = {arXiv preprint arXiv:2504.16813},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{fayyazi_llm_2025,
  title = {{LLM},
  author = {Fayyazi, R. and Zuzak, M. and Yang, S. J.},
  year = {2025},
  url = {https://arxiv.org/abs/2506.12100},
  journal = {arXiv preprint arXiv:2506.12100},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This analysis reveals how RAG affects the model’s confidence in generating specific tokens and provides insight into the contextual dependencies introduced by the retrieved …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{savage_large_2025,
  title = {Large language model uncertainty proxies: discrimination and calibration for medical diagnosis and treatment},
  author = {Savage, T. and Wang, J. and Gallo, R. and Boukil, A. and {...},
  year = {2025},
  url = {https://academic.oup.com/jamia/article-abstract/32/1/139/7819854},
  journal = {Journal of the …},
  note = {Publisher: academic.oup.com},
  keywords = {source: Google Scholar},
  abstract = {… context provided by RAG can errantly lead the … LLM uncertainty are important for building medical LLM-RAG systems. In this study we evaluate 3 proxies of LLM uncertainty: confidence …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_leveraging_2025,
  title = {Leveraging {FDA},
  author = {Wu, L. and Fang, H. and Qu, Y. and Xu, J. and Tong, W.},
  year = {2025},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC12098182/},
  journal = {Drug Safety},
  note = {Publisher: pmc.ncbi.nlm.nih.gov
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… will not risk a hallucination in the results… RAG, which restricted the reference used in the LLM inference solely to the specific labeling documents instead of using the full scope of the LLM’…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kalyuzhnaya_llm_2025,
  title = {{LLM},
  author = {Kalyuzhnaya, A. and Mityagin, S. and Lutsenko, E. and {...},
  year = {2025},
  url = {https://search.ebscohost.com/login.aspx?direct=true\&profile=ehost\&scope=site\&authtype=crawler\&jrnl=26246511\&AN=183336541\&h=0pICgql%2Bey7ZL1YAQE4iib5dbwaBiWjyLc6tqZE3sd6vLsiLGJ4U0smDycxB2boqa%2FC0RX3jT2MvCufSaSMFnw%3D%3D\&crl=c},
  journal = {… Cities (2624-6511 …},
  note = {Publisher: search.ebscohost.com},
  keywords = {source: Google Scholar},
  abstract = {… Effectiveness In this subsection, we discuss the results of using RAG to answer questions. First, … Unlike LLMs generating answers without context, the LLM-based system relies on factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mukherjee_llm-driven_2025,
  title = {{LLM},
  author = {Mukherjee, K. and Kantarcioglu, M.},
  year = {2025},
  url = {https://arxiv.org/abs/2508.21323},
  journal = {arXiv preprint arXiv:2508.21323},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We highlight how LLM and RAG introduce new opportunities and challenges in provenance-… A recurring risk with LLM-driven systems is hallucination when generated outputs diverge …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{aratchige_llms_2025,
  title = {Llms working in harmony: {A},
  author = {Aratchige, R. M. and Ilmini, W.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.01963},
  journal = {arXiv preprint arXiv:2504.01963},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To tackle the issue of LLM hallucinations, the authors implement a code… RAG’s impact goes beyond immediate performance gains; it laid the groundwork for future developments in LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_large_2025-1,
  title = {Large language model as a catalyst: {A},
  author = {Wang, Y. and Afzal, M. M. and Li, Z. and Zhou, J. and Feng, C. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10915543/},
  journal = {IEEE Transactions …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… , our framework integrates retrieval-augmented generation (RAG), … to explore the use of LLM and RAG to solve the BSS problem… LLM interaction can mitigate the impact of hallucinations, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{paul_llm-assisted_2025,
  title = {{LLM},
  author = {Paul, S. and Alemi, F. and Macwan, R.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.00428},
  journal = {arXiv preprint arXiv:2504.00428},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… reduces hallucinations and enhances factual accuracy [5], [41]. Additionally, RAG allows … This review of the literature evaluates the leading technologies in LLM and RAG techniques …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{qian_large_2025,
  title = {Large language model-empowered paradigm for automated geotechnical site planning and geological characterization},
  author = {Qian, Z. and Shi, C.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0926580525001438},
  journal = {Automation in Construction},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… LLM-based agent named “Geologist” to streamline geotechnical site planning and subsequent geological interpretation. A Multihop-Retrieval-Augmented Generation … the proposed LLM-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{wang_llms_2025,
  title = {{LLMs},
  author = {Wang, Keheng and Duan, Feiyu and Li, Peiguang and Wang, Sirui and Cai, Xunliang},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218420242&partnerID=40&md5=d29b223c39c202b3a323bd2681bf7bdf},
  booktitle = {Proceedings - {International},
  pages = {2379 -- 2400},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 2},
}

@inproceedings{emonet_llm-based_2025,
  title = {{LLM},
  author = {Emonet, Vincent and Bolleman, Jerven Tjalling and Duvaud, Séverine and de Farias, Tarcisio Mendes and Sima, Ana Claudia},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003725579&partnerID=40&md5=b4f921224527cdda4878a73c92235b62},
  booktitle = {{CEUR},
  volume = {3953},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{li_traq_2023,
  title = {Traq: {Trustworthy},
  author = {Li, S. and Park, S. and Lee, I. and Bastani, O.},
  year = {2023},
  url = {https://arxiv.org/abs/2307.04642},
  journal = {arXiv preprint arXiv:2307.04642},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG reduces hallucinations by retrieving passages from a knowledge base such as Wikipedia and then using an LLM to … rates of retriever and LLM sets (named Ret and LLM), and with …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_detecting_2025-1,
  title = {Detecting emergencies in patient portal messages using large language models and knowledge graph-based retrieval-augmented generation},
  author = {Liu, S. and Wright, A. P. and McCoy, A. B. and Huang, S. S. and {...},
  year = {2025},
  journal = {Journal of the …},
  note = {Publisher: Oxford Academic
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xiong_dr-rag_2025,
  title = {{DR},
  author = {Xiong, X. and Cai, H. and Yu, H. and Shen, B. and Hu, P.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1474034625005816},
  journal = {Advanced Engineering Informatics},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… data using a hybrid R2D-LLM approach. We create a rule … a retrieval-augmented generation pipeline with a binary classifier guides rule selection and prompt templates enhance LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{pyae_developing_2025,
  title = {Developing a {RAG},
  author = {Pyae, M. S. and Phyo, S. S. and Kyaw, STMM and Lin, T. S. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10961967/},
  journal = {… on Digital Arts …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… retrieves and generates sequentially, while Advanced RAG ensures factual accuracy and … SouLLMate, an adaptive LLM-driven system that leverages advanced technologies like RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cremaschi_decoding_2025,
  title = {Decoding the mind: {A},
  author = {Cremaschi, M. and Ditolve, D. and Curcio, C. and Panzeri, A. and {...},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417425008139},
  journal = {Expert Systems with …},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… LLMind Chat leverages a Retrieval Augmented Generation (RAG) model based on the Gemma 2 (27B parameters), specifically adapted to the context of the ICD-11. This RAG model …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xu_does_2025,
  title = {Does context matter? contextualjudgebench for evaluating llm-based judges in contextual settings},
  author = {Xu, A. and Bansal, S. and Ming, Y. and Yavuz, S. and Joty, S.},
  year = {2025},
  url = {https://arxiv.org/abs/2503.15620},
  journal = {arXiv preprint arXiv:2503.15620},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… survey specifically cite trust loss due to hallucinations as a top concern. Hallucinations are … For QA-Feedback, we use approach H to create preference pairs from RAG responses …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lee_performance_2024,
  title = {Performance comparison of retrieval-augmented generation and fine-tuned large language models for construction safety management knowledge retrieval},
  author = {Lee, J. and Ahn, S. and Kim, D. and Kim, D.},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S092658052400582X},
  journal = {Automation in Construction},
  note = {Publisher: Elsevier},
  keywords = {source: Google Scholar},
  abstract = {… fine-tuned LLM was fine-… RAG model improving by 21.5 \% and the fine-tuned LLM by 26 \%. The findings highlight the relative strengths and weaknesses of the RAG and fine-tuned LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kang_prompt-rag_2024-1,
  title = {Prompt-rag: {Pioneering},
  author = {Kang, B. and Kim, J. and Yun, T. R. and Kim, C. E.},
  year = {2024},
  url = {https://arxiv.org/abs/2401.11246},
  journal = {arXiv preprint arXiv:2401.11246},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… ChatGPT and conventional vector embedding-based RAG models. This study not only highlights the challenges of conventional RAG … thereby aiming to minimize hallucination. In cases …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{naik_probabilistic_2024,
  title = {Probabilistic consensus through ensemble validation: {A},
  author = {Naik, N.},
  year = {2024},
  url = {https://arxiv.org/abs/2411.06535},
  journal = {arXiv preprint arXiv:2411.06535},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Izacard \&Grave, 2021), aim to enhance factual accuracy by grounding LLM outputs in trusted … context, RAG systems have …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{baek_probing-rag_2024,
  title = {Probing-rag: {Self},
  author = {Baek, I. and Chang, H. and Kim, B. and Lee, J. and Lee, H.},
  year = {2024},
  url = {https://arxiv.org/abs/2410.13339},
  journal = {arXiv preprint arXiv:2410.13339},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… of Figure 1, Probing-RAG focuses on the hidden states of the LLM’s intermediate layers. … : external classifier based, LLM-based feedback, and confidence-based techniques. External …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wu_pa-rag_2024,
  title = {Pa-rag: {Rag},
  author = {Wu, J. and Cai, H. and Yan, L. and Sun, H. and Li, X. and Wang, S. and Yin, D. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.14510},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… When constructing the training data, we employ ChatGPT-3.5 (GPT-3.5-Turbo-1106) and introduce a citation rewrite mechanism to create near-perfect responses. An overview of the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bai_pistis-rag_2024,
  title = {Pistis-{RAG},
  author = {Bai, Y. and Miao, Y. and Chen, L. and Li, D. and Ren, Y. and Xie, H. and {...},
  year = {2024},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240700072B/abstract},
  journal = {arXiv e …},
  note = {Publisher: ui.adsabs.harvard.edu},
  keywords = {source: Google Scholar},
  abstract = {… , trust, and reliability, echoing the core principles of RAG in … RAG systems. It adheres to information retrieval principles while considering the unique business scenario captured by LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wen_perception_2024,
  title = {Perception of knowledge boundary for large language models through semi-open-ended question answering},
  author = {Wen, Z. and Tian, Z. and Jian, Z. and Huang, Z. and Ke, P. and {...},
  year = {2024},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/a1e0d6fa0c30b7d4f75dd9c7ed6189f2-Abstract-Conference.html},
  journal = {Advances in …},
  note = {Publisher: proceedings.neurips.cc},
  keywords = {source: Google Scholar},
  abstract = {… the results from the RAG-based evaluation and LLM self-… the knowledge boundary of the target LLM. Following our method, … that advanced LLM (ie GPT-4) is easy to hallucinate on semi-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{razafinirina_pedagogical_2024,
  title = {Pedagogical alignment of large language models (llm) for personalized learning: a survey, trends and challenges},
  author = {Razafinirina, M. A. and Dimbisoa, W. G. and {...},
  year = {2024},
  journal = {Journal of …},
  note = {Publisher: Scientific Research Publishing
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{maio_pirates_2024,
  title = {Pirates of the rag: {Adaptively},
  author = {Maio, C. Di and Cosci, C. and Maggini, M. and Poggioni, V. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.18295},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Given a pre-trained LLM, we describe a RAG system by an architectures composed of four … dentiality within the system, undermining its trustworthiness and security guarantees not only …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{garza_privcomp-kg_2024-1,
  title = {Privcomp-kg: {Leveraging},
  author = {Garza, L. and Elluri, L. and Kotal, A. and Piplai, A. and Gupta, D. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2404.19744},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… In our approach, we have utilized the power of RAG to limit the possibilities of hallucinations … of the retrieved article numbers by the LLM. As RAG is integral to the LLM, we assess the …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{lin_pe-gpt_2024,
  title = {{PE},
  author = {Lin, F. and Li, X. and Lei, W. and Rodriguez-Andina, J. J. and {...},
  year = {2024},
  url = {https://ieeexplore.ieee.org/abstract/document/10701612/},
  journal = {IEEE Transactions …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM tailored for PE design applications, named PE-GPT. The methodology involves enhancing PE-GPT with retrieval augmented generation … LLM,s hallucination by the PE-tailored RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{noauthor_proceedings_2024,
  title = {Proceedings of {ALTRUIST},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217185920&partnerID=40&md5=b2db96197f85c74983379a12427933bf},
  booktitle = {{CEUR},
  volume = {3906},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_proceedings_2024-1,
  title = {Proceedings - 2024 {International},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213952249&partnerID=40&md5=d66d551d5ec8c17f8e34e6a396ac21f6},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{zerhoudi_personarag_2024,
  title = {{PersonaRAG},
  author = {Zerhoudi, Saber and Granitzer, Michael},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207482267&partnerID=40&md5=00e17c80859e082ded877a219383d2f5},
  booktitle = {{CEUR},
  volume = {3784},
  pages = {1 -- 11},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{chu_patent_2024,
  title = {Patent {Response},
  author = {Chu, Jungmei and Lo, Haocheng and Hsiang, Jieh and Cho, Chunchieh},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204918162&partnerID=40&md5=0ee730886a4380a0c00ae2074002599c},
  pages = {146 -- 155},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 4},
}

@inproceedings{noauthor_proceedings_2024-3,
  title = {Proceedings of the 2024 {AAAI},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203829053&partnerID=40&md5=03c425dfd05b375a2763743c28f0c36b},
  booktitle = {Proceedings of {Machine},
  volume = {257},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{chandrasekhar_nanogpt_2025,
  title = {{NANOGPT},
  author = {Chandrasekhar, A. and Farimani, O. B. and Ajenifujah, O. T. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.20541},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… , RAG significantly reduces the likelihood of hallucinations and inaccuracies compared to raw LLM … The LLM-RAG system presented in this work utilizes the LLaMA3.1-8b-Instruct model, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{tufino_notebooklm_2025,
  title = {{NotebookLM},
  author = {Tufino, E.},
  year = {2025},
  url = {http://203.160.84.158:9988/papers/2504.09720v1.NotebookLM__An_LLM_with_RAG_for_active_learning_and_collaborative_tutoring.pdf},
  journal = {arXiv preprint arXiv:2504.09720},
  note = {Publisher: 203.160.84.158
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… on internal training data, a RAG-based system actively retrieves relevant documents to ground its responses in factual information. Remarkable examples of RAG-based applications in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{akarajaradwong_nitibench_2025,
  title = {Nitibench: {A},
  author = {Akarajaradwong, P. and Pothavorn, P. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.10868},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We evaluate retrieval-augmented generation (RAG) and long-context LLM-based … In order to control the quality of judge LLM when assessing coverage and citation score, we iteratively …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{christodoulou_nlpdame_2025,
  title = {{NLPDame},
  author = {Christodoulou, C.},
  year = {2025},
  url = {https://ceur-ws.org/Vol-4038/paper_147.pdf},
  journal = {Working Notes of CLEF},
  note = {Publisher: ceur-ws.org
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… RAG was employed as it provides external knowledge from the most relevant texts and minimizes LLM hallucinations. It was used to provide context to assist the LLM in performing …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xiao_network_2025,
  title = {Network for knowledge {Organization},
  author = {Xiao, Z. and Pakrasi, H. B. and Chen, Y. and Tang, Y. J.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1096717624001484},
  journal = {Metabolic Engineering},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… pipeline to store and distillate factual knowledge. Knowledge graphs have emerged as a … an example LLM in this study due to its exceptional Retrieval Augmented Generation (RAG) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{bender_next_2025,
  title = {Next {Sentence},
  author = {Bender, Alexandre Thurow and Gomes, Gabriel A. and Corrêa, Ulisses Brisolara and Araújo, Ricardo Matsumura},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007896132&partnerID=40&md5=10af8b2ba73502a14194d3fad0ea674b},
  booktitle = {Proceedings of the {International},
  volume = {38},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{dodgson_establishing_2023,
  title = {Establishing performance baselines in fine-tuning, retrieval-augmented generation and soft-prompting for non-specialist llm users},
  author = {Dodgson, J. and Nanzheng, L. and Peh, J. and Pattirane, A. R. J. and {...},
  year = {2023},
  url = {https://arxiv.org/abs/2311.05903},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… The LLM Deployer Module incorporates multi-model collaborative agents and instructional … LLM analysis engines. In this context, different LLMs are used to retrieve user-relevant factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kang_ever_2023,
  title = {Ever: {Mitigating},
  author = {Kang, H. and Ni, J. and Yao, H.},
  year = {2023},
  url = {https://mk322.github.io/papers/KangEVER2024.pdf},
  journal = {arXiv preprint arXiv:2311.09114},
  note = {Publisher: mk322.github.io
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… better preference data to essentially enhance the factuality LLM by preference tuning. Here, as … we’ve identified for RAG and post-hoc edit methods? (2) Can EVER effectively reduce …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chavva_enhanced_2023,
  title = {Enhanced {Hybrid},
  author = {Chavva, M. and Veera, S.},
  year = {2023},
  url = {https://journals.theusinsight.com/index.php/AJAI/article/view/70},
  journal = {American Journal of AI \&Innovation},
  note = {Publisher: journals.theusinsight.com},
  keywords = {source: Google Scholar},
  abstract = {… This paper presents an enhanced Hybrid Retrieval-Augmented Generation with Large Language Model (RAG-LLM) architecture tailored for domain-specific cloud infrastructure …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{garigliotti_explainable_2023,
  title = {Explainable {LLM},
  author = {Garigliotti, D.},
  year = {2023},
  url = {https://ceur-ws.org/Vol-3953/368.pdf},
  publisher = {ceur-ws.org},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… LLM is also requested to cite the passages that support the correctness of the generated answer, and answer and citation … through the entire RAG assessment while the LLM has access …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kazoom_vault_2025,
  title = {Vault: {Vigilant},
  author = {Kazoom, R. and Cohen, O. and Puzis, R. and Shabtai, A. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2508.00965},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We introduce VAULT, a fully automated adversarial RAG pipeline … Next, we assemble these contexts into LLM prompts to generate ad… Then use high-confidence examples for training; …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cao_video_2025,
  title = {Video simpleqa: {Towards},
  author = {Cao, M. and Hu, P. and Wang, Y. and Gu, J. and Tang, H. and Zhao, H. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2503.18923},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… To further enhance the reliability, we train expert annotators to refine the LLM-generated QA … RAG yields significant gains at the cost of inference efficiency: We explore RAG to facilitate …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{chung_verifact_2025,
  title = {Verifact: {Verifying},
  author = {Chung, P. and Swaminathan, A. and Goodell, A. J. and Kim, Y. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2501.16672},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… factuality of LLM-generated text given the needle-in-a-haystack challenge of identifying subtle errors and hallucinations[… that atomic claims improve information retrieval for RAG[29], but …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_veridebug_2025,
  title = {Veridebug: {A},
  author = {Wang, N. and Yao, B. and Zhou, J. and Hu, Y. and Wang, X. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11106068/},
  journal = {… Conference on LLM …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… to enhance the LLM’s outputs during debugging tasks [5]. However, RAG methods introduce … Figure 5 illustrates hallucination in LLM debugging. Instruction (Figure 5a) shows the actual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kang_deficiency_2023,
  title = {Deficiency of large language models in finance: {An},
  author = {Kang, H. and Liu, X. Y.},
  year = {2023},
  url = {https://arxiv.org/abs/2311.15548},
  journal = {arXiv preprint arXiv:2311.15548},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… LLM models’ capacity of querying historical stock prices. Third, to alleviate the hallucination … To mitigate hallucinations, we show the effectiveness of the RAG method and prompt-based …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{yue_democratizing_2023,
  title = {Democratizing financial knowledge with {ChatGPT},
  author = {Yue, T. and Au, D. and Au, C. C. and Iu, K. Y.},
  year = {2023},
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4346152},
  journal = {Available at SSRN 4346152},
  note = {Publisher: papers.ssrn.com},
  keywords = {source: Google Scholar},
  abstract = {… Within the RAG framework, the LLM model is allowed to reference external data during … significant issues with AI hallucinations. We recognize that LLM hallucinations are likely to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mansurova_development_2023,
  title = {Development of a question answering chatbot for blockchain domain},
  author = {Mansurova, A. and Nugumanova, A. and {...},
  year = {2023},
  url = {https://sj.astanait.edu.kz/wp-content/uploads/2023/11/Journal_AITU_15vol_sept23-%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D1%8F-4-27-40.pdf},
  journal = {Scientific Journal of …},
  note = {Publisher: sj.astanait.edu.kz
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… of simplified reports created using ChatGPT was mostly accurate. … They do, however, tend to generate hallucinations. Therefore… Retrieval-augmented generation (RAG) is a technique in …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_judge_2025,
  title = {Judge as a judge: {Improving},
  author = {Liu, S. and Li, X. and Liu, Z. and Yan, Y. and Yang, C. and Zeng, Z. and Liu, Z. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2502.18817},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… that enhances LLM-based judgment models to generate more accurate evaluations for RAG … Additionally, the vanilla LLM prioritizes factual correctness, while the SFT model focuses on …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@book{wu_joint_2025,
  title = {Joint modeling of intelligent retrieval-augmented generation in {LLM},
  author = {Wu, D. and Pan, S.},
  year = {2025},
  url = {https://www.researchsquare.com/article/rs-7739042/latest},
  publisher = {researchsquare.com},
  keywords = {source: Google Scholar},
  abstract = {… utilization by proposing a retrieval-augmented generation method enhanced with intelligent … generation and ensuring semantic coverage, factual consistency, and contextual coherence. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{suri_visdom_2024,
  title = {Visdom: {Multi},
  author = {Suri, M. and Mathur, P. and Dernoncourt, F. and Goswami, K. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2412.10704},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Evidence Curation: As a first step, we prompt the LLM to extract relevant evidence from the … the model’s reasoning abilities by filtering out noise and helps mitigate LLM hallucinations. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{an_vitality_2024,
  title = {Vitality 2: {Reviewing},
  author = {An, H. and Narechania, A. and Wall, E. and Xu, K.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.13450},
  journal = {arXiv preprint arXiv:2408.13450},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… uses a Large Language Model or LLM-… RAG mitigates LLMs hallucinations by providing additional contextual information. In future work, we can further reduce hallucinations in RAG by …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{friel_chainpoll_2023,
  title = {Chainpoll: {A},
  author = {Friel, R. and Sanyal, A.},
  year = {2023},
  url = {https://arxiv.org/abs/2310.18344},
  journal = {arXiv preprint arXiv:2310.18344},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… We propose 2 new metrics to quantify LLM hallucinations - Adherence and Correctness. The former pertinent to Retrieval Augmented Generation (RAG) workflows measuring an LLM’s …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{xu_context-aware_2023,
  title = {Context-aware decoding reduces hallucination in query-focused summarization},
  author = {Xu, Z.},
  year = {2023},
  url = {https://arxiv.org/abs/2312.14335},
  journal = {arXiv preprint arXiv:2312.14335},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… However, applying large language models (LLM) potentially leads to hallucinations, especially … In retrieval augmented generation, we mainly focus on intrinsic hallucination. Pre-trained …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{cao_learn_2023,
  title = {Learn to refuse: {Making},
  author = {Cao, L.},
  year = {2023},
  url = {https://arxiv.org/abs/2311.01041},
  journal = {arXiv preprint arXiv:2311.01041},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… type of hallucination, namely fact-conflicting hallucination, … Our objective is for the LLM to function solely as a machine … with the general retrieval augmented generation (RAG) method. In …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shethiya_llm-powered_2023,
  title = {{LLM},
  author = {Shethiya, A. S.},
  year = {2023},
  url = {http://academianexusjournal.com/index.php/anj/article/view/21},
  journal = {Academia Nexus Journal},
  note = {Publisher: academianexusjournal.com},
  keywords = {source: Google Scholar},
  abstract = {… is the Retrieval-Augmented Generation (RAG) architecture. RAG augments the LLM with … and capabilities of the LLM, possibly offering "why" explanations or citations where feasible. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{_llm_2023,
  title = {{LLM},
  author = {{정천수},
  year = {2023},
  url = {https://www.researchgate.net/profile/Cheonsu-Jeong/publication/376893991_Generative_AI_service_implementation_using_LLM_application_architecture_based_on_RAG_model_and_LangChain_framework/links/658e9c136f6e450f19b310b1/Generative-AI-service-implementation-using-LLM-application-architecture-based-on-RAG-model-and-LangChain-framework.pdf},
  journal = {지능정보연구},
  note = {Publisher: researchgate.net
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… Accordingly, this study presents a method of implementing generative AI services using the LLM application architecture using the most widely used LangChain framework. To this end, …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@inproceedings{cheng_lift_2023,
  title = {Lift {Yourself},
  author = {Cheng, Xin and Luo, Di and Chen, Xiuying and Liu, Lemao and Zhao, Dong-Yan and Yan, Rui},
  year = {2023},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205561550&partnerID=40&md5=f01f9f055070db2780dc35daed6a4fbd},
  booktitle = {Advances in {Neural},
  volume = {36},
  note = {Type: Conference paper},
  keywords = {source: Scopus},
  annote = {Cited by: 41},
}

@article{chen_you_2025,
  title = {You {Don},
  author = {Chen, S. and Zhou, C. and Yuan, Z. and Zhang, Q. and Cui, Z. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2508.06105},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM … becomes prematurely confident and produces an …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wang_jmlr_2024,
  title = {Jmlr: {Joint},
  author = {Wang, J. and Yang, Z. and Yao, Z. and Yu, H.},
  year = {2024},
  url = {https://arxiv.org/abs/2402.17887},
  journal = {arXiv preprint arXiv:2402.17887},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… (RAG) has limited success in addressing hallucinations. Unlike previous methods in RAG … separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{liu_xrag_2025,
  title = {{XRAG},
  author = {Liu, W. and Trenous, S. and Ribeiro, L. F. R. and Byrne, B. and {...},
  year = {2025},
  url = {https://arxiv.org/abs/2505.10089},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… RAG setting (GPT-4o achieves only 62.4\% accuracy, see Table 4). We develop a novel LLM-… 2024), the Q\&A pairs generated by the LLM may contain factual errors. Therefore, we ask a …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{mohammed_diabetiq_nodate,
  title = {{DiabetIQ},
  author = {Mohammed, S. and Nabil, N. I. and Nipa, H. R. and Setu, U. S. H.},
  url = {https://www.researchgate.net/profile/Saif-Mohammed-18/publication/391479329_DiabetIQ_An_Intelligent_Diabetes_Management_Application_with_an_Integrated_LLM-Augmented_RAG_Chatbot_and_ML-Based_Risk_Early_Prediction/links/6819c216df0e3f544f52211e/DiabetIQ-An-Intelligent-Diabetes-Management-Application-with-an-Integrated-LLM-Augmented-RAG-Chatbot-and-ML-Based-Risk-Early-Prediction.pdf},
  journal = {researchgate.net},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… is the Retrieval-Augmented Generation (RAG) pipeline, … base Large Language Model (LLM) while ensuring factual grounding. … the LLM with both the user’s question and relevant factual …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kloker_new_2024,
  title = {New {Curriculum},
  author = {Kloker, S. and Bukoli, H. and Kateete, T.},
  year = {2024},
  url = {https://arxiv.org/abs/2408.07542},
  journal = {arXiv preprint arXiv:2408.07542},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… the "confidence conundrum": the LLM presents information … of RAG for lesson plan creation in the context of Ugandan secondary schools. We developed a prototype, that utilizes an LLM …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{wilcock_new_2024,
  title = {New technologies for spoken dialogue systems: {LLMs},
  author = {Wilcock, G.},
  year = {2024},
  url = {https://researchportal.helsinki.fi/files/320056326/IWSDS-short-final-v10.pdf},
  journal = {14th International Workshop on Spoken …},
  note = {Publisher: researchportal.helsinki.fi
Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… RAG with Llama2 to generate natural language answers to user questions about the document contents. Note that the LLM generates confident … uses vector-based RAG with Llama2 to …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{kang_nadine_2024,
  title = {Nadine: an {LLM},
  author = {Kang, H. and Moussa, M. B. and {...},
  year = {2024},
  url = {https://arxiv.org/abs/2405.20189},
  journal = {arXiv preprint arXiv …},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… This approach aims to reduce LLM hallucinations and incorporate data from external … In this section, we demonstrate the RAG system adopted in our interaction module. The RAG …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{shin_qa_2023,
  title = {{QA},
  author = {Shin, J. and Lee, J. and Kim, K. and Lee, T. and {...},
  year = {2023},
  journal = {Annual …},
  note = {Publisher: Human and Language Technology
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{martineau_what_2023,
  title = {What is retrieval-augmented generation?},
  author = {Martineau, K. and Explainable, A. I. and Generative, A. I.},
  year = {2023},
  url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG?ref=blog.zatrok.com},
  journal = {IBM Research Blog},
  note = {Publisher: research.ibm.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Implementing RAG in an LLM-based question answering … By grounding an LLM on a set of external, verifiable facts, the … that an LLM will leak sensitive data, or ‘hallucinate’ incorrect …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jeong_generative_2023,
  title = {Generative {AI},
  author = {Jeong, C.},
  year = {2023},
  journal = {Journal of Intelligence and Information …},
  note = {Publisher: Korea Intelligent Information System …
Type: CITATION},
  keywords = {source: Google Scholar},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zarza_optimized_2023,
  title = {Optimized financial planning: integrating individual and cooperative budgeting models with {LLM},
  author = {Zarzà, I. De and Curtò, J. De and Roig, G. and Calafate, C. T.},
  year = {2023},
  url = {https://www.mdpi.com/2673-2688/5/1/6},
  journal = {AI},
  note = {Publisher: mdpi.com
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… potential for LLM hallucination, the methodology could integrate a retrieval augmented generation (RAG) … The incorporation of RAG into our recommendation generation process can be …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{hannah_legal_2025,
  title = {On the legal implications of {Large},
  author = {Hannah, G. and Sousa, R. T. and Dasoulas, I. and d'Amato, C.},
  year = {2025},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826824000295},
  journal = {Journal of Web Semantics},
  note = {Publisher: Elsevier
Type: HTML},
  keywords = {source: Google Scholar},
  abstract = {… Nevertheless, there are cases where the LLM recommends actions that have potential legal … the LLM answers, is also able to provide actual evidence for them by supplying citations of …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{bilal_onrl-rag_2025-1,
  title = {Onrl-rag: {Real},
  author = {Bilal, A. and Lin, B.},
  year = {2025},
  url = {https://arxiv.org/abs/2504.02894},
  journal = {arXiv preprint arXiv:2504.02894},
  note = {Publisher: arxiv.org},
  keywords = {source: Google Scholar},
  abstract = {… While RAG offers the correct information, it may not best … -based Retrieval-Augmented Generation (OnRL-RAG) system … compared to standard RAG and simple LLM via GPT-4o, GPT-4o-…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{gajjar_oran-bench-13k_2025,
  title = {Oran-bench-13k: {An},
  author = {Gajjar, P. and Shah, V. K.},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/10975994/},
  journal = {2025 IEEE 22nd Consumer …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Our findings indicate that current popular LLM models are not … We also propose a RAG-based pipeline named ORANSight … Fung, “Towards mitigating llm hallucination via self reflection,…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{jain_mitigating_2025-1,
  title = {On mitigating code {LLM},
  author = {Jain, N. and Kwiatkowski, R. and Ray, B. and {...},
  year = {2025},
  url = {https://ieeexplore.ieee.org/abstract/document/11121691/},
  journal = {2025 IEEE/ACM 47th …},
  note = {Publisher: ieeexplore.ieee.org},
  keywords = {source: Google Scholar},
  abstract = {… Hence, to address API hallucinations, we adopt retrieval augmented generation with documentation, ie, Documentation Augmented Generation (DAG), which has shown early promise […},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{zheng_-device_nodate,
  title = {On-{Device},
  author = {Zheng, D.},
  url = {https://dqzheng.com/wp-content/uploads/2025/10/SEA_2025-2.pdf},
  journal = {dqzheng.com},
  note = {Type: PDF},
  keywords = {source: Google Scholar},
  abstract = {… The agentric system uses a hybrid LLM-RAG architecture with … we demonstrate 90\% LLM success rate with 10\% RAG fallback, … confidence scores fall below threshold (confidence {\textless},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{__2024,
  title = {검색 증강 생성 ({RAG},
  author = {{이은빈},
  year = {2024},
  url = {https://kiss.kstudy.com/Detail/Ar?key=4118382},
  journal = {정보처리학회 논문지 (KTSDE)},
  note = {Publisher: kiss.kstudy.com},
  keywords = {source: Google Scholar},
  abstract = {… databases, thereby reducing the hallucination phenomenon often seen in LLMs … RAG, reviews recent research trends aimed at enhancing the retrieval capabilities of LLMs through RAG…},
  annote = {Query date: 2025-10-25 20:50:36},
}

@article{__2023,
  title = {カスタマーサポートにおける {LLM},
  author = {{二宮大空},
  year = {2023},
  url = {https://www.jstage.jst.go.jp/article/jsaislud/99/0/99_191/_article/-char/ja/},
  journal = {人工知能学会研究会資料 言語・音声理解と対話処理研究 …},
  note = {Publisher: jstage.jst.go.jp},
  keywords = {source: Google Scholar},
  abstract = {… , including issues like hallucination, necessitating … an LLM-simulated user interacts with the RAG-based dialogue system, and the resulting dialogue data is evaluated using the LLM. …},
  annote = {Query date: 2025-10-25 20:50:36},
}

@misc{noauthor_scopus_nodate,
  title = {Scopus - {Document},
  url = {https://www.scopus.com/results/results.uri?sort=plf-f&src=s&sid=004b5458677e4972e2b44cdf9b71d3cc&sot=a&sdt=a&cluster=scolang%2C%22English%22%2Ct%2C%22Spanish%22%2Ct&sl=756&s=TITLE-ABS-KEY%28+%28%22retrieval-augmented+generation%22+OR+RAG%29+AND+%28%22large+language+model*%22+OR+LLM*+OR+%22generative+AI%22+OR+ChatGPT%29+AND+%28+%28trust+OR+confidence+OR+credibility+OR+%22user+perception%22+OR+%22perceived+reliability%22+OR+%22overtrust%22+OR+calibration%29+OR+%28hallucination*+OR+factuality+OR+%22factual+accuracy%22+OR+faithfulness+OR+citation+OR+citations+OR+reference+OR+%22source+attribution%22+OR+provenance+OR+grounding%29+OR+%28%22explainable+artificial+intelligence%22+OR+%22explainable+AI%22+OR+XAI+OR+%22interpretable+AI%22+OR+%22interpretable+artificial+intelligence%22+OR+%22AI+explainability%22+OR+%22AI+interpretability%22+OR+%22transparent+AI%22+OR+%22AI+transparency%22+OR+%22model+interpretability%22+OR+%22post-hoc+explanation%22+OR+%22feature+attribution%22%29%29+%29+AND+PUBYEAR+%26gt%3B+2019+AND+PUBYEAR+%26lt%3B+2026&origin=searchadvanced&editSaveSearch=&txGid=e9ebf783d0fd1690e1ec752586b783f5&sessionSearchId=004b5458677e4972e2b44cdf9b71d3cc&limit=10},
  keywords = {source: Scopus},
  file = {Scopus - Document search results | Signed in:C\:\\Users\\Marco\\Zotero\\storage\\3W35T745\\results.html:text/html},
  urldate = {2025-10-26},
}

@article{noauthor_43rd_2025,
  title = {43rd {International},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209551458&partnerID=40&md5=8beac58109dd074a36359432bcbc8d8a},
  journal = {Lecture Notes in Computer Science},
  volume = {15238 LNCS},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_2025_2025,
  title = {2025 11th {International},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011583044&partnerID=40&md5=587b88ef744122f032bc2e9c067d04a6},
  journal = {International Conference on Web Research, ICWR},
  number = {2025},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_8th_2025,
  title = {8th {International},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002920066&partnerID=40&md5=42ad37c3f901a0db6bf011b5720de306},
  journal = {Lecture Notes in Computer Science},
  volume = {15545 LNCS},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_11th_2025,
  title = {11th {International},
  year = {2025},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001862693&partnerID=40&md5=feb70896a8fec78cc8baada933229ca5},
  booktitle = {International {Conference},
  volume = {1},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@inproceedings{noauthor_2024_2024,
  title = {2024 {Design},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215933813&partnerID=40&md5=29034e1cfb47df740ae0cff805094709},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

@article{noauthor_32nd_2024,
  title = {32nd {International},
  year = {2024},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198482399&partnerID=40&md5=909e46f468f0730419b64b5c01b1f99e},
  journal = {Lecture Notes in Computer Science},
  volume = {14775 LNAI},
  note = {Type: Conference review},
  keywords = {source: Scopus},
  annote = {Cited by: 0},
}

